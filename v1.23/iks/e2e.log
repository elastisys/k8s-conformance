I0602 21:12:24.646389      21 e2e.go:132] Starting e2e run "68dd39b1-7a69-44e6-8e74-b60df5359f38" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1654204344 - Will randomize all specs
Will run 346 of 7044 specs

Jun  2 21:12:26.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
E0602 21:12:26.905747      21 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun  2 21:12:26.909: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun  2 21:12:26.994: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  2 21:12:27.123: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  2 21:12:27.123: INFO: expected 15 pod replicas in namespace 'kube-system', 15 are Running and Ready.
Jun  2 21:12:27.123: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  2 21:12:27.156: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun  2 21:12:27.156: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun  2 21:12:27.156: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jun  2 21:12:27.156: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jun  2 21:12:27.156: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Jun  2 21:12:27.156: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jun  2 21:12:27.156: INFO: e2e test version: v1.23.7
Jun  2 21:12:27.161: INFO: kube-apiserver version: v1.23.7+IKS
Jun  2 21:12:27.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:12:27.175: INFO: Cluster IP family: ipv4
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:12:27.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
W0602 21:12:27.251027      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jun  2 21:12:27.251: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jun  2 21:12:27.282: INFO: PSP annotation exists on dry run pod: "ibm-privileged-psp"; assuming PodSecurityPolicy is enabled
W0602 21:12:27.313934      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0602 21:12:27.329807      21 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jun  2 21:12:27.411: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8476
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  2 21:12:27.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:12:27.800: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:28.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:12:28.830: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:29.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:12:29.836: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:30.841: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:12:30.841: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:31.834: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:12:31.834: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:32.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  2 21:12:32.832: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:33.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 21:12:33.835: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:12:34.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 21:12:34.833: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Jun  2 21:12:34.924: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17842"},"items":null}

Jun  2 21:12:34.935: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17842"},"items":[{"metadata":{"name":"daemon-set-ck5fc","generateName":"daemon-set-","namespace":"daemonsets-8476","uid":"2b6f297a-fe8a-410f-932b-8c2d5e2e7250","resourceVersion":"17835","creationTimestamp":"2022-06-02T21:12:27Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"8ce3115c42c7d4d30254ee71795ff751ceef41e6df15ffb73f3cf065e9137497","cni.projectcalico.org/podIP":"172.30.220.240/32","cni.projectcalico.org/podIPs":"172.30.220.240/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0017f412-2ba6-48af-920c-8da3e204593e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0017f412-2ba6-48af-920c-8da3e204593e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qvkls","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qvkls","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.134.156.253","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.134.156.253"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:27Z"}],"hostIP":"10.134.156.253","podIP":"172.30.220.240","podIPs":[{"ip":"172.30.220.240"}],"startTime":"2022-06-02T21:12:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-02T21:12:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3eb28db0378ec8cc3c14719c87aa561c909c2549c0c3af14ecc28b1f22eb00f6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-d2x7s","generateName":"daemon-set-","namespace":"daemonsets-8476","uid":"2e50e8c5-30c1-437d-986e-21bd2a481182","resourceVersion":"17830","creationTimestamp":"2022-06-02T21:12:27Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6cfb13abf7dbb3ca9569a1a0d4b68636ef2013377e5f740b2458f4e67e36e417","cni.projectcalico.org/podIP":"172.30.118.14/32","cni.projectcalico.org/podIPs":"172.30.118.14/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0017f412-2ba6-48af-920c-8da3e204593e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0017f412-2ba6-48af-920c-8da3e204593e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.118.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jg2jb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jg2jb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.134.156.247","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.134.156.247"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:32Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:32Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:27Z"}],"hostIP":"10.134.156.247","podIP":"172.30.118.14","podIPs":[{"ip":"172.30.118.14"}],"startTime":"2022-06-02T21:12:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-02T21:12:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://874fb3976b0d90db35a1217c265277fd666177b2ad8088354d96a55031092e16","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ffwdf","generateName":"daemon-set-","namespace":"daemonsets-8476","uid":"2329d7b5-9270-4e0e-98cd-827685301f65","resourceVersion":"17838","creationTimestamp":"2022-06-02T21:12:27Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"44c42c2def7784b18b368c12e4b731cfbb3044396394ca92db84071169cc6503","cni.projectcalico.org/podIP":"172.30.170.181/32","cni.projectcalico.org/podIPs":"172.30.170.181/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0017f412-2ba6-48af-920c-8da3e204593e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0017f412-2ba6-48af-920c-8da3e204593e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-06-02T21:12:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.170.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4b9wt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4b9wt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.134.156.209","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.134.156.209"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-06-02T21:12:27Z"}],"hostIP":"10.134.156.209","podIP":"172.30.170.181","podIPs":[{"ip":"172.30.170.181"}],"startTime":"2022-06-02T21:12:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-06-02T21:12:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://1bb99e9830d38bc0280137456c746f8d0f09d48f7f3d09b308bc8e258ed65c84","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:12:34.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8476" for this suite.

• [SLOW TEST:7.845 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":1,"skipped":12,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:12:35.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2729
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-2729
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  2 21:12:35.228: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  2 21:12:35.338: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:12:37.363: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:12:39.351: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:12:41.358: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:12:43.362: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:45.354: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:47.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:49.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:51.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:53.361: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:55.357: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:12:57.357: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  2 21:12:57.376: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  2 21:12:57.426: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  2 21:13:01.501: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  2 21:13:01.502: INFO: Breadth first check of 172.30.170.182 on host 10.134.156.209...
Jun  2 21:13:01.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.118.16:9080/dial?request=hostname&protocol=http&host=172.30.170.182&port=8083&tries=1'] Namespace:pod-network-test-2729 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:13:01.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:13:01.516: INFO: ExecWithOptions: Clientset creation
Jun  2 21:13:01.517: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2729/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.118.16%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.170.182%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:13:01.755: INFO: Waiting for responses: map[]
Jun  2 21:13:01.755: INFO: reached 172.30.170.182 after 0/1 tries
Jun  2 21:13:01.755: INFO: Breadth first check of 172.30.118.15 on host 10.134.156.247...
Jun  2 21:13:01.799: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.118.16:9080/dial?request=hostname&protocol=http&host=172.30.118.15&port=8083&tries=1'] Namespace:pod-network-test-2729 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:13:01.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:13:01.802: INFO: ExecWithOptions: Clientset creation
Jun  2 21:13:01.802: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2729/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.118.16%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.118.15%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:13:02.119: INFO: Waiting for responses: map[]
Jun  2 21:13:02.119: INFO: reached 172.30.118.15 after 0/1 tries
Jun  2 21:13:02.119: INFO: Breadth first check of 172.30.220.241 on host 10.134.156.253...
Jun  2 21:13:02.134: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.118.16:9080/dial?request=hostname&protocol=http&host=172.30.220.241&port=8083&tries=1'] Namespace:pod-network-test-2729 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:13:02.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:13:02.135: INFO: ExecWithOptions: Clientset creation
Jun  2 21:13:02.136: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2729/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.118.16%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.220.241%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:13:02.295: INFO: Waiting for responses: map[]
Jun  2 21:13:02.295: INFO: reached 172.30.220.241 after 0/1 tries
Jun  2 21:13:02.295: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:13:02.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2729" for this suite.

• [SLOW TEST:27.317 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":46,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:13:02.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2364
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:13:02.562: INFO: Creating deployment "webserver-deployment"
Jun  2 21:13:02.574: INFO: Waiting for observed generation 1
Jun  2 21:13:04.602: INFO: Waiting for all required pods to come up
Jun  2 21:13:04.620: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun  2 21:13:06.673: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  2 21:13:06.692: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  2 21:13:06.722: INFO: Updating deployment webserver-deployment
Jun  2 21:13:06.722: INFO: Waiting for observed generation 2
Jun  2 21:13:08.753: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  2 21:13:08.763: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  2 21:13:08.770: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  2 21:13:08.822: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  2 21:13:08.822: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  2 21:13:08.830: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  2 21:13:08.848: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  2 21:13:08.848: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  2 21:13:08.873: INFO: Updating deployment webserver-deployment
Jun  2 21:13:08.873: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  2 21:13:08.895: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  2 21:13:10.921: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 21:13:10.969: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2364  45c2270d-dc80-44c5-b0c0-626ca2d46fc2 18330 3 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002491578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-02 21:13:08 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-06-02 21:13:09 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  2 21:13:10.979: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-2364  bc58e272-286f-4dc5-a744-6b085fda5cb6 18327 3 2022-06-02 21:13:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 45c2270d-dc80-44c5-b0c0-626ca2d46fc2 0xc002491947 0xc002491948}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45c2270d-dc80-44c5-b0c0-626ca2d46fc2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0024919e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 21:13:10.979: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  2 21:13:10.979: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-2364  cff4b6af-eb1a-4ab3-a066-2c58458f25f1 18317 3 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 45c2270d-dc80-44c5-b0c0-626ca2d46fc2 0xc002491a47 0xc002491a48}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"45c2270d-dc80-44c5-b0c0-626ca2d46fc2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002491ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  2 21:13:11.042: INFO: Pod "webserver-deployment-566f96c878-8fg4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-8fg4z webserver-deployment-566f96c878- deployment-2364  4798c45f-d329-4d88-b9e2-42525fa68b58 18228 0 2022-06-02 21:13:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:5492ebeeaa880a1b0c4687b8a532b9856205b1f93f0d82f214aab3c4353b2c56 cni.projectcalico.org/podIP:172.30.220.246/32 cni.projectcalico.org/podIPs:172.30.220.246/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002491fb7 0xc002491fb8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2ntg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2ntg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.042: INFO: Pod "webserver-deployment-566f96c878-9nwll" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9nwll webserver-deployment-566f96c878- deployment-2364  07d11659-60c5-4220-b378-5181888cd035 18387 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:fee905866f5faecd9580affda6fd3cb9cea94abe460bcf16a3119dcf4f887e0b cni.projectcalico.org/podIP:172.30.220.248/32 cni.projectcalico.org/podIPs:172.30.220.248/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc0028321c7 0xc0028321c8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-npjbg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-npjbg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.043: INFO: Pod "webserver-deployment-566f96c878-b926b" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-b926b webserver-deployment-566f96c878- deployment-2364  3186024a-9e07-46d3-949c-7f6d24f99659 18218 0 2022-06-02 21:13:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:08c344ecfe5ae6d4374fbfcddf06450f0dd69348ba339153006791582fc7c40f cni.projectcalico.org/podIP:172.30.170.186/32 cni.projectcalico.org/podIPs:172.30.170.186/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc0028323d7 0xc0028323d8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6s7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6s7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.043: INFO: Pod "webserver-deployment-566f96c878-d5dj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-d5dj5 webserver-deployment-566f96c878- deployment-2364  e6cc7ea6-be9f-4d9a-8b09-18f135a3cd1c 18374 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc0028325e7 0xc0028325e8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x6g67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x6g67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.043: INFO: Pod "webserver-deployment-566f96c878-gs59c" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-gs59c webserver-deployment-566f96c878- deployment-2364  0743af2e-bd59-4cf9-9bc3-3196d4a15c84 18239 0 2022-06-02 21:13:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:d14d386e64bf4664d9e3c30edd6164f3f5f2d1e3ab1fddf86062d537d6c0ad38 cni.projectcalico.org/podIP:172.30.220.247/32 cni.projectcalico.org/podIPs:172.30.220.247/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc0028327d7 0xc0028327d8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9kk2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9kk2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.043: INFO: Pod "webserver-deployment-566f96c878-j5qwk" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-j5qwk webserver-deployment-566f96c878- deployment-2364  548629af-3333-4df4-8193-6a0bffaff0ef 18413 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:bb03c86310c0eadb8589dde31e204a70c95a00f17ddbda6a8d561e23f6c95e3c cni.projectcalico.org/podIP:172.30.170.189/32 cni.projectcalico.org/podIPs:172.30.170.189/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc0028329e7 0xc0028329e8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8n6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8n6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.044: INFO: Pod "webserver-deployment-566f96c878-jf7c5" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-jf7c5 webserver-deployment-566f96c878- deployment-2364  70724924-29db-4367-87e8-0efaee227ed3 18403 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:430d47857088eca9d8e6a8a62e1e29bf5d03f0186f3b8c2391f92622ffa0d6bc cni.projectcalico.org/podIP:172.30.170.188/32 cni.projectcalico.org/podIPs:172.30.170.188/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002832bf7 0xc002832bf8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfh9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfh9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.044: INFO: Pod "webserver-deployment-566f96c878-kmmfw" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-kmmfw webserver-deployment-566f96c878- deployment-2364  58ed78a1-6a4e-44c3-b465-5d1b929cfad8 18363 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002832e07 0xc002832e08}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f6rbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f6rbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.044: INFO: Pod "webserver-deployment-566f96c878-lkl6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-lkl6c webserver-deployment-566f96c878- deployment-2364  ad34ed94-8fbb-4a55-8ca6-38e579ca77e2 18414 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:310de013699fd244cce8f21913ac140ccbd89bfe3f714766fa5ead1cb17667d1 cni.projectcalico.org/podIP:172.30.220.250/32 cni.projectcalico.org/podIPs:172.30.220.250/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002832ff7 0xc002832ff8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9bxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9bxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.044: INFO: Pod "webserver-deployment-566f96c878-msqkn" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-msqkn webserver-deployment-566f96c878- deployment-2364  451a44e6-cc80-4684-b89c-948f23b13784 18365 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002833207 0xc002833208}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2sxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2sxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.045: INFO: Pod "webserver-deployment-566f96c878-pbkl8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-pbkl8 webserver-deployment-566f96c878- deployment-2364  f85df3be-2912-4e3d-bcf1-a88267849101 18411 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:ca1f41971c2b592ec8f3709a5361a3fd5341700e0bf30a539ba7e336e1486591 cni.projectcalico.org/podIP:172.30.118.23/32 cni.projectcalico.org/podIPs:172.30.118.23/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002833417 0xc002833418}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68w7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68w7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.045: INFO: Pod "webserver-deployment-566f96c878-x5fbs" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-x5fbs webserver-deployment-566f96c878- deployment-2364  cc958a8c-983b-46c1-b352-162e1dcb9312 18225 0 2022-06-02 21:13:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:1bba8501bd1198805ae41548b1d228c49dd65fe7f3987f92bb89dbfa739e9e08 cni.projectcalico.org/podIP:172.30.118.21/32 cni.projectcalico.org/podIPs:172.30.118.21/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002833647 0xc002833648}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c98wm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c98wm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.045: INFO: Pod "webserver-deployment-566f96c878-x9flg" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-x9flg webserver-deployment-566f96c878- deployment-2364  b24aaf01-6d9a-4ade-8e4f-228f3662511d 18211 0 2022-06-02 21:13:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:f0bc3d94784e86f50282c23640bd43d78d1c5b173b1425e5a1876c88eae09d6b cni.projectcalico.org/podIP:172.30.118.17/32 cni.projectcalico.org/podIPs:172.30.118.17/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 bc58e272-286f-4dc5-a744-6b085fda5cb6 0xc002833877 0xc002833878}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc58e272-286f-4dc5-a744-6b085fda5cb6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8p8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8p8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.045: INFO: Pod "webserver-deployment-5d9fdcc779-47v9l" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-47v9l webserver-deployment-5d9fdcc779- deployment-2364  dbe4cbbd-c9e0-43e4-89ff-801295e709bd 18102 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:ef3ba609531b430f2e1b816581c4900c93ec393dcfcf601b8071c1ae791219c0 cni.projectcalico.org/podIP:172.30.118.20/32 cni.projectcalico.org/podIPs:172.30.118.20/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc002833aa7 0xc002833aa8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.118.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tq7h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tq7h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:172.30.118.20,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7ee77813e0c7e90376283a7307e4e361aadf50c3e344e13c77e5297a365ae78e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.118.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.045: INFO: Pod "webserver-deployment-5d9fdcc779-56glf" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-56glf webserver-deployment-5d9fdcc779- deployment-2364  0a5e28ec-73c3-46f2-b77c-631e63754736 18116 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:84478f6595724c2293747596549e5678ef14a4b2ac6254818665ba82e0ef5ed4 cni.projectcalico.org/podIP:172.30.170.185/32 cni.projectcalico.org/podIPs:172.30.170.185/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc002833cb7 0xc002833cb8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.170.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7z2wx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7z2wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:172.30.170.185,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://91eb8fc28e7e486e293512c78df0b67898ff78387287147c46078dba2917a32e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.170.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.045: INFO: Pod "webserver-deployment-5d9fdcc779-592mb" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-592mb webserver-deployment-5d9fdcc779- deployment-2364  1e0574d3-770e-40eb-90a2-592e3d973c7a 18135 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:5bd1d5ee108eec809eb496014456420248e356b3cd36ceb954165ad8ca1ae0dc cni.projectcalico.org/podIP:172.30.220.245/32 cni.projectcalico.org/podIPs:172.30.220.245/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc002833ec7 0xc002833ec8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6d46l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6d46l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.245,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1fc3a4ddf3c1681782473dca4fd698d7673fc86c106dd7c226437c81d042763b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.046: INFO: Pod "webserver-deployment-5d9fdcc779-66wvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-66wvv webserver-deployment-5d9fdcc779- deployment-2364  bbe5eb42-60a9-486f-ae2b-7ab32908e450 18402 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:14d7cb0f51639de0ff0d24316ca57da5cdd490fcb17e55b15e824823f09bbce7 cni.projectcalico.org/podIP:172.30.220.249/32 cni.projectcalico.org/podIPs:172.30.220.249/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a40d7 0xc0028a40d8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8f4sn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8f4sn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.046: INFO: Pod "webserver-deployment-5d9fdcc779-7r86b" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7r86b webserver-deployment-5d9fdcc779- deployment-2364  7d35daf9-6c5a-495e-af13-d4a37b584b3f 18359 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a42c7 0xc0028a42c8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsxss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsxss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.048: INFO: Pod "webserver-deployment-5d9fdcc779-7shbq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7shbq webserver-deployment-5d9fdcc779- deployment-2364  7ee14b7a-eefd-4f88-a912-f553a83bbe2c 18390 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:3048c01577eac20f41b2276f1de91fc520f70d29d90035cded9b0256553c631d cni.projectcalico.org/podIP:172.30.170.187/32 cni.projectcalico.org/podIPs:172.30.170.187/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a4497 0xc0028a4498}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2pmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2pmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.048: INFO: Pod "webserver-deployment-5d9fdcc779-7z7k8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7z7k8 webserver-deployment-5d9fdcc779- deployment-2364  76f44dda-7edc-45d8-8369-ff4e13d21fdd 18399 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:df1d842f1fb8a692a4d618e9b8cc3b53e2c016ca4a9e72fc44d4e5ef1bb62a5b cni.projectcalico.org/podIP:172.30.118.22/32 cni.projectcalico.org/podIPs:172.30.118.22/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a46a7 0xc0028a46a8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cflf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cflf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.049: INFO: Pod "webserver-deployment-5d9fdcc779-8hhq2" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-8hhq2 webserver-deployment-5d9fdcc779- deployment-2364  31934b6d-b2dd-41e8-910e-c8457c3fce0d 18098 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:c3975273dd50f5af46277463c0f00837e070b05d0ab838eb2b6756832f6aaa9a cni.projectcalico.org/podIP:172.30.118.19/32 cni.projectcalico.org/podIPs:172.30.118.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a48b7 0xc0028a48b8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.118.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qb996,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qb996,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:172.30.118.19,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ca64df00ecf3a91379c3d2b4ab98283bc0117296030d41b0ba7ce383cb21ef78,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.118.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.049: INFO: Pod "webserver-deployment-5d9fdcc779-dngtc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-dngtc webserver-deployment-5d9fdcc779- deployment-2364  9bc361cb-2a6f-4e0e-887e-6b9db93b4608 18295 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a4ac7 0xc0028a4ac8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pskll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pskll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.049: INFO: Pod "webserver-deployment-5d9fdcc779-gv8v8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gv8v8 webserver-deployment-5d9fdcc779- deployment-2364  cbdd0494-9dc0-4eec-ad6e-004b30c3b7ee 18362 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a4c97 0xc0028a4c98}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmng9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmng9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.050: INFO: Pod "webserver-deployment-5d9fdcc779-hvxv9" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hvxv9 webserver-deployment-5d9fdcc779- deployment-2364  5a7d86c6-34ab-4826-99c5-3faee5a37367 18132 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:6d30c693883bdbb4758ea1c20b5b6e1df70c565b4470057d0600de144d34c920 cni.projectcalico.org/podIP:172.30.220.242/32 cni.projectcalico.org/podIPs:172.30.220.242/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a4e67 0xc0028a4e68}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5652f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5652f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.242,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://64848ff2f9dc722e27857ad18f18b9979e9afcf23d3ec647717e4e99ea30ea38,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.050: INFO: Pod "webserver-deployment-5d9fdcc779-jph5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jph5f webserver-deployment-5d9fdcc779- deployment-2364  a559b312-8093-44a0-8be4-3cd700b8ee6c 18367 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5077 0xc0028a5078}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d42nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d42nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.050: INFO: Pod "webserver-deployment-5d9fdcc779-k48z6" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-k48z6 webserver-deployment-5d9fdcc779- deployment-2364  de016104-adab-4a52-b187-ca224a93ad25 18121 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:55d0b60b96923a1180966dee53f5175e1cc4f00d2b9ff1d2122890f25a9ebfc7 cni.projectcalico.org/podIP:172.30.170.184/32 cni.projectcalico.org/podIPs:172.30.170.184/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5247 0xc0028a5248}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.170.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5lk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5lk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:172.30.170.184,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://248949aa2f3aad5d38a26fcd0526c0098e46af161dc716ecdbf7a4ca384d78d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.170.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.051: INFO: Pod "webserver-deployment-5d9fdcc779-lbpt5" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-lbpt5 webserver-deployment-5d9fdcc779- deployment-2364  825d5216-c13d-4597-8b1e-59f1e1c99bd9 18344 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5457 0xc0028a5458}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frksc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frksc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.051: INFO: Pod "webserver-deployment-5d9fdcc779-lmdzq" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-lmdzq webserver-deployment-5d9fdcc779- deployment-2364  eaa6be8c-31cd-4964-b730-4dd0a7701a8c 18118 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:4f68f1135138aee62a5e14eaaf3d14edf3b68710f6766d486b3061c6693661f8 cni.projectcalico.org/podIP:172.30.170.183/32 cni.projectcalico.org/podIPs:172.30.170.183/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5627 0xc0028a5628}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.170.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4xwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4xwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:172.30.170.183,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9572c0d23876db14e7c9a390a655a2ce6416ece5666ac159c8de1e1416bcccd3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.170.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.051: INFO: Pod "webserver-deployment-5d9fdcc779-rczxv" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rczxv webserver-deployment-5d9fdcc779- deployment-2364  88ab67ac-b950-49b7-90c8-ce372ad1646f 18370 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5837 0xc0028a5838}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x7c5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x7c5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.051: INFO: Pod "webserver-deployment-5d9fdcc779-rtbqm" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rtbqm webserver-deployment-5d9fdcc779- deployment-2364  c3047c75-b160-4810-94c3-e2e64c79a4d7 18326 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5a07 0xc0028a5a08}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t67q4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t67q4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.052: INFO: Pod "webserver-deployment-5d9fdcc779-t5xsk" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-t5xsk webserver-deployment-5d9fdcc779- deployment-2364  75d426f6-cea0-47ca-bf3f-8cca6f3c1386 18139 0 2022-06-02 21:13:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:3b9f2ad2e49967de2165109e76c2fff683af413d24a5644806b06d445d3638ef cni.projectcalico.org/podIP:172.30.118.18/32 cni.projectcalico.org/podIPs:172.30.118.18/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5bf7 0xc0028a5bf8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.118.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4jsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4jsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:172.30.118.18,StartTime:2022-06-02 21:13:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ef4763099985af7fb005f153a37e96a2f482fd8b6ee9fbe9214437dbf31d66b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.118.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.052: INFO: Pod "webserver-deployment-5d9fdcc779-vdtdc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vdtdc webserver-deployment-5d9fdcc779- deployment-2364  f9840c00-6a00-439c-ba2f-15bfb71caf6c 18364 0 2022-06-02 21:13:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5e07 0xc0028a5e08}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx9tm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx9tm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:13:11.052: INFO: Pod "webserver-deployment-5d9fdcc779-xsk84" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xsk84 webserver-deployment-5d9fdcc779- deployment-2364  739db67f-40b0-45d4-9d3c-4af9d0cdc64a 18430 0 2022-06-02 21:13:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:07ea5a2a36f545946d3fb074a54977b0bc2c915a02d53e13dece9576af885419 cni.projectcalico.org/podIP:172.30.170.190/32 cni.projectcalico.org/podIPs:172.30.170.190/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 cff4b6af-eb1a-4ab3-a066-2c58458f25f1 0xc0028a5fd7 0xc0028a5fd8}] []  [{kube-controller-manager Update v1 2022-06-02 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cff4b6af-eb1a-4ab3-a066-2c58458f25f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:13:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-06-02 21:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lr9tv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lr9tv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:,StartTime:2022-06-02 21:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:13:11.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2364" for this suite.

• [SLOW TEST:8.864 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":3,"skipped":93,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:13:11.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9308
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun  2 21:13:11.416: INFO: Waiting up to 5m0s for pod "pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1" in namespace "emptydir-9308" to be "Succeeded or Failed"
Jun  2 21:13:11.427: INFO: Pod "pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.074803ms
Jun  2 21:13:13.448: INFO: Pod "pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.032650807s
Jun  2 21:13:15.465: INFO: Pod "pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1": Phase="Running", Reason="", readiness=false. Elapsed: 4.049454092s
Jun  2 21:13:17.486: INFO: Pod "pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070156622s
STEP: Saw pod success
Jun  2 21:13:17.486: INFO: Pod "pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1" satisfied condition "Succeeded or Failed"
Jun  2 21:13:17.496: INFO: Trying to get logs from node 10.134.156.247 pod pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1 container test-container: <nil>
STEP: delete the pod
Jun  2 21:13:17.668: INFO: Waiting for pod pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1 to disappear
Jun  2 21:13:17.678: INFO: Pod pod-f1a8cbd4-6293-4445-a17f-1e8fec5461e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:13:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9308" for this suite.

• [SLOW TEST:6.526 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":114,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:13:17.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9588
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:15:02.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9588" for this suite.

• [SLOW TEST:104.443 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":5,"skipped":118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:15:02.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-451
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-451
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:15:02.504: INFO: Found 0 stateful pods, waiting for 1
Jun  2 21:15:12.527: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jun  2 21:15:12.615: INFO: Found 1 stateful pods, waiting for 2
Jun  2 21:15:22.639: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 21:15:22.639: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 21:15:22.725: INFO: Deleting all statefulset in ns statefulset-451
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:15:22.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-451" for this suite.

• [SLOW TEST:20.641 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":6,"skipped":143,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:15:22.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3894
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-p5mk
STEP: Creating a pod to test atomic-volume-subpath
Jun  2 21:15:23.143: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p5mk" in namespace "subpath-3894" to be "Succeeded or Failed"
Jun  2 21:15:23.153: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026316ms
Jun  2 21:15:25.181: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 2.038563664s
Jun  2 21:15:27.208: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 4.065160323s
Jun  2 21:15:29.251: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 6.10826969s
Jun  2 21:15:31.269: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 8.126217099s
Jun  2 21:15:33.296: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 10.153718009s
Jun  2 21:15:35.319: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 12.176500289s
Jun  2 21:15:37.343: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 14.199968332s
Jun  2 21:15:39.357: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 16.214063253s
Jun  2 21:15:41.374: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 18.231368854s
Jun  2 21:15:43.395: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=true. Elapsed: 20.251844882s
Jun  2 21:15:45.409: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Running", Reason="", readiness=false. Elapsed: 22.265738363s
Jun  2 21:15:47.434: INFO: Pod "pod-subpath-test-configmap-p5mk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.291218364s
STEP: Saw pod success
Jun  2 21:15:47.434: INFO: Pod "pod-subpath-test-configmap-p5mk" satisfied condition "Succeeded or Failed"
Jun  2 21:15:47.444: INFO: Trying to get logs from node 10.134.156.209 pod pod-subpath-test-configmap-p5mk container test-container-subpath-configmap-p5mk: <nil>
STEP: delete the pod
Jun  2 21:15:47.574: INFO: Waiting for pod pod-subpath-test-configmap-p5mk to disappear
Jun  2 21:15:47.584: INFO: Pod pod-subpath-test-configmap-p5mk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p5mk
Jun  2 21:15:47.584: INFO: Deleting pod "pod-subpath-test-configmap-p5mk" in namespace "subpath-3894"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:15:47.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3894" for this suite.

• [SLOW TEST:24.842 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":7,"skipped":155,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:15:47.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4163
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:15:48.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4163" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":8,"skipped":176,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:15:48.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6266
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-8fc3f52d-de31-4253-b08f-59d13f9778f9
STEP: Creating a pod to test consume configMaps
Jun  2 21:15:48.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1" in namespace "configmap-6266" to be "Succeeded or Failed"
Jun  2 21:15:48.490: INFO: Pod "pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.485485ms
Jun  2 21:15:50.511: INFO: Pod "pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032973801s
Jun  2 21:15:52.531: INFO: Pod "pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052316144s
Jun  2 21:15:54.542: INFO: Pod "pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063510917s
STEP: Saw pod success
Jun  2 21:15:54.542: INFO: Pod "pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1" satisfied condition "Succeeded or Failed"
Jun  2 21:15:54.551: INFO: Trying to get logs from node 10.134.156.253 pod pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 21:15:54.715: INFO: Waiting for pod pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1 to disappear
Jun  2 21:15:54.725: INFO: Pod pod-configmaps-e4484259-14c5-4d6b-a6d6-60553fd3d1e1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:15:54.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6266" for this suite.

• [SLOW TEST:6.545 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":9,"skipped":190,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:15:54.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2794
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:15:55.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:15:58.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 15, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 15, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 15, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 15, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:16:01.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun  2 21:16:05.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=webhook-2794 attach --namespace=webhook-2794 to-be-attached-pod -i -c=container1'
Jun  2 21:16:05.614: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:16:05.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2794" for this suite.
STEP: Destroying namespace "webhook-2794-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:11.023 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":10,"skipped":208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:16:05.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5882
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:16:17.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5882" for this suite.

• [SLOW TEST:11.398 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":11,"skipped":294,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:16:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3293
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-3293
STEP: creating service affinity-nodeport-transition in namespace services-3293
STEP: creating replication controller affinity-nodeport-transition in namespace services-3293
I0602 21:16:17.481464      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3293, replica count: 3
I0602 21:16:20.532540      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 21:16:20.610: INFO: Creating new exec pod
Jun  2 21:16:25.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun  2 21:16:26.094: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun  2 21:16:26.094: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:16:26.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.189.13 80'
Jun  2 21:16:26.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.189.13 80\nConnection to 172.21.189.13 80 port [tcp/http] succeeded!\n"
Jun  2 21:16:26.433: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:16:26.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.247 32316'
Jun  2 21:16:26.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.247 32316\nConnection to 10.134.156.247 32316 port [tcp/*] succeeded!\n"
Jun  2 21:16:26.772: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:16:26.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.209 32316'
Jun  2 21:16:27.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.209 32316\nConnection to 10.134.156.209 32316 port [tcp/*] succeeded!\n"
Jun  2 21:16:27.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:16:27.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.134.156.209:32316/ ; done'
Jun  2 21:16:27.708: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n"
Jun  2 21:16:27.708: INFO: stdout: "\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t"
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:27.709: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:27.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.134.156.209:32316/ ; done'
Jun  2 21:16:28.300: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n"
Jun  2 21:16:28.300: INFO: stdout: "\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-q4zxm\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-v7vv2\naffinity-nodeport-transition-v7vv2"
Jun  2 21:16:28.300: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.300: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.300: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:28.300: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-q4zxm
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:28.301: INFO: Received response from host: affinity-nodeport-transition-v7vv2
Jun  2 21:16:58.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3293 exec execpod-affinityl92td -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.134.156.209:32316/ ; done'
Jun  2 21:16:58.789: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32316/\n"
Jun  2 21:16:58.789: INFO: stdout: "\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t\naffinity-nodeport-transition-jhh4t"
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Received response from host: affinity-nodeport-transition-jhh4t
Jun  2 21:16:58.789: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3293, will wait for the garbage collector to delete the pods
Jun  2 21:16:58.904: INFO: Deleting ReplicationController affinity-nodeport-transition took: 23.009572ms
Jun  2 21:16:59.105: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.794728ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:02.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3293" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:44.932 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":12,"skipped":308,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7523
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jun  2 21:17:02.369: INFO: Waiting up to 5m0s for pod "downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe" in namespace "downward-api-7523" to be "Succeeded or Failed"
Jun  2 21:17:02.384: INFO: Pod "downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.282631ms
Jun  2 21:17:04.399: INFO: Pod "downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029944501s
Jun  2 21:17:06.416: INFO: Pod "downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046693389s
Jun  2 21:17:08.434: INFO: Pod "downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06515277s
STEP: Saw pod success
Jun  2 21:17:08.434: INFO: Pod "downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe" satisfied condition "Succeeded or Failed"
Jun  2 21:17:08.446: INFO: Trying to get logs from node 10.134.156.247 pod downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe container dapi-container: <nil>
STEP: delete the pod
Jun  2 21:17:08.579: INFO: Waiting for pod downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe to disappear
Jun  2 21:17:08.595: INFO: Pod downward-api-2f7d38a0-a138-4410-8c34-5f80984e16fe no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:08.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7523" for this suite.

• [SLOW TEST:6.499 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":329,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:08.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5542
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  2 21:17:08.817: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  2 21:17:08.843: INFO: Waiting for terminating namespaces to be deleted...
Jun  2 21:17:08.854: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.209 before test
Jun  2 21:17:08.890: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-qxhvw from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.890: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 21:17:08.890: INFO: calico-node-xc26s from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.890: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:17:08.890: INFO: calico-typha-7684bb556f-zfmb4 from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.890: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:17:08.890: INFO: coredns-74bf9bd988-v6qcf from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.890: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:17:08.890: INFO: ibm-keepalived-watcher-5wwlj from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.890: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:17:08.890: INFO: ibm-master-proxy-static-10.134.156.209 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:08.890: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:17:08.890: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:17:08.891: INFO: konnectivity-agent-wmghg from kube-system started at 2022-06-02 19:25:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.891: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:17:08.891: INFO: metrics-server-6bc784d6c-ztp8q from kube-system started at 2022-06-02 20:01:37 +0000 UTC (3 container statuses recorded)
Jun  2 21:17:08.891: INFO: 	Container config-watcher ready: true, restart count 0
Jun  2 21:17:08.891: INFO: 	Container metrics-server ready: true, restart count 0
Jun  2 21:17:08.891: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun  2 21:17:08.891: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-j82hw from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.891: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 21:17:08.891: INFO: sonobuoy-e2e-job-d730fbd8ba3a4c8f from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:08.891: INFO: 	Container e2e ready: true, restart count 0
Jun  2 21:17:08.891: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:17:08.891: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-kd2m7 from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:08.891: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:17:08.891: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 21:17:08.891: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.247 before test
Jun  2 21:17:08.985: INFO: calico-kube-controllers-648794b58-n4vlx from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.985: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  2 21:17:08.985: INFO: calico-node-l6vdp from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.985: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:17:08.986: INFO: calico-typha-7684bb556f-k9h4f from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.986: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:17:08.986: INFO: coredns-74bf9bd988-l2b48 from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.986: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:17:08.986: INFO: coredns-autoscaler-867cd8fddb-rxjf6 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.987: INFO: 	Container autoscaler ready: true, restart count 0
Jun  2 21:17:08.987: INFO: dashboard-metrics-scraper-7f68fbcb5b-j8jwr from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.987: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun  2 21:17:08.987: INFO: ibm-file-plugin-7795fdcfdb-cd4r2 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.987: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun  2 21:17:08.987: INFO: ibm-keepalived-watcher-t9x4p from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.987: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:17:08.987: INFO: ibm-master-proxy-static-10.134.156.247 from kube-system started at 2022-06-02 19:15:43 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:08.988: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:17:08.988: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:17:08.988: INFO: ibm-storage-watcher-75494444c9-jf6zn from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.988: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun  2 21:17:08.988: INFO: konnectivity-agent-tpmz2 from kube-system started at 2022-06-02 19:25:23 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.989: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:17:08.989: INFO: kubernetes-dashboard-7bbdd6b4cd-z6cv7 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:08.989: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  2 21:17:08.989: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-jfbjf from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:08.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:17:08.990: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 21:17:08.990: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.253 before test
Jun  2 21:17:09.024: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-02 19:19:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.024: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun  2 21:17:09.024: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-8hb9r from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.024: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 21:17:09.024: INFO: calico-node-xwlqr from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.024: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:17:09.025: INFO: calico-typha-7684bb556f-bpb2x from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:17:09.025: INFO: coredns-74bf9bd988-5wlnt from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:17:09.025: INFO: ibm-keepalived-watcher-mcrwm from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:17:09.025: INFO: ibm-master-proxy-static-10.134.156.253 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:17:09.025: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:17:09.025: INFO: konnectivity-agent-swbk5 from kube-system started at 2022-06-02 19:25:19 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:17:09.025: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-t7zkd from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 21:17:09.025: INFO: sonobuoy from sonobuoy started at 2022-06-02 21:12:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  2 21:17:09.025: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-x9xlv from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:17:09.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:17:09.025: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f4ea58884d6e0a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5542" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":14,"skipped":348,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2547
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-8f9a31f9-c389-4dc5-ab53-d4921f1e9bce
STEP: Creating a pod to test consume secrets
Jun  2 21:17:10.405: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5" in namespace "projected-2547" to be "Succeeded or Failed"
Jun  2 21:17:10.419: INFO: Pod "pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.767619ms
Jun  2 21:17:12.436: INFO: Pod "pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03080965s
Jun  2 21:17:14.454: INFO: Pod "pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049284756s
Jun  2 21:17:16.487: INFO: Pod "pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081944826s
STEP: Saw pod success
Jun  2 21:17:16.487: INFO: Pod "pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5" satisfied condition "Succeeded or Failed"
Jun  2 21:17:16.520: INFO: Trying to get logs from node 10.134.156.253 pod pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5 container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 21:17:16.652: INFO: Waiting for pod pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5 to disappear
Jun  2 21:17:16.666: INFO: Pod pod-projected-secrets-8d6e75b7-9b9e-46b2-884c-0825090de2b5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:16.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2547" for this suite.

• [SLOW TEST:6.531 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":358,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:16.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6175
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:17:16.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df" in namespace "downward-api-6175" to be "Succeeded or Failed"
Jun  2 21:17:16.940: INFO: Pod "downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.367218ms
Jun  2 21:17:19.004: INFO: Pod "downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074800767s
Jun  2 21:17:21.022: INFO: Pod "downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.092477522s
STEP: Saw pod success
Jun  2 21:17:21.022: INFO: Pod "downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df" satisfied condition "Succeeded or Failed"
Jun  2 21:17:21.033: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df container client-container: <nil>
STEP: delete the pod
Jun  2 21:17:21.113: INFO: Waiting for pod downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df to disappear
Jun  2 21:17:21.125: INFO: Pod downwardapi-volume-fcb8b983-07f6-46e5-a5e4-5cdab712d0df no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:21.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6175" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":364,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:21.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8775
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0602 21:17:22.557552      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun  2 21:17:22.558: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:22.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8775" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":17,"skipped":375,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:22.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8212
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
Jun  2 21:17:22.806: INFO: created test-podtemplate-1
Jun  2 21:17:22.819: INFO: created test-podtemplate-2
Jun  2 21:17:22.833: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun  2 21:17:22.845: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun  2 21:17:22.902: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:22.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8212" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":18,"skipped":377,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:22.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-8488
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jun  2 21:17:25.244: INFO: pods: 0 < 3
Jun  2 21:17:27.271: INFO: running pods: 1 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jun  2 21:17:33.555: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:35.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8488" for this suite.

• [SLOW TEST:12.794 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":19,"skipped":391,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:35.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2546
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:17:35.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  2 21:17:35.996: INFO: The status of Pod pod-logs-websocket-465d8cbf-1772-4407-ae4f-f67edfcaccec is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:17:38.010: INFO: The status of Pod pod-logs-websocket-465d8cbf-1772-4407-ae4f-f67edfcaccec is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:38.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2546" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":20,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6920
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:17:38.330: INFO: The status of Pod server-envvars-7c2fccc5-b747-4acd-9404-6a0f0f2ef956 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:17:40.348: INFO: The status of Pod server-envvars-7c2fccc5-b747-4acd-9404-6a0f0f2ef956 is Running (Ready = true)
Jun  2 21:17:40.409: INFO: Waiting up to 5m0s for pod "client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9" in namespace "pods-6920" to be "Succeeded or Failed"
Jun  2 21:17:40.421: INFO: Pod "client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.579501ms
Jun  2 21:17:42.439: INFO: Pod "client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030013208s
Jun  2 21:17:44.458: INFO: Pod "client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049194205s
Jun  2 21:17:46.479: INFO: Pod "client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069794825s
STEP: Saw pod success
Jun  2 21:17:46.479: INFO: Pod "client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9" satisfied condition "Succeeded or Failed"
Jun  2 21:17:46.491: INFO: Trying to get logs from node 10.134.156.247 pod client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9 container env3cont: <nil>
STEP: delete the pod
Jun  2 21:17:46.571: INFO: Waiting for pod client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9 to disappear
Jun  2 21:17:46.582: INFO: Pod client-envvars-3e9515fb-96a6-4ac8-93a5-7e40c4da94d9 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:46.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6920" for this suite.

• [SLOW TEST:8.526 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":421,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:46.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1511
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-6a802b3b-adc7-4029-b47b-128100432b55
STEP: Creating a pod to test consume configMaps
Jun  2 21:17:46.870: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e" in namespace "projected-1511" to be "Succeeded or Failed"
Jun  2 21:17:46.880: INFO: Pod "pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.312115ms
Jun  2 21:17:48.929: INFO: Pod "pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059007839s
Jun  2 21:17:50.947: INFO: Pod "pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077256667s
STEP: Saw pod success
Jun  2 21:17:50.947: INFO: Pod "pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e" satisfied condition "Succeeded or Failed"
Jun  2 21:17:50.959: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e container agnhost-container: <nil>
STEP: delete the pod
Jun  2 21:17:51.013: INFO: Waiting for pod pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e to disappear
Jun  2 21:17:51.024: INFO: Pod pod-projected-configmaps-bcf0cb7e-b86b-410a-8059-edd327d1615e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:51.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1511" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":425,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:51.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9291
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:17:51.249: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun  2 21:17:53.373: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:17:54.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9291" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":23,"skipped":428,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:17:54.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6007
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:17:55.257: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:17:57.293: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 17, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 17, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 17, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 17, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:18:00.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:18:00.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1332-crds.webhook.example.com via the AdmissionRegistration API
Jun  2 21:18:06.058: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:18:06.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6007" for this suite.
STEP: Destroying namespace "webhook-6007-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.754 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":24,"skipped":431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:18:07.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-1466
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun  2 21:18:07.440: INFO: Waiting up to 5m0s for pod "security-context-7094131a-11dd-4710-b309-135f2ed893a0" in namespace "security-context-1466" to be "Succeeded or Failed"
Jun  2 21:18:07.497: INFO: Pod "security-context-7094131a-11dd-4710-b309-135f2ed893a0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.589259ms
Jun  2 21:18:09.513: INFO: Pod "security-context-7094131a-11dd-4710-b309-135f2ed893a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07322574s
Jun  2 21:18:11.531: INFO: Pod "security-context-7094131a-11dd-4710-b309-135f2ed893a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090779087s
STEP: Saw pod success
Jun  2 21:18:11.531: INFO: Pod "security-context-7094131a-11dd-4710-b309-135f2ed893a0" satisfied condition "Succeeded or Failed"
Jun  2 21:18:11.542: INFO: Trying to get logs from node 10.134.156.253 pod security-context-7094131a-11dd-4710-b309-135f2ed893a0 container test-container: <nil>
STEP: delete the pod
Jun  2 21:18:11.599: INFO: Waiting for pod security-context-7094131a-11dd-4710-b309-135f2ed893a0 to disappear
Jun  2 21:18:11.610: INFO: Pod security-context-7094131a-11dd-4710-b309-135f2ed893a0 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:18:11.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1466" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":25,"skipped":454,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:18:11.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-6cef3c4b-cc5a-4e43-a9ce-438111e0a9f8 in namespace container-probe-9671
Jun  2 21:18:15.915: INFO: Started pod busybox-6cef3c4b-cc5a-4e43-a9ce-438111e0a9f8 in namespace container-probe-9671
STEP: checking the pod's current state and verifying that restartCount is present
Jun  2 21:18:15.926: INFO: Initial restart count of pod busybox-6cef3c4b-cc5a-4e43-a9ce-438111e0a9f8 is 0
Jun  2 21:19:04.424: INFO: Restart count of pod container-probe-9671/busybox-6cef3c4b-cc5a-4e43-a9ce-438111e0a9f8 is now 1 (48.497522941s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:19:04.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9671" for this suite.

• [SLOW TEST:52.890 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:19:04.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9385
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:19:21.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9385" for this suite.

• [SLOW TEST:16.584 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":27,"skipped":493,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:19:21.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6689
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-8rgq
STEP: Creating a pod to test atomic-volume-subpath
Jun  2 21:19:21.376: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8rgq" in namespace "subpath-6689" to be "Succeeded or Failed"
Jun  2 21:19:21.388: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535478ms
Jun  2 21:19:23.408: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03159825s
Jun  2 21:19:25.429: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 4.051832394s
Jun  2 21:19:27.446: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 6.068909223s
Jun  2 21:19:29.462: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 8.085609715s
Jun  2 21:19:31.479: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 10.102108931s
Jun  2 21:19:33.498: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 12.121372406s
Jun  2 21:19:35.520: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 14.143591798s
Jun  2 21:19:37.538: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 16.161016995s
Jun  2 21:19:39.555: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 18.17808984s
Jun  2 21:19:41.585: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 20.208754962s
Jun  2 21:19:43.607: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=true. Elapsed: 22.230205094s
Jun  2 21:19:45.624: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Running", Reason="", readiness=false. Elapsed: 24.247342444s
Jun  2 21:19:47.641: INFO: Pod "pod-subpath-test-projected-8rgq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.264214438s
STEP: Saw pod success
Jun  2 21:19:47.641: INFO: Pod "pod-subpath-test-projected-8rgq" satisfied condition "Succeeded or Failed"
Jun  2 21:19:47.685: INFO: Trying to get logs from node 10.134.156.253 pod pod-subpath-test-projected-8rgq container test-container-subpath-projected-8rgq: <nil>
STEP: delete the pod
Jun  2 21:19:47.825: INFO: Waiting for pod pod-subpath-test-projected-8rgq to disappear
Jun  2 21:19:47.840: INFO: Pod pod-subpath-test-projected-8rgq no longer exists
STEP: Deleting pod pod-subpath-test-projected-8rgq
Jun  2 21:19:47.840: INFO: Deleting pod "pod-subpath-test-projected-8rgq" in namespace "subpath-6689"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:19:47.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6689" for this suite.

• [SLOW TEST:26.767 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":28,"skipped":501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:19:47.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3816
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  2 21:19:48.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:19:48.268: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:19:49.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:19:49.343: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:19:50.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  2 21:19:50.295: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:19:51.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 21:19:51.298: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Jun  2 21:19:51.322: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jun  2 21:19:51.348: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jun  2 21:19:51.353: INFO: Observed &DaemonSet event: ADDED
Jun  2 21:19:51.354: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.356: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.356: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.356: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.356: INFO: Found daemon set daemon-set in namespace daemonsets-3816 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  2 21:19:51.356: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jun  2 21:19:51.391: INFO: Observed &DaemonSet event: ADDED
Jun  2 21:19:51.391: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.392: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.393: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.393: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.394: INFO: Observed daemon set daemon-set in namespace daemonsets-3816 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  2 21:19:51.394: INFO: Observed &DaemonSet event: MODIFIED
Jun  2 21:19:51.394: INFO: Found daemon set daemon-set in namespace daemonsets-3816 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun  2 21:19:51.395: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3816, will wait for the garbage collector to delete the pods
Jun  2 21:19:51.488: INFO: Deleting DaemonSet.extensions daemon-set took: 20.821491ms
Jun  2 21:19:51.689: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.817048ms
Jun  2 21:19:54.308: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:19:54.308: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  2 21:19:54.321: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20831"},"items":null}

Jun  2 21:19:54.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20831"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:19:54.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3816" for this suite.

• [SLOW TEST:6.513 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":29,"skipped":564,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:19:54.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Jun  2 21:19:54.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 create -f -'
Jun  2 21:19:55.989: INFO: stderr: ""
Jun  2 21:19:55.989: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  2 21:19:55.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 21:19:56.096: INFO: stderr: ""
Jun  2 21:19:56.096: INFO: stdout: "update-demo-nautilus-t8xjg update-demo-nautilus-w59vt "
Jun  2 21:19:56.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods update-demo-nautilus-t8xjg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 21:19:56.188: INFO: stderr: ""
Jun  2 21:19:56.188: INFO: stdout: ""
Jun  2 21:19:56.188: INFO: update-demo-nautilus-t8xjg is created but not running
Jun  2 21:20:01.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 21:20:01.333: INFO: stderr: ""
Jun  2 21:20:01.333: INFO: stdout: "update-demo-nautilus-t8xjg update-demo-nautilus-w59vt "
Jun  2 21:20:01.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods update-demo-nautilus-t8xjg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 21:20:01.481: INFO: stderr: ""
Jun  2 21:20:01.481: INFO: stdout: "true"
Jun  2 21:20:01.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods update-demo-nautilus-t8xjg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 21:20:01.621: INFO: stderr: ""
Jun  2 21:20:01.621: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 21:20:01.621: INFO: validating pod update-demo-nautilus-t8xjg
Jun  2 21:20:01.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 21:20:01.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 21:20:01.678: INFO: update-demo-nautilus-t8xjg is verified up and running
Jun  2 21:20:01.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods update-demo-nautilus-w59vt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 21:20:01.826: INFO: stderr: ""
Jun  2 21:20:01.827: INFO: stdout: "true"
Jun  2 21:20:01.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods update-demo-nautilus-w59vt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 21:20:01.954: INFO: stderr: ""
Jun  2 21:20:01.954: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 21:20:01.954: INFO: validating pod update-demo-nautilus-w59vt
Jun  2 21:20:01.993: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 21:20:01.993: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 21:20:01.993: INFO: update-demo-nautilus-w59vt is verified up and running
STEP: using delete to clean up resources
Jun  2 21:20:01.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 delete --grace-period=0 --force -f -'
Jun  2 21:20:02.114: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 21:20:02.114: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  2 21:20:02.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get rc,svc -l name=update-demo --no-headers'
Jun  2 21:20:02.247: INFO: stderr: "No resources found in kubectl-7180 namespace.\n"
Jun  2 21:20:02.247: INFO: stdout: ""
Jun  2 21:20:02.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7180 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  2 21:20:02.377: INFO: stderr: ""
Jun  2 21:20:02.377: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:02.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7180" for this suite.

• [SLOW TEST:8.004 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":30,"skipped":576,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:02.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8420
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8420.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8420.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8420.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8420.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:20:10.894: INFO: DNS probes using dns-8420/dns-test-edcb5f85-7371-4585-ac01-835a972b1523 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:10.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8420" for this suite.

• [SLOW TEST:8.570 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":31,"skipped":577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:10.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3523
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-3523
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3523
STEP: Deleting pre-stop pod
Jun  2 21:20:24.392: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:24.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3523" for this suite.

• [SLOW TEST:13.482 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":32,"skipped":623,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:24.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2052
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
Jun  2 21:20:24.723: INFO: Found Service test-service-fkg68 in namespace services-2052 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun  2 21:20:24.723: INFO: Service test-service-fkg68 created
STEP: Getting /status
Jun  2 21:20:24.736: INFO: Service test-service-fkg68 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jun  2 21:20:24.756: INFO: observed Service test-service-fkg68 in namespace services-2052 with annotations: map[] & LoadBalancer: {[]}
Jun  2 21:20:24.756: INFO: Found Service test-service-fkg68 in namespace services-2052 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun  2 21:20:24.756: INFO: Service test-service-fkg68 has service status patched
STEP: updating the ServiceStatus
Jun  2 21:20:24.792: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jun  2 21:20:24.797: INFO: Observed Service test-service-fkg68 in namespace services-2052 with annotations: map[] & Conditions: {[]}
Jun  2 21:20:24.797: INFO: Observed event: &Service{ObjectMeta:{test-service-fkg68  services-2052  825f4fe8-e79a-4192-b688-4850e5400e74 21058 0 2022-06-02 21:20:24 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-06-02 21:20:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-06-02 21:20:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.236.26,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.236.26],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun  2 21:20:24.798: INFO: Found Service test-service-fkg68 in namespace services-2052 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  2 21:20:24.798: INFO: Service test-service-fkg68 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jun  2 21:20:24.829: INFO: observed Service test-service-fkg68 in namespace services-2052 with labels: map[test-service-static:true]
Jun  2 21:20:24.829: INFO: observed Service test-service-fkg68 in namespace services-2052 with labels: map[test-service-static:true]
Jun  2 21:20:24.830: INFO: observed Service test-service-fkg68 in namespace services-2052 with labels: map[test-service-static:true]
Jun  2 21:20:24.830: INFO: Found Service test-service-fkg68 in namespace services-2052 with labels: map[test-service:patched test-service-static:true]
Jun  2 21:20:24.830: INFO: Service test-service-fkg68 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jun  2 21:20:24.872: INFO: Observed event: ADDED
Jun  2 21:20:24.872: INFO: Observed event: MODIFIED
Jun  2 21:20:24.873: INFO: Observed event: MODIFIED
Jun  2 21:20:24.873: INFO: Observed event: MODIFIED
Jun  2 21:20:24.873: INFO: Found Service test-service-fkg68 in namespace services-2052 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun  2 21:20:24.873: INFO: Service test-service-fkg68 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:24.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2052" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":33,"skipped":628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:24.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3811
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jun  2 21:20:25.132: INFO: Waiting up to 5m0s for pod "downward-api-546154b5-587a-4806-a538-588ebc0896fd" in namespace "downward-api-3811" to be "Succeeded or Failed"
Jun  2 21:20:25.146: INFO: Pod "downward-api-546154b5-587a-4806-a538-588ebc0896fd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.786442ms
Jun  2 21:20:27.165: INFO: Pod "downward-api-546154b5-587a-4806-a538-588ebc0896fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032827067s
Jun  2 21:20:29.182: INFO: Pod "downward-api-546154b5-587a-4806-a538-588ebc0896fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05016504s
Jun  2 21:20:31.197: INFO: Pod "downward-api-546154b5-587a-4806-a538-588ebc0896fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06547686s
STEP: Saw pod success
Jun  2 21:20:31.198: INFO: Pod "downward-api-546154b5-587a-4806-a538-588ebc0896fd" satisfied condition "Succeeded or Failed"
Jun  2 21:20:31.208: INFO: Trying to get logs from node 10.134.156.247 pod downward-api-546154b5-587a-4806-a538-588ebc0896fd container dapi-container: <nil>
STEP: delete the pod
Jun  2 21:20:31.324: INFO: Waiting for pod downward-api-546154b5-587a-4806-a538-588ebc0896fd to disappear
Jun  2 21:20:31.336: INFO: Pod downward-api-546154b5-587a-4806-a538-588ebc0896fd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:31.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3811" for this suite.

• [SLOW TEST:6.462 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:31.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6784
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:20:31.755: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a7845d67-bea2-4a32-b011-238bf23a20b8", Controller:(*bool)(0xc004c6e386), BlockOwnerDeletion:(*bool)(0xc004c6e387)}}
Jun  2 21:20:31.831: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6f6d3fda-45fc-4af3-b1ee-04aadb696278", Controller:(*bool)(0xc004aa07e6), BlockOwnerDeletion:(*bool)(0xc004aa07e7)}}
Jun  2 21:20:31.845: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a4aef6e6-e6e1-4623-88cf-4845257764dc", Controller:(*bool)(0xc004aa0a6e), BlockOwnerDeletion:(*bool)(0xc004aa0a6f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:36.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6784" for this suite.

• [SLOW TEST:5.561 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":35,"skipped":707,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:36.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3966
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:20:37.166: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe" in namespace "projected-3966" to be "Succeeded or Failed"
Jun  2 21:20:37.181: INFO: Pod "downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.434678ms
Jun  2 21:20:39.196: INFO: Pod "downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029391906s
Jun  2 21:20:41.213: INFO: Pod "downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046099135s
Jun  2 21:20:43.229: INFO: Pod "downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062646932s
STEP: Saw pod success
Jun  2 21:20:43.229: INFO: Pod "downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe" satisfied condition "Succeeded or Failed"
Jun  2 21:20:43.242: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe container client-container: <nil>
STEP: delete the pod
Jun  2 21:20:43.377: INFO: Waiting for pod downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe to disappear
Jun  2 21:20:43.409: INFO: Pod downwardapi-volume-df070fc6-1d5f-4330-a087-6f374fa1bdfe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:20:43.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3966" for this suite.

• [SLOW TEST:6.510 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":36,"skipped":714,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:20:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5320
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jun  2 21:20:43.632: INFO: PodSpec: initContainers in spec.initContainers
Jun  2 21:21:30.403: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6663586a-567c-475e-bf43-c78b1904bd7e", GenerateName:"", Namespace:"init-container-5320", SelfLink:"", UID:"e01b3fe8-96b3-455e-a56a-ecc36a5d36ef", ResourceVersion:"21313", Generation:0, CreationTimestamp:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"632076695"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"2a1e9ed2f09cc2ced0f52bb71a36010a4f4bf219f8e2737c80faa333f7c5e41c", "cni.projectcalico.org/podIP":"172.30.118.48/32", "cni.projectcalico.org/podIPs":"172.30.118.48/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004baa540), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.June, 2, 21, 20, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004baa570), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.June, 2, 21, 20, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004baa5a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-7w6x4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0025f1640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7w6x4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7w6x4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7w6x4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003783ed0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.134.156.247", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003b64930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003783f60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003783f80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003783f88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003783f8c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004b77ff0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.134.156.247", PodIP:"172.30.118.48", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.118.48"}}, StartTime:time.Date(2022, time.June, 2, 21, 20, 43, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003b64a10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003b64a80)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://e1fd3e40cf05f0ecceee8baf5db28ff6277e93f2f79d2bdfe4123ad9ccbef697", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0025f1740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0025f16e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc0009c6124)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:21:30.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5320" for this suite.

• [SLOW TEST:46.998 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":37,"skipped":731,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:21:30.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4784
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  2 21:21:30.710: INFO: Waiting up to 5m0s for pod "pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8" in namespace "emptydir-4784" to be "Succeeded or Failed"
Jun  2 21:21:30.722: INFO: Pod "pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.985357ms
Jun  2 21:21:32.741: INFO: Pod "pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030950708s
Jun  2 21:21:34.759: INFO: Pod "pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049254394s
Jun  2 21:21:36.782: INFO: Pod "pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072248989s
STEP: Saw pod success
Jun  2 21:21:36.782: INFO: Pod "pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8" satisfied condition "Succeeded or Failed"
Jun  2 21:21:36.795: INFO: Trying to get logs from node 10.134.156.253 pod pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8 container test-container: <nil>
STEP: delete the pod
Jun  2 21:21:36.940: INFO: Waiting for pod pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8 to disappear
Jun  2 21:21:36.951: INFO: Pod pod-dbf609a5-8ac3-4b6a-8752-bcb221c9bac8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:21:36.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4784" for this suite.

• [SLOW TEST:6.535 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":751,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:21:36.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-889
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:21:37.175: INFO: Creating ReplicaSet my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a
Jun  2 21:21:37.202: INFO: Pod name my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a: Found 0 pods out of 1
Jun  2 21:21:42.224: INFO: Pod name my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a: Found 1 pods out of 1
Jun  2 21:21:42.224: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a" is running
Jun  2 21:21:42.235: INFO: Pod "my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a-s4cf5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:21:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:21:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:21:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:21:37 +0000 UTC Reason: Message:}])
Jun  2 21:21:42.235: INFO: Trying to dial the pod
Jun  2 21:21:47.323: INFO: Controller my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a: Got expected result from replica 1 [my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a-s4cf5]: "my-hostname-basic-8da62239-5ae9-4274-aa26-15f05aaad47a-s4cf5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:21:47.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-889" for this suite.

• [SLOW TEST:10.377 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":39,"skipped":759,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:21:47.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9057
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jun  2 21:21:53.805: INFO: 32 pods remaining
Jun  2 21:21:53.805: INFO: 31 pods has nil DeletionTimestamp
Jun  2 21:21:53.805: INFO: 
STEP: Gathering metrics
Jun  2 21:21:54.761: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:21:54.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0602 21:21:54.761778      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-9057" for this suite.

• [SLOW TEST:7.462 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":40,"skipped":763,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:21:54.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3541
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3541
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-3541
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3541
Jun  2 21:21:55.248: INFO: Found 0 stateful pods, waiting for 1
Jun  2 21:22:05.280: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun  2 21:22:05.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 21:22:05.613: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 21:22:05.613: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 21:22:05.613: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 21:22:05.629: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  2 21:22:15.656: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 21:22:15.656: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 21:22:15.726: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun  2 21:22:15.726: INFO: ss-0  10.134.156.247  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:21:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:21:55 +0000 UTC  }]
Jun  2 21:22:15.726: INFO: 
Jun  2 21:22:15.726: INFO: StatefulSet ss has not reached scale 3, at 1
Jun  2 21:22:16.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987093186s
Jun  2 21:22:17.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9665832s
Jun  2 21:22:18.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.944833044s
Jun  2 21:22:19.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.920913099s
Jun  2 21:22:20.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.888228054s
Jun  2 21:22:21.859: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.871915778s
Jun  2 21:22:22.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.85378968s
Jun  2 21:22:23.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.822082609s
Jun  2 21:22:24.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 804.992184ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3541
Jun  2 21:22:25.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 21:22:26.268: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  2 21:22:26.269: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 21:22:26.269: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 21:22:26.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 21:22:26.668: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  2 21:22:26.668: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 21:22:26.668: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 21:22:26.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 21:22:27.029: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  2 21:22:27.029: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 21:22:27.029: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 21:22:27.048: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 21:22:27.048: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 21:22:27.048: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun  2 21:22:27.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 21:22:27.424: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 21:22:27.424: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 21:22:27.424: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 21:22:27.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 21:22:27.744: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 21:22:27.744: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 21:22:27.744: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 21:22:27.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3541 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 21:22:28.075: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 21:22:28.075: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 21:22:28.075: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 21:22:28.075: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 21:22:28.089: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  2 21:22:38.155: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 21:22:38.155: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 21:22:38.155: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 21:22:38.221: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun  2 21:22:38.221: INFO: ss-0  10.134.156.247  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:21:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:21:55 +0000 UTC  }]
Jun  2 21:22:38.221: INFO: ss-1  10.134.156.253  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  }]
Jun  2 21:22:38.221: INFO: ss-2  10.134.156.209  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  }]
Jun  2 21:22:38.221: INFO: 
Jun  2 21:22:38.222: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  2 21:22:39.239: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun  2 21:22:39.239: INFO: ss-0  10.134.156.247  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:21:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:21:55 +0000 UTC  }]
Jun  2 21:22:39.239: INFO: ss-1  10.134.156.253  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  }]
Jun  2 21:22:39.240: INFO: ss-2  10.134.156.209  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 21:22:15 +0000 UTC  }]
Jun  2 21:22:39.240: INFO: 
Jun  2 21:22:39.240: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  2 21:22:40.255: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.964832249s
Jun  2 21:22:41.273: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.948978309s
Jun  2 21:22:42.293: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.931392184s
Jun  2 21:22:43.306: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.911774861s
Jun  2 21:22:44.322: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.897896406s
Jun  2 21:22:45.364: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.881027181s
Jun  2 21:22:46.381: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.839209531s
Jun  2 21:22:47.398: INFO: Verifying statefulset ss doesn't scale past 0 for another 822.859384ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3541
Jun  2 21:22:48.414: INFO: Scaling statefulset ss to 0
Jun  2 21:22:48.455: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 21:22:48.467: INFO: Deleting all statefulset in ns statefulset-3541
Jun  2 21:22:48.480: INFO: Scaling statefulset ss to 0
Jun  2 21:22:48.519: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 21:22:48.531: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:22:48.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3541" for this suite.

• [SLOW TEST:53.809 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":41,"skipped":779,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:22:48.634: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2383
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2383
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-2383
Jun  2 21:22:48.941: INFO: Found 0 stateful pods, waiting for 1
Jun  2 21:22:58.978: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jun  2 21:22:59.034: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jun  2 21:22:59.073: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jun  2 21:22:59.081: INFO: Observed &StatefulSet event: ADDED
Jun  2 21:22:59.081: INFO: Found Statefulset ss in namespace statefulset-2383 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  2 21:22:59.081: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jun  2 21:22:59.081: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  2 21:22:59.128: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jun  2 21:22:59.134: INFO: Observed &StatefulSet event: ADDED
Jun  2 21:22:59.135: INFO: Observed Statefulset ss in namespace statefulset-2383 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  2 21:22:59.136: INFO: Observed &StatefulSet event: MODIFIED
Jun  2 21:22:59.136: INFO: Found Statefulset ss in namespace statefulset-2383 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 21:22:59.138: INFO: Deleting all statefulset in ns statefulset-2383
Jun  2 21:22:59.153: INFO: Scaling statefulset ss to 0
Jun  2 21:23:09.279: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 21:23:09.290: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:09.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2383" for this suite.

• [SLOW TEST:20.760 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":42,"skipped":780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:09.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9285
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:23:09.742: INFO: Waiting up to 5m0s for pod "downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37" in namespace "downward-api-9285" to be "Succeeded or Failed"
Jun  2 21:23:09.754: INFO: Pod "downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37": Phase="Pending", Reason="", readiness=false. Elapsed: 11.68023ms
Jun  2 21:23:11.772: INFO: Pod "downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029575907s
Jun  2 21:23:13.819: INFO: Pod "downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077282402s
Jun  2 21:23:15.837: INFO: Pod "downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.094778773s
STEP: Saw pod success
Jun  2 21:23:15.837: INFO: Pod "downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37" satisfied condition "Succeeded or Failed"
Jun  2 21:23:15.851: INFO: Trying to get logs from node 10.134.156.253 pod downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37 container client-container: <nil>
STEP: delete the pod
Jun  2 21:23:15.975: INFO: Waiting for pod downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37 to disappear
Jun  2 21:23:15.987: INFO: Pod downwardapi-volume-821a6934-193a-4a3c-901b-5153684ffc37 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:15.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9285" for this suite.

• [SLOW TEST:6.673 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":43,"skipped":810,"failed":0}
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:16.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8870
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jun  2 21:23:16.276: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:22.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8870" for this suite.

• [SLOW TEST:6.242 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":44,"skipped":815,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:22.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1047
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:23:23.065: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:23:25.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 23, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 23, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 23, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 23, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:23:28.170: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:28.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1047" for this suite.
STEP: Destroying namespace "webhook-1047-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.207 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":45,"skipped":836,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:28.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1016
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:23:28.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618" in namespace "downward-api-1016" to be "Succeeded or Failed"
Jun  2 21:23:28.748: INFO: Pod "downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618": Phase="Pending", Reason="", readiness=false. Elapsed: 16.779162ms
Jun  2 21:23:30.763: INFO: Pod "downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031756532s
Jun  2 21:23:32.783: INFO: Pod "downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051860276s
Jun  2 21:23:34.801: INFO: Pod "downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069163679s
STEP: Saw pod success
Jun  2 21:23:34.801: INFO: Pod "downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618" satisfied condition "Succeeded or Failed"
Jun  2 21:23:34.823: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618 container client-container: <nil>
STEP: delete the pod
Jun  2 21:23:34.932: INFO: Waiting for pod downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618 to disappear
Jun  2 21:23:34.943: INFO: Pod downwardapi-volume-f379b53e-8e2d-418c-93ef-f196ff221618 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:34.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1016" for this suite.

• [SLOW TEST:6.454 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:34.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7162
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:23:35.245: INFO: Create a RollingUpdate DaemonSet
Jun  2 21:23:35.259: INFO: Check that daemon pods launch on every node of the cluster
Jun  2 21:23:35.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:23:35.286: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:23:36.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:23:36.314: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:23:37.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 21:23:37.314: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 21:23:38.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 21:23:38.314: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jun  2 21:23:38.314: INFO: Update the DaemonSet to trigger a rollout
Jun  2 21:23:38.345: INFO: Updating DaemonSet daemon-set
Jun  2 21:23:41.397: INFO: Roll back the DaemonSet before rollout is complete
Jun  2 21:23:41.431: INFO: Updating DaemonSet daemon-set
Jun  2 21:23:41.431: INFO: Make sure DaemonSet rollback is complete
Jun  2 21:23:41.468: INFO: Wrong image for pod: daemon-set-f7khb. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jun  2 21:23:41.468: INFO: Pod daemon-set-f7khb is not available
Jun  2 21:23:47.504: INFO: Pod daemon-set-2pn7v is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7162, will wait for the garbage collector to delete the pods
Jun  2 21:23:47.650: INFO: Deleting DaemonSet.extensions daemon-set took: 24.339089ms
Jun  2 21:23:47.852: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.491139ms
Jun  2 21:23:50.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:23:50.473: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  2 21:23:50.485: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23629"},"items":null}

Jun  2 21:23:50.496: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23629"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:50.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7162" for this suite.

• [SLOW TEST:15.593 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":47,"skipped":891,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:50.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7224
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jun  2 21:23:50.841: INFO: Waiting up to 5m0s for pod "downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206" in namespace "downward-api-7224" to be "Succeeded or Failed"
Jun  2 21:23:50.853: INFO: Pod "downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206": Phase="Pending", Reason="", readiness=false. Elapsed: 11.838594ms
Jun  2 21:23:52.872: INFO: Pod "downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030420713s
Jun  2 21:23:54.891: INFO: Pod "downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050023481s
Jun  2 21:23:56.908: INFO: Pod "downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066954994s
STEP: Saw pod success
Jun  2 21:23:56.908: INFO: Pod "downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206" satisfied condition "Succeeded or Failed"
Jun  2 21:23:56.920: INFO: Trying to get logs from node 10.134.156.247 pod downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206 container dapi-container: <nil>
STEP: delete the pod
Jun  2 21:23:56.980: INFO: Waiting for pod downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206 to disappear
Jun  2 21:23:56.991: INFO: Pod downward-api-2aabb101-616a-41a8-9ebe-d693c8b91206 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:23:56.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7224" for this suite.

• [SLOW TEST:6.453 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:23:57.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8085
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jun  2 21:24:17.639: INFO: EndpointSlice for Service endpointslice-8085/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:24:27.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8085" for this suite.

• [SLOW TEST:30.699 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":49,"skipped":946,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:24:27.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9191
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9191 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9191;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9191 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9191;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9191.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9191.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9191.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9191.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9191.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9191.svc;check="$$(dig +notcp +noall +answer +search 211.140.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.140.211_udp@PTR;check="$$(dig +tcp +noall +answer +search 211.140.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.140.211_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9191 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9191;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9191 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9191;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9191.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9191.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9191.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9191.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9191.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9191.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9191.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9191.svc;check="$$(dig +notcp +noall +answer +search 211.140.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.140.211_udp@PTR;check="$$(dig +tcp +noall +answer +search 211.140.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.140.211_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:24:32.132: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.150: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.168: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.187: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.212: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.229: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.251: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.372: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.392: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.408: INFO: Unable to read jessie_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.441: INFO: Unable to read jessie_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.459: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.476: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.493: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:32.564: INFO: Lookups using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9191 wheezy_tcp@dns-test-service.dns-9191 wheezy_udp@dns-test-service.dns-9191.svc wheezy_tcp@dns-test-service.dns-9191.svc wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9191 jessie_tcp@dns-test-service.dns-9191 jessie_udp@dns-test-service.dns-9191.svc jessie_tcp@dns-test-service.dns-9191.svc jessie_udp@_http._tcp.dns-test-service.dns-9191.svc jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc]

Jun  2 21:24:37.584: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.601: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.620: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.662: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.705: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.725: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.766: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.783: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.973: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:37.987: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.003: INFO: Unable to read jessie_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.022: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.056: INFO: Unable to read jessie_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.072: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.088: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.104: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:38.230: INFO: Lookups using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9191 wheezy_tcp@dns-test-service.dns-9191 wheezy_udp@dns-test-service.dns-9191.svc wheezy_tcp@dns-test-service.dns-9191.svc wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9191 jessie_tcp@dns-test-service.dns-9191 jessie_udp@dns-test-service.dns-9191.svc jessie_tcp@dns-test-service.dns-9191.svc jessie_udp@_http._tcp.dns-test-service.dns-9191.svc jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc]

Jun  2 21:24:42.585: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.605: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.622: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.658: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.716: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.736: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.830: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.851: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.869: INFO: Unable to read jessie_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.891: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.914: INFO: Unable to read jessie_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.960: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:42.986: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:43.025: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:43.091: INFO: Lookups using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9191 wheezy_tcp@dns-test-service.dns-9191 wheezy_udp@dns-test-service.dns-9191.svc wheezy_tcp@dns-test-service.dns-9191.svc wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9191 jessie_tcp@dns-test-service.dns-9191 jessie_udp@dns-test-service.dns-9191.svc jessie_tcp@dns-test-service.dns-9191.svc jessie_udp@_http._tcp.dns-test-service.dns-9191.svc jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc]

Jun  2 21:24:47.584: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.608: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.627: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.646: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.665: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.682: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.705: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.725: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.858: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.875: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.893: INFO: Unable to read jessie_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.928: INFO: Unable to read jessie_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.948: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.967: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:47.987: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:48.086: INFO: Lookups using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9191 wheezy_tcp@dns-test-service.dns-9191 wheezy_udp@dns-test-service.dns-9191.svc wheezy_tcp@dns-test-service.dns-9191.svc wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9191 jessie_tcp@dns-test-service.dns-9191 jessie_udp@dns-test-service.dns-9191.svc jessie_tcp@dns-test-service.dns-9191.svc jessie_udp@_http._tcp.dns-test-service.dns-9191.svc jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc]

Jun  2 21:24:52.601: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.619: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.637: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.660: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.675: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.694: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.720: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.741: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.863: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.901: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.920: INFO: Unable to read jessie_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.947: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:52.982: INFO: Unable to read jessie_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:53.007: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:53.024: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:53.042: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:53.269: INFO: Lookups using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9191 wheezy_tcp@dns-test-service.dns-9191 wheezy_udp@dns-test-service.dns-9191.svc wheezy_tcp@dns-test-service.dns-9191.svc wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9191 jessie_tcp@dns-test-service.dns-9191 jessie_udp@dns-test-service.dns-9191.svc jessie_tcp@dns-test-service.dns-9191.svc jessie_udp@_http._tcp.dns-test-service.dns-9191.svc jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc]

Jun  2 21:24:57.586: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.606: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.624: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.659: INFO: Unable to read wheezy_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.679: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.696: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.714: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.809: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.837: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.853: INFO: Unable to read jessie_udp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191 from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.889: INFO: Unable to read jessie_udp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.931: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:57.949: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc from pod dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286: the server could not find the requested resource (get pods dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286)
Jun  2 21:24:58.024: INFO: Lookups using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9191 wheezy_tcp@dns-test-service.dns-9191 wheezy_udp@dns-test-service.dns-9191.svc wheezy_tcp@dns-test-service.dns-9191.svc wheezy_udp@_http._tcp.dns-test-service.dns-9191.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9191.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9191 jessie_tcp@dns-test-service.dns-9191 jessie_udp@dns-test-service.dns-9191.svc jessie_tcp@dns-test-service.dns-9191.svc jessie_udp@_http._tcp.dns-test-service.dns-9191.svc jessie_tcp@_http._tcp.dns-test-service.dns-9191.svc]

Jun  2 21:25:03.077: INFO: DNS probes using dns-9191/dns-test-cf5163b9-554b-4013-8f9c-6f5ace4a7286 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:03.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9191" for this suite.

• [SLOW TEST:35.585 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":50,"skipped":959,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:03.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-5055
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:25:03.588: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Jun  2 21:25:03.588: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:03.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5055" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":51,"skipped":972,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:03.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9761
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:25:04.191: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:25:06.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 25, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 25, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 25, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 25, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:25:09.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:25:09.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Registering the custom resource webhook via the AdmissionRegistration API
Jun  2 21:25:10.984: INFO: Waiting for webhook configuration to be ready...
Jun  2 21:25:16.163: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:17.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9761" for this suite.
STEP: Destroying namespace "webhook-9761-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.706 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":52,"skipped":974,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:17.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2465
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
Jun  2 21:25:17.584: INFO: Waiting up to 5m0s for pod "client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755" in namespace "containers-2465" to be "Succeeded or Failed"
Jun  2 21:25:17.596: INFO: Pod "client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755": Phase="Pending", Reason="", readiness=false. Elapsed: 11.972963ms
Jun  2 21:25:19.615: INFO: Pod "client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031338147s
Jun  2 21:25:21.632: INFO: Pod "client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048121685s
STEP: Saw pod success
Jun  2 21:25:21.632: INFO: Pod "client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755" satisfied condition "Succeeded or Failed"
Jun  2 21:25:21.644: INFO: Trying to get logs from node 10.134.156.247 pod client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 21:25:21.721: INFO: Waiting for pod client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755 to disappear
Jun  2 21:25:21.735: INFO: Pod client-containers-8d3c066e-44c5-4d3a-9faf-dcd6302f9755 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2465" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:21.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9090
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:25:21.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  2 21:25:22.043: INFO: The status of Pod pod-exec-websocket-b03f2fba-1b33-4acc-acd7-97f6c0b50eb7 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:25:24.061: INFO: The status of Pod pod-exec-websocket-b03f2fba-1b33-4acc-acd7-97f6c0b50eb7 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:24.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9090" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":54,"skipped":1014,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:24.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5327
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:24.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5327" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":55,"skipped":1026,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:24.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-1339
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:26.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1339" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":56,"skipped":1050,"failed":0}
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:26.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6778
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:25:27.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6778
I0602 21:25:27.205403      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6778, replica count: 1
I0602 21:25:28.257402      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0602 21:25:29.257695      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0602 21:25:30.258046      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 21:25:30.384: INFO: Created: latency-svc-fnvtb
Jun  2 21:25:30.397: INFO: Got endpoints: latency-svc-fnvtb [38.200644ms]
Jun  2 21:25:30.451: INFO: Created: latency-svc-tdrdm
Jun  2 21:25:30.483: INFO: Got endpoints: latency-svc-tdrdm [85.788812ms]
Jun  2 21:25:30.485: INFO: Created: latency-svc-mhc2f
Jun  2 21:25:30.495: INFO: Created: latency-svc-qmbtd
Jun  2 21:25:30.502: INFO: Created: latency-svc-g27sz
Jun  2 21:25:30.511: INFO: Created: latency-svc-sn2vq
Jun  2 21:25:30.549: INFO: Created: latency-svc-b297s
Jun  2 21:25:30.556: INFO: Got endpoints: latency-svc-g27sz [158.791945ms]
Jun  2 21:25:30.556: INFO: Got endpoints: latency-svc-mhc2f [159.127224ms]
Jun  2 21:25:30.564: INFO: Created: latency-svc-xn9hh
Jun  2 21:25:30.573: INFO: Got endpoints: latency-svc-qmbtd [175.896355ms]
Jun  2 21:25:30.578: INFO: Got endpoints: latency-svc-b297s [180.334546ms]
Jun  2 21:25:30.578: INFO: Got endpoints: latency-svc-sn2vq [180.803888ms]
Jun  2 21:25:30.580: INFO: Created: latency-svc-ndccz
Jun  2 21:25:30.608: INFO: Got endpoints: latency-svc-xn9hh [209.937822ms]
Jun  2 21:25:30.609: INFO: Got endpoints: latency-svc-ndccz [211.005436ms]
Jun  2 21:25:30.610: INFO: Created: latency-svc-8ff9n
Jun  2 21:25:30.614: INFO: Created: latency-svc-24cdr
Jun  2 21:25:30.622: INFO: Created: latency-svc-6fz7x
Jun  2 21:25:30.653: INFO: Created: latency-svc-nhds8
Jun  2 21:25:30.659: INFO: Got endpoints: latency-svc-8ff9n [261.622984ms]
Jun  2 21:25:30.679: INFO: Created: latency-svc-4c7td
Jun  2 21:25:30.689: INFO: Got endpoints: latency-svc-24cdr [290.847315ms]
Jun  2 21:25:30.689: INFO: Got endpoints: latency-svc-6fz7x [291.026707ms]
Jun  2 21:25:30.690: INFO: Got endpoints: latency-svc-nhds8 [292.177284ms]
Jun  2 21:25:30.692: INFO: Created: latency-svc-zn7fw
Jun  2 21:25:30.704: INFO: Created: latency-svc-njgkm
Jun  2 21:25:30.716: INFO: Created: latency-svc-l2xnc
Jun  2 21:25:30.719: INFO: Got endpoints: latency-svc-4c7td [320.333228ms]
Jun  2 21:25:30.725: INFO: Created: latency-svc-rmvvw
Jun  2 21:25:30.731: INFO: Created: latency-svc-6ljh2
Jun  2 21:25:30.743: INFO: Got endpoints: latency-svc-zn7fw [344.533827ms]
Jun  2 21:25:30.751: INFO: Got endpoints: latency-svc-rmvvw [194.826339ms]
Jun  2 21:25:30.752: INFO: Got endpoints: latency-svc-njgkm [354.519476ms]
Jun  2 21:25:30.754: INFO: Created: latency-svc-j72zt
Jun  2 21:25:30.755: INFO: Created: latency-svc-78rzz
Jun  2 21:25:30.756: INFO: Got endpoints: latency-svc-l2xnc [273.424511ms]
Jun  2 21:25:30.757: INFO: Got endpoints: latency-svc-6ljh2 [200.056824ms]
Jun  2 21:25:30.760: INFO: Created: latency-svc-k7x7t
Jun  2 21:25:30.763: INFO: Created: latency-svc-pxvqf
Jun  2 21:25:30.772: INFO: Created: latency-svc-9vv7t
Jun  2 21:25:30.781: INFO: Created: latency-svc-5xvtl
Jun  2 21:25:30.796: INFO: Created: latency-svc-ld84c
Jun  2 21:25:30.798: INFO: Got endpoints: latency-svc-j72zt [220.203286ms]
Jun  2 21:25:30.799: INFO: Got endpoints: latency-svc-78rzz [225.856469ms]
Jun  2 21:25:30.799: INFO: Got endpoints: latency-svc-k7x7t [220.915812ms]
Jun  2 21:25:30.802: INFO: Created: latency-svc-cdhrf
Jun  2 21:25:30.828: INFO: Got endpoints: latency-svc-9vv7t [218.858222ms]
Jun  2 21:25:30.828: INFO: Got endpoints: latency-svc-pxvqf [220.564687ms]
Jun  2 21:25:30.830: INFO: Created: latency-svc-zbcmq
Jun  2 21:25:30.831: INFO: Created: latency-svc-r5x6k
Jun  2 21:25:30.835: INFO: Got endpoints: latency-svc-5xvtl [175.41187ms]
Jun  2 21:25:30.835: INFO: Got endpoints: latency-svc-ld84c [146.315853ms]
Jun  2 21:25:30.843: INFO: Created: latency-svc-wwzwk
Jun  2 21:25:30.849: INFO: Got endpoints: latency-svc-zbcmq [159.338636ms]
Jun  2 21:25:30.848: INFO: Got endpoints: latency-svc-cdhrf [157.112575ms]
Jun  2 21:25:30.851: INFO: Got endpoints: latency-svc-r5x6k [131.422536ms]
Jun  2 21:25:30.852: INFO: Created: latency-svc-5gpwj
Jun  2 21:25:30.863: INFO: Created: latency-svc-7wcm4
Jun  2 21:25:30.870: INFO: Created: latency-svc-p8kd4
Jun  2 21:25:30.880: INFO: Created: latency-svc-sc2cp
Jun  2 21:25:30.888: INFO: Created: latency-svc-5r6m4
Jun  2 21:25:30.898: INFO: Got endpoints: latency-svc-wwzwk [154.471084ms]
Jun  2 21:25:30.903: INFO: Created: latency-svc-7kgxd
Jun  2 21:25:30.905: INFO: Got endpoints: latency-svc-5gpwj [154.474998ms]
Jun  2 21:25:30.906: INFO: Got endpoints: latency-svc-p8kd4 [149.278862ms]
Jun  2 21:25:30.906: INFO: Got endpoints: latency-svc-7wcm4 [154.102008ms]
Jun  2 21:25:30.941: INFO: Created: latency-svc-7zrmb
Jun  2 21:25:30.943: INFO: Got endpoints: latency-svc-sc2cp [184.345011ms]
Jun  2 21:25:30.947: INFO: Got endpoints: latency-svc-5r6m4 [148.201516ms]
Jun  2 21:25:30.947: INFO: Got endpoints: latency-svc-7kgxd [148.715245ms]
Jun  2 21:25:30.952: INFO: Got endpoints: latency-svc-7zrmb [152.622878ms]
Jun  2 21:25:30.953: INFO: Created: latency-svc-4jw92
Jun  2 21:25:30.966: INFO: Created: latency-svc-5z7bd
Jun  2 21:25:30.987: INFO: Created: latency-svc-lj64z
Jun  2 21:25:30.988: INFO: Got endpoints: latency-svc-4jw92 [160.626108ms]
Jun  2 21:25:30.988: INFO: Got endpoints: latency-svc-5z7bd [159.906544ms]
Jun  2 21:25:30.993: INFO: Created: latency-svc-j49fp
Jun  2 21:25:31.005: INFO: Created: latency-svc-4n5rj
Jun  2 21:25:31.024: INFO: Got endpoints: latency-svc-lj64z [188.776198ms]
Jun  2 21:25:31.025: INFO: Got endpoints: latency-svc-j49fp [189.997354ms]
Jun  2 21:25:31.025: INFO: Got endpoints: latency-svc-4n5rj [176.466979ms]
Jun  2 21:25:31.171: INFO: Created: latency-svc-8r5n9
Jun  2 21:25:31.171: INFO: Created: latency-svc-znsms
Jun  2 21:25:31.171: INFO: Created: latency-svc-6dqpg
Jun  2 21:25:31.173: INFO: Created: latency-svc-wg92n
Jun  2 21:25:31.174: INFO: Created: latency-svc-sm25r
Jun  2 21:25:31.174: INFO: Created: latency-svc-qcd4k
Jun  2 21:25:31.174: INFO: Created: latency-svc-kcbg6
Jun  2 21:25:31.174: INFO: Created: latency-svc-z64pg
Jun  2 21:25:31.175: INFO: Created: latency-svc-crf5c
Jun  2 21:25:31.175: INFO: Created: latency-svc-6wz4r
Jun  2 21:25:31.175: INFO: Created: latency-svc-vcfzq
Jun  2 21:25:31.176: INFO: Created: latency-svc-lvpqd
Jun  2 21:25:31.176: INFO: Created: latency-svc-tbhrv
Jun  2 21:25:31.178: INFO: Created: latency-svc-g8nrx
Jun  2 21:25:31.178: INFO: Created: latency-svc-x9m22
Jun  2 21:25:31.182: INFO: Got endpoints: latency-svc-6dqpg [234.879762ms]
Jun  2 21:25:31.184: INFO: Got endpoints: latency-svc-znsms [285.230301ms]
Jun  2 21:25:31.184: INFO: Got endpoints: latency-svc-wg92n [158.692693ms]
Jun  2 21:25:31.185: INFO: Got endpoints: latency-svc-8r5n9 [233.277705ms]
Jun  2 21:25:31.188: INFO: Got endpoints: latency-svc-6wz4r [161.943063ms]
Jun  2 21:25:31.204: INFO: Got endpoints: latency-svc-x9m22 [256.503317ms]
Jun  2 21:25:31.205: INFO: Got endpoints: latency-svc-g8nrx [216.255187ms]
Jun  2 21:25:31.205: INFO: Got endpoints: latency-svc-vcfzq [298.653611ms]
Jun  2 21:25:31.205: INFO: Got endpoints: latency-svc-lvpqd [216.786406ms]
Jun  2 21:25:31.208: INFO: Got endpoints: latency-svc-z64pg [356.39439ms]
Jun  2 21:25:31.228: INFO: Got endpoints: latency-svc-sm25r [322.788074ms]
Jun  2 21:25:31.233: INFO: Got endpoints: latency-svc-tbhrv [326.498246ms]
Jun  2 21:25:31.262: INFO: Created: latency-svc-9gj5j
Jun  2 21:25:31.263: INFO: Created: latency-svc-4c458
Jun  2 21:25:31.265: INFO: Created: latency-svc-2hkll
Jun  2 21:25:31.265: INFO: Created: latency-svc-85ldw
Jun  2 21:25:31.278: INFO: Got endpoints: latency-svc-crf5c [334.112418ms]
Jun  2 21:25:31.278: INFO: Got endpoints: latency-svc-qcd4k [253.988827ms]
Jun  2 21:25:31.282: INFO: Got endpoints: latency-svc-kcbg6 [432.612717ms]
Jun  2 21:25:31.306: INFO: Created: latency-svc-hwjmv
Jun  2 21:25:31.307: INFO: Got endpoints: latency-svc-9gj5j [124.783202ms]
Jun  2 21:25:31.310: INFO: Created: latency-svc-hc8dj
Jun  2 21:25:31.311: INFO: Got endpoints: latency-svc-4c458 [123.2513ms]
Jun  2 21:25:31.312: INFO: Got endpoints: latency-svc-2hkll [128.376244ms]
Jun  2 21:25:31.315: INFO: Got endpoints: latency-svc-85ldw [130.700575ms]
Jun  2 21:25:31.317: INFO: Got endpoints: latency-svc-hc8dj [113.375569ms]
Jun  2 21:25:31.318: INFO: Created: latency-svc-h64tj
Jun  2 21:25:31.318: INFO: Got endpoints: latency-svc-hwjmv [132.099399ms]
Jun  2 21:25:31.324: INFO: Created: latency-svc-7c9r9
Jun  2 21:25:31.326: INFO: Got endpoints: latency-svc-h64tj [120.978878ms]
Jun  2 21:25:31.332: INFO: Got endpoints: latency-svc-7c9r9 [126.417687ms]
Jun  2 21:25:31.334: INFO: Created: latency-svc-hm9d5
Jun  2 21:25:31.367: INFO: Got endpoints: latency-svc-hm9d5 [161.287148ms]
Jun  2 21:25:31.399: INFO: Created: latency-svc-b5kb8
Jun  2 21:25:31.401: INFO: Created: latency-svc-rftxk
Jun  2 21:25:31.401: INFO: Created: latency-svc-c76g6
Jun  2 21:25:31.404: INFO: Created: latency-svc-jbspk
Jun  2 21:25:31.414: INFO: Got endpoints: latency-svc-b5kb8 [185.123885ms]
Jun  2 21:25:31.414: INFO: Created: latency-svc-7tvdk
Jun  2 21:25:31.420: INFO: Got endpoints: latency-svc-rftxk [212.620861ms]
Jun  2 21:25:31.421: INFO: Got endpoints: latency-svc-c76g6 [187.516104ms]
Jun  2 21:25:31.451: INFO: Got endpoints: latency-svc-jbspk [172.768723ms]
Jun  2 21:25:31.461: INFO: Got endpoints: latency-svc-7tvdk [181.602566ms]
Jun  2 21:25:31.477: INFO: Created: latency-svc-l99cd
Jun  2 21:25:31.483: INFO: Got endpoints: latency-svc-l99cd [200.851695ms]
Jun  2 21:25:31.483: INFO: Created: latency-svc-m5pqw
Jun  2 21:25:31.519: INFO: Got endpoints: latency-svc-m5pqw [211.96615ms]
Jun  2 21:25:31.522: INFO: Created: latency-svc-nrjrr
Jun  2 21:25:31.537: INFO: Got endpoints: latency-svc-nrjrr [225.892595ms]
Jun  2 21:25:31.539: INFO: Created: latency-svc-jnggq
Jun  2 21:25:31.551: INFO: Created: latency-svc-tf74p
Jun  2 21:25:31.555: INFO: Got endpoints: latency-svc-jnggq [242.875922ms]
Jun  2 21:25:31.556: INFO: Got endpoints: latency-svc-tf74p [239.453155ms]
Jun  2 21:25:31.561: INFO: Created: latency-svc-rqfss
Jun  2 21:25:31.584: INFO: Got endpoints: latency-svc-rqfss [266.901096ms]
Jun  2 21:25:31.594: INFO: Created: latency-svc-bs8dx
Jun  2 21:25:31.597: INFO: Created: latency-svc-whs9r
Jun  2 21:25:31.638: INFO: Created: latency-svc-h8x9w
Jun  2 21:25:31.638: INFO: Got endpoints: latency-svc-bs8dx [320.597886ms]
Jun  2 21:25:31.643: INFO: Got endpoints: latency-svc-whs9r [317.320494ms]
Jun  2 21:25:31.649: INFO: Created: latency-svc-hv8dw
Jun  2 21:25:31.685: INFO: Got endpoints: latency-svc-h8x9w [353.623581ms]
Jun  2 21:25:31.686: INFO: Got endpoints: latency-svc-hv8dw [318.392529ms]
Jun  2 21:25:31.703: INFO: Created: latency-svc-xddk9
Jun  2 21:25:31.705: INFO: Created: latency-svc-z5kw5
Jun  2 21:25:31.719: INFO: Created: latency-svc-mqhcw
Jun  2 21:25:31.740: INFO: Created: latency-svc-lvkmf
Jun  2 21:25:31.745: INFO: Created: latency-svc-j9lls
Jun  2 21:25:31.745: INFO: Got endpoints: latency-svc-mqhcw [324.113647ms]
Jun  2 21:25:31.746: INFO: Got endpoints: latency-svc-z5kw5 [325.715597ms]
Jun  2 21:25:31.746: INFO: Got endpoints: latency-svc-xddk9 [332.482725ms]
Jun  2 21:25:31.790: INFO: Got endpoints: latency-svc-j9lls [329.292584ms]
Jun  2 21:25:31.791: INFO: Got endpoints: latency-svc-lvkmf [339.420577ms]
Jun  2 21:25:31.810: INFO: Created: latency-svc-w7bvb
Jun  2 21:25:31.811: INFO: Got endpoints: latency-svc-w7bvb [290.952835ms]
Jun  2 21:25:31.811: INFO: Created: latency-svc-wk6cb
Jun  2 21:25:31.811: INFO: Got endpoints: latency-svc-wk6cb [328.035052ms]
Jun  2 21:25:31.830: INFO: Created: latency-svc-d2xjq
Jun  2 21:25:31.831: INFO: Created: latency-svc-fhhgc
Jun  2 21:25:31.831: INFO: Got endpoints: latency-svc-fhhgc [294.231346ms]
Jun  2 21:25:31.831: INFO: Got endpoints: latency-svc-d2xjq [274.483792ms]
Jun  2 21:25:31.830: INFO: Created: latency-svc-9x4dw
Jun  2 21:25:31.832: INFO: Got endpoints: latency-svc-9x4dw [276.810498ms]
Jun  2 21:25:31.920: INFO: Created: latency-svc-l5zj7
Jun  2 21:25:31.922: INFO: Created: latency-svc-hpd4k
Jun  2 21:25:31.922: INFO: Created: latency-svc-b2jcc
Jun  2 21:25:31.922: INFO: Created: latency-svc-kzksn
Jun  2 21:25:31.945: INFO: Got endpoints: latency-svc-hpd4k [360.696889ms]
Jun  2 21:25:31.945: INFO: Got endpoints: latency-svc-b2jcc [306.707157ms]
Jun  2 21:25:31.947: INFO: Got endpoints: latency-svc-l5zj7 [261.829354ms]
Jun  2 21:25:31.947: INFO: Got endpoints: latency-svc-kzksn [304.190825ms]
Jun  2 21:25:31.953: INFO: Created: latency-svc-w4qvb
Jun  2 21:25:31.972: INFO: Got endpoints: latency-svc-w4qvb [286.128751ms]
Jun  2 21:25:31.986: INFO: Created: latency-svc-plhdr
Jun  2 21:25:31.987: INFO: Created: latency-svc-cf47h
Jun  2 21:25:32.005: INFO: Got endpoints: latency-svc-cf47h [259.776201ms]
Jun  2 21:25:32.007: INFO: Got endpoints: latency-svc-plhdr [260.524136ms]
Jun  2 21:25:32.076: INFO: Created: latency-svc-svkz5
Jun  2 21:25:32.076: INFO: Created: latency-svc-q9gdf
Jun  2 21:25:32.086: INFO: Created: latency-svc-vq4hc
Jun  2 21:25:32.115: INFO: Got endpoints: latency-svc-q9gdf [324.763049ms]
Jun  2 21:25:32.115: INFO: Got endpoints: latency-svc-vq4hc [323.892814ms]
Jun  2 21:25:32.116: INFO: Got endpoints: latency-svc-svkz5 [369.848423ms]
Jun  2 21:25:32.117: INFO: Created: latency-svc-kljmk
Jun  2 21:25:32.118: INFO: Created: latency-svc-pgtj2
Jun  2 21:25:32.119: INFO: Created: latency-svc-mb88b
Jun  2 21:25:32.121: INFO: Got endpoints: latency-svc-kljmk [309.652303ms]
Jun  2 21:25:32.121: INFO: Created: latency-svc-hx5b6
Jun  2 21:25:32.121: INFO: Created: latency-svc-bkqv7
Jun  2 21:25:32.125: INFO: Got endpoints: latency-svc-hx5b6 [314.546395ms]
Jun  2 21:25:32.125: INFO: Got endpoints: latency-svc-pgtj2 [294.057751ms]
Jun  2 21:25:32.132: INFO: Got endpoints: latency-svc-mb88b [299.651167ms]
Jun  2 21:25:32.132: INFO: Got endpoints: latency-svc-bkqv7 [300.228116ms]
Jun  2 21:25:32.133: INFO: Created: latency-svc-7nkkz
Jun  2 21:25:32.135: INFO: Created: latency-svc-dln9s
Jun  2 21:25:32.184: INFO: Created: latency-svc-rgpvn
Jun  2 21:25:32.185: INFO: Created: latency-svc-wlwmf
Jun  2 21:25:32.185: INFO: Got endpoints: latency-svc-7nkkz [239.544141ms]
Jun  2 21:25:32.185: INFO: Got endpoints: latency-svc-dln9s [240.096535ms]
Jun  2 21:25:32.206: INFO: Got endpoints: latency-svc-rgpvn [258.571215ms]
Jun  2 21:25:32.207: INFO: Created: latency-svc-pt5c9
Jun  2 21:25:32.208: INFO: Created: latency-svc-xrrj6
Jun  2 21:25:32.218: INFO: Got endpoints: latency-svc-wlwmf [270.182397ms]
Jun  2 21:25:32.227: INFO: Created: latency-svc-89kz7
Jun  2 21:25:32.265: INFO: Got endpoints: latency-svc-89kz7 [257.964418ms]
Jun  2 21:25:32.265: INFO: Got endpoints: latency-svc-xrrj6 [259.584849ms]
Jun  2 21:25:32.266: INFO: Got endpoints: latency-svc-pt5c9 [294.005311ms]
Jun  2 21:25:32.447: INFO: Created: latency-svc-zph9n
Jun  2 21:25:32.451: INFO: Created: latency-svc-rhmv8
Jun  2 21:25:32.451: INFO: Created: latency-svc-s7cdn
Jun  2 21:25:32.451: INFO: Created: latency-svc-9s7k9
Jun  2 21:25:32.453: INFO: Created: latency-svc-w5bvc
Jun  2 21:25:32.453: INFO: Created: latency-svc-qzxr7
Jun  2 21:25:32.453: INFO: Created: latency-svc-bdhfv
Jun  2 21:25:32.453: INFO: Created: latency-svc-x7968
Jun  2 21:25:32.454: INFO: Created: latency-svc-jd6k7
Jun  2 21:25:32.454: INFO: Created: latency-svc-852l2
Jun  2 21:25:32.454: INFO: Created: latency-svc-2vfbs
Jun  2 21:25:32.454: INFO: Created: latency-svc-px97m
Jun  2 21:25:32.454: INFO: Created: latency-svc-6cldb
Jun  2 21:25:32.455: INFO: Created: latency-svc-7rzxg
Jun  2 21:25:32.456: INFO: Created: latency-svc-8mnnl
Jun  2 21:25:32.476: INFO: Got endpoints: latency-svc-zph9n [210.773449ms]
Jun  2 21:25:32.477: INFO: Got endpoints: latency-svc-s7cdn [270.465212ms]
Jun  2 21:25:32.477: INFO: Got endpoints: latency-svc-2vfbs [290.547108ms]
Jun  2 21:25:32.501: INFO: Created: latency-svc-m786k
Jun  2 21:25:32.510: INFO: Got endpoints: latency-svc-9s7k9 [244.838533ms]
Jun  2 21:25:32.511: INFO: Got endpoints: latency-svc-852l2 [394.762943ms]
Jun  2 21:25:32.511: INFO: Got endpoints: latency-svc-rhmv8 [385.638172ms]
Jun  2 21:25:32.512: INFO: Got endpoints: latency-svc-8mnnl [391.194423ms]
Jun  2 21:25:32.513: INFO: Got endpoints: latency-svc-qzxr7 [295.365966ms]
Jun  2 21:25:32.515: INFO: Got endpoints: latency-svc-x7968 [399.411203ms]
Jun  2 21:25:32.515: INFO: Got endpoints: latency-svc-jd6k7 [383.33368ms]
Jun  2 21:25:32.519: INFO: Got endpoints: latency-svc-7rzxg [252.993611ms]
Jun  2 21:25:32.522: INFO: Got endpoints: latency-svc-px97m [390.17048ms]
Jun  2 21:25:32.524: INFO: Created: latency-svc-cmj79
Jun  2 21:25:32.529: INFO: Created: latency-svc-z4fln
Jun  2 21:25:32.530: INFO: Got endpoints: latency-svc-6cldb [344.070009ms]
Jun  2 21:25:32.541: INFO: Got endpoints: latency-svc-w5bvc [426.25386ms]
Jun  2 21:25:32.571: INFO: Created: latency-svc-74m8f
Jun  2 21:25:32.577: INFO: Got endpoints: latency-svc-m786k [100.914889ms]
Jun  2 21:25:32.577: INFO: Got endpoints: latency-svc-cmj79 [100.333562ms]
Jun  2 21:25:32.577: INFO: Got endpoints: latency-svc-z4fln [99.321487ms]
Jun  2 21:25:32.577: INFO: Got endpoints: latency-svc-bdhfv [451.977426ms]
Jun  2 21:25:32.588: INFO: Got endpoints: latency-svc-74m8f [77.21336ms]
Jun  2 21:25:32.606: INFO: Created: latency-svc-bhpjl
Jun  2 21:25:32.607: INFO: Created: latency-svc-2xd4g
Jun  2 21:25:32.646: INFO: Got endpoints: latency-svc-bhpjl [135.455797ms]
Jun  2 21:25:32.646: INFO: Got endpoints: latency-svc-2xd4g [132.723427ms]
Jun  2 21:25:32.666: INFO: Created: latency-svc-7xjgj
Jun  2 21:25:32.684: INFO: Got endpoints: latency-svc-7xjgj [172.685377ms]
Jun  2 21:25:32.688: INFO: Created: latency-svc-hc957
Jun  2 21:25:32.723: INFO: Created: latency-svc-rpgwm
Jun  2 21:25:32.724: INFO: Got endpoints: latency-svc-hc957 [211.999964ms]
Jun  2 21:25:32.736: INFO: Got endpoints: latency-svc-rpgwm [221.221472ms]
Jun  2 21:25:32.740: INFO: Created: latency-svc-jb9bx
Jun  2 21:25:32.777: INFO: Created: latency-svc-hzxbp
Jun  2 21:25:32.778: INFO: Got endpoints: latency-svc-jb9bx [262.949644ms]
Jun  2 21:25:32.784: INFO: Created: latency-svc-b6nbt
Jun  2 21:25:32.795: INFO: Got endpoints: latency-svc-hzxbp [275.737298ms]
Jun  2 21:25:32.800: INFO: Got endpoints: latency-svc-b6nbt [277.960009ms]
Jun  2 21:25:32.826: INFO: Created: latency-svc-4ch4z
Jun  2 21:25:32.826: INFO: Created: latency-svc-z9m9t
Jun  2 21:25:32.826: INFO: Created: latency-svc-p2mww
Jun  2 21:25:32.840: INFO: Created: latency-svc-42h7x
Jun  2 21:25:32.850: INFO: Got endpoints: latency-svc-p2mww [308.361394ms]
Jun  2 21:25:32.851: INFO: Created: latency-svc-7d5lq
Jun  2 21:25:32.852: INFO: Got endpoints: latency-svc-z9m9t [273.999337ms]
Jun  2 21:25:32.853: INFO: Got endpoints: latency-svc-4ch4z [323.089115ms]
Jun  2 21:25:32.855: INFO: Got endpoints: latency-svc-42h7x [276.90025ms]
Jun  2 21:25:32.857: INFO: Got endpoints: latency-svc-7d5lq [278.931737ms]
Jun  2 21:25:32.900: INFO: Created: latency-svc-gh5vz
Jun  2 21:25:32.900: INFO: Created: latency-svc-dc7wk
Jun  2 21:25:32.900: INFO: Created: latency-svc-h9pmj
Jun  2 21:25:32.907: INFO: Got endpoints: latency-svc-gh5vz [329.378739ms]
Jun  2 21:25:32.916: INFO: Got endpoints: latency-svc-dc7wk [327.290613ms]
Jun  2 21:25:32.917: INFO: Got endpoints: latency-svc-h9pmj [271.311727ms]
Jun  2 21:25:32.920: INFO: Created: latency-svc-448tq
Jun  2 21:25:32.951: INFO: Created: latency-svc-7sblf
Jun  2 21:25:32.951: INFO: Created: latency-svc-2tdtk
Jun  2 21:25:32.952: INFO: Got endpoints: latency-svc-448tq [305.490156ms]
Jun  2 21:25:32.987: INFO: Created: latency-svc-nx7jq
Jun  2 21:25:32.988: INFO: Created: latency-svc-zbnl6
Jun  2 21:25:32.989: INFO: Got endpoints: latency-svc-7sblf [264.570667ms]
Jun  2 21:25:32.990: INFO: Got endpoints: latency-svc-2tdtk [306.064172ms]
Jun  2 21:25:33.001: INFO: Got endpoints: latency-svc-zbnl6 [222.219123ms]
Jun  2 21:25:33.001: INFO: Got endpoints: latency-svc-nx7jq [264.63281ms]
Jun  2 21:25:33.209: INFO: Created: latency-svc-d74k4
Jun  2 21:25:33.219: INFO: Created: latency-svc-jf4cb
Jun  2 21:25:33.225: INFO: Created: latency-svc-8vmv5
Jun  2 21:25:33.226: INFO: Created: latency-svc-6z7d9
Jun  2 21:25:33.227: INFO: Created: latency-svc-6nrqc
Jun  2 21:25:33.227: INFO: Created: latency-svc-tfv7t
Jun  2 21:25:33.227: INFO: Created: latency-svc-bvm8w
Jun  2 21:25:33.227: INFO: Created: latency-svc-djqnq
Jun  2 21:25:33.227: INFO: Created: latency-svc-p8g2r
Jun  2 21:25:33.227: INFO: Created: latency-svc-7jmgb
Jun  2 21:25:33.228: INFO: Created: latency-svc-lxszn
Jun  2 21:25:33.228: INFO: Created: latency-svc-mdwrb
Jun  2 21:25:33.228: INFO: Created: latency-svc-46pm8
Jun  2 21:25:33.228: INFO: Created: latency-svc-sb8kh
Jun  2 21:25:33.228: INFO: Created: latency-svc-nvnz6
Jun  2 21:25:33.275: INFO: Got endpoints: latency-svc-tfv7t [273.266349ms]
Jun  2 21:25:33.277: INFO: Got endpoints: latency-svc-jf4cb [369.652972ms]
Jun  2 21:25:33.279: INFO: Got endpoints: latency-svc-8vmv5 [289.943621ms]
Jun  2 21:25:33.282: INFO: Got endpoints: latency-svc-6z7d9 [281.402305ms]
Jun  2 21:25:33.283: INFO: Got endpoints: latency-svc-d74k4 [432.872848ms]
Jun  2 21:25:33.283: INFO: Got endpoints: latency-svc-lxszn [331.744758ms]
Jun  2 21:25:33.284: INFO: Got endpoints: latency-svc-bvm8w [431.883615ms]
Jun  2 21:25:33.284: INFO: Got endpoints: latency-svc-6nrqc [367.770319ms]
Jun  2 21:25:33.284: INFO: Got endpoints: latency-svc-7jmgb [293.905981ms]
Jun  2 21:25:33.284: INFO: Got endpoints: latency-svc-nvnz6 [431.209482ms]
Jun  2 21:25:33.284: INFO: Got endpoints: latency-svc-sb8kh [488.633677ms]
Jun  2 21:25:33.284: INFO: Got endpoints: latency-svc-mdwrb [483.751701ms]
Jun  2 21:25:33.301: INFO: Got endpoints: latency-svc-djqnq [445.814086ms]
Jun  2 21:25:33.308: INFO: Got endpoints: latency-svc-46pm8 [450.934ms]
Jun  2 21:25:33.311: INFO: Got endpoints: latency-svc-p8g2r [394.142883ms]
Jun  2 21:25:33.325: INFO: Created: latency-svc-5bxg4
Jun  2 21:25:33.335: INFO: Got endpoints: latency-svc-5bxg4 [60.170213ms]
Jun  2 21:25:33.337: INFO: Created: latency-svc-8cktv
Jun  2 21:25:33.348: INFO: Created: latency-svc-pkj6k
Jun  2 21:25:33.358: INFO: Got endpoints: latency-svc-8cktv [81.05542ms]
Jun  2 21:25:33.359: INFO: Got endpoints: latency-svc-pkj6k [78.667304ms]
Jun  2 21:25:33.361: INFO: Created: latency-svc-2blkc
Jun  2 21:25:33.371: INFO: Got endpoints: latency-svc-2blkc [88.606633ms]
Jun  2 21:25:33.375: INFO: Created: latency-svc-gdmkn
Jun  2 21:25:33.394: INFO: Created: latency-svc-9gcbm
Jun  2 21:25:33.394: INFO: Got endpoints: latency-svc-gdmkn [109.042159ms]
Jun  2 21:25:33.414: INFO: Got endpoints: latency-svc-9gcbm [129.271665ms]
Jun  2 21:25:33.422: INFO: Created: latency-svc-pmw95
Jun  2 21:25:33.433: INFO: Got endpoints: latency-svc-pmw95 [150.682423ms]
Jun  2 21:25:33.435: INFO: Created: latency-svc-lrvhl
Jun  2 21:25:33.442: INFO: Created: latency-svc-5c882
Jun  2 21:25:33.446: INFO: Got endpoints: latency-svc-lrvhl [162.228277ms]
Jun  2 21:25:33.452: INFO: Got endpoints: latency-svc-5c882 [167.946376ms]
Jun  2 21:25:33.460: INFO: Created: latency-svc-5xsgp
Jun  2 21:25:33.470: INFO: Created: latency-svc-z8zc5
Jun  2 21:25:33.471: INFO: Got endpoints: latency-svc-5xsgp [187.283504ms]
Jun  2 21:25:33.481: INFO: Got endpoints: latency-svc-z8zc5 [196.370079ms]
Jun  2 21:25:33.512: INFO: Created: latency-svc-dk277
Jun  2 21:25:33.513: INFO: Created: latency-svc-bfrxc
Jun  2 21:25:33.519: INFO: Created: latency-svc-76wvt
Jun  2 21:25:33.533: INFO: Got endpoints: latency-svc-dk277 [247.746629ms]
Jun  2 21:25:33.534: INFO: Got endpoints: latency-svc-bfrxc [232.369621ms]
Jun  2 21:25:33.557: INFO: Created: latency-svc-wvx4s
Jun  2 21:25:33.559: INFO: Got endpoints: latency-svc-76wvt [251.289292ms]
Jun  2 21:25:33.605: INFO: Created: latency-svc-dbj6q
Jun  2 21:25:33.605: INFO: Got endpoints: latency-svc-wvx4s [294.040029ms]
Jun  2 21:25:33.606: INFO: Created: latency-svc-8ntz2
Jun  2 21:25:33.640: INFO: Created: latency-svc-hqsd7
Jun  2 21:25:33.642: INFO: Got endpoints: latency-svc-8ntz2 [307.202768ms]
Jun  2 21:25:33.643: INFO: Got endpoints: latency-svc-dbj6q [284.605274ms]
Jun  2 21:25:33.645: INFO: Created: latency-svc-qh8vv
Jun  2 21:25:33.651: INFO: Got endpoints: latency-svc-hqsd7 [292.414255ms]
Jun  2 21:25:33.656: INFO: Created: latency-svc-rl6gd
Jun  2 21:25:33.662: INFO: Got endpoints: latency-svc-qh8vv [290.497969ms]
Jun  2 21:25:33.668: INFO: Created: latency-svc-n4r8p
Jun  2 21:25:33.669: INFO: Got endpoints: latency-svc-rl6gd [274.516073ms]
Jun  2 21:25:33.679: INFO: Got endpoints: latency-svc-n4r8p [264.506476ms]
Jun  2 21:25:33.680: INFO: Created: latency-svc-tbfzz
Jun  2 21:25:33.690: INFO: Got endpoints: latency-svc-tbfzz [256.722229ms]
Jun  2 21:25:33.696: INFO: Created: latency-svc-2n2vt
Jun  2 21:25:33.705: INFO: Created: latency-svc-z4m8c
Jun  2 21:25:33.708: INFO: Got endpoints: latency-svc-2n2vt [261.664328ms]
Jun  2 21:25:33.717: INFO: Created: latency-svc-kcmx6
Jun  2 21:25:33.718: INFO: Got endpoints: latency-svc-z4m8c [265.828665ms]
Jun  2 21:25:33.729: INFO: Got endpoints: latency-svc-kcmx6 [257.582738ms]
Jun  2 21:25:33.731: INFO: Created: latency-svc-q7q4n
Jun  2 21:25:33.772: INFO: Created: latency-svc-ckkr9
Jun  2 21:25:33.772: INFO: Created: latency-svc-k4fpx
Jun  2 21:25:33.773: INFO: Got endpoints: latency-svc-q7q4n [291.717672ms]
Jun  2 21:25:33.805: INFO: Created: latency-svc-h9k6z
Jun  2 21:25:33.806: INFO: Created: latency-svc-jrwj9
Jun  2 21:25:33.807: INFO: Got endpoints: latency-svc-k4fpx [273.889245ms]
Jun  2 21:25:33.807: INFO: Got endpoints: latency-svc-ckkr9 [272.798818ms]
Jun  2 21:25:33.850: INFO: Got endpoints: latency-svc-jrwj9 [244.885368ms]
Jun  2 21:25:33.850: INFO: Got endpoints: latency-svc-h9k6z [291.453758ms]
Jun  2 21:25:33.853: INFO: Created: latency-svc-zz4b5
Jun  2 21:25:33.855: INFO: Created: latency-svc-r5tlk
Jun  2 21:25:33.855: INFO: Created: latency-svc-fnf6f
Jun  2 21:25:33.856: INFO: Got endpoints: latency-svc-zz4b5 [212.549315ms]
Jun  2 21:25:33.856: INFO: Got endpoints: latency-svc-r5tlk [213.670409ms]
Jun  2 21:25:33.858: INFO: Got endpoints: latency-svc-fnf6f [206.384214ms]
Jun  2 21:25:33.858: INFO: Latencies: [60.170213ms 77.21336ms 78.667304ms 81.05542ms 85.788812ms 88.606633ms 99.321487ms 100.333562ms 100.914889ms 109.042159ms 113.375569ms 120.978878ms 123.2513ms 124.783202ms 126.417687ms 128.376244ms 129.271665ms 130.700575ms 131.422536ms 132.099399ms 132.723427ms 135.455797ms 146.315853ms 148.201516ms 148.715245ms 149.278862ms 150.682423ms 152.622878ms 154.102008ms 154.471084ms 154.474998ms 157.112575ms 158.692693ms 158.791945ms 159.127224ms 159.338636ms 159.906544ms 160.626108ms 161.287148ms 161.943063ms 162.228277ms 167.946376ms 172.685377ms 172.768723ms 175.41187ms 175.896355ms 176.466979ms 180.334546ms 180.803888ms 181.602566ms 184.345011ms 185.123885ms 187.283504ms 187.516104ms 188.776198ms 189.997354ms 194.826339ms 196.370079ms 200.056824ms 200.851695ms 206.384214ms 209.937822ms 210.773449ms 211.005436ms 211.96615ms 211.999964ms 212.549315ms 212.620861ms 213.670409ms 216.255187ms 216.786406ms 218.858222ms 220.203286ms 220.564687ms 220.915812ms 221.221472ms 222.219123ms 225.856469ms 225.892595ms 232.369621ms 233.277705ms 234.879762ms 239.453155ms 239.544141ms 240.096535ms 242.875922ms 244.838533ms 244.885368ms 247.746629ms 251.289292ms 252.993611ms 253.988827ms 256.503317ms 256.722229ms 257.582738ms 257.964418ms 258.571215ms 259.584849ms 259.776201ms 260.524136ms 261.622984ms 261.664328ms 261.829354ms 262.949644ms 264.506476ms 264.570667ms 264.63281ms 265.828665ms 266.901096ms 270.182397ms 270.465212ms 271.311727ms 272.798818ms 273.266349ms 273.424511ms 273.889245ms 273.999337ms 274.483792ms 274.516073ms 275.737298ms 276.810498ms 276.90025ms 277.960009ms 278.931737ms 281.402305ms 284.605274ms 285.230301ms 286.128751ms 289.943621ms 290.497969ms 290.547108ms 290.847315ms 290.952835ms 291.026707ms 291.453758ms 291.717672ms 292.177284ms 292.414255ms 293.905981ms 294.005311ms 294.040029ms 294.057751ms 294.231346ms 295.365966ms 298.653611ms 299.651167ms 300.228116ms 304.190825ms 305.490156ms 306.064172ms 306.707157ms 307.202768ms 308.361394ms 309.652303ms 314.546395ms 317.320494ms 318.392529ms 320.333228ms 320.597886ms 322.788074ms 323.089115ms 323.892814ms 324.113647ms 324.763049ms 325.715597ms 326.498246ms 327.290613ms 328.035052ms 329.292584ms 329.378739ms 331.744758ms 332.482725ms 334.112418ms 339.420577ms 344.070009ms 344.533827ms 353.623581ms 354.519476ms 356.39439ms 360.696889ms 367.770319ms 369.652972ms 369.848423ms 383.33368ms 385.638172ms 390.17048ms 391.194423ms 394.142883ms 394.762943ms 399.411203ms 426.25386ms 431.209482ms 431.883615ms 432.612717ms 432.872848ms 445.814086ms 450.934ms 451.977426ms 483.751701ms 488.633677ms]
Jun  2 21:25:33.859: INFO: 50 %ile: 261.622984ms
Jun  2 21:25:33.859: INFO: 90 %ile: 367.770319ms
Jun  2 21:25:33.859: INFO: 99 %ile: 483.751701ms
Jun  2 21:25:33.859: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:33.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6778" for this suite.

• [SLOW TEST:6.908 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":57,"skipped":1053,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:33.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6935
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:25:34.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7" in namespace "downward-api-6935" to be "Succeeded or Failed"
Jun  2 21:25:34.156: INFO: Pod "downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.814002ms
Jun  2 21:25:36.170: INFO: Pod "downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023780519s
Jun  2 21:25:38.184: INFO: Pod "downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03761556s
Jun  2 21:25:40.198: INFO: Pod "downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051998326s
STEP: Saw pod success
Jun  2 21:25:40.198: INFO: Pod "downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7" satisfied condition "Succeeded or Failed"
Jun  2 21:25:40.211: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7 container client-container: <nil>
STEP: delete the pod
Jun  2 21:25:40.374: INFO: Waiting for pod downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7 to disappear
Jun  2 21:25:40.383: INFO: Pod downwardapi-volume-f421d312-42ce-4454-b775-c86cd7354ea7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:40.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6935" for this suite.

• [SLOW TEST:6.506 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":1065,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:40.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2946
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2946
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2946
STEP: Waiting until pod test-pod will start running in namespace statefulset-2946
STEP: Creating statefulset with conflicting port in namespace statefulset-2946
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2946
Jun  2 21:25:44.790: INFO: Observed stateful pod in namespace: statefulset-2946, name: ss-0, uid: a349e007-2834-4c7b-8369-c8158ec60813, status phase: Pending. Waiting for statefulset controller to delete.
Jun  2 21:25:44.844: INFO: Observed stateful pod in namespace: statefulset-2946, name: ss-0, uid: a349e007-2834-4c7b-8369-c8158ec60813, status phase: Failed. Waiting for statefulset controller to delete.
Jun  2 21:25:44.870: INFO: Observed stateful pod in namespace: statefulset-2946, name: ss-0, uid: a349e007-2834-4c7b-8369-c8158ec60813, status phase: Failed. Waiting for statefulset controller to delete.
Jun  2 21:25:44.886: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2946
STEP: Removing pod with conflicting port in namespace statefulset-2946
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2946 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 21:25:49.033: INFO: Deleting all statefulset in ns statefulset-2946
Jun  2 21:25:49.044: INFO: Scaling statefulset ss to 0
Jun  2 21:25:59.113: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 21:25:59.125: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:25:59.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2946" for this suite.

• [SLOW TEST:18.789 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":59,"skipped":1072,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:25:59.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5086
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Jun  2 21:25:59.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-5086 create -f -'
Jun  2 21:26:00.731: INFO: stderr: ""
Jun  2 21:26:00.731: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  2 21:26:01.783: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 21:26:01.783: INFO: Found 0 / 1
Jun  2 21:26:02.753: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 21:26:02.753: INFO: Found 0 / 1
Jun  2 21:26:03.750: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 21:26:03.750: INFO: Found 1 / 1
Jun  2 21:26:03.750: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun  2 21:26:03.762: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 21:26:03.762: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  2 21:26:03.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-5086 patch pod agnhost-primary-t4sgq -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  2 21:26:03.914: INFO: stderr: ""
Jun  2 21:26:03.914: INFO: stdout: "pod/agnhost-primary-t4sgq patched\n"
STEP: checking annotations
Jun  2 21:26:03.942: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 21:26:03.943: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:03.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5086" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":60,"skipped":1089,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:04.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-6642
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
Jun  2 21:26:04.265: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Jun  2 21:26:06.308: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jun  2 21:26:08.404: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:10.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6642" for this suite.

• [SLOW TEST:6.457 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":61,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:10.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4481
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-be01775b-f5c3-4805-8e30-de8e167705d3
STEP: Creating a pod to test consume secrets
Jun  2 21:26:10.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e" in namespace "projected-4481" to be "Succeeded or Failed"
Jun  2 21:26:10.725: INFO: Pod "pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.853929ms
Jun  2 21:26:12.745: INFO: Pod "pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030005877s
Jun  2 21:26:14.757: INFO: Pod "pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042303923s
STEP: Saw pod success
Jun  2 21:26:14.757: INFO: Pod "pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e" satisfied condition "Succeeded or Failed"
Jun  2 21:26:14.766: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  2 21:26:14.818: INFO: Waiting for pod pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e to disappear
Jun  2 21:26:14.830: INFO: Pod pod-projected-secrets-5ce8762e-ee0e-407f-bd51-bfebe5107e4e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:14.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4481" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":1140,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:14.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3346
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:15.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-3834
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3346
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:15.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-3834" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:15.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3346" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":63,"skipped":1150,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:15.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9901
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:26:15.916: INFO: Waiting up to 5m0s for pod "busybox-user-65534-563ea5cb-de32-45bd-bdd5-9fc4bdf46828" in namespace "security-context-test-9901" to be "Succeeded or Failed"
Jun  2 21:26:15.924: INFO: Pod "busybox-user-65534-563ea5cb-de32-45bd-bdd5-9fc4bdf46828": Phase="Pending", Reason="", readiness=false. Elapsed: 7.707445ms
Jun  2 21:26:17.949: INFO: Pod "busybox-user-65534-563ea5cb-de32-45bd-bdd5-9fc4bdf46828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032680334s
Jun  2 21:26:19.968: INFO: Pod "busybox-user-65534-563ea5cb-de32-45bd-bdd5-9fc4bdf46828": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051332056s
Jun  2 21:26:21.991: INFO: Pod "busybox-user-65534-563ea5cb-de32-45bd-bdd5-9fc4bdf46828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074665652s
Jun  2 21:26:21.991: INFO: Pod "busybox-user-65534-563ea5cb-de32-45bd-bdd5-9fc4bdf46828" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:21.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9901" for this suite.

• [SLOW TEST:6.394 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1152,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:22.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3429
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:26:26.491: INFO: DNS probes using dns-3429/dns-test-23556d45-8409-424c-b49d-d7447474ffa1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:26.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3429" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":65,"skipped":1152,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:26.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6395
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
Jun  2 21:26:26.862: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6395 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:26.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6395" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":66,"skipped":1166,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:26.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9850
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9850.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9850.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9850.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9850.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:26:31.439: INFO: DNS probes using dns-9850/dns-test-586ce6a5-0fbd-4277-ab8d-bb31c1763ee9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:31.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9850" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":67,"skipped":1185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:31.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8490
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:26:32.238: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun  2 21:26:34.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 26, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 26, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 26, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 26, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:26:37.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:37.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8490" for this suite.
STEP: Destroying namespace "webhook-8490-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.207 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":68,"skipped":1215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:37.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9597
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-34aae074-c453-47b9-9b58-6596895d2f6b
STEP: Creating a pod to test consume configMaps
Jun  2 21:26:38.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8" in namespace "configmap-9597" to be "Succeeded or Failed"
Jun  2 21:26:38.064: INFO: Pod "pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.765431ms
Jun  2 21:26:40.087: INFO: Pod "pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037565706s
Jun  2 21:26:42.112: INFO: Pod "pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0623708s
Jun  2 21:26:44.138: INFO: Pod "pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088139743s
STEP: Saw pod success
Jun  2 21:26:44.138: INFO: Pod "pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8" satisfied condition "Succeeded or Failed"
Jun  2 21:26:44.150: INFO: Trying to get logs from node 10.134.156.253 pod pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 21:26:44.306: INFO: Waiting for pod pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8 to disappear
Jun  2 21:26:44.315: INFO: Pod pod-configmaps-ed0c908d-cf11-4d0f-9d5a-d3115e001af8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:26:44.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9597" for this suite.

• [SLOW TEST:6.566 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1240,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:26:44.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5999
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5999
STEP: creating service affinity-clusterip-transition in namespace services-5999
STEP: creating replication controller affinity-clusterip-transition in namespace services-5999
I0602 21:26:44.648959      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5999, replica count: 3
I0602 21:26:47.708186      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 21:26:47.740: INFO: Creating new exec pod
Jun  2 21:26:52.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5999 exec execpod-affinitydf2ph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun  2 21:26:53.352: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun  2 21:26:53.352: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:26:53.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5999 exec execpod-affinitydf2ph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.52.116 80'
Jun  2 21:26:53.769: INFO: stderr: "+ + ncecho hostName -v\n -t -w 2 172.21.52.116 80\nConnection to 172.21.52.116 80 port [tcp/http] succeeded!\n"
Jun  2 21:26:53.769: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:26:53.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5999 exec execpod-affinitydf2ph -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.52.116:80/ ; done'
Jun  2 21:26:54.306: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n"
Jun  2 21:26:54.306: INFO: stdout: "\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27"
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:26:54.306: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:27:24.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5999 exec execpod-affinitydf2ph -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.52.116:80/ ; done'
Jun  2 21:27:24.803: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n"
Jun  2 21:27:24.803: INFO: stdout: "\naffinity-clusterip-transition-2c7hc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-2c7hc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-2c7hc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-2c7hc\naffinity-clusterip-transition-nrv27\naffinity-clusterip-transition-w7dsc"
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-2c7hc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-2c7hc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-2c7hc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-2c7hc
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-nrv27
Jun  2 21:27:24.803: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:24.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5999 exec execpod-affinitydf2ph -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.52.116:80/ ; done'
Jun  2 21:27:25.287: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.52.116:80/\n"
Jun  2 21:27:25.287: INFO: stdout: "\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc\naffinity-clusterip-transition-w7dsc"
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Received response from host: affinity-clusterip-transition-w7dsc
Jun  2 21:27:25.287: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5999, will wait for the garbage collector to delete the pods
Jun  2 21:27:25.419: INFO: Deleting ReplicationController affinity-clusterip-transition took: 46.627635ms
Jun  2 21:27:25.620: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.846919ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:27:28.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5999" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:44.281 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":70,"skipped":1245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:27:28.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7561
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:27:29.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7561" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":71,"skipped":1278,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:27:29.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9923
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:27:29.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42" in namespace "projected-9923" to be "Succeeded or Failed"
Jun  2 21:27:29.357: INFO: Pod "downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42": Phase="Pending", Reason="", readiness=false. Elapsed: 36.751551ms
Jun  2 21:27:31.381: INFO: Pod "downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059950197s
Jun  2 21:27:33.405: INFO: Pod "downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084007984s
Jun  2 21:27:35.445: INFO: Pod "downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.124500111s
STEP: Saw pod success
Jun  2 21:27:35.446: INFO: Pod "downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42" satisfied condition "Succeeded or Failed"
Jun  2 21:27:35.457: INFO: Trying to get logs from node 10.134.156.253 pod downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42 container client-container: <nil>
STEP: delete the pod
Jun  2 21:27:35.517: INFO: Waiting for pod downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42 to disappear
Jun  2 21:27:35.525: INFO: Pod downwardapi-volume-d30c922f-6c9a-4f19-bffd-35dccb740c42 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:27:35.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9923" for this suite.

• [SLOW TEST:6.502 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:27:35.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7738
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7738
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-7738
Jun  2 21:27:35.824: INFO: Found 0 stateful pods, waiting for 1
Jun  2 21:27:45.855: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 21:27:45.978: INFO: Deleting all statefulset in ns statefulset-7738
Jun  2 21:27:45.989: INFO: Scaling statefulset ss to 0
Jun  2 21:27:56.134: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 21:27:56.145: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:27:56.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7738" for this suite.

• [SLOW TEST:20.704 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":73,"skipped":1314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:27:56.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5647
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:27:56.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:27:59.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5647" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":74,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:27:59.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1952
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jun  2 21:28:00.280: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:28:02.307: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:28:04.291: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun  2 21:28:05.358: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:28:06.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1952" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":75,"skipped":1379,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:28:06.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4183
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4183.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4183.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4183.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4183.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 85.76.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.76.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.76.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.76.85_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4183.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4183.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4183.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4183.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4183.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 85.76.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.76.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.76.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.76.85_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:28:17.104: INFO: Unable to read wheezy_udp@dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.119: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.160: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.173: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.247: INFO: Unable to read jessie_udp@dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.278: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.293: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:17.409: INFO: Lookups using dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e failed for: [wheezy_udp@dns-test-service.dns-4183.svc.cluster.local wheezy_tcp@dns-test-service.dns-4183.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_udp@dns-test-service.dns-4183.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local]

Jun  2 21:28:22.458: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:22.474: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:22.559: INFO: Unable to read jessie_udp@dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:22.588: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:22.605: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:22.663: INFO: Lookups using dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_udp@dns-test-service.dns-4183.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local]

Jun  2 21:28:27.459: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:27.472: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:27.603: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:27.618: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:27.676: INFO: Lookups using dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local]

Jun  2 21:28:32.455: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:32.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:32.566: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:32.584: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:32.652: INFO: Lookups using dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local]

Jun  2 21:28:37.461: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:37.474: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:37.584: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:37.602: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local from pod dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e: the server could not find the requested resource (get pods dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e)
Jun  2 21:28:37.654: INFO: Lookups using dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4183.svc.cluster.local]

Jun  2 21:28:42.721: INFO: DNS probes using dns-4183/dns-test-7054d198-dd8a-4611-bd02-99b09a7db33e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:28:42.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4183" for this suite.

• [SLOW TEST:36.294 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":76,"skipped":1430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:28:42.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-325
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  2 21:28:43.118: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  2 21:28:43.182: INFO: Waiting for terminating namespaces to be deleted...
Jun  2 21:28:43.193: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.209 before test
Jun  2 21:28:43.234: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-qxhvw from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.234: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 21:28:43.234: INFO: calico-node-xc26s from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:28:43.235: INFO: calico-typha-7684bb556f-zfmb4 from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:28:43.235: INFO: coredns-74bf9bd988-v6qcf from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:28:43.235: INFO: ibm-keepalived-watcher-5wwlj from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:28:43.235: INFO: ibm-master-proxy-static-10.134.156.209 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:28:43.235: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:28:43.235: INFO: konnectivity-agent-wmghg from kube-system started at 2022-06-02 19:25:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:28:43.235: INFO: metrics-server-6bc784d6c-ztp8q from kube-system started at 2022-06-02 20:01:37 +0000 UTC (3 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container config-watcher ready: true, restart count 0
Jun  2 21:28:43.235: INFO: 	Container metrics-server ready: true, restart count 0
Jun  2 21:28:43.235: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun  2 21:28:43.235: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-j82hw from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 21:28:43.235: INFO: sonobuoy-e2e-job-d730fbd8ba3a4c8f from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container e2e ready: true, restart count 0
Jun  2 21:28:43.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:28:43.235: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-kd2m7 from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:28:43.235: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 21:28:43.235: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.247 before test
Jun  2 21:28:43.272: INFO: calico-kube-controllers-648794b58-n4vlx from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  2 21:28:43.272: INFO: calico-node-l6vdp from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:28:43.272: INFO: calico-typha-7684bb556f-k9h4f from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:28:43.272: INFO: coredns-74bf9bd988-l2b48 from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:28:43.272: INFO: coredns-autoscaler-867cd8fddb-rxjf6 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container autoscaler ready: true, restart count 0
Jun  2 21:28:43.272: INFO: dashboard-metrics-scraper-7f68fbcb5b-j8jwr from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun  2 21:28:43.272: INFO: ibm-file-plugin-7795fdcfdb-cd4r2 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun  2 21:28:43.272: INFO: ibm-keepalived-watcher-t9x4p from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:28:43.272: INFO: ibm-master-proxy-static-10.134.156.247 from kube-system started at 2022-06-02 19:15:43 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:28:43.272: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:28:43.272: INFO: ibm-storage-watcher-75494444c9-jf6zn from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun  2 21:28:43.272: INFO: konnectivity-agent-tpmz2 from kube-system started at 2022-06-02 19:25:23 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:28:43.272: INFO: kubernetes-dashboard-7bbdd6b4cd-z6cv7 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  2 21:28:43.272: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-jfbjf from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.272: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:28:43.272: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 21:28:43.272: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.253 before test
Jun  2 21:28:43.311: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-02 19:19:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.311: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun  2 21:28:43.311: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-8hb9r from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.311: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 21:28:43.311: INFO: calico-node-xwlqr from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.311: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:28:43.311: INFO: calico-typha-7684bb556f-bpb2x from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.311: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:28:43.311: INFO: coredns-74bf9bd988-5wlnt from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.311: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:28:43.311: INFO: ibm-keepalived-watcher-mcrwm from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.311: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:28:43.311: INFO: ibm-master-proxy-static-10.134.156.253 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.312: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:28:43.312: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:28:43.312: INFO: konnectivity-agent-swbk5 from kube-system started at 2022-06-02 19:25:19 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.312: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:28:43.312: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-t7zkd from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.312: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 21:28:43.312: INFO: sonobuoy from sonobuoy started at 2022-06-02 21:12:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:28:43.312: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  2 21:28:43.312: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-x9xlv from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:28:43.312: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:28:43.312: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-10ac8ef7-9946-4bb8-8629-ceb75057fa0e 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.134.156.253 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-10ac8ef7-9946-4bb8-8629-ceb75057fa0e off the node 10.134.156.253
STEP: verifying the node doesn't have the label kubernetes.io/e2e-10ac8ef7-9946-4bb8-8629-ceb75057fa0e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:33:51.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-325" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.807 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":77,"skipped":1462,"failed":0}
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:33:51.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-7521
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  2 21:33:52.056: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  2 21:33:52.077: INFO: starting watch
STEP: patching
STEP: updating
Jun  2 21:33:52.137: INFO: waiting for watch events with expected annotations
Jun  2 21:33:52.137: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:33:52.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7521" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":78,"skipped":1462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:33:52.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8546
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-55df72ce-ba30-4fc9-8f4f-689dcf97f05e
STEP: Creating a pod to test consume configMaps
Jun  2 21:33:52.715: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828" in namespace "projected-8546" to be "Succeeded or Failed"
Jun  2 21:33:52.725: INFO: Pod "pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828": Phase="Pending", Reason="", readiness=false. Elapsed: 10.288975ms
Jun  2 21:33:54.740: INFO: Pod "pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025036245s
Jun  2 21:33:56.757: INFO: Pod "pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042093125s
Jun  2 21:33:58.794: INFO: Pod "pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07902261s
STEP: Saw pod success
Jun  2 21:33:58.794: INFO: Pod "pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828" satisfied condition "Succeeded or Failed"
Jun  2 21:33:58.806: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 21:33:58.942: INFO: Waiting for pod pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828 to disappear
Jun  2 21:33:58.951: INFO: Pod pod-projected-configmaps-604f47e1-8a36-4797-962c-adaeebec0828 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:33:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8546" for this suite.

• [SLOW TEST:6.578 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:33:58.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8080
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
Jun  2 21:33:59.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8080 create -f -'
Jun  2 21:33:59.461: INFO: stderr: ""
Jun  2 21:33:59.461: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun  2 21:33:59.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8080 diff -f -'
Jun  2 21:33:59.745: INFO: rc: 1
Jun  2 21:33:59.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8080 delete -f -'
Jun  2 21:33:59.865: INFO: stderr: ""
Jun  2 21:33:59.865: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:33:59.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8080" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":80,"skipped":1548,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:33:59.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8487
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun  2 21:34:04.348: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8487 PodName:pod-sharedvolume-c76f8a74-5140-43f5-bc5e-5547be41c248 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:34:04.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:34:04.349: INFO: ExecWithOptions: Clientset creation
Jun  2 21:34:04.349: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-8487/pods/pod-sharedvolume-c76f8a74-5140-43f5-bc5e-5547be41c248/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:34:04.665: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:04.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8487" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":81,"skipped":1550,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:04.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8626
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:34:05.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:34:07.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:34:10.669: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:34:10.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7587-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:14.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8626" for this suite.
STEP: Destroying namespace "webhook-8626-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.589 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":82,"skipped":1556,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:14.301: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4557
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:34:14.532: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:34:16.558: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:34:18.549: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:20.547: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:22.557: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:24.547: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:26.566: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:28.562: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:30.548: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:32.558: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:34.546: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = false)
Jun  2 21:34:36.556: INFO: The status of Pod test-webserver-7d52a3e5-673d-42c5-9741-5984352836cd is Running (Ready = true)
Jun  2 21:34:36.567: INFO: Container started at 2022-06-02 21:34:15 +0000 UTC, pod became ready at 2022-06-02 21:34:34 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:36.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4557" for this suite.

• [SLOW TEST:22.325 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1562,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:36.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4013
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jun  2 21:34:36.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
Jun  2 21:34:37.291: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun  2 21:34:39.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 21:34:41.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 21:34:43.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 21:34:45.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 21:34:47.744: INFO: Waited 312.456559ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jun  2 21:34:48.231: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:48.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4013" for this suite.

• [SLOW TEST:12.241 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":84,"skipped":1568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:48.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-861
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  2 21:34:49.200: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  2 21:34:49.218: INFO: starting watch
STEP: patching
STEP: updating
Jun  2 21:34:49.267: INFO: waiting for watch events with expected annotations
Jun  2 21:34:49.267: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:49.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-861" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":85,"skipped":1604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:49.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-589
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
Jun  2 21:34:49.602: INFO: created test-event-1
Jun  2 21:34:49.614: INFO: created test-event-2
Jun  2 21:34:49.628: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun  2 21:34:49.640: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun  2 21:34:49.697: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:49.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-589" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":86,"skipped":1628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:49.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1653
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
Jun  2 21:34:49.969: INFO: The status of Pod pod-hostip-e980827b-dd49-42d8-a171-d7357645c3e5 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:34:51.987: INFO: The status of Pod pod-hostip-e980827b-dd49-42d8-a171-d7357645c3e5 is Running (Ready = true)
Jun  2 21:34:52.008: INFO: Pod pod-hostip-e980827b-dd49-42d8-a171-d7357645c3e5 has hostIP: 10.134.156.247
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:52.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1653" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1660,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9219
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:34:52.303: INFO: The status of Pod busybox-scheduling-5425240a-7665-47ca-b58f-7a87fd2bc70f is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:34:54.315: INFO: The status of Pod busybox-scheduling-5425240a-7665-47ca-b58f-7a87fd2bc70f is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:34:56.341: INFO: The status of Pod busybox-scheduling-5425240a-7665-47ca-b58f-7a87fd2bc70f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:34:56.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9219" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:34:56.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3729
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:34:57.043: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun  2 21:34:59.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 34, 57, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 34, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:35:02.143: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun  2 21:35:02.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:35:02.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3729" for this suite.
STEP: Destroying namespace "webhook-3729-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.094 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":89,"skipped":1697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:35:02.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4590
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jun  2 21:35:03.050: INFO: running pods: 0 < 3
Jun  2 21:35:05.063: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:35:07.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4590" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":90,"skipped":1729,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:35:07.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1079
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jun  2 21:35:07.345: INFO: Waiting up to 5m0s for pod "downward-api-9058b833-7d24-417c-9196-2f5099015001" in namespace "downward-api-1079" to be "Succeeded or Failed"
Jun  2 21:35:07.358: INFO: Pod "downward-api-9058b833-7d24-417c-9196-2f5099015001": Phase="Pending", Reason="", readiness=false. Elapsed: 12.388631ms
Jun  2 21:35:09.371: INFO: Pod "downward-api-9058b833-7d24-417c-9196-2f5099015001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025649836s
Jun  2 21:35:11.389: INFO: Pod "downward-api-9058b833-7d24-417c-9196-2f5099015001": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043775944s
Jun  2 21:35:13.405: INFO: Pod "downward-api-9058b833-7d24-417c-9196-2f5099015001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059949072s
STEP: Saw pod success
Jun  2 21:35:13.405: INFO: Pod "downward-api-9058b833-7d24-417c-9196-2f5099015001" satisfied condition "Succeeded or Failed"
Jun  2 21:35:13.416: INFO: Trying to get logs from node 10.134.156.209 pod downward-api-9058b833-7d24-417c-9196-2f5099015001 container dapi-container: <nil>
STEP: delete the pod
Jun  2 21:35:13.544: INFO: Waiting for pod downward-api-9058b833-7d24-417c-9196-2f5099015001 to disappear
Jun  2 21:35:13.553: INFO: Pod downward-api-9058b833-7d24-417c-9196-2f5099015001 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:35:13.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1079" for this suite.

• [SLOW TEST:6.453 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:35:13.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1029
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-1029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1029 to expose endpoints map[]
Jun  2 21:35:13.851: INFO: successfully validated that service endpoint-test2 in namespace services-1029 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1029
Jun  2 21:35:13.890: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:35:15.915: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1029 to expose endpoints map[pod1:[80]]
Jun  2 21:35:15.970: INFO: successfully validated that service endpoint-test2 in namespace services-1029 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jun  2 21:35:15.971: INFO: Creating new exec pod
Jun  2 21:35:19.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1029 exec execpodgf6jg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun  2 21:35:19.328: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  2 21:35:19.328: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:35:19.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1029 exec execpodgf6jg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.223 80'
Jun  2 21:35:19.672: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.223 80\nConnection to 172.21.109.223 80 port [tcp/http] succeeded!\n"
Jun  2 21:35:19.672: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1029
Jun  2 21:35:19.704: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:35:21.746: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:35:23.728: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1029 to expose endpoints map[pod1:[80] pod2:[80]]
Jun  2 21:35:23.789: INFO: successfully validated that service endpoint-test2 in namespace services-1029 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jun  2 21:35:24.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1029 exec execpodgf6jg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun  2 21:35:25.095: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  2 21:35:25.095: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:35:25.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1029 exec execpodgf6jg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.223 80'
Jun  2 21:35:25.343: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.223 80\nConnection to 172.21.109.223 80 port [tcp/http] succeeded!\n"
Jun  2 21:35:25.344: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1029 to expose endpoints map[pod2:[80]]
Jun  2 21:35:26.452: INFO: successfully validated that service endpoint-test2 in namespace services-1029 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jun  2 21:35:27.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1029 exec execpodgf6jg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun  2 21:35:27.747: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun  2 21:35:27.747: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:35:27.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1029 exec execpodgf6jg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.223 80'
Jun  2 21:35:28.060: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.223 80\nConnection to 172.21.109.223 80 port [tcp/http] succeeded!\n"
Jun  2 21:35:28.060: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1029
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1029 to expose endpoints map[]
Jun  2 21:35:29.159: INFO: successfully validated that service endpoint-test2 in namespace services-1029 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:35:29.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1029" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.642 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":92,"skipped":1761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:35:29.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9007
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:35:30.000: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:35:33.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
Jun  2 21:35:38.249: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:35:50.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9007" for this suite.
STEP: Destroying namespace "webhook-9007-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:21.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":93,"skipped":1801,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:35:51.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6822
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-629
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5311
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:35:57.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6822" for this suite.
STEP: Destroying namespace "nsdeletetest-629" for this suite.
Jun  2 21:35:58.036: INFO: Namespace nsdeletetest-629 was already deleted
STEP: Destroying namespace "nsdeletetest-5311" for this suite.

• [SLOW TEST:6.963 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":94,"skipped":1808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:35:58.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7730
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:35:58.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572" in namespace "projected-7730" to be "Succeeded or Failed"
Jun  2 21:35:58.355: INFO: Pod "downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572": Phase="Pending", Reason="", readiness=false. Elapsed: 8.942533ms
Jun  2 21:36:00.371: INFO: Pod "downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024743905s
Jun  2 21:36:02.387: INFO: Pod "downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040345215s
STEP: Saw pod success
Jun  2 21:36:02.387: INFO: Pod "downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572" satisfied condition "Succeeded or Failed"
Jun  2 21:36:02.398: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572 container client-container: <nil>
STEP: delete the pod
Jun  2 21:36:02.528: INFO: Waiting for pod downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572 to disappear
Jun  2 21:36:02.544: INFO: Pod downwardapi-volume-cb278bed-9759-4e13-b9b8-2efa08aad572 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:36:02.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7730" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:36:02.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-461
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:36:02.844: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b" in namespace "security-context-test-461" to be "Succeeded or Failed"
Jun  2 21:36:02.856: INFO: Pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.40523ms
Jun  2 21:36:04.869: INFO: Pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02471614s
Jun  2 21:36:06.894: INFO: Pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049811013s
Jun  2 21:36:08.919: INFO: Pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074440198s
Jun  2 21:36:08.919: INFO: Pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b" satisfied condition "Succeeded or Failed"
Jun  2 21:36:08.944: INFO: Got logs for pod "busybox-privileged-false-25730099-f95c-4c93-9fc3-09572ac8c03b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:36:08.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-461" for this suite.

• [SLOW TEST:6.412 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:232
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":96,"skipped":1867,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:36:09.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7214
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7214
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7214
STEP: creating replication controller externalsvc in namespace services-7214
I0602 21:36:09.320553      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7214, replica count: 2
I0602 21:36:12.372248      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun  2 21:36:12.431: INFO: Creating new exec pod
Jun  2 21:36:16.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7214 exec execpodh9qw7 -- /bin/sh -x -c nslookup clusterip-service.services-7214.svc.cluster.local'
Jun  2 21:36:16.886: INFO: stderr: "+ nslookup clusterip-service.services-7214.svc.cluster.local\n"
Jun  2 21:36:16.886: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7214.svc.cluster.local\tcanonical name = externalsvc.services-7214.svc.cluster.local.\nName:\texternalsvc.services-7214.svc.cluster.local\nAddress: 172.21.231.56\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7214, will wait for the garbage collector to delete the pods
Jun  2 21:36:16.977: INFO: Deleting ReplicationController externalsvc took: 29.275027ms
Jun  2 21:36:17.177: INFO: Terminating ReplicationController externalsvc pods took: 200.280514ms
Jun  2 21:36:19.581: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:36:19.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7214" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.758 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":97,"skipped":1886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:36:19.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-560
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  2 21:36:20.038: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  2 21:37:20.184: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:20.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1710
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:37:20.465: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun  2 21:37:20.477: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:20.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1710" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:20.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-560" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.071 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":98,"skipped":1945,"failed":0}
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:20.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2672
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:37:21.118: INFO: Got root ca configmap in namespace "svcaccounts-2672"
Jun  2 21:37:21.150: INFO: Deleted root ca configmap in namespace "svcaccounts-2672"
STEP: waiting for a new root ca configmap created
Jun  2 21:37:21.668: INFO: Recreated root ca configmap in namespace "svcaccounts-2672"
Jun  2 21:37:21.684: INFO: Updated root ca configmap in namespace "svcaccounts-2672"
STEP: waiting for the root ca configmap reconciled
Jun  2 21:37:22.203: INFO: Reconciled root ca configmap in namespace "svcaccounts-2672"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:22.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2672" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":99,"skipped":1945,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6248
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
Jun  2 21:37:22.506: INFO: Waiting up to 5m0s for pod "var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613" in namespace "var-expansion-6248" to be "Succeeded or Failed"
Jun  2 21:37:22.518: INFO: Pod "var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613": Phase="Pending", Reason="", readiness=false. Elapsed: 11.689489ms
Jun  2 21:37:24.530: INFO: Pod "var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02385202s
Jun  2 21:37:26.554: INFO: Pod "var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047414858s
STEP: Saw pod success
Jun  2 21:37:26.554: INFO: Pod "var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613" satisfied condition "Succeeded or Failed"
Jun  2 21:37:26.564: INFO: Trying to get logs from node 10.134.156.247 pod var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613 container dapi-container: <nil>
STEP: delete the pod
Jun  2 21:37:26.613: INFO: Waiting for pod var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613 to disappear
Jun  2 21:37:26.626: INFO: Pod var-expansion-47a0f74e-41d7-4322-9b92-30726e0b3613 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:26.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6248" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":1951,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:26.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7952
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
Jun  2 21:37:26.913: INFO: Waiting up to 5m0s for pod "var-expansion-8ec7164a-4b84-4385-827c-880e642c2366" in namespace "var-expansion-7952" to be "Succeeded or Failed"
Jun  2 21:37:26.924: INFO: Pod "var-expansion-8ec7164a-4b84-4385-827c-880e642c2366": Phase="Pending", Reason="", readiness=false. Elapsed: 10.61046ms
Jun  2 21:37:28.941: INFO: Pod "var-expansion-8ec7164a-4b84-4385-827c-880e642c2366": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02743402s
Jun  2 21:37:30.958: INFO: Pod "var-expansion-8ec7164a-4b84-4385-827c-880e642c2366": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044791587s
Jun  2 21:37:33.014: INFO: Pod "var-expansion-8ec7164a-4b84-4385-827c-880e642c2366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100559497s
STEP: Saw pod success
Jun  2 21:37:33.014: INFO: Pod "var-expansion-8ec7164a-4b84-4385-827c-880e642c2366" satisfied condition "Succeeded or Failed"
Jun  2 21:37:33.024: INFO: Trying to get logs from node 10.134.156.253 pod var-expansion-8ec7164a-4b84-4385-827c-880e642c2366 container dapi-container: <nil>
STEP: delete the pod
Jun  2 21:37:33.072: INFO: Waiting for pod var-expansion-8ec7164a-4b84-4385-827c-880e642c2366 to disappear
Jun  2 21:37:33.104: INFO: Pod var-expansion-8ec7164a-4b84-4385-827c-880e642c2366 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7952" for this suite.

• [SLOW TEST:6.477 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":101,"skipped":1951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-37
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-37
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-37
STEP: creating replication controller externalsvc in namespace services-37
I0602 21:37:33.503701      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-37, replica count: 2
I0602 21:37:36.555000      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0602 21:37:39.555532      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun  2 21:37:39.617: INFO: Creating new exec pod
Jun  2 21:37:41.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-37 exec execpodszjcw -- /bin/sh -x -c nslookup nodeport-service.services-37.svc.cluster.local'
Jun  2 21:37:42.157: INFO: stderr: "+ nslookup nodeport-service.services-37.svc.cluster.local\n"
Jun  2 21:37:42.157: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-37.svc.cluster.local\tcanonical name = externalsvc.services-37.svc.cluster.local.\nName:\texternalsvc.services-37.svc.cluster.local\nAddress: 172.21.87.94\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-37, will wait for the garbage collector to delete the pods
Jun  2 21:37:42.238: INFO: Deleting ReplicationController externalsvc took: 18.677774ms
Jun  2 21:37:42.439: INFO: Terminating ReplicationController externalsvc pods took: 200.263629ms
Jun  2 21:37:44.607: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:44.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-37" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.529 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":102,"skipped":1975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:44.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6886
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:37:44.915: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a" in namespace "downward-api-6886" to be "Succeeded or Failed"
Jun  2 21:37:44.928: INFO: Pod "downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.748848ms
Jun  2 21:37:46.946: INFO: Pod "downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031896089s
Jun  2 21:37:48.965: INFO: Pod "downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050859254s
Jun  2 21:37:50.982: INFO: Pod "downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067705916s
STEP: Saw pod success
Jun  2 21:37:50.982: INFO: Pod "downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a" satisfied condition "Succeeded or Failed"
Jun  2 21:37:50.993: INFO: Trying to get logs from node 10.134.156.253 pod downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a container client-container: <nil>
STEP: delete the pod
Jun  2 21:37:51.104: INFO: Waiting for pod downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a to disappear
Jun  2 21:37:51.115: INFO: Pod downwardapi-volume-ebc58731-b9aa-4c2f-ac2b-79e95966078a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:51.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6886" for this suite.

• [SLOW TEST:6.474 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":103,"skipped":2000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:51.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-9087
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun  2 21:37:51.380: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun  2 21:37:51.399: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  2 21:37:51.399: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun  2 21:37:51.426: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  2 21:37:51.426: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun  2 21:37:51.488: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun  2 21:37:51.488: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun  2 21:37:58.602: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:37:58.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9087" for this suite.

• [SLOW TEST:7.496 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":104,"skipped":2041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:37:58.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9066
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-6b1467ad-705f-4350-8c06-81a86e98da38
STEP: Creating a pod to test consume secrets
Jun  2 21:37:58.896: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0" in namespace "projected-9066" to be "Succeeded or Failed"
Jun  2 21:37:58.908: INFO: Pod "pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.246354ms
Jun  2 21:38:00.924: INFO: Pod "pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027710142s
Jun  2 21:38:02.949: INFO: Pod "pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053112116s
STEP: Saw pod success
Jun  2 21:38:02.950: INFO: Pod "pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0" satisfied condition "Succeeded or Failed"
Jun  2 21:38:02.962: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  2 21:38:03.065: INFO: Waiting for pod pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0 to disappear
Jun  2 21:38:03.089: INFO: Pod pod-projected-secrets-56e4e552-47d8-45a3-ae0f-f97048bd4ac0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:03.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9066" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":2063,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9876
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:15.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9876" for this suite.

• [SLOW TEST:12.282 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":106,"skipped":2070,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:15.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-3008
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:38:16.473: INFO: Checking APIGroup: apiregistration.k8s.io
Jun  2 21:38:16.477: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun  2 21:38:16.477: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun  2 21:38:16.477: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun  2 21:38:16.477: INFO: Checking APIGroup: apps
Jun  2 21:38:16.481: INFO: PreferredVersion.GroupVersion: apps/v1
Jun  2 21:38:16.481: INFO: Versions found [{apps/v1 v1}]
Jun  2 21:38:16.481: INFO: apps/v1 matches apps/v1
Jun  2 21:38:16.481: INFO: Checking APIGroup: events.k8s.io
Jun  2 21:38:16.497: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun  2 21:38:16.497: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.497: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun  2 21:38:16.497: INFO: Checking APIGroup: authentication.k8s.io
Jun  2 21:38:16.518: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun  2 21:38:16.518: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun  2 21:38:16.518: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun  2 21:38:16.518: INFO: Checking APIGroup: authorization.k8s.io
Jun  2 21:38:16.537: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun  2 21:38:16.537: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun  2 21:38:16.538: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun  2 21:38:16.538: INFO: Checking APIGroup: autoscaling
Jun  2 21:38:16.542: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun  2 21:38:16.542: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun  2 21:38:16.542: INFO: autoscaling/v2 matches autoscaling/v2
Jun  2 21:38:16.542: INFO: Checking APIGroup: batch
Jun  2 21:38:16.577: INFO: PreferredVersion.GroupVersion: batch/v1
Jun  2 21:38:16.578: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun  2 21:38:16.578: INFO: batch/v1 matches batch/v1
Jun  2 21:38:16.578: INFO: Checking APIGroup: certificates.k8s.io
Jun  2 21:38:16.583: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun  2 21:38:16.583: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun  2 21:38:16.583: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun  2 21:38:16.583: INFO: Checking APIGroup: networking.k8s.io
Jun  2 21:38:16.600: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun  2 21:38:16.600: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun  2 21:38:16.600: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun  2 21:38:16.600: INFO: Checking APIGroup: policy
Jun  2 21:38:16.604: INFO: PreferredVersion.GroupVersion: policy/v1
Jun  2 21:38:16.604: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jun  2 21:38:16.604: INFO: policy/v1 matches policy/v1
Jun  2 21:38:16.604: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun  2 21:38:16.608: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun  2 21:38:16.609: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun  2 21:38:16.609: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun  2 21:38:16.609: INFO: Checking APIGroup: storage.k8s.io
Jun  2 21:38:16.613: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun  2 21:38:16.613: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.613: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun  2 21:38:16.613: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun  2 21:38:16.619: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun  2 21:38:16.619: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun  2 21:38:16.619: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun  2 21:38:16.619: INFO: Checking APIGroup: apiextensions.k8s.io
Jun  2 21:38:16.651: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun  2 21:38:16.651: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun  2 21:38:16.651: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun  2 21:38:16.651: INFO: Checking APIGroup: scheduling.k8s.io
Jun  2 21:38:16.682: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun  2 21:38:16.682: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun  2 21:38:16.682: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun  2 21:38:16.682: INFO: Checking APIGroup: coordination.k8s.io
Jun  2 21:38:16.688: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun  2 21:38:16.688: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun  2 21:38:16.688: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun  2 21:38:16.688: INFO: Checking APIGroup: node.k8s.io
Jun  2 21:38:16.692: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun  2 21:38:16.692: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.692: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun  2 21:38:16.692: INFO: Checking APIGroup: discovery.k8s.io
Jun  2 21:38:16.696: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun  2 21:38:16.696: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.696: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun  2 21:38:16.696: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun  2 21:38:16.700: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jun  2 21:38:16.700: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.700: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jun  2 21:38:16.700: INFO: Checking APIGroup: crd.projectcalico.org
Jun  2 21:38:16.704: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun  2 21:38:16.704: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun  2 21:38:16.704: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun  2 21:38:16.705: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun  2 21:38:16.709: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jun  2 21:38:16.710: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.710: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jun  2 21:38:16.710: INFO: Checking APIGroup: ibm.com
Jun  2 21:38:16.715: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jun  2 21:38:16.715: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jun  2 21:38:16.716: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jun  2 21:38:16.716: INFO: Checking APIGroup: metrics.k8s.io
Jun  2 21:38:16.721: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun  2 21:38:16.721: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun  2 21:38:16.722: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:16.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3008" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":107,"skipped":2072,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:16.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7932
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  2 21:38:16.985: INFO: Waiting up to 5m0s for pod "pod-67de6901-994e-4bcf-abee-b51b341b34ce" in namespace "emptydir-7932" to be "Succeeded or Failed"
Jun  2 21:38:16.995: INFO: Pod "pod-67de6901-994e-4bcf-abee-b51b341b34ce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.889128ms
Jun  2 21:38:19.016: INFO: Pod "pod-67de6901-994e-4bcf-abee-b51b341b34ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.030999904s
Jun  2 21:38:21.036: INFO: Pod "pod-67de6901-994e-4bcf-abee-b51b341b34ce": Phase="Running", Reason="", readiness=false. Elapsed: 4.050955528s
Jun  2 21:38:23.062: INFO: Pod "pod-67de6901-994e-4bcf-abee-b51b341b34ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076215328s
STEP: Saw pod success
Jun  2 21:38:23.062: INFO: Pod "pod-67de6901-994e-4bcf-abee-b51b341b34ce" satisfied condition "Succeeded or Failed"
Jun  2 21:38:23.102: INFO: Trying to get logs from node 10.134.156.247 pod pod-67de6901-994e-4bcf-abee-b51b341b34ce container test-container: <nil>
STEP: delete the pod
Jun  2 21:38:23.170: INFO: Waiting for pod pod-67de6901-994e-4bcf-abee-b51b341b34ce to disappear
Jun  2 21:38:23.182: INFO: Pod pod-67de6901-994e-4bcf-abee-b51b341b34ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:23.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7932" for this suite.

• [SLOW TEST:6.452 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":108,"skipped":2085,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:23.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9727
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun  2 21:38:23.527: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9727  576eae0a-3a70-4fa2-99b8-f1d431222ce3 30200 0 2022-06-02 21:38:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-02 21:38:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 21:38:23.527: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9727  576eae0a-3a70-4fa2-99b8-f1d431222ce3 30201 0 2022-06-02 21:38:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-02 21:38:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 21:38:23.527: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9727  576eae0a-3a70-4fa2-99b8-f1d431222ce3 30202 0 2022-06-02 21:38:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-02 21:38:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun  2 21:38:33.659: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9727  576eae0a-3a70-4fa2-99b8-f1d431222ce3 30225 0 2022-06-02 21:38:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-02 21:38:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 21:38:33.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9727  576eae0a-3a70-4fa2-99b8-f1d431222ce3 30226 0 2022-06-02 21:38:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-02 21:38:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 21:38:33.661: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9727  576eae0a-3a70-4fa2-99b8-f1d431222ce3 30227 0 2022-06-02 21:38:23 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-06-02 21:38:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:33.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9727" for this suite.

• [SLOW TEST:10.479 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":109,"skipped":2106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:33.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-1851
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1851" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":110,"skipped":2132,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:33.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1457
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1457
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-1457
I0602 21:38:34.243174      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1457, replica count: 2
I0602 21:38:37.294389      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 21:38:37.294: INFO: Creating new exec pod
Jun  2 21:38:42.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1457 exec execpodqxk62 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun  2 21:38:42.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  2 21:38:42.691: INFO: stdout: "externalname-service-nkg62"
Jun  2 21:38:42.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1457 exec execpodqxk62 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.171.80 80'
Jun  2 21:38:43.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.171.80 80\nConnection to 172.21.171.80 80 port [tcp/http] succeeded!\n"
Jun  2 21:38:43.015: INFO: stdout: "externalname-service-m9xsx"
Jun  2 21:38:43.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1457 exec execpodqxk62 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.253 32720'
Jun  2 21:38:43.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.253 32720\nConnection to 10.134.156.253 32720 port [tcp/*] succeeded!\n"
Jun  2 21:38:43.376: INFO: stdout: "externalname-service-m9xsx"
Jun  2 21:38:43.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1457 exec execpodqxk62 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.209 32720'
Jun  2 21:38:43.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.209 32720\nConnection to 10.134.156.209 32720 port [tcp/*] succeeded!\n"
Jun  2 21:38:43.639: INFO: stdout: "externalname-service-m9xsx"
Jun  2 21:38:43.640: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:43.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1457" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:9.750 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":111,"skipped":2143,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:43.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8811
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
Jun  2 21:38:44.097: INFO: Waiting up to 5m0s for pod "pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81" in namespace "emptydir-8811" to be "Succeeded or Failed"
Jun  2 21:38:44.107: INFO: Pod "pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81": Phase="Pending", Reason="", readiness=false. Elapsed: 10.448341ms
Jun  2 21:38:46.125: INFO: Pod "pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81": Phase="Running", Reason="", readiness=true. Elapsed: 2.027836398s
Jun  2 21:38:48.150: INFO: Pod "pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81": Phase="Running", Reason="", readiness=false. Elapsed: 4.053221904s
Jun  2 21:38:50.167: INFO: Pod "pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070302833s
STEP: Saw pod success
Jun  2 21:38:50.167: INFO: Pod "pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81" satisfied condition "Succeeded or Failed"
Jun  2 21:38:50.177: INFO: Trying to get logs from node 10.134.156.253 pod pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81 container test-container: <nil>
STEP: delete the pod
Jun  2 21:38:50.287: INFO: Waiting for pod pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81 to disappear
Jun  2 21:38:50.297: INFO: Pod pod-dece0bd2-4346-49e4-bcd6-c6d755fa2f81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:38:50.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8811" for this suite.

• [SLOW TEST:6.594 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2146,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:38:50.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1769
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:07.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1769" for this suite.

• [SLOW TEST:17.516 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":113,"skipped":2150,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:07.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9277
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun  2 21:39:10.689: INFO: Successfully updated pod "adopt-release-jstdm"
STEP: Checking that the Job readopts the Pod
Jun  2 21:39:10.689: INFO: Waiting up to 15m0s for pod "adopt-release-jstdm" in namespace "job-9277" to be "adopted"
Jun  2 21:39:10.703: INFO: Pod "adopt-release-jstdm": Phase="Running", Reason="", readiness=true. Elapsed: 13.849696ms
Jun  2 21:39:12.718: INFO: Pod "adopt-release-jstdm": Phase="Running", Reason="", readiness=true. Elapsed: 2.029437998s
Jun  2 21:39:12.718: INFO: Pod "adopt-release-jstdm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun  2 21:39:13.279: INFO: Successfully updated pod "adopt-release-jstdm"
STEP: Checking that the Job releases the Pod
Jun  2 21:39:13.280: INFO: Waiting up to 15m0s for pod "adopt-release-jstdm" in namespace "job-9277" to be "released"
Jun  2 21:39:13.290: INFO: Pod "adopt-release-jstdm": Phase="Running", Reason="", readiness=true. Elapsed: 10.599645ms
Jun  2 21:39:15.333: INFO: Pod "adopt-release-jstdm": Phase="Running", Reason="", readiness=true. Elapsed: 2.053343039s
Jun  2 21:39:15.333: INFO: Pod "adopt-release-jstdm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9277" for this suite.

• [SLOW TEST:7.550 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":114,"skipped":2153,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:15.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9324
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:39:15.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:16.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9324" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":115,"skipped":2153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:17.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9584
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5285
STEP: Creating secret with name secret-test-a303ee33-cfa6-4bce-8af5-9452a90405a7
STEP: Creating a pod to test consume secrets
Jun  2 21:39:17.553: INFO: Waiting up to 5m0s for pod "pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c" in namespace "secrets-9584" to be "Succeeded or Failed"
Jun  2 21:39:17.562: INFO: Pod "pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.169198ms
Jun  2 21:39:19.577: INFO: Pod "pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023229704s
Jun  2 21:39:21.608: INFO: Pod "pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05423248s
Jun  2 21:39:23.634: INFO: Pod "pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079941845s
STEP: Saw pod success
Jun  2 21:39:23.634: INFO: Pod "pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c" satisfied condition "Succeeded or Failed"
Jun  2 21:39:23.647: INFO: Trying to get logs from node 10.134.156.253 pod pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 21:39:23.710: INFO: Waiting for pod pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c to disappear
Jun  2 21:39:23.719: INFO: Pod pod-secrets-38777c58-20c4-4c66-a7bf-9f6d996e7e7c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:23.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9584" for this suite.
STEP: Destroying namespace "secret-namespace-5285" for this suite.

• [SLOW TEST:6.792 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2225,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3207
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:24.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3207" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":117,"skipped":2229,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:24.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-113
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:39:25.001: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun  2 21:39:27.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 39, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 39, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 39, 25, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 39, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:39:30.120: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:30.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-113" for this suite.
STEP: Destroying namespace "webhook-113-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.517 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":118,"skipped":2232,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:30.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-9030
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun  2 21:39:31.119: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun  2 21:39:31.210: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:31.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9030" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":119,"skipped":2248,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:31.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4660
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  2 21:39:31.669: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  2 21:39:31.714: INFO: starting watch
STEP: patching
STEP: updating
Jun  2 21:39:31.762: INFO: waiting for watch events with expected annotations
Jun  2 21:39:31.762: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:31.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4660" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":120,"skipped":2263,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:31.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7323
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-1ad55445-1cff-4198-809e-53c52c1c7cb7
STEP: Creating a pod to test consume secrets
Jun  2 21:39:32.205: INFO: Waiting up to 5m0s for pod "pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15" in namespace "secrets-7323" to be "Succeeded or Failed"
Jun  2 21:39:32.215: INFO: Pod "pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15": Phase="Pending", Reason="", readiness=false. Elapsed: 10.615611ms
Jun  2 21:39:34.267: INFO: Pod "pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062608353s
Jun  2 21:39:36.293: INFO: Pod "pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088391144s
STEP: Saw pod success
Jun  2 21:39:36.293: INFO: Pod "pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15" satisfied condition "Succeeded or Failed"
Jun  2 21:39:36.303: INFO: Trying to get logs from node 10.134.156.253 pod pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15 container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 21:39:36.385: INFO: Waiting for pod pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15 to disappear
Jun  2 21:39:36.411: INFO: Pod pod-secrets-6d19cdee-89fe-484d-aa67-18ca4c8d0e15 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:36.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7323" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":121,"skipped":2287,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:36.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-720
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-720
STEP: creating service affinity-nodeport in namespace services-720
STEP: creating replication controller affinity-nodeport in namespace services-720
I0602 21:39:36.786854      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-720, replica count: 3
I0602 21:39:39.837714      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 21:39:39.884: INFO: Creating new exec pod
Jun  2 21:39:44.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-720 exec execpod-affinity96gq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun  2 21:39:45.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun  2 21:39:45.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:39:45.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-720 exec execpod-affinity96gq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.100.52 80'
Jun  2 21:39:45.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.100.52 80\nConnection to 172.21.100.52 80 port [tcp/http] succeeded!\n"
Jun  2 21:39:45.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:39:45.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-720 exec execpod-affinity96gq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.253 30694'
Jun  2 21:39:46.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.253 30694\nConnection to 10.134.156.253 30694 port [tcp/*] succeeded!\n"
Jun  2 21:39:46.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:39:46.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-720 exec execpod-affinity96gq8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.209 30694'
Jun  2 21:39:46.614: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.209 30694\nConnection to 10.134.156.209 30694 port [tcp/*] succeeded!\n"
Jun  2 21:39:46.614: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 21:39:46.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-720 exec execpod-affinity96gq8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.134.156.209:30694/ ; done'
Jun  2 21:39:47.105: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:30694/\n"
Jun  2 21:39:47.105: INFO: stdout: "\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df\naffinity-nodeport-v84df"
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.105: INFO: Received response from host: affinity-nodeport-v84df
Jun  2 21:39:47.107: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-720, will wait for the garbage collector to delete the pods
Jun  2 21:39:47.221: INFO: Deleting ReplicationController affinity-nodeport took: 17.893021ms
Jun  2 21:39:47.422: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.70703ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:39:50.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-720" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.969 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":122,"skipped":2301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:39:50.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4639
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-962f1df0-5336-459b-bfab-46ea3f8d8203 in namespace container-probe-4639
Jun  2 21:39:54.696: INFO: Started pod busybox-962f1df0-5336-459b-bfab-46ea3f8d8203 in namespace container-probe-4639
STEP: checking the pod's current state and verifying that restartCount is present
Jun  2 21:39:54.705: INFO: Initial restart count of pod busybox-962f1df0-5336-459b-bfab-46ea3f8d8203 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:43:55.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4639" for this suite.

• [SLOW TEST:245.092 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:43:55.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7923
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:43:55.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7923" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":124,"skipped":2374,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:43:55.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7093
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:43:56.103: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  2 21:44:01.116: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  2 21:44:01.116: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 21:44:01.179: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7093  81d55425-68d8-4f46-b77f-4d93de98a961 31313 1 2022-06-02 21:44:01 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-06-02 21:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a5c738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun  2 21:44:01.190: INFO: New ReplicaSet "test-cleanup-deployment-56cd759769" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-56cd759769  deployment-7093  ad1273b1-a1b1-4cd2-a3d2-9b7088deeb50 31315 1 2022-06-02 21:44:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 81d55425-68d8-4f46-b77f-4d93de98a961 0xc000a5cbd7 0xc000a5cbd8}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81d55425-68d8-4f46-b77f-4d93de98a961\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 56cd759769,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000a5cc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 21:44:01.190: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun  2 21:44:01.190: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7093  61f95fc6-2fa7-4959-8415-b820c99450a1 31314 1 2022-06-02 21:43:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 81d55425-68d8-4f46-b77f-4d93de98a961 0xc000a5caa7 0xc000a5caa8}] []  [{e2e.test Update apps/v1 2022-06-02 21:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:43:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-06-02 21:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"81d55425-68d8-4f46-b77f-4d93de98a961\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000a5cb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  2 21:44:01.207: INFO: Pod "test-cleanup-controller-mslsr" is available:
&Pod{ObjectMeta:{test-cleanup-controller-mslsr test-cleanup-controller- deployment-7093  bd5174d3-89a3-4f78-929a-c84bd77b0f83 31295 0 2022-06-02 21:43:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:9b46d174427188b17489727064b26b8f588711e3bd973281408fc58fce7c21c8 cni.projectcalico.org/podIP:172.30.118.11/32 cni.projectcalico.org/podIPs:172.30.118.11/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller 61f95fc6-2fa7-4959-8415-b820c99450a1 0xc002491647 0xc002491648}] []  [{kube-controller-manager Update v1 2022-06-02 21:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61f95fc6-2fa7-4959-8415-b820c99450a1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:43:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:43:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.118.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-874kl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-874kl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:43:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:43:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:43:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:43:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:172.30.118.11,StartTime:2022-06-02 21:43:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:43:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1d2366eb498c1980acab501234cb9f467d2c3d02514f5f249f977002cf208e6a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.118.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:44:01.207: INFO: Pod "test-cleanup-deployment-56cd759769-6bp5p" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-56cd759769-6bp5p test-cleanup-deployment-56cd759769- deployment-7093  d190cc65-0fd3-4993-9d7f-3ad80fec4191 31318 0 2022-06-02 21:44:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-56cd759769 ad1273b1-a1b1-4cd2-a3d2-9b7088deeb50 0xc002491867 0xc002491868}] []  [{kube-controller-manager Update v1 2022-06-02 21:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ad1273b1-a1b1-4cd2-a3d2-9b7088deeb50\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kkhnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kkhnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:44:01.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7093" for this suite.

• [SLOW TEST:5.382 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":125,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:44:01.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8198
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-7f427445-9014-425b-b314-d61404832978
Jun  2 21:44:01.469: INFO: Pod name my-hostname-basic-7f427445-9014-425b-b314-d61404832978: Found 0 pods out of 1
Jun  2 21:44:06.489: INFO: Pod name my-hostname-basic-7f427445-9014-425b-b314-d61404832978: Found 1 pods out of 1
Jun  2 21:44:06.489: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7f427445-9014-425b-b314-d61404832978" are running
Jun  2 21:44:06.513: INFO: Pod "my-hostname-basic-7f427445-9014-425b-b314-d61404832978-pj2q6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:44:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:44:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:44:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-06-02 21:44:01 +0000 UTC Reason: Message:}])
Jun  2 21:44:06.514: INFO: Trying to dial the pod
Jun  2 21:44:11.607: INFO: Controller my-hostname-basic-7f427445-9014-425b-b314-d61404832978: Got expected result from replica 1 [my-hostname-basic-7f427445-9014-425b-b314-d61404832978-pj2q6]: "my-hostname-basic-7f427445-9014-425b-b314-d61404832978-pj2q6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:44:11.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8198" for this suite.

• [SLOW TEST:10.404 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":126,"skipped":2389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:44:11.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-8229
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:44:11.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Creating first CR 
Jun  2 21:44:14.572: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-02T21:44:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-02T21:44:14Z]] name:name1 resourceVersion:31405 uid:c5e1559f-e202-45f7-bf1b-1f2a12e44798] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun  2 21:44:24.599: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-02T21:44:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-02T21:44:24Z]] name:name2 resourceVersion:31437 uid:8f40f485-d37c-445c-8157-408e372312b4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun  2 21:44:34.648: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-02T21:44:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-02T21:44:34Z]] name:name1 resourceVersion:31451 uid:c5e1559f-e202-45f7-bf1b-1f2a12e44798] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun  2 21:44:44.675: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-02T21:44:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-02T21:44:44Z]] name:name2 resourceVersion:31459 uid:8f40f485-d37c-445c-8157-408e372312b4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun  2 21:44:54.704: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-02T21:44:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-02T21:44:34Z]] name:name1 resourceVersion:31471 uid:c5e1559f-e202-45f7-bf1b-1f2a12e44798] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun  2 21:45:04.736: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-06-02T21:44:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-06-02T21:44:44Z]] name:name2 resourceVersion:31482 uid:8f40f485-d37c-445c-8157-408e372312b4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:45:15.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8229" for this suite.

• [SLOW TEST:63.673 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":127,"skipped":2419,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:45:15.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6742
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6742.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6742.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:45:19.785: INFO: DNS probes using dns-test-0be8077e-4f1d-4246-8d32-38886eaf8ae8 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6742.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6742.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:45:23.993: INFO: File wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:24.011: INFO: File jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:24.011: INFO: Lookups using dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 failed for: [wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local]

Jun  2 21:45:29.032: INFO: File wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:29.077: INFO: File jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:29.077: INFO: Lookups using dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 failed for: [wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local]

Jun  2 21:45:34.028: INFO: File wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:34.043: INFO: File jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:34.043: INFO: Lookups using dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 failed for: [wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local]

Jun  2 21:45:39.037: INFO: File wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:39.055: INFO: File jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:39.055: INFO: Lookups using dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 failed for: [wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local]

Jun  2 21:45:44.032: INFO: File wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:44.048: INFO: File jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:44.048: INFO: Lookups using dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 failed for: [wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local]

Jun  2 21:45:49.043: INFO: File jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local from pod  dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun  2 21:45:49.043: INFO: Lookups using dns-6742/dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 failed for: [jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local]

Jun  2 21:45:54.045: INFO: DNS probes using dns-test-2847d21f-e9b1-45fe-b45e-b61a5430cb30 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6742.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6742.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6742.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6742.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 21:45:58.303: INFO: DNS probes using dns-test-d108ab15-9cff-435f-b624-8bde6e890e09 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:45:58.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6742" for this suite.

• [SLOW TEST:43.090 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":128,"skipped":2424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:45:58.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5118
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:45:58.626: INFO: Creating pod...
Jun  2 21:45:58.661: INFO: Pod Quantity: 1 Status: Pending
Jun  2 21:45:59.675: INFO: Pod Quantity: 1 Status: Pending
Jun  2 21:46:00.676: INFO: Pod Quantity: 1 Status: Pending
Jun  2 21:46:01.674: INFO: Pod Status: Running
Jun  2 21:46:01.674: INFO: Creating service...
Jun  2 21:46:01.734: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/DELETE
Jun  2 21:46:01.830: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  2 21:46:01.830: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/GET
Jun  2 21:46:01.854: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  2 21:46:01.854: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/HEAD
Jun  2 21:46:01.870: INFO: http.Client request:HEAD | StatusCode:200
Jun  2 21:46:01.870: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/OPTIONS
Jun  2 21:46:01.902: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  2 21:46:01.902: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/PATCH
Jun  2 21:46:01.921: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  2 21:46:01.921: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/POST
Jun  2 21:46:01.934: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  2 21:46:01.935: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/pods/agnhost/proxy/some/path/with/PUT
Jun  2 21:46:01.964: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun  2 21:46:01.965: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/DELETE
Jun  2 21:46:01.994: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun  2 21:46:01.994: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/GET
Jun  2 21:46:02.036: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun  2 21:46:02.036: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/HEAD
Jun  2 21:46:02.056: INFO: http.Client request:HEAD | StatusCode:200
Jun  2 21:46:02.056: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/OPTIONS
Jun  2 21:46:02.076: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun  2 21:46:02.076: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/PATCH
Jun  2 21:46:02.106: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun  2 21:46:02.107: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/POST
Jun  2 21:46:02.128: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun  2 21:46:02.129: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-5118/services/test-service/proxy/some/path/with/PUT
Jun  2 21:46:02.154: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:02.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5118" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":129,"skipped":2464,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:02.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-6786
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:02.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6786" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":130,"skipped":2476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:02.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5160
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:46:03.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29" in namespace "downward-api-5160" to be "Succeeded or Failed"
Jun  2 21:46:03.038: INFO: Pod "downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29": Phase="Pending", Reason="", readiness=false. Elapsed: 15.702096ms
Jun  2 21:46:05.052: INFO: Pod "downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29": Phase="Running", Reason="", readiness=true. Elapsed: 2.029517776s
Jun  2 21:46:07.062: INFO: Pod "downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29": Phase="Running", Reason="", readiness=false. Elapsed: 4.039142727s
Jun  2 21:46:09.081: INFO: Pod "downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058762937s
STEP: Saw pod success
Jun  2 21:46:09.081: INFO: Pod "downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29" satisfied condition "Succeeded or Failed"
Jun  2 21:46:09.091: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29 container client-container: <nil>
STEP: delete the pod
Jun  2 21:46:09.224: INFO: Waiting for pod downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29 to disappear
Jun  2 21:46:09.234: INFO: Pod downwardapi-volume-c290e2d5-0224-48ee-805d-6f697b7eac29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:09.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5160" for this suite.

• [SLOW TEST:6.526 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2554,"failed":0}
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:09.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2775
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:46:13.508: INFO: Deleting pod "var-expansion-be9bde45-660b-45d8-bbe0-6c6457114495" in namespace "var-expansion-2775"
Jun  2 21:46:13.526: INFO: Wait up to 5m0s for pod "var-expansion-be9bde45-660b-45d8-bbe0-6c6457114495" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:15.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2775" for this suite.

• [SLOW TEST:6.328 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":132,"skipped":2554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:15.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3949
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:46:16.154: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:46:18.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 46, 16, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 46, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 46, 16, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 46, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:46:21.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:31.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3949" for this suite.
STEP: Destroying namespace "webhook-3949-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.489 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":133,"skipped":2598,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:32.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3573
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:46:32.389: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun  2 21:46:32.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:32.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jun  2 21:46:32.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:32.533: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:33.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:33.546: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:34.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:34.546: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:35.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  2 21:46:35.549: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun  2 21:46:35.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  2 21:46:35.607: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun  2 21:46:36.632: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:36.632: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun  2 21:46:36.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:36.694: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:37.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:37.708: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:38.721: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:38.721: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:39.712: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:39.712: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 21:46:40.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  2 21:46:40.709: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3573, will wait for the garbage collector to delete the pods
Jun  2 21:46:40.811: INFO: Deleting DaemonSet.extensions daemon-set took: 15.887887ms
Jun  2 21:46:40.912: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.364906ms
Jun  2 21:46:44.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 21:46:44.323: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  2 21:46:44.336: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32030"},"items":null}

Jun  2 21:46:44.346: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32030"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:44.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3573" for this suite.

• [SLOW TEST:12.374 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":134,"skipped":2609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:44.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5170
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:46:44.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun  2 21:46:49.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 create -f -'
Jun  2 21:46:50.485: INFO: stderr: ""
Jun  2 21:46:50.485: INFO: stdout: "e2e-test-crd-publish-openapi-7171-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  2 21:46:50.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 delete e2e-test-crd-publish-openapi-7171-crds test-foo'
Jun  2 21:46:50.662: INFO: stderr: ""
Jun  2 21:46:50.662: INFO: stdout: "e2e-test-crd-publish-openapi-7171-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  2 21:46:50.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 apply -f -'
Jun  2 21:46:50.975: INFO: stderr: ""
Jun  2 21:46:50.975: INFO: stdout: "e2e-test-crd-publish-openapi-7171-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  2 21:46:50.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 delete e2e-test-crd-publish-openapi-7171-crds test-foo'
Jun  2 21:46:51.114: INFO: stderr: ""
Jun  2 21:46:51.115: INFO: stdout: "e2e-test-crd-publish-openapi-7171-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Jun  2 21:46:51.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 create -f -'
Jun  2 21:46:51.813: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun  2 21:46:51.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 create -f -'
Jun  2 21:46:52.048: INFO: rc: 1
Jun  2 21:46:52.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 apply -f -'
Jun  2 21:46:52.283: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun  2 21:46:52.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 create -f -'
Jun  2 21:46:52.501: INFO: rc: 1
Jun  2 21:46:52.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 --namespace=crd-publish-openapi-5170 apply -f -'
Jun  2 21:46:52.724: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun  2 21:46:52.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 explain e2e-test-crd-publish-openapi-7171-crds'
Jun  2 21:46:52.964: INFO: stderr: ""
Jun  2 21:46:52.964: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7171-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun  2 21:46:52.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 explain e2e-test-crd-publish-openapi-7171-crds.metadata'
Jun  2 21:46:53.223: INFO: stderr: ""
Jun  2 21:46:53.223: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7171-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  2 21:46:53.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 explain e2e-test-crd-publish-openapi-7171-crds.spec'
Jun  2 21:46:53.544: INFO: stderr: ""
Jun  2 21:46:53.545: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7171-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  2 21:46:53.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 explain e2e-test-crd-publish-openapi-7171-crds.spec.bars'
Jun  2 21:46:53.785: INFO: stderr: ""
Jun  2 21:46:53.785: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7171-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun  2 21:46:53.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-5170 explain e2e-test-crd-publish-openapi-7171-crds.spec.bars2'
Jun  2 21:46:54.050: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:46:58.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5170" for this suite.

• [SLOW TEST:13.964 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":135,"skipped":2680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:46:58.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5909
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  2 21:47:03.830: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:47:03.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5909" for this suite.

• [SLOW TEST:5.483 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:47:03.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-1486
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  2 21:47:04.601: INFO: starting watch
STEP: patching
STEP: updating
Jun  2 21:47:04.654: INFO: waiting for watch events with expected annotations
Jun  2 21:47:04.654: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:47:04.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1486" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":137,"skipped":2859,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:47:04.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5001
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun  2 21:47:05.111: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5001  223bcc4f-7413-437f-853b-7dce10a83a7b 32149 0 2022-06-02 21:47:05 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2022-06-02 21:47:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5xg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5xg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 21:47:05.124: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:47:07.138: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun  2 21:47:07.138: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5001 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:47:07.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:47:07.139: INFO: ExecWithOptions: Clientset creation
Jun  2 21:47:07.139: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5001/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Jun  2 21:47:07.377: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5001 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:47:07.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:47:07.379: INFO: ExecWithOptions: Clientset creation
Jun  2 21:47:07.379: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5001/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:47:07.584: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:47:07.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5001" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":138,"skipped":2879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:47:07.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-961
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:47:07.960: INFO: Creating deployment "test-recreate-deployment"
Jun  2 21:47:07.996: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  2 21:47:08.018: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun  2 21:47:10.041: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  2 21:47:10.049: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  2 21:47:10.077: INFO: Updating deployment test-recreate-deployment
Jun  2 21:47:10.077: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 21:47:10.361: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-961  8de52ff2-81a1-48c0-b3be-179253e33e7b 32232 2 2022-06-02 21:47:07 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00805a4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-06-02 21:47:10 +0000 UTC,LastTransitionTime:2022-06-02 21:47:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-06-02 21:47:10 +0000 UTC,LastTransitionTime:2022-06-02 21:47:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  2 21:47:10.373: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-961  31629a3f-ce22-4863-bb30-3bd226edf9b6 32231 1 2022-06-02 21:47:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8de52ff2-81a1-48c0-b3be-179253e33e7b 0xc007d4b7d7 0xc007d4b7d8}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8de52ff2-81a1-48c0-b3be-179253e33e7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d4b8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 21:47:10.374: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  2 21:47:10.374: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-961  1871ae87-da3e-4392-ab1e-95a8bb621711 32220 2 2022-06-02 21:47:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8de52ff2-81a1-48c0-b3be-179253e33e7b 0xc007d4b937 0xc007d4b938}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:47:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8de52ff2-81a1-48c0-b3be-179253e33e7b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d4b9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 21:47:10.386: INFO: Pod "test-recreate-deployment-5b99bd5487-h42wz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-h42wz test-recreate-deployment-5b99bd5487- deployment-961  cf9a7eec-e77c-4615-a7cd-c7bbefe8ab23 32230 0 2022-06-02 21:47:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 31629a3f-ce22-4863-bb30-3bd226edf9b6 0xc007d4be97 0xc007d4be98}] []  [{kube-controller-manager Update v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31629a3f-ce22-4863-bb30-3bd226edf9b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-06-02 21:47:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v22fg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v22fg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:47:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:47:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:47:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:47:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:,StartTime:2022-06-02 21:47:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:47:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-961" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":139,"skipped":2912,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:47:10.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6059
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-6059
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  2 21:47:10.649: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  2 21:47:10.790: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:47:12.805: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:14.809: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:16.816: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:18.806: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:20.808: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:22.814: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:24.814: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:26.809: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:28.803: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:30.814: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 21:47:32.813: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  2 21:47:32.834: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  2 21:47:32.853: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  2 21:47:34.953: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  2 21:47:34.954: INFO: Going to poll 172.30.170.166 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun  2 21:47:34.965: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.170.166:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6059 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:47:34.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:47:34.966: INFO: ExecWithOptions: Clientset creation
Jun  2 21:47:34.966: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6059/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.170.166%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:47:35.182: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  2 21:47:35.182: INFO: Going to poll 172.30.118.61 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun  2 21:47:35.197: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.118.61:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6059 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:47:35.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:47:35.199: INFO: ExecWithOptions: Clientset creation
Jun  2 21:47:35.199: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6059/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.118.61%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:47:35.419: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  2 21:47:35.419: INFO: Going to poll 172.30.220.229 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun  2 21:47:35.434: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.220.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6059 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 21:47:35.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:47:35.435: INFO: ExecWithOptions: Clientset creation
Jun  2 21:47:35.435: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-6059/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.220.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 21:47:35.652: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:47:35.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6059" for this suite.

• [SLOW TEST:25.275 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2921,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:47:35.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3446
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:47:37.974: INFO: Deleting pod "var-expansion-e2351874-0f5c-40cd-8b23-e2f6d14ad7bb" in namespace "var-expansion-3446"
Jun  2 21:47:37.998: INFO: Wait up to 5m0s for pod "var-expansion-e2351874-0f5c-40cd-8b23-e2f6d14ad7bb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:47:42.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3446" for this suite.

• [SLOW TEST:6.397 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":141,"skipped":2933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:47:42.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8994
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:47:42.352: INFO: created pod
Jun  2 21:47:42.352: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8994" to be "Succeeded or Failed"
Jun  2 21:47:42.362: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048982ms
Jun  2 21:47:44.375: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022410023s
Jun  2 21:47:46.401: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048674136s
Jun  2 21:47:48.411: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059009738s
STEP: Saw pod success
Jun  2 21:47:48.411: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun  2 21:48:18.413: INFO: polling logs
Jun  2 21:48:18.565: INFO: Pod logs: 
2022/06/02 21:47:44 OK: Got token
2022/06/02 21:47:44 validating with in-cluster discovery
2022/06/02 21:47:44 OK: got issuer https://kubernetes.default.svc
2022/06/02 21:47:44 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8994:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654207062, NotBefore:1654206462, IssuedAt:1654206462, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8994", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"dc855ed5-6891-4197-912a-375c20516197"}}}
2022/06/02 21:47:44 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
2022/06/02 21:47:44 OK: Validated signature on JWT
2022/06/02 21:47:44 OK: Got valid claims from token!
2022/06/02 21:47:44 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8994:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654207062, NotBefore:1654206462, IssuedAt:1654206462, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8994", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"dc855ed5-6891-4197-912a-375c20516197"}}}

Jun  2 21:48:18.565: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:48:18.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8994" for this suite.

• [SLOW TEST:36.541 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":142,"skipped":2979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:48:18.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6327
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 21:48:18.899: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63" in namespace "projected-6327" to be "Succeeded or Failed"
Jun  2 21:48:18.910: INFO: Pod "downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63": Phase="Pending", Reason="", readiness=false. Elapsed: 10.953566ms
Jun  2 21:48:20.927: INFO: Pod "downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63": Phase="Running", Reason="", readiness=true. Elapsed: 2.027687386s
Jun  2 21:48:22.941: INFO: Pod "downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63": Phase="Running", Reason="", readiness=false. Elapsed: 4.041431824s
Jun  2 21:48:24.960: INFO: Pod "downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060794112s
STEP: Saw pod success
Jun  2 21:48:24.960: INFO: Pod "downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63" satisfied condition "Succeeded or Failed"
Jun  2 21:48:24.971: INFO: Trying to get logs from node 10.134.156.253 pod downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63 container client-container: <nil>
STEP: delete the pod
Jun  2 21:48:25.041: INFO: Waiting for pod downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63 to disappear
Jun  2 21:48:25.051: INFO: Pod downwardapi-volume-95204f56-a773-4e4f-a63f-5a42982f0a63 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:48:25.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6327" for this suite.

• [SLOW TEST:6.439 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":3009,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:48:25.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-942
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Jun  2 21:48:25.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:48:45.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-942" for this suite.

• [SLOW TEST:20.627 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":144,"skipped":3030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:48:45.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4313
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun  2 21:48:47.089: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0602 21:48:47.088933      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:48:47.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4313" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":145,"skipped":3060,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:48:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:53:47.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8773" for this suite.

• [SLOW TEST:300.330 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":146,"skipped":3061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:53:47.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1841
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1841
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1841
I0602 21:53:47.763365      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1841, replica count: 2
I0602 21:53:50.815564      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 21:53:50.815: INFO: Creating new exec pod
Jun  2 21:53:53.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1841 exec execpodkzjkk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun  2 21:53:54.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  2 21:53:54.201: INFO: stdout: "externalname-service-fql2t"
Jun  2 21:53:54.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-1841 exec execpodkzjkk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.16.173 80'
Jun  2 21:53:54.518: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.16.173 80\nConnection to 172.21.16.173 80 port [tcp/http] succeeded!\n"
Jun  2 21:53:54.519: INFO: stdout: "externalname-service-qwdqw"
Jun  2 21:53:54.519: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:53:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1841" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.137 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":147,"skipped":3084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:53:54.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-9898
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:53:54.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9898" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":148,"skipped":3116,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:53:54.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3405
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-c7ce6d80-2d91-432d-a1a9-03fc03f64a30
STEP: Creating a pod to test consume configMaps
Jun  2 21:53:55.124: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e" in namespace "projected-3405" to be "Succeeded or Failed"
Jun  2 21:53:55.133: INFO: Pod "pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.451055ms
Jun  2 21:53:57.167: INFO: Pod "pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04216213s
Jun  2 21:53:59.179: INFO: Pod "pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054146214s
STEP: Saw pod success
Jun  2 21:53:59.179: INFO: Pod "pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e" satisfied condition "Succeeded or Failed"
Jun  2 21:53:59.187: INFO: Trying to get logs from node 10.134.156.253 pod pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  2 21:53:59.372: INFO: Waiting for pod pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e to disappear
Jun  2 21:53:59.383: INFO: Pod pod-projected-configmaps-c6fa42af-5a4b-4964-b091-e34272455b8e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:53:59.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3405" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":3130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:53:59.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4140
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:54:12.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4140" for this suite.

• [SLOW TEST:13.446 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":150,"skipped":3174,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:54:12.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3545
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
Jun  2 21:54:13.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3545 api-versions'
Jun  2 21:54:13.214: INFO: stderr: ""
Jun  2 21:54:13.214: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:54:13.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3545" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":151,"skipped":3194,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:54:13.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3188
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun  2 21:54:13.506: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.506: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.543: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.543: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.569: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.569: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.645: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:13.645: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  2 21:54:15.848: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  2 21:54:15.848: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  2 21:54:16.269: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun  2 21:54:16.309: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun  2 21:54:16.317: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.317: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.317: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.317: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.317: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.317: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.318: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.318: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 0
Jun  2 21:54:16.318: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:16.318: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:16.318: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.318: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.319: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.319: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.328: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.328: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.359: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.359: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.492: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.493: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:16.509: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:16.509: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:18.327: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:18.327: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:18.424: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
STEP: listing Deployments
Jun  2 21:54:18.452: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun  2 21:54:18.480: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun  2 21:54:18.500: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:18.502: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:18.526: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:18.549: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:18.827: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:20.891: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:21.624: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:21.704: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:21.716: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun  2 21:54:23.971: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun  2 21:54:24.179: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:24.179: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:24.179: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:24.180: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:24.180: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 1
Jun  2 21:54:24.181: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:24.181: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 3
Jun  2 21:54:24.181: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:24.181: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 2
Jun  2 21:54:24.182: INFO: observed Deployment test-deployment in namespace deployment-3188 with ReadyReplicas 3
STEP: deleting the Deployment
Jun  2 21:54:24.208: INFO: observed event type MODIFIED
Jun  2 21:54:24.209: INFO: observed event type MODIFIED
Jun  2 21:54:24.209: INFO: observed event type MODIFIED
Jun  2 21:54:24.210: INFO: observed event type MODIFIED
Jun  2 21:54:24.211: INFO: observed event type MODIFIED
Jun  2 21:54:24.211: INFO: observed event type MODIFIED
Jun  2 21:54:24.212: INFO: observed event type MODIFIED
Jun  2 21:54:24.212: INFO: observed event type MODIFIED
Jun  2 21:54:24.212: INFO: observed event type MODIFIED
Jun  2 21:54:24.213: INFO: observed event type MODIFIED
Jun  2 21:54:24.213: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 21:54:24.224: INFO: Log out all the ReplicaSets if there is no deployment created
Jun  2 21:54:24.234: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-3188  24f1e631-555b-4621-a705-790ad4ebd1e3 33411 4 2022-06-02 21:54:16 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 098a4ff6-bee8-4498-9e50-3c7a33e0135b 0xc004aa1637 0xc004aa1638}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:54:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098a4ff6-bee8-4498-9e50-3c7a33e0135b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:54:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa16c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  2 21:54:24.244: INFO: pod: "test-deployment-5ddd8b47d8-fznhk":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-fznhk test-deployment-5ddd8b47d8- deployment-3188  1ce39cba-f498-483a-a74b-9ca51c228d79 33407 0 2022-06-02 21:54:16 +0000 UTC 2022-06-02 21:54:24 +0000 UTC 0xc0029a2f88 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:de8eabf5826f44cc485322e4a5c9cb9aca76caf8bbc764bf2db69f5150056bc3 cni.projectcalico.org/podIP:172.30.220.246/32 cni.projectcalico.org/podIPs:172.30.220.246/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 24f1e631-555b-4621-a705-790ad4ebd1e3 0xc0029a2fb7 0xc0029a2fb8}] []  [{kube-controller-manager Update v1 2022-06-02 21:54:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24f1e631-555b-4621-a705-790ad4ebd1e3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:54:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:54:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m958z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m958z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.246,StartTime:2022-06-02 21:54:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:54:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:containerd://97ac1aaac09203e0fd14ef417eb7a87042dce31ed42cc58b3b81ae5c6fa33852,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  2 21:54:24.245: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-3188  4f27b6c7-8baa-4fb1-9c44-7a2a4134341c 33296 3 2022-06-02 21:54:13 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 098a4ff6-bee8-4498-9e50-3c7a33e0135b 0xc004aa1727 0xc004aa1728}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:54:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098a4ff6-bee8-4498-9e50-3c7a33e0135b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:54:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa17b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  2 21:54:24.257: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-3188  2a684687-7a5a-4fc3-b0d5-34479b26be7b 33403 2 2022-06-02 21:54:18 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 098a4ff6-bee8-4498-9e50-3c7a33e0135b 0xc004aa1817 0xc004aa1818}] []  [{kube-controller-manager Update apps/v1 2022-06-02 21:54:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098a4ff6-bee8-4498-9e50-3c7a33e0135b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 21:54:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004aa18b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun  2 21:54:24.268: INFO: pod: "test-deployment-854fdc678-q4b6k":
&Pod{ObjectMeta:{test-deployment-854fdc678-q4b6k test-deployment-854fdc678- deployment-3188  b30f332b-2999-4d63-bdc9-60b65067bddb 33402 0 2022-06-02 21:54:21 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:356f0fa033a4d9546243f67022d25af540d9d298d5be815e8c36f3baf3310304 cni.projectcalico.org/podIP:172.30.118.24/32 cni.projectcalico.org/podIPs:172.30.118.24/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-854fdc678 2a684687-7a5a-4fc3-b0d5-34479b26be7b 0xc004aa1e47 0xc004aa1e48}] []  [{kube-controller-manager Update v1 2022-06-02 21:54:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a684687-7a5a-4fc3-b0d5-34479b26be7b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:54:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:54:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.118.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhvs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhvs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.247,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.247,PodIP:172.30.118.24,StartTime:2022-06-02 21:54:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:54:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://147c530d52cec47b6b1ef5c35e485115d1c6997791f45f5be58c0038b8a2c558,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.118.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  2 21:54:24.270: INFO: pod: "test-deployment-854fdc678-tzztd":
&Pod{ObjectMeta:{test-deployment-854fdc678-tzztd test-deployment-854fdc678- deployment-3188  a15eb67b-05ae-4c5e-9b2d-316d731e7f6f 33367 0 2022-06-02 21:54:18 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:39aa714656fd9ff5a0901ded572b4c4a101c6a164ecfb5bc879bf5fbedca1fc4 cni.projectcalico.org/podIP:172.30.170.169/32 cni.projectcalico.org/podIPs:172.30.170.169/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-854fdc678 2a684687-7a5a-4fc3-b0d5-34479b26be7b 0xc003561567 0xc003561568}] []  [{kube-controller-manager Update v1 2022-06-02 21:54:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a684687-7a5a-4fc3-b0d5-34479b26be7b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 21:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 21:54:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.170.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qp45w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qp45w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 21:54:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.209,PodIP:172.30.170.169,StartTime:2022-06-02 21:54:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 21:54:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a701e7e0f39514c7d002b637a5c792ef7e3371dba44a61e33c656646b263e4db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.170.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:54:24.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3188" for this suite.

• [SLOW TEST:11.050 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":152,"skipped":3203,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:54:24.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8036
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:54:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8036" for this suite.

• [SLOW TEST:33.194 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":3210,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:54:57.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3061
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun  2 21:54:57.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun  2 21:55:11.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 21:55:14.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:29.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3061" for this suite.

• [SLOW TEST:32.473 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":154,"skipped":3219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:29.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6551
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-6551/configmap-test-94ef9991-7f47-4c2d-b1f3-d3b7a1307a29
STEP: Creating a pod to test consume configMaps
Jun  2 21:55:30.218: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357" in namespace "configmap-6551" to be "Succeeded or Failed"
Jun  2 21:55:30.233: INFO: Pod "pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357": Phase="Pending", Reason="", readiness=false. Elapsed: 14.1172ms
Jun  2 21:55:32.261: INFO: Pod "pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042156011s
Jun  2 21:55:34.284: INFO: Pod "pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065863066s
Jun  2 21:55:36.311: INFO: Pod "pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092242746s
STEP: Saw pod success
Jun  2 21:55:36.311: INFO: Pod "pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357" satisfied condition "Succeeded or Failed"
Jun  2 21:55:36.322: INFO: Trying to get logs from node 10.134.156.253 pod pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357 container env-test: <nil>
STEP: delete the pod
Jun  2 21:55:36.515: INFO: Waiting for pod pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357 to disappear
Jun  2 21:55:36.525: INFO: Pod pod-configmaps-d3c5e006-ed9f-41e9-90ef-a4f12f004357 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6551" for this suite.

• [SLOW TEST:6.568 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":3277,"failed":0}
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3206
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:36.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3206" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":156,"skipped":3277,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:37.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1755
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  2 21:55:37.321: INFO: Waiting up to 5m0s for pod "pod-11fcb395-cbaf-4409-917f-5ca318f358c0" in namespace "emptydir-1755" to be "Succeeded or Failed"
Jun  2 21:55:37.339: INFO: Pod "pod-11fcb395-cbaf-4409-917f-5ca318f358c0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.990574ms
Jun  2 21:55:39.356: INFO: Pod "pod-11fcb395-cbaf-4409-917f-5ca318f358c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034589203s
Jun  2 21:55:41.376: INFO: Pod "pod-11fcb395-cbaf-4409-917f-5ca318f358c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054554588s
Jun  2 21:55:43.401: INFO: Pod "pod-11fcb395-cbaf-4409-917f-5ca318f358c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079233076s
STEP: Saw pod success
Jun  2 21:55:43.401: INFO: Pod "pod-11fcb395-cbaf-4409-917f-5ca318f358c0" satisfied condition "Succeeded or Failed"
Jun  2 21:55:43.413: INFO: Trying to get logs from node 10.134.156.253 pod pod-11fcb395-cbaf-4409-917f-5ca318f358c0 container test-container: <nil>
STEP: delete the pod
Jun  2 21:55:43.487: INFO: Waiting for pod pod-11fcb395-cbaf-4409-917f-5ca318f358c0 to disappear
Jun  2 21:55:43.501: INFO: Pod pod-11fcb395-cbaf-4409-917f-5ca318f358c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:43.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1755" for this suite.

• [SLOW TEST:6.533 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":3289,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:43.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9208
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun  2 21:55:43.824: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9208  ecd62372-00cf-44b6-a1f8-147cee0bfc0b 33803 0 2022-06-02 21:55:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-02 21:55:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 21:55:43.824: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9208  ecd62372-00cf-44b6-a1f8-147cee0bfc0b 33804 0 2022-06-02 21:55:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-06-02 21:55:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:43.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9208" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":158,"skipped":3299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:43.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3360
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
Jun  2 21:55:44.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  2 21:55:44.256: INFO: stderr: ""
Jun  2 21:55:44.256: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
Jun  2 21:55:44.256: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  2 21:55:44.256: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3360" to be "running and ready, or succeeded"
Jun  2 21:55:44.269: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.003064ms
Jun  2 21:55:46.294: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.037761135s
Jun  2 21:55:46.294: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  2 21:55:46.294: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun  2 21:55:46.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 logs logs-generator logs-generator'
Jun  2 21:55:46.481: INFO: stderr: ""
Jun  2 21:55:46.482: INFO: stdout: "I0602 21:55:45.671433       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/6hd 251\nI0602 21:55:45.871847       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/vjv 247\nI0602 21:55:46.072243       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/bwvf 464\nI0602 21:55:46.271588       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/pwhj 450\nI0602 21:55:46.471938       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/b87p 527\n"
STEP: limiting log lines
Jun  2 21:55:46.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 logs logs-generator logs-generator --tail=1'
Jun  2 21:55:46.635: INFO: stderr: ""
Jun  2 21:55:46.635: INFO: stdout: "I0602 21:55:46.471938       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/b87p 527\n"
Jun  2 21:55:46.635: INFO: got output "I0602 21:55:46.471938       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/b87p 527\n"
STEP: limiting log bytes
Jun  2 21:55:46.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 logs logs-generator logs-generator --limit-bytes=1'
Jun  2 21:55:46.834: INFO: stderr: ""
Jun  2 21:55:46.834: INFO: stdout: "I"
Jun  2 21:55:46.834: INFO: got output "I"
STEP: exposing timestamps
Jun  2 21:55:46.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 logs logs-generator logs-generator --tail=1 --timestamps'
Jun  2 21:55:46.999: INFO: stderr: ""
Jun  2 21:55:46.999: INFO: stdout: "2022-06-02T21:55:46.871807847Z I0602 21:55:46.871612       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5lbn 507\n"
Jun  2 21:55:46.999: INFO: got output "2022-06-02T21:55:46.871807847Z I0602 21:55:46.871612       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5lbn 507\n"
STEP: restricting to a time range
Jun  2 21:55:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 logs logs-generator logs-generator --since=1s'
Jun  2 21:55:49.643: INFO: stderr: ""
Jun  2 21:55:49.643: INFO: stdout: "I0602 21:55:48.671938       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/5vvx 482\nI0602 21:55:48.872309       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/72n6 308\nI0602 21:55:49.071625       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/bxr 566\nI0602 21:55:49.271991       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/4p5 417\nI0602 21:55:49.472364       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vfck 386\n"
Jun  2 21:55:49.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 logs logs-generator logs-generator --since=24h'
Jun  2 21:55:49.780: INFO: stderr: ""
Jun  2 21:55:49.780: INFO: stdout: "I0602 21:55:45.671433       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/6hd 251\nI0602 21:55:45.871847       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/vjv 247\nI0602 21:55:46.072243       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/bwvf 464\nI0602 21:55:46.271588       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/pwhj 450\nI0602 21:55:46.471938       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/b87p 527\nI0602 21:55:46.672301       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/d4f 489\nI0602 21:55:46.871612       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/5lbn 507\nI0602 21:55:47.072010       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/ndf 390\nI0602 21:55:47.272386       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/zpm 543\nI0602 21:55:47.471651       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/kps 303\nI0602 21:55:47.672032       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/mkg 218\nI0602 21:55:47.872403       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/j8ws 595\nI0602 21:55:48.071799       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/5fn 576\nI0602 21:55:48.272206       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/lpn 279\nI0602 21:55:48.471578       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/pj8 387\nI0602 21:55:48.671938       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/5vvx 482\nI0602 21:55:48.872309       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/72n6 308\nI0602 21:55:49.071625       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/bxr 566\nI0602 21:55:49.271991       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/4p5 417\nI0602 21:55:49.472364       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/vfck 386\nI0602 21:55:49.671652       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/s7vr 326\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
Jun  2 21:55:49.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-3360 delete pod logs-generator'
Jun  2 21:55:51.150: INFO: stderr: ""
Jun  2 21:55:51.150: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:51.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3360" for this suite.

• [SLOW TEST:7.333 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":159,"skipped":3328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:51.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9804
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-1b1ae537-7abc-4d56-89ae-c8750764797a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:55:55.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9804" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":3467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:55:55.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2228
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-9652c65c-b211-4ba3-8969-524fac2a2f28
STEP: Creating secret with name s-test-opt-upd-6cc94e46-5ed0-42cd-b9d9-512e30ede70b
STEP: Creating the pod
Jun  2 21:55:55.913: INFO: The status of Pod pod-projected-secrets-ad475868-86e2-466a-82c6-e5db66aa21d9 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:55:57.952: INFO: The status of Pod pod-projected-secrets-ad475868-86e2-466a-82c6-e5db66aa21d9 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:55:59.929: INFO: The status of Pod pod-projected-secrets-ad475868-86e2-466a-82c6-e5db66aa21d9 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-9652c65c-b211-4ba3-8969-524fac2a2f28
STEP: Updating secret s-test-opt-upd-6cc94e46-5ed0-42cd-b9d9-512e30ede70b
STEP: Creating secret with name s-test-opt-create-9e8b7415-d447-456f-8d45-574d8b45afc5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:57:23.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2228" for this suite.

• [SLOW TEST:87.982 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":3547,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:57:23.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2118
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:57:27.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2118" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":3566,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:57:27.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3592
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 21:57:28.167: INFO: The status of Pod busybox-readonly-fs10a07d65-73ea-4817-ae95-6ad9d4b67544 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:57:30.185: INFO: The status of Pod busybox-readonly-fs10a07d65-73ea-4817-ae95-6ad9d4b67544 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:57:32.192: INFO: The status of Pod busybox-readonly-fs10a07d65-73ea-4817-ae95-6ad9d4b67544 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:57:32.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3592" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3576,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:57:32.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-391
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jun  2 21:57:32.577: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:57:38.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-391" for this suite.

• [SLOW TEST:6.062 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":164,"skipped":3591,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:57:38.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3185
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jun  2 21:57:38.700: INFO: The status of Pod annotationupdate49e5e358-9fc6-4f51-aca6-4aff01dcfd83 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 21:57:40.763: INFO: The status of Pod annotationupdate49e5e358-9fc6-4f51-aca6-4aff01dcfd83 is Running (Ready = true)
Jun  2 21:57:41.357: INFO: Successfully updated pod "annotationupdate49e5e358-9fc6-4f51-aca6-4aff01dcfd83"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:57:43.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3185" for this suite.

• [SLOW TEST:5.016 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":165,"skipped":3592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:57:43.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6585
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:58:11.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6585" for this suite.

• [SLOW TEST:28.499 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":166,"skipped":3628,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:58:11.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7972
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  2 21:58:12.221: INFO: Waiting up to 5m0s for pod "pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663" in namespace "emptydir-7972" to be "Succeeded or Failed"
Jun  2 21:58:12.268: INFO: Pod "pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663": Phase="Pending", Reason="", readiness=false. Elapsed: 46.505215ms
Jun  2 21:58:14.293: INFO: Pod "pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071280158s
Jun  2 21:58:16.315: INFO: Pod "pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093567919s
Jun  2 21:58:18.345: INFO: Pod "pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.123532415s
STEP: Saw pod success
Jun  2 21:58:18.345: INFO: Pod "pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663" satisfied condition "Succeeded or Failed"
Jun  2 21:58:18.368: INFO: Trying to get logs from node 10.134.156.247 pod pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663 container test-container: <nil>
STEP: delete the pod
Jun  2 21:58:18.464: INFO: Waiting for pod pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663 to disappear
Jun  2 21:58:18.477: INFO: Pod pod-f04e5c81-2422-4e9a-98f0-6eea17ddd663 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:58:18.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7972" for this suite.

• [SLOW TEST:6.551 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":3638,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:58:18.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9021
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  2 21:58:18.732: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  2 21:58:18.767: INFO: Waiting for terminating namespaces to be deleted...
Jun  2 21:58:18.779: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.209 before test
Jun  2 21:58:18.848: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-qxhvw from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 21:58:18.848: INFO: calico-node-xc26s from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:58:18.848: INFO: calico-typha-7684bb556f-zfmb4 from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:58:18.848: INFO: coredns-74bf9bd988-v6qcf from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:58:18.848: INFO: ibm-keepalived-watcher-5wwlj from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:58:18.848: INFO: ibm-master-proxy-static-10.134.156.209 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:58:18.848: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:58:18.848: INFO: konnectivity-agent-wmghg from kube-system started at 2022-06-02 19:25:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:58:18.848: INFO: metrics-server-6bc784d6c-ztp8q from kube-system started at 2022-06-02 20:01:37 +0000 UTC (3 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container config-watcher ready: true, restart count 0
Jun  2 21:58:18.848: INFO: 	Container metrics-server ready: true, restart count 0
Jun  2 21:58:18.848: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun  2 21:58:18.848: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-j82hw from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 21:58:18.848: INFO: sonobuoy-e2e-job-d730fbd8ba3a4c8f from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.848: INFO: 	Container e2e ready: true, restart count 0
Jun  2 21:58:18.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:58:18.849: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-kd2m7 from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:58:18.849: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 21:58:18.849: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.247 before test
Jun  2 21:58:18.907: INFO: calico-kube-controllers-648794b58-n4vlx from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  2 21:58:18.907: INFO: calico-node-l6vdp from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:58:18.907: INFO: calico-typha-7684bb556f-k9h4f from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:58:18.907: INFO: coredns-74bf9bd988-l2b48 from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:58:18.907: INFO: coredns-autoscaler-867cd8fddb-rxjf6 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container autoscaler ready: true, restart count 0
Jun  2 21:58:18.907: INFO: dashboard-metrics-scraper-7f68fbcb5b-j8jwr from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun  2 21:58:18.907: INFO: ibm-file-plugin-7795fdcfdb-cd4r2 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.907: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun  2 21:58:18.907: INFO: ibm-keepalived-watcher-t9x4p from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.908: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:58:18.908: INFO: ibm-master-proxy-static-10.134.156.247 from kube-system started at 2022-06-02 19:15:43 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.908: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:58:18.908: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:58:18.908: INFO: ibm-storage-watcher-75494444c9-jf6zn from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.908: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun  2 21:58:18.908: INFO: konnectivity-agent-tpmz2 from kube-system started at 2022-06-02 19:25:23 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.908: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:58:18.908: INFO: kubernetes-dashboard-7bbdd6b4cd-z6cv7 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.908: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  2 21:58:18.908: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-jfbjf from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:58:18.908: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 21:58:18.908: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.253 before test
Jun  2 21:58:18.972: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-02 19:19:01 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun  2 21:58:18.973: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-8hb9r from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 21:58:18.973: INFO: calico-node-xwlqr from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 21:58:18.973: INFO: calico-typha-7684bb556f-bpb2x from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 21:58:18.973: INFO: coredns-74bf9bd988-5wlnt from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container coredns ready: true, restart count 0
Jun  2 21:58:18.973: INFO: ibm-keepalived-watcher-mcrwm from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 21:58:18.973: INFO: ibm-master-proxy-static-10.134.156.253 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 21:58:18.973: INFO: 	Container pause ready: true, restart count 0
Jun  2 21:58:18.973: INFO: konnectivity-agent-swbk5 from kube-system started at 2022-06-02 19:25:19 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 21:58:18.973: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-t7zkd from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 21:58:18.973: INFO: sonobuoy from sonobuoy started at 2022-06-02 21:12:04 +0000 UTC (1 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  2 21:58:18.973: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-x9xlv from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 21:58:18.973: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 21:58:18.973: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a44e6414-45a8-4916-bc47-af3dc2014ade 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a44e6414-45a8-4916-bc47-af3dc2014ade off the node 10.134.156.247
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a44e6414-45a8-4916-bc47-af3dc2014ade
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:58:27.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9021" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.926 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":168,"skipped":3638,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:58:27.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4551
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-lbtp
STEP: Creating a pod to test atomic-volume-subpath
Jun  2 21:58:27.698: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lbtp" in namespace "subpath-4551" to be "Succeeded or Failed"
Jun  2 21:58:27.714: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Pending", Reason="", readiness=false. Elapsed: 15.614402ms
Jun  2 21:58:29.769: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070077259s
Jun  2 21:58:31.806: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 4.107034465s
Jun  2 21:58:33.830: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 6.131660495s
Jun  2 21:58:35.882: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 8.183404047s
Jun  2 21:58:37.922: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 10.223208919s
Jun  2 21:58:39.950: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 12.251274781s
Jun  2 21:58:41.971: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 14.272310624s
Jun  2 21:58:43.995: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 16.296002908s
Jun  2 21:58:46.040: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 18.341476919s
Jun  2 21:58:48.061: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 20.362279455s
Jun  2 21:58:50.099: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=true. Elapsed: 22.400436461s
Jun  2 21:58:52.134: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Running", Reason="", readiness=false. Elapsed: 24.434983983s
Jun  2 21:58:54.164: INFO: Pod "pod-subpath-test-secret-lbtp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.465582257s
STEP: Saw pod success
Jun  2 21:58:54.164: INFO: Pod "pod-subpath-test-secret-lbtp" satisfied condition "Succeeded or Failed"
Jun  2 21:58:54.174: INFO: Trying to get logs from node 10.134.156.253 pod pod-subpath-test-secret-lbtp container test-container-subpath-secret-lbtp: <nil>
STEP: delete the pod
Jun  2 21:58:54.327: INFO: Waiting for pod pod-subpath-test-secret-lbtp to disappear
Jun  2 21:58:54.340: INFO: Pod pod-subpath-test-secret-lbtp no longer exists
STEP: Deleting pod pod-subpath-test-secret-lbtp
Jun  2 21:58:54.340: INFO: Deleting pod "pod-subpath-test-secret-lbtp" in namespace "subpath-4551"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:58:54.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4551" for this suite.

• [SLOW TEST:26.988 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":169,"skipped":3644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:58:54.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4829
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:58:55.312: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 21:58:57.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 21, 58, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 58, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 21, 58, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 21, 58, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:59:00.425: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:59:01.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4829" for this suite.
STEP: Destroying namespace "webhook-4829-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.915 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":170,"skipped":3683,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:59:01.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8240
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
Jun  2 21:59:01.582: INFO: Creating e2e-svc-a-qkfgc
Jun  2 21:59:01.625: INFO: Creating e2e-svc-b-d46lk
Jun  2 21:59:01.657: INFO: Creating e2e-svc-c-gs224
STEP: deleting service collection
Jun  2 21:59:01.983: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:59:01.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8240" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":171,"skipped":3697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:59:02.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9760
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 21:59:03.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 21:59:06.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Jun  2 21:59:11.399: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:59:11.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9760" for this suite.
STEP: Destroying namespace "webhook-9760-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.935 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":172,"skipped":3722,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:59:11.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4224
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-921d1aa2-118b-4d68-a57d-5ff83d31c255
STEP: Creating a pod to test consume secrets
Jun  2 21:59:12.345: INFO: Waiting up to 5m0s for pod "pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb" in namespace "secrets-4224" to be "Succeeded or Failed"
Jun  2 21:59:12.361: INFO: Pod "pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.028623ms
Jun  2 21:59:14.390: INFO: Pod "pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044627027s
Jun  2 21:59:16.415: INFO: Pod "pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069888405s
Jun  2 21:59:18.473: INFO: Pod "pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.128298143s
STEP: Saw pod success
Jun  2 21:59:18.474: INFO: Pod "pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb" satisfied condition "Succeeded or Failed"
Jun  2 21:59:18.489: INFO: Trying to get logs from node 10.134.156.247 pod pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb container secret-env-test: <nil>
STEP: delete the pod
Jun  2 21:59:18.591: INFO: Waiting for pod pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb to disappear
Jun  2 21:59:18.603: INFO: Pod pod-secrets-9d6b6ae0-2a7d-441e-b185-caff2999adbb no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 21:59:18.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4224" for this suite.

• [SLOW TEST:6.653 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3730,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 21:59:18.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2244
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun  2 21:59:59.189: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  2 21:59:59.189: INFO: Deleting pod "simpletest.rc-242vq" in namespace "gc-2244"
W0602 21:59:59.188961      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun  2 21:59:59.291: INFO: Deleting pod "simpletest.rc-26jq9" in namespace "gc-2244"
Jun  2 21:59:59.362: INFO: Deleting pod "simpletest.rc-2jfn7" in namespace "gc-2244"
Jun  2 21:59:59.407: INFO: Deleting pod "simpletest.rc-2krpf" in namespace "gc-2244"
Jun  2 21:59:59.482: INFO: Deleting pod "simpletest.rc-2n8lw" in namespace "gc-2244"
Jun  2 21:59:59.560: INFO: Deleting pod "simpletest.rc-2pvd4" in namespace "gc-2244"
Jun  2 21:59:59.647: INFO: Deleting pod "simpletest.rc-2sgcd" in namespace "gc-2244"
Jun  2 21:59:59.732: INFO: Deleting pod "simpletest.rc-2xmk5" in namespace "gc-2244"
Jun  2 21:59:59.768: INFO: Deleting pod "simpletest.rc-4658m" in namespace "gc-2244"
Jun  2 21:59:59.826: INFO: Deleting pod "simpletest.rc-4jtxx" in namespace "gc-2244"
Jun  2 21:59:59.905: INFO: Deleting pod "simpletest.rc-5f4pn" in namespace "gc-2244"
Jun  2 21:59:59.960: INFO: Deleting pod "simpletest.rc-5nnj6" in namespace "gc-2244"
Jun  2 22:00:00.000: INFO: Deleting pod "simpletest.rc-5q8d4" in namespace "gc-2244"
Jun  2 22:00:00.097: INFO: Deleting pod "simpletest.rc-5qdnv" in namespace "gc-2244"
Jun  2 22:00:00.138: INFO: Deleting pod "simpletest.rc-5tn9s" in namespace "gc-2244"
Jun  2 22:00:00.250: INFO: Deleting pod "simpletest.rc-665fs" in namespace "gc-2244"
Jun  2 22:00:00.354: INFO: Deleting pod "simpletest.rc-6vqrf" in namespace "gc-2244"
Jun  2 22:00:00.411: INFO: Deleting pod "simpletest.rc-75b4c" in namespace "gc-2244"
Jun  2 22:00:00.544: INFO: Deleting pod "simpletest.rc-7b9x8" in namespace "gc-2244"
Jun  2 22:00:00.666: INFO: Deleting pod "simpletest.rc-7cm2c" in namespace "gc-2244"
Jun  2 22:00:00.699: INFO: Deleting pod "simpletest.rc-7fqkl" in namespace "gc-2244"
Jun  2 22:00:00.788: INFO: Deleting pod "simpletest.rc-7hmnp" in namespace "gc-2244"
Jun  2 22:00:00.851: INFO: Deleting pod "simpletest.rc-7jxgn" in namespace "gc-2244"
Jun  2 22:00:00.986: INFO: Deleting pod "simpletest.rc-7m759" in namespace "gc-2244"
Jun  2 22:00:01.117: INFO: Deleting pod "simpletest.rc-7vf65" in namespace "gc-2244"
Jun  2 22:00:01.205: INFO: Deleting pod "simpletest.rc-8spml" in namespace "gc-2244"
Jun  2 22:00:01.309: INFO: Deleting pod "simpletest.rc-9sqpg" in namespace "gc-2244"
Jun  2 22:00:01.513: INFO: Deleting pod "simpletest.rc-9xsgt" in namespace "gc-2244"
Jun  2 22:00:01.673: INFO: Deleting pod "simpletest.rc-9z4w9" in namespace "gc-2244"
Jun  2 22:00:01.831: INFO: Deleting pod "simpletest.rc-bsx9r" in namespace "gc-2244"
Jun  2 22:00:01.890: INFO: Deleting pod "simpletest.rc-btv99" in namespace "gc-2244"
Jun  2 22:00:02.048: INFO: Deleting pod "simpletest.rc-bxvbt" in namespace "gc-2244"
Jun  2 22:00:02.157: INFO: Deleting pod "simpletest.rc-cw9qz" in namespace "gc-2244"
Jun  2 22:00:02.238: INFO: Deleting pod "simpletest.rc-dgpdr" in namespace "gc-2244"
Jun  2 22:00:02.286: INFO: Deleting pod "simpletest.rc-dhqw7" in namespace "gc-2244"
Jun  2 22:00:02.487: INFO: Deleting pod "simpletest.rc-g5lng" in namespace "gc-2244"
Jun  2 22:00:02.568: INFO: Deleting pod "simpletest.rc-g6nh5" in namespace "gc-2244"
Jun  2 22:00:02.660: INFO: Deleting pod "simpletest.rc-ggn8w" in namespace "gc-2244"
Jun  2 22:00:02.760: INFO: Deleting pod "simpletest.rc-gtrcf" in namespace "gc-2244"
Jun  2 22:00:02.823: INFO: Deleting pod "simpletest.rc-h2szg" in namespace "gc-2244"
Jun  2 22:00:02.965: INFO: Deleting pod "simpletest.rc-hd2fw" in namespace "gc-2244"
Jun  2 22:00:03.030: INFO: Deleting pod "simpletest.rc-hjwfr" in namespace "gc-2244"
Jun  2 22:00:03.130: INFO: Deleting pod "simpletest.rc-hlf2g" in namespace "gc-2244"
Jun  2 22:00:03.169: INFO: Deleting pod "simpletest.rc-hpnlz" in namespace "gc-2244"
Jun  2 22:00:03.246: INFO: Deleting pod "simpletest.rc-hrq9t" in namespace "gc-2244"
Jun  2 22:00:03.448: INFO: Deleting pod "simpletest.rc-jkk72" in namespace "gc-2244"
Jun  2 22:00:03.509: INFO: Deleting pod "simpletest.rc-jqmpx" in namespace "gc-2244"
Jun  2 22:00:03.565: INFO: Deleting pod "simpletest.rc-jqpsw" in namespace "gc-2244"
Jun  2 22:00:03.612: INFO: Deleting pod "simpletest.rc-k4dhf" in namespace "gc-2244"
Jun  2 22:00:03.656: INFO: Deleting pod "simpletest.rc-k5xd5" in namespace "gc-2244"
Jun  2 22:00:03.695: INFO: Deleting pod "simpletest.rc-k7jbq" in namespace "gc-2244"
Jun  2 22:00:03.732: INFO: Deleting pod "simpletest.rc-kcjxj" in namespace "gc-2244"
Jun  2 22:00:03.795: INFO: Deleting pod "simpletest.rc-kf6xn" in namespace "gc-2244"
Jun  2 22:00:03.836: INFO: Deleting pod "simpletest.rc-kg2sf" in namespace "gc-2244"
Jun  2 22:00:03.912: INFO: Deleting pod "simpletest.rc-khbtk" in namespace "gc-2244"
Jun  2 22:00:03.992: INFO: Deleting pod "simpletest.rc-krfh6" in namespace "gc-2244"
Jun  2 22:00:04.036: INFO: Deleting pod "simpletest.rc-kwc57" in namespace "gc-2244"
Jun  2 22:00:04.111: INFO: Deleting pod "simpletest.rc-l6m4c" in namespace "gc-2244"
Jun  2 22:00:04.165: INFO: Deleting pod "simpletest.rc-lbmf2" in namespace "gc-2244"
Jun  2 22:00:04.256: INFO: Deleting pod "simpletest.rc-lmfdr" in namespace "gc-2244"
Jun  2 22:00:04.369: INFO: Deleting pod "simpletest.rc-lw4nq" in namespace "gc-2244"
Jun  2 22:00:04.436: INFO: Deleting pod "simpletest.rc-mspx2" in namespace "gc-2244"
Jun  2 22:00:04.548: INFO: Deleting pod "simpletest.rc-n5ngb" in namespace "gc-2244"
Jun  2 22:00:04.662: INFO: Deleting pod "simpletest.rc-ncw66" in namespace "gc-2244"
Jun  2 22:00:04.691: INFO: Deleting pod "simpletest.rc-nlffs" in namespace "gc-2244"
Jun  2 22:00:04.751: INFO: Deleting pod "simpletest.rc-npsh9" in namespace "gc-2244"
Jun  2 22:00:04.835: INFO: Deleting pod "simpletest.rc-nrjww" in namespace "gc-2244"
Jun  2 22:00:04.940: INFO: Deleting pod "simpletest.rc-nwzkj" in namespace "gc-2244"
Jun  2 22:00:05.012: INFO: Deleting pod "simpletest.rc-p6mr5" in namespace "gc-2244"
Jun  2 22:00:05.059: INFO: Deleting pod "simpletest.rc-pvlhl" in namespace "gc-2244"
Jun  2 22:00:05.123: INFO: Deleting pod "simpletest.rc-pwclz" in namespace "gc-2244"
Jun  2 22:00:05.152: INFO: Deleting pod "simpletest.rc-px2qr" in namespace "gc-2244"
Jun  2 22:00:05.193: INFO: Deleting pod "simpletest.rc-pzkt2" in namespace "gc-2244"
Jun  2 22:00:05.268: INFO: Deleting pod "simpletest.rc-pzmx5" in namespace "gc-2244"
Jun  2 22:00:05.298: INFO: Deleting pod "simpletest.rc-q7kjk" in namespace "gc-2244"
Jun  2 22:00:05.334: INFO: Deleting pod "simpletest.rc-qgrb4" in namespace "gc-2244"
Jun  2 22:00:05.368: INFO: Deleting pod "simpletest.rc-qp96j" in namespace "gc-2244"
Jun  2 22:00:05.433: INFO: Deleting pod "simpletest.rc-r5zmx" in namespace "gc-2244"
Jun  2 22:00:05.480: INFO: Deleting pod "simpletest.rc-rh6qc" in namespace "gc-2244"
Jun  2 22:00:05.517: INFO: Deleting pod "simpletest.rc-sbbpg" in namespace "gc-2244"
Jun  2 22:00:05.552: INFO: Deleting pod "simpletest.rc-sjl25" in namespace "gc-2244"
Jun  2 22:00:05.598: INFO: Deleting pod "simpletest.rc-sprc4" in namespace "gc-2244"
Jun  2 22:00:05.693: INFO: Deleting pod "simpletest.rc-tbv2l" in namespace "gc-2244"
Jun  2 22:00:05.727: INFO: Deleting pod "simpletest.rc-tjnn7" in namespace "gc-2244"
Jun  2 22:00:05.754: INFO: Deleting pod "simpletest.rc-txmkr" in namespace "gc-2244"
Jun  2 22:00:05.815: INFO: Deleting pod "simpletest.rc-v8sxk" in namespace "gc-2244"
Jun  2 22:00:05.849: INFO: Deleting pod "simpletest.rc-vhwr2" in namespace "gc-2244"
Jun  2 22:00:05.915: INFO: Deleting pod "simpletest.rc-vndz5" in namespace "gc-2244"
Jun  2 22:00:05.990: INFO: Deleting pod "simpletest.rc-wcf8h" in namespace "gc-2244"
Jun  2 22:00:06.041: INFO: Deleting pod "simpletest.rc-wmjll" in namespace "gc-2244"
Jun  2 22:00:06.113: INFO: Deleting pod "simpletest.rc-wsqvh" in namespace "gc-2244"
Jun  2 22:00:06.146: INFO: Deleting pod "simpletest.rc-xcxvg" in namespace "gc-2244"
Jun  2 22:00:06.198: INFO: Deleting pod "simpletest.rc-xqfm2" in namespace "gc-2244"
Jun  2 22:00:06.246: INFO: Deleting pod "simpletest.rc-xx896" in namespace "gc-2244"
Jun  2 22:00:06.344: INFO: Deleting pod "simpletest.rc-xzc2k" in namespace "gc-2244"
Jun  2 22:00:06.390: INFO: Deleting pod "simpletest.rc-z6q6k" in namespace "gc-2244"
Jun  2 22:00:06.463: INFO: Deleting pod "simpletest.rc-z7qfc" in namespace "gc-2244"
Jun  2 22:00:06.548: INFO: Deleting pod "simpletest.rc-zl28r" in namespace "gc-2244"
Jun  2 22:00:06.623: INFO: Deleting pod "simpletest.rc-zl4bn" in namespace "gc-2244"
Jun  2 22:00:06.717: INFO: Deleting pod "simpletest.rc-zsdhr" in namespace "gc-2244"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:00:06.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2244" for this suite.

• [SLOW TEST:48.176 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":174,"skipped":3731,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:00:06.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7180
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:00:07.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  2 22:00:10.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-7180 --namespace=crd-publish-openapi-7180 create -f -'
Jun  2 22:00:11.781: INFO: stderr: ""
Jun  2 22:00:11.781: INFO: stdout: "e2e-test-crd-publish-openapi-2044-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  2 22:00:11.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-7180 --namespace=crd-publish-openapi-7180 delete e2e-test-crd-publish-openapi-2044-crds test-cr'
Jun  2 22:00:11.896: INFO: stderr: ""
Jun  2 22:00:11.896: INFO: stdout: "e2e-test-crd-publish-openapi-2044-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  2 22:00:11.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-7180 --namespace=crd-publish-openapi-7180 apply -f -'
Jun  2 22:00:12.838: INFO: stderr: ""
Jun  2 22:00:12.838: INFO: stdout: "e2e-test-crd-publish-openapi-2044-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  2 22:00:12.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-7180 --namespace=crd-publish-openapi-7180 delete e2e-test-crd-publish-openapi-2044-crds test-cr'
Jun  2 22:00:12.989: INFO: stderr: ""
Jun  2 22:00:12.989: INFO: stdout: "e2e-test-crd-publish-openapi-2044-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  2 22:00:12.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-7180 explain e2e-test-crd-publish-openapi-2044-crds'
Jun  2 22:00:13.654: INFO: stderr: ""
Jun  2 22:00:13.654: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2044-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:00:17.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7180" for this suite.

• [SLOW TEST:10.428 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":175,"skipped":3740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:00:17.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2817
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun  2 22:00:17.553: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun  2 22:00:17.561: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  }]
Jun  2 22:00:17.656: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  }]
Jun  2 22:00:18.924: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  }]
Jun  2 22:00:20.255: INFO: Found Pod pod-test in namespace pods-2817 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-06-02 22:00:17 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jun  2 22:00:20.303: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun  2 22:00:20.407: INFO: observed event type ADDED
Jun  2 22:00:20.407: INFO: observed event type MODIFIED
Jun  2 22:00:20.408: INFO: observed event type MODIFIED
Jun  2 22:00:20.408: INFO: observed event type MODIFIED
Jun  2 22:00:20.408: INFO: observed event type MODIFIED
Jun  2 22:00:20.409: INFO: observed event type MODIFIED
Jun  2 22:00:20.410: INFO: observed event type MODIFIED
Jun  2 22:00:20.410: INFO: observed event type MODIFIED
Jun  2 22:00:22.286: INFO: observed event type MODIFIED
Jun  2 22:00:22.587: INFO: observed event type MODIFIED
Jun  2 22:00:23.280: INFO: observed event type MODIFIED
Jun  2 22:00:23.282: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:00:23.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2817" for this suite.

• [SLOW TEST:6.096 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":176,"skipped":3763,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:00:23.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8848
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:00:27.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8848" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3765,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:00:27.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3656
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3656
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Jun  2 22:00:28.048: INFO: Found 0 stateful pods, waiting for 3
Jun  2 22:00:38.077: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:00:38.077: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:00:38.077: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:00:38.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3656 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 22:00:38.470: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 22:00:38.470: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 22:00:38.470: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jun  2 22:00:48.590: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun  2 22:00:58.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3656 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:00:59.016: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  2 22:00:59.016: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 22:00:59.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 22:01:19.165: INFO: Waiting for StatefulSet statefulset-3656/ss2 to complete update
STEP: Rolling back to a previous revision
Jun  2 22:01:29.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3656 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 22:01:29.577: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 22:01:29.577: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 22:01:29.577: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 22:01:39.701: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun  2 22:01:49.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-3656 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:01:50.115: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  2 22:01:50.115: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 22:01:50.115: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 22:02:00.237: INFO: Waiting for StatefulSet statefulset-3656/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 22:02:10.288: INFO: Deleting all statefulset in ns statefulset-3656
Jun  2 22:02:10.299: INFO: Scaling statefulset ss2 to 0
Jun  2 22:02:20.413: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 22:02:20.429: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:02:20.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3656" for this suite.

• [SLOW TEST:112.949 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":178,"skipped":3777,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:02:20.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4977
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-4977
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  2 22:02:20.899: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  2 22:02:20.989: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:02:23.017: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:02:25.020: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:27.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:29.012: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:31.013: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:33.025: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:35.007: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:37.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:39.010: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:41.015: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:02:43.016: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  2 22:02:43.043: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  2 22:02:43.065: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  2 22:02:45.211: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  2 22:02:45.211: INFO: Going to poll 172.30.170.143 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  2 22:02:45.221: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.170.143 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4977 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:02:45.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:02:45.222: INFO: ExecWithOptions: Clientset creation
Jun  2 22:02:45.222: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4977/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.170.143+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:02:46.447: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  2 22:02:46.448: INFO: Going to poll 172.30.118.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  2 22:02:46.473: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.118.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4977 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:02:46.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:02:46.475: INFO: ExecWithOptions: Clientset creation
Jun  2 22:02:46.475: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4977/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.118.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:02:47.683: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  2 22:02:47.683: INFO: Going to poll 172.30.220.237 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  2 22:02:47.733: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.220.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4977 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:02:47.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:02:47.734: INFO: ExecWithOptions: Clientset creation
Jun  2 22:02:47.735: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4977/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.220.237+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:02:48.981: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:02:48.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4977" for this suite.

• [SLOW TEST:28.320 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3781,"failed":0}
S
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:02:49.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9338
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:02:49.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9338" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":180,"skipped":3782,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:02:49.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2635
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun  2 22:02:49.704: INFO: Pod name pod-release: Found 0 pods out of 1
Jun  2 22:02:54.724: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:02:55.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2635" for this suite.

• [SLOW TEST:6.399 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":181,"skipped":3791,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:02:55.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3687
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun  2 22:02:56.081: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38120 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:02:56.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38120 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun  2 22:02:56.104: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38121 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:02:56.104: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38121 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun  2 22:02:56.128: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38122 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:02:56.128: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38122 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun  2 22:02:56.149: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38123 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:02:56.149: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3687  4e460b43-3208-4e9e-8acd-d0faaf477b60 38123 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun  2 22:02:56.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3687  e4faaf72-2337-4c9e-a786-f48224e702bd 38124 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:02:56.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3687  e4faaf72-2337-4c9e-a786-f48224e702bd 38124 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun  2 22:03:06.233: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3687  e4faaf72-2337-4c9e-a786-f48224e702bd 38173 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:03:06.234: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3687  e4faaf72-2337-4c9e-a786-f48224e702bd 38173 0 2022-06-02 22:02:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-06-02 22:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:03:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3687" for this suite.

• [SLOW TEST:20.432 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":182,"skipped":3791,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:03:16.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9759
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-7498fcdb-4f91-42e7-843d-763a91de9508 in namespace container-probe-9759
Jun  2 22:03:20.594: INFO: Started pod liveness-7498fcdb-4f91-42e7-843d-763a91de9508 in namespace container-probe-9759
STEP: checking the pod's current state and verifying that restartCount is present
Jun  2 22:03:20.607: INFO: Initial restart count of pod liveness-7498fcdb-4f91-42e7-843d-763a91de9508 is 0
Jun  2 22:03:38.833: INFO: Restart count of pod container-probe-9759/liveness-7498fcdb-4f91-42e7-843d-763a91de9508 is now 1 (18.226410124s elapsed)
Jun  2 22:03:59.145: INFO: Restart count of pod container-probe-9759/liveness-7498fcdb-4f91-42e7-843d-763a91de9508 is now 2 (38.538162072s elapsed)
Jun  2 22:04:17.378: INFO: Restart count of pod container-probe-9759/liveness-7498fcdb-4f91-42e7-843d-763a91de9508 is now 3 (56.771051398s elapsed)
Jun  2 22:04:37.629: INFO: Restart count of pod container-probe-9759/liveness-7498fcdb-4f91-42e7-843d-763a91de9508 is now 4 (1m17.022538951s elapsed)
Jun  2 22:05:48.604: INFO: Restart count of pod container-probe-9759/liveness-7498fcdb-4f91-42e7-843d-763a91de9508 is now 5 (2m27.997491744s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:05:48.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9759" for this suite.

• [SLOW TEST:152.425 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3795,"failed":0}
S
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:05:48.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8935
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
Jun  2 22:05:48.982: INFO: Waiting up to 5m0s for pod "client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf" in namespace "containers-8935" to be "Succeeded or Failed"
Jun  2 22:05:48.994: INFO: Pod "client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.368106ms
Jun  2 22:05:51.020: INFO: Pod "client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038218907s
Jun  2 22:05:53.042: INFO: Pod "client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059488034s
Jun  2 22:05:55.089: INFO: Pod "client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107147454s
STEP: Saw pod success
Jun  2 22:05:55.089: INFO: Pod "client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf" satisfied condition "Succeeded or Failed"
Jun  2 22:05:55.102: INFO: Trying to get logs from node 10.134.156.247 pod client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:05:55.255: INFO: Waiting for pod client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf to disappear
Jun  2 22:05:55.266: INFO: Pod client-containers-17236350-5a8a-41b3-b388-2fea724ecbdf no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:05:55.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8935" for this suite.

• [SLOW TEST:6.599 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3796,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:05:55.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1139
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1139.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1139.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1139.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1139.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1139.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  2 22:05:59.765: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:05:59.816: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:05:59.885: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:05:59.908: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:05:59.926: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:05:59.942: INFO: Lookups using dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1139.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local jessie_udp@dns-test-service-2.dns-1139.svc.cluster.local]

Jun  2 22:06:04.976: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:06:05.049: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local from pod dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb: the server could not find the requested resource (get pods dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb)
Jun  2 22:06:05.080: INFO: Lookups using dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1139.svc.cluster.local]

Jun  2 22:06:10.102: INFO: DNS probes using dns-1139/dns-test-2a6f8435-8d51-4ca2-a560-be2db87939fb succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:06:10.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1139" for this suite.

• [SLOW TEST:14.914 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":185,"skipped":3817,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:06:10.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3092
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:06:26.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3092" for this suite.

• [SLOW TEST:16.710 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":186,"skipped":3830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:06:26.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-2234
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  2 22:06:27.260: INFO: starting watch
STEP: patching
STEP: updating
Jun  2 22:06:27.299: INFO: waiting for watch events with expected annotations
Jun  2 22:06:27.299: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:06:27.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2234" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":187,"skipped":3852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:06:27.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5425
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-2a6f4ac7-09a9-43b8-bbe4-265aa845508b
STEP: Creating a pod to test consume configMaps
Jun  2 22:06:27.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54" in namespace "configmap-5425" to be "Succeeded or Failed"
Jun  2 22:06:27.700: INFO: Pod "pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54": Phase="Pending", Reason="", readiness=false. Elapsed: 10.172912ms
Jun  2 22:06:29.713: INFO: Pod "pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023386183s
Jun  2 22:06:31.751: INFO: Pod "pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061459758s
Jun  2 22:06:33.769: INFO: Pod "pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079250328s
STEP: Saw pod success
Jun  2 22:06:33.769: INFO: Pod "pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54" satisfied condition "Succeeded or Failed"
Jun  2 22:06:33.778: INFO: Trying to get logs from node 10.134.156.253 pod pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:06:33.931: INFO: Waiting for pod pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54 to disappear
Jun  2 22:06:33.942: INFO: Pod pod-configmaps-54cebe09-3582-42eb-bdfa-81b17252ee54 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:06:33.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5425" for this suite.

• [SLOW TEST:6.557 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3930,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:06:33.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3496
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  2 22:06:34.203: INFO: Waiting up to 5m0s for pod "pod-af99a91d-97ba-4794-a768-3a99cabb2b57" in namespace "emptydir-3496" to be "Succeeded or Failed"
Jun  2 22:06:34.254: INFO: Pod "pod-af99a91d-97ba-4794-a768-3a99cabb2b57": Phase="Pending", Reason="", readiness=false. Elapsed: 50.89511ms
Jun  2 22:06:36.276: INFO: Pod "pod-af99a91d-97ba-4794-a768-3a99cabb2b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072966983s
Jun  2 22:06:38.304: INFO: Pod "pod-af99a91d-97ba-4794-a768-3a99cabb2b57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100865443s
Jun  2 22:06:40.325: INFO: Pod "pod-af99a91d-97ba-4794-a768-3a99cabb2b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.121705292s
STEP: Saw pod success
Jun  2 22:06:40.326: INFO: Pod "pod-af99a91d-97ba-4794-a768-3a99cabb2b57" satisfied condition "Succeeded or Failed"
Jun  2 22:06:40.336: INFO: Trying to get logs from node 10.134.156.247 pod pod-af99a91d-97ba-4794-a768-3a99cabb2b57 container test-container: <nil>
STEP: delete the pod
Jun  2 22:06:40.505: INFO: Waiting for pod pod-af99a91d-97ba-4794-a768-3a99cabb2b57 to disappear
Jun  2 22:06:40.536: INFO: Pod pod-af99a91d-97ba-4794-a768-3a99cabb2b57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:06:40.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3496" for this suite.

• [SLOW TEST:6.642 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3963,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:06:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2016
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-454b1b34-90cd-445c-9aae-0d37b485c8f5 in namespace container-probe-2016
Jun  2 22:06:44.974: INFO: Started pod liveness-454b1b34-90cd-445c-9aae-0d37b485c8f5 in namespace container-probe-2016
STEP: checking the pod's current state and verifying that restartCount is present
Jun  2 22:06:44.984: INFO: Initial restart count of pod liveness-454b1b34-90cd-445c-9aae-0d37b485c8f5 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:10:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2016" for this suite.

• [SLOW TEST:245.075 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3974,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:10:45.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1628
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:10:45.980: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518" in namespace "downward-api-1628" to be "Succeeded or Failed"
Jun  2 22:10:45.992: INFO: Pod "downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 11.38708ms
Jun  2 22:10:48.021: INFO: Pod "downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040460586s
Jun  2 22:10:50.058: INFO: Pod "downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07806759s
STEP: Saw pod success
Jun  2 22:10:50.058: INFO: Pod "downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518" satisfied condition "Succeeded or Failed"
Jun  2 22:10:50.074: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518 container client-container: <nil>
STEP: delete the pod
Jun  2 22:10:50.194: INFO: Waiting for pod downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518 to disappear
Jun  2 22:10:50.207: INFO: Pod downwardapi-volume-8bf26dbe-f59f-4098-9c4d-3cd8b22f0518 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:10:50.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1628" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3991,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:10:50.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6678
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun  2 22:11:00.613: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
W0602 22:11:00.613031      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun  2 22:11:00.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6678" for this suite.

• [SLOW TEST:10.413 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":192,"skipped":3992,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:11:00.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7326
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-7326
Jun  2 22:11:00.936: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:11:02.959: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun  2 22:11:02.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun  2 22:11:03.514: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun  2 22:11:03.514: INFO: stdout: "iptables"
Jun  2 22:11:03.514: INFO: proxyMode: iptables
Jun  2 22:11:03.551: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun  2 22:11:03.563: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-7326
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7326
I0602 22:11:03.621317      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7326, replica count: 3
I0602 22:11:06.673503      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 22:11:06.709: INFO: Creating new exec pod
Jun  2 22:11:11.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec execpod-affinityvfskl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jun  2 22:11:12.125: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun  2 22:11:12.126: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:11:12.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec execpod-affinityvfskl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.139.68 80'
Jun  2 22:11:12.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.139.68 80\nConnection to 172.21.139.68 80 port [tcp/http] succeeded!\n"
Jun  2 22:11:12.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:11:12.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec execpod-affinityvfskl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.139.68:80/ ; done'
Jun  2 22:11:12.951: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n"
Jun  2 22:11:12.951: INFO: stdout: "\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl\naffinity-clusterip-timeout-8jbsl"
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.951: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Received response from host: affinity-clusterip-timeout-8jbsl
Jun  2 22:11:12.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec execpod-affinityvfskl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.139.68:80/'
Jun  2 22:11:13.241: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n"
Jun  2 22:11:13.241: INFO: stdout: "affinity-clusterip-timeout-8jbsl"
Jun  2 22:11:33.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec execpod-affinityvfskl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.139.68:80/'
Jun  2 22:11:33.623: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n"
Jun  2 22:11:33.623: INFO: stdout: "affinity-clusterip-timeout-8jbsl"
Jun  2 22:11:53.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7326 exec execpod-affinityvfskl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.139.68:80/'
Jun  2 22:11:53.969: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.139.68:80/\n"
Jun  2 22:11:53.969: INFO: stdout: "affinity-clusterip-timeout-tcdmm"
Jun  2 22:11:53.969: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7326, will wait for the garbage collector to delete the pods
Jun  2 22:11:54.130: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 20.095038ms
Jun  2 22:11:54.331: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 201.347275ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:11:57.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7326" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:56.399 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":193,"skipped":4002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:11:57.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3742
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-3742
STEP: creating service affinity-clusterip in namespace services-3742
STEP: creating replication controller affinity-clusterip in namespace services-3742
I0602 22:11:57.338564      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3742, replica count: 3
I0602 22:12:00.390108      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 22:12:00.423: INFO: Creating new exec pod
Jun  2 22:12:03.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3742 exec execpod-affinityg75dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun  2 22:12:03.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun  2 22:12:03.846: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:12:03.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3742 exec execpod-affinityg75dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.88.248 80'
Jun  2 22:12:04.156: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.88.248 80\nConnection to 172.21.88.248 80 port [tcp/http] succeeded!\n"
Jun  2 22:12:04.156: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:12:04.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-3742 exec execpod-affinityg75dq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.88.248:80/ ; done'
Jun  2 22:12:04.538: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.88.248:80/\n"
Jun  2 22:12:04.538: INFO: stdout: "\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz\naffinity-clusterip-b7vcz"
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Received response from host: affinity-clusterip-b7vcz
Jun  2 22:12:04.538: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3742, will wait for the garbage collector to delete the pods
Jun  2 22:12:04.704: INFO: Deleting ReplicationController affinity-clusterip took: 22.32362ms
Jun  2 22:12:04.805: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.083081ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:12:07.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3742" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.257 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":194,"skipped":4025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:12:07.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7763
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:12:07.509: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  2 22:12:07.534: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  2 22:12:12.560: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  2 22:12:12.561: INFO: Creating deployment "test-rolling-update-deployment"
Jun  2 22:12:12.574: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  2 22:12:12.600: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  2 22:12:14.642: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  2 22:12:14.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 12, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 12, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 12, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-796dbc4547\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:12:16.683: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 22:12:16.719: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7763  99f1c0ef-8a54-4b2d-8f15-6742407f525f 39519 1 2022-06-02 22:12:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-06-02 22:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035f1898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-02 22:12:12 +0000 UTC,LastTransitionTime:2022-06-02 22:12:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-06-02 22:12:15 +0000 UTC,LastTransitionTime:2022-06-02 22:12:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  2 22:12:16.732: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-7763  ea826414-6103-46a5-af9e-f1d61577036c 39509 1 2022-06-02 22:12:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 99f1c0ef-8a54-4b2d-8f15-6742407f525f 0xc0035f1d97 0xc0035f1d98}] []  [{kube-controller-manager Update apps/v1 2022-06-02 22:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99f1c0ef-8a54-4b2d-8f15-6742407f525f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:12:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0006aa048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:12:16.732: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  2 22:12:16.732: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7763  b87fa1d6-8a42-4ea8-9360-1623832bf54c 39518 2 2022-06-02 22:12:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 99f1c0ef-8a54-4b2d-8f15-6742407f525f 0xc0035f1c67 0xc0035f1c68}] []  [{e2e.test Update apps/v1 2022-06-02 22:12:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99f1c0ef-8a54-4b2d-8f15-6742407f525f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:12:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035f1d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:12:16.747: INFO: Pod "test-rolling-update-deployment-796dbc4547-gsbqc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-gsbqc test-rolling-update-deployment-796dbc4547- deployment-7763  a168b567-2a14-4094-b251-c2a8955ef082 39508 0 2022-06-02 22:12:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/containerID:262e7d9fce3ed2c933d5aad29f289c0643d2501e1d5147bfb216d8855cf5c242 cni.projectcalico.org/podIP:172.30.220.241/32 cni.projectcalico.org/podIPs:172.30.220.241/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 ea826414-6103-46a5-af9e-f1d61577036c 0xc0058236b7 0xc0058236b8}] []  [{kube-controller-manager Update v1 2022-06-02 22:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea826414-6103-46a5-af9e-f1d61577036c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 22:12:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 22:12:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-224nq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-224nq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:12:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:12:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.241,StartTime:2022-06-02 22:12:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 22:12:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://b0618fedeadeb010523d07d57b2d61d0fcc21340bd5cd1de77c5b8d69ba10a5e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:12:16.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7763" for this suite.

• [SLOW TEST:9.470 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":195,"skipped":4098,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:12:16.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1759
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jun  2 22:12:17.005: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  2 22:12:17.074: INFO: Waiting for terminating namespaces to be deleted...
Jun  2 22:12:17.085: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.209 before test
Jun  2 22:12:17.136: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-qxhvw from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 22:12:17.136: INFO: calico-node-xc26s from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 22:12:17.136: INFO: calico-typha-7684bb556f-zfmb4 from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 22:12:17.136: INFO: coredns-74bf9bd988-v6qcf from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container coredns ready: true, restart count 0
Jun  2 22:12:17.136: INFO: ibm-keepalived-watcher-5wwlj from kube-system started at 2022-06-02 19:16:11 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 22:12:17.136: INFO: ibm-master-proxy-static-10.134.156.209 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 22:12:17.136: INFO: 	Container pause ready: true, restart count 0
Jun  2 22:12:17.136: INFO: konnectivity-agent-wmghg from kube-system started at 2022-06-02 19:25:26 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 22:12:17.136: INFO: metrics-server-6bc784d6c-ztp8q from kube-system started at 2022-06-02 20:01:37 +0000 UTC (3 container statuses recorded)
Jun  2 22:12:17.136: INFO: 	Container config-watcher ready: true, restart count 0
Jun  2 22:12:17.136: INFO: 	Container metrics-server ready: true, restart count 0
Jun  2 22:12:17.136: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jun  2 22:12:17.136: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-j82hw from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.137: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 22:12:17.137: INFO: sonobuoy-e2e-job-d730fbd8ba3a4c8f from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.137: INFO: 	Container e2e ready: true, restart count 0
Jun  2 22:12:17.137: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 22:12:17.137: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-kd2m7 from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.137: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 22:12:17.137: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 22:12:17.137: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.247 before test
Jun  2 22:12:17.165: INFO: calico-kube-controllers-648794b58-n4vlx from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.166: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  2 22:12:17.166: INFO: calico-node-l6vdp from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.166: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 22:12:17.166: INFO: calico-typha-7684bb556f-k9h4f from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.166: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 22:12:17.166: INFO: coredns-74bf9bd988-l2b48 from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.166: INFO: 	Container coredns ready: true, restart count 0
Jun  2 22:12:17.167: INFO: coredns-autoscaler-867cd8fddb-rxjf6 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.167: INFO: 	Container autoscaler ready: true, restart count 0
Jun  2 22:12:17.167: INFO: dashboard-metrics-scraper-7f68fbcb5b-j8jwr from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.167: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jun  2 22:12:17.167: INFO: ibm-file-plugin-7795fdcfdb-cd4r2 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.167: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun  2 22:12:17.168: INFO: ibm-keepalived-watcher-t9x4p from kube-system started at 2022-06-02 19:16:01 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.168: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 22:12:17.168: INFO: ibm-master-proxy-static-10.134.156.247 from kube-system started at 2022-06-02 19:15:43 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.168: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 22:12:17.168: INFO: 	Container pause ready: true, restart count 0
Jun  2 22:12:17.168: INFO: ibm-storage-watcher-75494444c9-jf6zn from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.168: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun  2 22:12:17.169: INFO: konnectivity-agent-tpmz2 from kube-system started at 2022-06-02 19:25:23 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.169: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 22:12:17.169: INFO: kubernetes-dashboard-7bbdd6b4cd-z6cv7 from kube-system started at 2022-06-02 19:16:16 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.169: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jun  2 22:12:17.169: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-jfbjf from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 22:12:17.170: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  2 22:12:17.170: INFO: 
Logging pods the apiserver thinks is on node 10.134.156.253 before test
Jun  2 22:12:17.201: INFO: test-k8s-e2e-pvg-master-verification from default started at 2022-06-02 19:19:01 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.201: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun  2 22:12:17.201: INFO: test-rolling-update-deployment-796dbc4547-gsbqc from deployment-7763 started at 2022-06-02 22:12:12 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.201: INFO: 	Container agnhost ready: true, restart count 0
Jun  2 22:12:17.201: INFO: ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-8hb9r from ibm-system started at 2022-06-02 19:20:35 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.201: INFO: 	Container ibm-cloud-provider-ip-169-50-20-163 ready: true, restart count 0
Jun  2 22:12:17.201: INFO: calico-node-xwlqr from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.201: INFO: 	Container calico-node ready: true, restart count 0
Jun  2 22:12:17.201: INFO: calico-typha-7684bb556f-bpb2x from kube-system started at 2022-06-02 19:16:26 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.201: INFO: 	Container calico-typha ready: true, restart count 0
Jun  2 22:12:17.202: INFO: coredns-74bf9bd988-5wlnt from kube-system started at 2022-06-02 19:25:56 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container coredns ready: true, restart count 0
Jun  2 22:12:17.202: INFO: ibm-keepalived-watcher-mcrwm from kube-system started at 2022-06-02 19:16:04 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  2 22:12:17.202: INFO: ibm-master-proxy-static-10.134.156.253 from kube-system started at 2022-06-02 19:15:59 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  2 22:12:17.202: INFO: 	Container pause ready: true, restart count 0
Jun  2 22:12:17.202: INFO: konnectivity-agent-swbk5 from kube-system started at 2022-06-02 19:25:19 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jun  2 22:12:17.202: INFO: public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-t7zkd from kube-system started at 2022-06-02 19:20:28 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container nginx-ingress ready: true, restart count 0
Jun  2 22:12:17.202: INFO: sonobuoy from sonobuoy started at 2022-06-02 21:12:04 +0000 UTC (1 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  2 22:12:17.202: INFO: sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-x9xlv from sonobuoy started at 2022-06-02 21:12:10 +0000 UTC (2 container statuses recorded)
Jun  2 22:12:17.202: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  2 22:12:17.202: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node 10.134.156.209
STEP: verifying the node has the label node 10.134.156.247
STEP: verifying the node has the label node 10.134.156.253
Jun  2 22:12:17.506: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.134.156.253
Jun  2 22:12:17.506: INFO: Pod test-rolling-update-deployment-796dbc4547-gsbqc requesting resource cpu=0m on Node 10.134.156.253
Jun  2 22:12:17.506: INFO: Pod ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-8hb9r requesting resource cpu=5m on Node 10.134.156.253
Jun  2 22:12:17.506: INFO: Pod ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-qxhvw requesting resource cpu=5m on Node 10.134.156.209
Jun  2 22:12:17.507: INFO: Pod calico-kube-controllers-648794b58-n4vlx requesting resource cpu=10m on Node 10.134.156.247
Jun  2 22:12:17.507: INFO: Pod calico-node-l6vdp requesting resource cpu=250m on Node 10.134.156.247
Jun  2 22:12:17.507: INFO: Pod calico-node-xc26s requesting resource cpu=250m on Node 10.134.156.209
Jun  2 22:12:17.507: INFO: Pod calico-node-xwlqr requesting resource cpu=250m on Node 10.134.156.253
Jun  2 22:12:17.507: INFO: Pod calico-typha-7684bb556f-bpb2x requesting resource cpu=250m on Node 10.134.156.253
Jun  2 22:12:17.507: INFO: Pod calico-typha-7684bb556f-k9h4f requesting resource cpu=250m on Node 10.134.156.247
Jun  2 22:12:17.507: INFO: Pod calico-typha-7684bb556f-zfmb4 requesting resource cpu=250m on Node 10.134.156.209
Jun  2 22:12:17.508: INFO: Pod coredns-74bf9bd988-5wlnt requesting resource cpu=100m on Node 10.134.156.253
Jun  2 22:12:17.508: INFO: Pod coredns-74bf9bd988-l2b48 requesting resource cpu=100m on Node 10.134.156.247
Jun  2 22:12:17.508: INFO: Pod coredns-74bf9bd988-v6qcf requesting resource cpu=100m on Node 10.134.156.209
Jun  2 22:12:17.508: INFO: Pod coredns-autoscaler-867cd8fddb-rxjf6 requesting resource cpu=20m on Node 10.134.156.247
Jun  2 22:12:17.508: INFO: Pod dashboard-metrics-scraper-7f68fbcb5b-j8jwr requesting resource cpu=1m on Node 10.134.156.247
Jun  2 22:12:17.508: INFO: Pod ibm-file-plugin-7795fdcfdb-cd4r2 requesting resource cpu=50m on Node 10.134.156.247
Jun  2 22:12:17.508: INFO: Pod ibm-keepalived-watcher-5wwlj requesting resource cpu=5m on Node 10.134.156.209
Jun  2 22:12:17.509: INFO: Pod ibm-keepalived-watcher-mcrwm requesting resource cpu=5m on Node 10.134.156.253
Jun  2 22:12:17.509: INFO: Pod ibm-keepalived-watcher-t9x4p requesting resource cpu=5m on Node 10.134.156.247
Jun  2 22:12:17.509: INFO: Pod ibm-master-proxy-static-10.134.156.209 requesting resource cpu=25m on Node 10.134.156.209
Jun  2 22:12:17.509: INFO: Pod ibm-master-proxy-static-10.134.156.247 requesting resource cpu=25m on Node 10.134.156.247
Jun  2 22:12:17.510: INFO: Pod ibm-master-proxy-static-10.134.156.253 requesting resource cpu=25m on Node 10.134.156.253
Jun  2 22:12:17.510: INFO: Pod ibm-storage-watcher-75494444c9-jf6zn requesting resource cpu=50m on Node 10.134.156.247
Jun  2 22:12:17.510: INFO: Pod konnectivity-agent-swbk5 requesting resource cpu=10m on Node 10.134.156.253
Jun  2 22:12:17.510: INFO: Pod konnectivity-agent-tpmz2 requesting resource cpu=10m on Node 10.134.156.247
Jun  2 22:12:17.510: INFO: Pod konnectivity-agent-wmghg requesting resource cpu=10m on Node 10.134.156.209
Jun  2 22:12:17.510: INFO: Pod kubernetes-dashboard-7bbdd6b4cd-z6cv7 requesting resource cpu=50m on Node 10.134.156.247
Jun  2 22:12:17.510: INFO: Pod metrics-server-6bc784d6c-ztp8q requesting resource cpu=126m on Node 10.134.156.209
Jun  2 22:12:17.511: INFO: Pod public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-j82hw requesting resource cpu=10m on Node 10.134.156.209
Jun  2 22:12:17.511: INFO: Pod public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-t7zkd requesting resource cpu=10m on Node 10.134.156.253
Jun  2 22:12:17.511: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.134.156.253
Jun  2 22:12:17.511: INFO: Pod sonobuoy-e2e-job-d730fbd8ba3a4c8f requesting resource cpu=0m on Node 10.134.156.209
Jun  2 22:12:17.511: INFO: Pod sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-jfbjf requesting resource cpu=0m on Node 10.134.156.247
Jun  2 22:12:17.511: INFO: Pod sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-kd2m7 requesting resource cpu=0m on Node 10.134.156.209
Jun  2 22:12:17.511: INFO: Pod sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-x9xlv requesting resource cpu=0m on Node 10.134.156.253
STEP: Starting Pods to consume most of the cluster CPU.
Jun  2 22:12:17.512: INFO: Creating a pod which consumes cpu=2190m on Node 10.134.156.209
Jun  2 22:12:17.534: INFO: Creating a pod which consumes cpu=2162m on Node 10.134.156.247
Jun  2 22:12:17.564: INFO: Creating a pod which consumes cpu=2278m on Node 10.134.156.253
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170.16f4ed5ada3c74e7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1759/filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170 to 10.134.156.253]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170.16f4ed5b2a2fa729], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170.16f4ed5b2e495f28], Reason = [Created], Message = [Created container filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170.16f4ed5b39ea4655], Reason = [Started], Message = [Started container filler-pod-3988ae7a-820e-494d-bba1-6fc22effd170]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ca380e6-1164-440a-8749-73565f36e655.16f4ed5ad77fc403], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1759/filler-pod-3ca380e6-1164-440a-8749-73565f36e655 to 10.134.156.247]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ca380e6-1164-440a-8749-73565f36e655.16f4ed5b1c395cf6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ca380e6-1164-440a-8749-73565f36e655.16f4ed5b1eddefb1], Reason = [Created], Message = [Created container filler-pod-3ca380e6-1164-440a-8749-73565f36e655]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ca380e6-1164-440a-8749-73565f36e655.16f4ed5b276dbec3], Reason = [Started], Message = [Started container filler-pod-3ca380e6-1164-440a-8749-73565f36e655]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c.16f4ed5ad66f4870], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1759/filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c to 10.134.156.209]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c.16f4ed5b22abea9e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c.16f4ed5b261dadfa], Reason = [Created], Message = [Created container filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c.16f4ed5b327eed6a], Reason = [Started], Message = [Started container filler-pod-8680a0c0-6ff4-4301-ae83-e45b77d9c96c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f4ed5bcd35ce45], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.134.156.209
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.134.156.247
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.134.156.253
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:12:22.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1759" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.222 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":196,"skipped":4117,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:12:23.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8692
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun  2 22:12:23.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8692 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  2 22:12:23.357: INFO: stderr: ""
Jun  2 22:12:23.358: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun  2 22:12:28.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8692 get pod e2e-test-httpd-pod -o json'
Jun  2 22:12:28.530: INFO: stderr: ""
Jun  2 22:12:28.530: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"ff9e2295adc9f572ad2d50e1dcc08364e885a3af36dbc49bbb24c0086e3fa116\",\n            \"cni.projectcalico.org/podIP\": \"172.30.118.27/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.118.27/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2022-06-02T22:12:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8692\",\n        \"resourceVersion\": \"39631\",\n        \"uid\": \"28bf5a51-249a-43a8-91ba-fc7625c55dd0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-s4h9f\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.134.156.247\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-s4h9f\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-02T22:12:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-02T22:12:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-02T22:12:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-06-02T22:12:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f11436e0f509c5454afc89468422d1766844bdf4a939465239455c19cb4c0dfc\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-06-02T22:12:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.134.156.247\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.118.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.118.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-06-02T22:12:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun  2 22:12:28.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8692 replace -f -'
Jun  2 22:12:28.945: INFO: stderr: ""
Jun  2 22:12:28.945: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
Jun  2 22:12:28.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-8692 delete pods e2e-test-httpd-pod'
Jun  2 22:12:31.315: INFO: stderr: ""
Jun  2 22:12:31.316: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:12:31.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8692" for this suite.

• [SLOW TEST:8.347 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":197,"skipped":4119,"failed":0}
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:12:31.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-6160
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:12:31.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6160" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":198,"skipped":4126,"failed":0}
SS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:12:31.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7774
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:14:01.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7774" for this suite.

• [SLOW TEST:90.401 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":199,"skipped":4128,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:14:02.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9788
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-9788
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9788 to expose endpoints map[]
Jun  2 22:14:02.332: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun  2 22:14:03.384: INFO: successfully validated that service multi-endpoint-test in namespace services-9788 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9788
Jun  2 22:14:03.441: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:14:05.466: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:14:07.466: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9788 to expose endpoints map[pod1:[100]]
Jun  2 22:14:07.523: INFO: successfully validated that service multi-endpoint-test in namespace services-9788 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9788
Jun  2 22:14:07.553: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:14:09.573: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:14:11.580: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9788 to expose endpoints map[pod1:[100] pod2:[101]]
Jun  2 22:14:11.679: INFO: successfully validated that service multi-endpoint-test in namespace services-9788 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jun  2 22:14:11.679: INFO: Creating new exec pod
Jun  2 22:14:14.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-9788 exec execpod9lshk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun  2 22:14:15.121: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun  2 22:14:15.121: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:14:15.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-9788 exec execpod9lshk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.218.160 80'
Jun  2 22:14:15.443: INFO: stderr: "+ + nc -v -techo -w 2 172.21.218.160 hostName 80\n\nConnection to 172.21.218.160 80 port [tcp/http] succeeded!\n"
Jun  2 22:14:15.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:14:15.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-9788 exec execpod9lshk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun  2 22:14:15.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun  2 22:14:15.771: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:14:15.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-9788 exec execpod9lshk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.218.160 81'
Jun  2 22:14:16.092: INFO: stderr: "+ echo+  hostName\nnc -v -t -w 2 172.21.218.160 81\nConnection to 172.21.218.160 81 port [tcp/*] succeeded!\n"
Jun  2 22:14:16.092: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9788
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9788 to expose endpoints map[pod2:[101]]
Jun  2 22:14:16.229: INFO: successfully validated that service multi-endpoint-test in namespace services-9788 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9788
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9788 to expose endpoints map[]
Jun  2 22:14:16.299: INFO: successfully validated that service multi-endpoint-test in namespace services-9788 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:14:16.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9788" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.316 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":200,"skipped":4131,"failed":0}
SS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:14:16.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3346
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:14:22.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3346" for this suite.

• [SLOW TEST:6.470 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":201,"skipped":4133,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:14:22.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-2053
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  2 22:14:23.129: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  2 22:15:23.310: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:15:23.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-7933
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun  2 22:15:27.685: INFO: found a healthy node: 10.134.156.247
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:15:43.934: INFO: pods created so far: [1 1 1]
Jun  2 22:15:43.934: INFO: length of pods created so far: 3
Jun  2 22:15:49.976: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:15:56.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7933" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:15:57.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2053" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:94.563 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":202,"skipped":4139,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:15:57.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1824
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:15:57.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:16:04.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1824" for this suite.

• [SLOW TEST:7.184 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":203,"skipped":4151,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:16:04.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2334
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun  2 22:16:04.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-2334 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun  2 22:16:04.934: INFO: stderr: ""
Jun  2 22:16:04.934: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun  2 22:16:04.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-2334 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jun  2 22:16:06.209: INFO: stderr: ""
Jun  2 22:16:06.209: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun  2 22:16:06.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-2334 delete pods e2e-test-httpd-pod'
Jun  2 22:16:08.015: INFO: stderr: ""
Jun  2 22:16:08.015: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:16:08.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2334" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":204,"skipped":4153,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:16:08.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1363
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:16:15.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1363" for this suite.

• [SLOW TEST:7.335 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":205,"skipped":4159,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:16:15.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9518
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9518, will wait for the garbage collector to delete the pods
Jun  2 22:16:19.722: INFO: Deleting Job.batch foo took: 27.967211ms
Jun  2 22:16:19.923: INFO: Terminating Job.batch foo pods took: 200.669876ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:16:51.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9518" for this suite.

• [SLOW TEST:35.892 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":206,"skipped":4168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:16:51.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4759
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:22:01.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4759" for this suite.

• [SLOW TEST:310.421 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":207,"skipped":4198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:22:01.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-297
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
Jun  2 22:22:02.069: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:22:04.094: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:22:06.093: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:22:07.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-297" for this suite.

• [SLOW TEST:5.501 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":208,"skipped":4295,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:22:07.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1007
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-1007/secret-test-01d7958a-6331-4bf8-9f88-190d9c309123
STEP: Creating a pod to test consume secrets
Jun  2 22:22:07.522: INFO: Waiting up to 5m0s for pod "pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9" in namespace "secrets-1007" to be "Succeeded or Failed"
Jun  2 22:22:07.535: INFO: Pod "pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.300645ms
Jun  2 22:22:09.564: INFO: Pod "pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042302756s
Jun  2 22:22:11.589: INFO: Pod "pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066949967s
Jun  2 22:22:13.616: INFO: Pod "pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093880576s
STEP: Saw pod success
Jun  2 22:22:13.616: INFO: Pod "pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9" satisfied condition "Succeeded or Failed"
Jun  2 22:22:13.629: INFO: Trying to get logs from node 10.134.156.247 pod pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9 container env-test: <nil>
STEP: delete the pod
Jun  2 22:22:13.791: INFO: Waiting for pod pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9 to disappear
Jun  2 22:22:13.803: INFO: Pod pod-configmaps-55bfc29e-b465-4a1b-a704-d391ae3ca8b9 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:22:13.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1007" for this suite.

• [SLOW TEST:6.623 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":4301,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:22:13.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5298
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-1dd7e33b-3de8-485d-9626-9998308c6b69
STEP: Creating configMap with name cm-test-opt-upd-5931787f-4215-42ed-8b28-ae0643edee06
STEP: Creating the pod
Jun  2 22:22:14.170: INFO: The status of Pod pod-projected-configmaps-35baa5de-0327-43b7-a8a7-9a6fbc939f0b is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:22:16.192: INFO: The status of Pod pod-projected-configmaps-35baa5de-0327-43b7-a8a7-9a6fbc939f0b is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:22:18.191: INFO: The status of Pod pod-projected-configmaps-35baa5de-0327-43b7-a8a7-9a6fbc939f0b is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-1dd7e33b-3de8-485d-9626-9998308c6b69
STEP: Updating configmap cm-test-opt-upd-5931787f-4215-42ed-8b28-ae0643edee06
STEP: Creating configMap with name cm-test-opt-create-8d3e82bc-5588-4609-a1f7-ebc2ee36c6a2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:23:35.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5298" for this suite.

• [SLOW TEST:82.164 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":4317,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:23:36.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-225
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun  2 22:23:36.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:23:39.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:23:54.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-225" for this suite.

• [SLOW TEST:18.357 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":211,"skipped":4317,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:23:54.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5437
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Jun  2 22:23:54.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:24:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5437" for this suite.

• [SLOW TEST:20.674 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":212,"skipped":4324,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:24:15.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8670
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:24:15.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  2 22:24:19.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-8670 --namespace=crd-publish-openapi-8670 create -f -'
Jun  2 22:24:20.548: INFO: stderr: ""
Jun  2 22:24:20.548: INFO: stdout: "e2e-test-crd-publish-openapi-7664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  2 22:24:20.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-8670 --namespace=crd-publish-openapi-8670 delete e2e-test-crd-publish-openapi-7664-crds test-cr'
Jun  2 22:24:20.732: INFO: stderr: ""
Jun  2 22:24:20.732: INFO: stdout: "e2e-test-crd-publish-openapi-7664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  2 22:24:20.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-8670 --namespace=crd-publish-openapi-8670 apply -f -'
Jun  2 22:24:21.431: INFO: stderr: ""
Jun  2 22:24:21.431: INFO: stdout: "e2e-test-crd-publish-openapi-7664-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  2 22:24:21.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-8670 --namespace=crd-publish-openapi-8670 delete e2e-test-crd-publish-openapi-7664-crds test-cr'
Jun  2 22:24:21.536: INFO: stderr: ""
Jun  2 22:24:21.536: INFO: stdout: "e2e-test-crd-publish-openapi-7664-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun  2 22:24:21.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-8670 explain e2e-test-crd-publish-openapi-7664-crds'
Jun  2 22:24:22.475: INFO: stderr: ""
Jun  2 22:24:22.475: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7664-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:24:25.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8670" for this suite.

• [SLOW TEST:10.961 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":213,"skipped":4327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:24:26.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4185
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-83106424-f68d-432d-8d62-399727f01c2d
STEP: Creating configMap with name cm-test-opt-upd-ee3107e8-624e-4faf-96a4-8d962aea4480
STEP: Creating the pod
Jun  2 22:24:26.309: INFO: The status of Pod pod-configmaps-e9882a98-9f9b-4198-831a-a27608296618 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:24:28.332: INFO: The status of Pod pod-configmaps-e9882a98-9f9b-4198-831a-a27608296618 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:24:30.338: INFO: The status of Pod pod-configmaps-e9882a98-9f9b-4198-831a-a27608296618 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-83106424-f68d-432d-8d62-399727f01c2d
STEP: Updating configmap cm-test-opt-upd-ee3107e8-624e-4faf-96a4-8d962aea4480
STEP: Creating configMap with name cm-test-opt-create-b40ab4e7-9318-4130-8d5a-86f448d8d286
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:00.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4185" for this suite.

• [SLOW TEST:94.366 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4350,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:00.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-618
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 22:26:01.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78948c58f6\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:26:03.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 22:26:06.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:06.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-618" for this suite.
STEP: Destroying namespace "webhook-618-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":215,"skipped":4357,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:06.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-4692
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
Jun  2 22:26:06.724: INFO: Major version: 1
STEP: Confirm minor version
Jun  2 22:26:06.724: INFO: cleanMinorVersion: 23
Jun  2 22:26:06.724: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:06.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4692" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":216,"skipped":4366,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:06.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3262
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jun  2 22:26:07.053: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  2 22:26:12.084: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jun  2 22:26:12.095: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jun  2 22:26:12.121: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jun  2 22:26:12.127: INFO: Observed &ReplicaSet event: ADDED
Jun  2 22:26:12.127: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.128: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.128: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.128: INFO: Found replicaset test-rs in namespace replicaset-3262 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun  2 22:26:12.128: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jun  2 22:26:12.128: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  2 22:26:12.147: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jun  2 22:26:12.156: INFO: Observed &ReplicaSet event: ADDED
Jun  2 22:26:12.156: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.157: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.157: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.157: INFO: Observed replicaset test-rs in namespace replicaset-3262 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  2 22:26:12.158: INFO: Observed &ReplicaSet event: MODIFIED
Jun  2 22:26:12.158: INFO: Found replicaset test-rs in namespace replicaset-3262 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun  2 22:26:12.158: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:12.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3262" for this suite.

• [SLOW TEST:5.423 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":217,"skipped":4376,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:12.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1675
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
Jun  2 22:26:13.060: INFO: created pod pod-service-account-defaultsa
Jun  2 22:26:13.060: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  2 22:26:13.074: INFO: created pod pod-service-account-mountsa
Jun  2 22:26:13.074: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  2 22:26:13.093: INFO: created pod pod-service-account-nomountsa
Jun  2 22:26:13.093: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  2 22:26:13.106: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  2 22:26:13.106: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  2 22:26:13.125: INFO: created pod pod-service-account-mountsa-mountspec
Jun  2 22:26:13.126: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  2 22:26:13.170: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  2 22:26:13.170: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  2 22:26:13.189: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  2 22:26:13.189: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  2 22:26:13.210: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  2 22:26:13.210: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  2 22:26:13.235: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  2 22:26:13.235: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:13.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1675" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":218,"skipped":4438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8933
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:26:13.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df" in namespace "projected-8933" to be "Succeeded or Failed"
Jun  2 22:26:13.582: INFO: Pod "downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df": Phase="Pending", Reason="", readiness=false. Elapsed: 10.57642ms
Jun  2 22:26:15.608: INFO: Pod "downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036708829s
Jun  2 22:26:17.627: INFO: Pod "downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056081548s
STEP: Saw pod success
Jun  2 22:26:17.628: INFO: Pod "downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df" satisfied condition "Succeeded or Failed"
Jun  2 22:26:17.642: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df container client-container: <nil>
STEP: delete the pod
Jun  2 22:26:17.802: INFO: Waiting for pod downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df to disappear
Jun  2 22:26:17.815: INFO: Pod downwardapi-volume-4f896678-f404-4dc9-aa90-17335635f0df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:17.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8933" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":4461,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:17.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3651
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  2 22:26:23.272: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:23.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3651" for this suite.

• [SLOW TEST:5.530 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":220,"skipped":4475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:23.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1152
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 22:26:23.947: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 22:26:26.000: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 26, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 26, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 26, 24, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 26, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 22:26:29.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:29.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1152" for this suite.
STEP: Destroying namespace "webhook-1152-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.277 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":221,"skipped":4511,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5312
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-da98f46f-226e-42f2-be2d-540ae07fbc29 in namespace container-probe-5312
Jun  2 22:26:33.927: INFO: Started pod liveness-da98f46f-226e-42f2-be2d-540ae07fbc29 in namespace container-probe-5312
STEP: checking the pod's current state and verifying that restartCount is present
Jun  2 22:26:33.941: INFO: Initial restart count of pod liveness-da98f46f-226e-42f2-be2d-540ae07fbc29 is 0
Jun  2 22:26:52.230: INFO: Restart count of pod container-probe-5312/liveness-da98f46f-226e-42f2-be2d-540ae07fbc29 is now 1 (18.289451371s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:26:52.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5312" for this suite.

• [SLOW TEST:22.668 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":4528,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:26:52.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6373
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun  2 22:26:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:26:55.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6373" for this suite.

• [SLOW TEST:18.651 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":223,"skipped":4528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:11.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6166
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
Jun  2 22:27:11.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6166 cluster-info'
Jun  2 22:27:11.304: INFO: stderr: ""
Jun  2 22:27:11.304: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:11.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6166" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":224,"skipped":4628,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:11.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3813
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:11.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3813" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":225,"skipped":4629,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:11.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2411
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:11.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2411" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":226,"skipped":4639,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7462
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jun  2 22:27:12.212: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:14.248: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:16.228: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jun  2 22:27:16.306: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:18.339: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun  2 22:27:18.348: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:18.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:18.349: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:18.350: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:18.528: INFO: Exec stderr: ""
Jun  2 22:27:18.528: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:18.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:18.531: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:18.531: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:18.725: INFO: Exec stderr: ""
Jun  2 22:27:18.725: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:18.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:18.727: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:18.727: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:18.908: INFO: Exec stderr: ""
Jun  2 22:27:18.908: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:18.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:18.910: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:18.910: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:19.117: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun  2 22:27:19.117: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:19.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:19.119: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:19.119: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:19.361: INFO: Exec stderr: ""
Jun  2 22:27:19.361: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:19.362: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:19.362: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:19.550: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun  2 22:27:19.550: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:19.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:19.552: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:19.552: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:19.805: INFO: Exec stderr: ""
Jun  2 22:27:19.805: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:19.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:19.807: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:19.807: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:20.030: INFO: Exec stderr: ""
Jun  2 22:27:20.030: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:20.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:20.032: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:20.032: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:20.260: INFO: Exec stderr: ""
Jun  2 22:27:20.260: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7462 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:20.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:20.262: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:20.262: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7462/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:27:20.467: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:20.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7462" for this suite.

• [SLOW TEST:8.527 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4649,"failed":0}
S
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:20.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-935
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jun  2 22:27:20.788: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:22.808: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:24.806: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.134.156.209 on the node which pod1 resides and expect scheduled
Jun  2 22:27:24.865: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:26.886: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:28.880: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.134.156.209 but use UDP protocol on the node which pod2 resides
Jun  2 22:27:28.909: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:30.932: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:32.933: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:34.929: INFO: The status of Pod pod3 is Running (Ready = true)
Jun  2 22:27:34.952: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:36.991: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:27:38.970: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jun  2 22:27:38.979: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.134.156.209 http://127.0.0.1:54323/hostname] Namespace:hostport-935 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:38.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:38.981: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:38.981: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-935/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.134.156.209+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.134.156.209, port: 54323
Jun  2 22:27:39.251: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.134.156.209:54323/hostname] Namespace:hostport-935 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:39.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:39.252: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:39.252: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-935/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.134.156.209%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.134.156.209, port: 54323 UDP
Jun  2 22:27:39.541: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.134.156.209 54323] Namespace:hostport-935 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:27:39.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:27:39.542: INFO: ExecWithOptions: Clientset creation
Jun  2 22:27:39.542: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-935/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.134.156.209+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:44.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-935" for this suite.

• [SLOW TEST:24.253 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":228,"skipped":4650,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:44.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8372
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-cf62570e-e25f-43d3-ba40-48821c5f4cee
STEP: Creating a pod to test consume secrets
Jun  2 22:27:45.054: INFO: Waiting up to 5m0s for pod "pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3" in namespace "secrets-8372" to be "Succeeded or Failed"
Jun  2 22:27:45.063: INFO: Pod "pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.41873ms
Jun  2 22:27:47.086: INFO: Pod "pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031674006s
Jun  2 22:27:49.112: INFO: Pod "pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058386342s
Jun  2 22:27:51.122: INFO: Pod "pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067980668s
STEP: Saw pod success
Jun  2 22:27:51.122: INFO: Pod "pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3" satisfied condition "Succeeded or Failed"
Jun  2 22:27:51.130: INFO: Trying to get logs from node 10.134.156.247 pod pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3 container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 22:27:51.280: INFO: Waiting for pod pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3 to disappear
Jun  2 22:27:51.290: INFO: Pod pod-secrets-08c4eac3-dd71-4ee7-8498-1434c47b15e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:51.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8372" for this suite.

• [SLOW TEST:6.557 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:51.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3795
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:27:51.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58" in namespace "downward-api-3795" to be "Succeeded or Failed"
Jun  2 22:27:51.577: INFO: Pod "downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58": Phase="Pending", Reason="", readiness=false. Elapsed: 12.718618ms
Jun  2 22:27:53.603: INFO: Pod "downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038354635s
Jun  2 22:27:55.616: INFO: Pod "downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051907121s
STEP: Saw pod success
Jun  2 22:27:55.616: INFO: Pod "downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58" satisfied condition "Succeeded or Failed"
Jun  2 22:27:55.624: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58 container client-container: <nil>
STEP: delete the pod
Jun  2 22:27:55.670: INFO: Waiting for pod downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58 to disappear
Jun  2 22:27:55.681: INFO: Pod downwardapi-volume-7d6f66c9-34f3-4aa3-92ba-a0d2f7e55c58 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:55.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3795" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":230,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:55.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9408
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:27:55.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9408" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":231,"skipped":4693,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:27:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9380
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-zkwf
STEP: Creating a pod to test atomic-volume-subpath
Jun  2 22:27:56.304: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zkwf" in namespace "subpath-9380" to be "Succeeded or Failed"
Jun  2 22:27:56.316: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.76668ms
Jun  2 22:27:58.348: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043970297s
Jun  2 22:28:00.364: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 4.059842657s
Jun  2 22:28:02.377: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 6.073037806s
Jun  2 22:28:04.390: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.085309755s
Jun  2 22:28:06.406: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.101986409s
Jun  2 22:28:08.425: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 12.120156208s
Jun  2 22:28:10.439: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 14.134623719s
Jun  2 22:28:12.455: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 16.150531945s
Jun  2 22:28:14.469: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 18.164997049s
Jun  2 22:28:16.490: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 20.185872825s
Jun  2 22:28:18.515: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=true. Elapsed: 22.210871474s
Jun  2 22:28:20.528: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Running", Reason="", readiness=false. Elapsed: 24.223338154s
Jun  2 22:28:22.550: INFO: Pod "pod-subpath-test-configmap-zkwf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.246006293s
STEP: Saw pod success
Jun  2 22:28:22.550: INFO: Pod "pod-subpath-test-configmap-zkwf" satisfied condition "Succeeded or Failed"
Jun  2 22:28:22.560: INFO: Trying to get logs from node 10.134.156.247 pod pod-subpath-test-configmap-zkwf container test-container-subpath-configmap-zkwf: <nil>
STEP: delete the pod
Jun  2 22:28:22.646: INFO: Waiting for pod pod-subpath-test-configmap-zkwf to disappear
Jun  2 22:28:22.656: INFO: Pod pod-subpath-test-configmap-zkwf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zkwf
Jun  2 22:28:22.657: INFO: Deleting pod "pod-subpath-test-configmap-zkwf" in namespace "subpath-9380"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:22.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9380" for this suite.

• [SLOW TEST:26.767 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":232,"skipped":4696,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:22.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9368
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-fd16b40b-7a38-4778-a2ce-97f849e8ec2f
STEP: Creating a pod to test consume configMaps
Jun  2 22:28:23.008: INFO: Waiting up to 5m0s for pod "pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437" in namespace "configmap-9368" to be "Succeeded or Failed"
Jun  2 22:28:23.019: INFO: Pod "pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437": Phase="Pending", Reason="", readiness=false. Elapsed: 10.287905ms
Jun  2 22:28:25.031: INFO: Pod "pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022480225s
Jun  2 22:28:27.046: INFO: Pod "pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037356585s
Jun  2 22:28:29.076: INFO: Pod "pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067293711s
STEP: Saw pod success
Jun  2 22:28:29.076: INFO: Pod "pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437" satisfied condition "Succeeded or Failed"
Jun  2 22:28:29.086: INFO: Trying to get logs from node 10.134.156.253 pod pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:28:29.201: INFO: Waiting for pod pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437 to disappear
Jun  2 22:28:29.213: INFO: Pod pod-configmaps-d001777d-a110-4d42-968e-1540f4b01437 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:29.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9368" for this suite.

• [SLOW TEST:6.546 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":4717,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:29.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3383
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jun  2 22:28:29.590: INFO: The status of Pod labelsupdatedc39622d-6365-4315-8bf8-c0348d53309c is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:28:31.612: INFO: The status of Pod labelsupdatedc39622d-6365-4315-8bf8-c0348d53309c is Running (Ready = true)
Jun  2 22:28:32.188: INFO: Successfully updated pod "labelsupdatedc39622d-6365-4315-8bf8-c0348d53309c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:34.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3383" for this suite.

• [SLOW TEST:5.058 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4746,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:34.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7974
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:28:34.664: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2dc0068d-1ec9-4e7c-8230-d761d95b4310" in namespace "security-context-test-7974" to be "Succeeded or Failed"
Jun  2 22:28:34.675: INFO: Pod "busybox-readonly-false-2dc0068d-1ec9-4e7c-8230-d761d95b4310": Phase="Pending", Reason="", readiness=false. Elapsed: 10.736928ms
Jun  2 22:28:36.702: INFO: Pod "busybox-readonly-false-2dc0068d-1ec9-4e7c-8230-d761d95b4310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037948767s
Jun  2 22:28:38.740: INFO: Pod "busybox-readonly-false-2dc0068d-1ec9-4e7c-8230-d761d95b4310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076670452s
Jun  2 22:28:38.741: INFO: Pod "busybox-readonly-false-2dc0068d-1ec9-4e7c-8230-d761d95b4310" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:38.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7974" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":235,"skipped":4757,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5494
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:28:39.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:39.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5494" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":236,"skipped":4759,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:39.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6433
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
Jun  2 22:28:40.000: INFO: Waiting up to 5m0s for pod "var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c" in namespace "var-expansion-6433" to be "Succeeded or Failed"
Jun  2 22:28:40.010: INFO: Pod "var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.286942ms
Jun  2 22:28:42.032: INFO: Pod "var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031437931s
Jun  2 22:28:44.055: INFO: Pod "var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054805678s
Jun  2 22:28:46.070: INFO: Pod "var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069187602s
STEP: Saw pod success
Jun  2 22:28:46.070: INFO: Pod "var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c" satisfied condition "Succeeded or Failed"
Jun  2 22:28:46.078: INFO: Trying to get logs from node 10.134.156.253 pod var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c container dapi-container: <nil>
STEP: delete the pod
Jun  2 22:28:46.130: INFO: Waiting for pod var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c to disappear
Jun  2 22:28:46.141: INFO: Pod var-expansion-2eddfc24-b091-4ddc-9182-53eb85066b0c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:46.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6433" for this suite.

• [SLOW TEST:6.490 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":237,"skipped":4766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:46.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6002
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  2 22:28:46.459: INFO: Waiting up to 5m0s for pod "pod-4c2a6b13-7519-425d-84ee-472aed211a57" in namespace "emptydir-6002" to be "Succeeded or Failed"
Jun  2 22:28:46.470: INFO: Pod "pod-4c2a6b13-7519-425d-84ee-472aed211a57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.426418ms
Jun  2 22:28:48.489: INFO: Pod "pod-4c2a6b13-7519-425d-84ee-472aed211a57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028829026s
Jun  2 22:28:50.512: INFO: Pod "pod-4c2a6b13-7519-425d-84ee-472aed211a57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051722429s
STEP: Saw pod success
Jun  2 22:28:50.512: INFO: Pod "pod-4c2a6b13-7519-425d-84ee-472aed211a57" satisfied condition "Succeeded or Failed"
Jun  2 22:28:50.552: INFO: Trying to get logs from node 10.134.156.253 pod pod-4c2a6b13-7519-425d-84ee-472aed211a57 container test-container: <nil>
STEP: delete the pod
Jun  2 22:28:50.658: INFO: Waiting for pod pod-4c2a6b13-7519-425d-84ee-472aed211a57 to disappear
Jun  2 22:28:50.667: INFO: Pod pod-4c2a6b13-7519-425d-84ee-472aed211a57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:50.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6002" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7398
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  2 22:28:50.925: INFO: The status of Pod pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:28:52.950: INFO: The status of Pod pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:28:54.937: INFO: The status of Pod pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  2 22:28:55.522: INFO: Successfully updated pod "pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a"
Jun  2 22:28:55.522: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a" in namespace "pods-7398" to be "terminated due to deadline exceeded"
Jun  2 22:28:55.534: INFO: Pod "pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a": Phase="Running", Reason="", readiness=true. Elapsed: 11.341416ms
Jun  2 22:28:57.560: INFO: Pod "pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.037880241s
Jun  2 22:28:57.560: INFO: Pod "pod-update-activedeadlineseconds-25306940-ab56-4b05-924a-97674a7cee3a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:28:57.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7398" for this suite.

• [SLOW TEST:6.895 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:28:57.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1622
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jun  2 22:28:57.921: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:28:59.948: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jun  2 22:28:59.982: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:29:01.999: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  2 22:29:02.082: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  2 22:29:02.095: INFO: Pod pod-with-poststart-http-hook still exists
Jun  2 22:29:04.096: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  2 22:29:04.121: INFO: Pod pod-with-poststart-http-hook still exists
Jun  2 22:29:06.095: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  2 22:29:06.112: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:29:06.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1622" for this suite.

• [SLOW TEST:8.568 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":240,"skipped":4870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:29:06.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3494
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:29:06.390: INFO: Creating simple deployment test-new-deployment
Jun  2 22:29:06.428: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Jun  2 22:29:08.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 29, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 29, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 29, 6, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 29, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 22:29:10.648: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3494  0b5bb45b-8b2f-48e1-8432-89f4df8d43eb 42778 3 2022-06-02 22:29:06 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-06-02 22:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b11f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-02 22:29:08 +0000 UTC,LastTransitionTime:2022-06-02 22:29:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-06-02 22:29:08 +0000 UTC,LastTransitionTime:2022-06-02 22:29:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  2 22:29:10.663: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-3494  69086b8a-c18b-48ab-9db4-7bdddc426e63 42779 2 2022-06-02 22:29:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0b5bb45b-8b2f-48e1-8432-89f4df8d43eb 0xc008e4a3a7 0xc008e4a3a8}] []  [{kube-controller-manager Update apps/v1 2022-06-02 22:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b5bb45b-8b2f-48e1-8432-89f4df8d43eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:29:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008e4a438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:29:10.684: INFO: Pod "test-new-deployment-5d9fdcc779-ghmf8" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-ghmf8 test-new-deployment-5d9fdcc779- deployment-3494  5b26defb-1274-4955-b053-e8cb7e83513f 42770 0 2022-06-02 22:29:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:419e18c48460eb849b02a46401b206962bdcc119dbbcef0496d38bca42d4088b cni.projectcalico.org/podIP:172.30.220.207/32 cni.projectcalico.org/podIPs:172.30.220.207/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 69086b8a-c18b-48ab-9db4-7bdddc426e63 0xc004318737 0xc004318738}] []  [{kube-controller-manager Update v1 2022-06-02 22:29:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69086b8a-c18b-48ab-9db4-7bdddc426e63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 22:29:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 22:29:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6tvhs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6tvhs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.207,StartTime:2022-06-02 22:29:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 22:29:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://158baced0df9ba36af63f565e585382849d2995a1e4de99fe61c0f92be7a443b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  2 22:29:10.684: INFO: Pod "test-new-deployment-5d9fdcc779-vpqc8" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-vpqc8 test-new-deployment-5d9fdcc779- deployment-3494  2471b0df-4f27-4dea-aaf0-a04a46dbbdfd 42784 0 2022-06-02 22:29:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 69086b8a-c18b-48ab-9db4-7bdddc426e63 0xc004318f47 0xc004318f48}] []  [{kube-controller-manager Update v1 2022-06-02 22:29:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69086b8a-c18b-48ab-9db4-7bdddc426e63\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwwz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwwz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.209,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:29:10.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3494" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":241,"skipped":4895,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:29:10.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-538
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
Jun  2 22:29:10.927: INFO: Creating simple deployment test-deployment-67xw9
Jun  2 22:29:10.963: INFO: deployment "test-deployment-67xw9" doesn't have the required revision set
Jun  2 22:29:13.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 29, 10, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 29, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 29, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 29, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-67xw9-764bc7c4b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Jun  2 22:29:15.048: INFO: Deployment test-deployment-67xw9 has Conditions: [{Available True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-67xw9-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Jun  2 22:29:15.075: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 29, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 29, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 29, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 29, 10, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-67xw9-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jun  2 22:29:15.083: INFO: Observed &Deployment event: ADDED
Jun  2 22:29:15.084: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-67xw9-764bc7c4b7"}
Jun  2 22:29:15.084: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.084: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-67xw9-764bc7c4b7"}
Jun  2 22:29:15.085: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  2 22:29:15.086: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.086: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  2 22:29:15.086: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:11 +0000 UTC 2022-06-02 22:29:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-67xw9-764bc7c4b7" is progressing.}
Jun  2 22:29:15.086: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.087: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  2 22:29:15.087: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-67xw9-764bc7c4b7" has successfully progressed.}
Jun  2 22:29:15.087: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.087: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  2 22:29:15.088: INFO: Observed Deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-67xw9-764bc7c4b7" has successfully progressed.}
Jun  2 22:29:15.088: INFO: Found Deployment test-deployment-67xw9 in namespace deployment-538 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  2 22:29:15.088: INFO: Deployment test-deployment-67xw9 has an updated status
STEP: patching the Statefulset Status
Jun  2 22:29:15.089: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun  2 22:29:15.106: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jun  2 22:29:15.112: INFO: Observed &Deployment event: ADDED
Jun  2 22:29:15.112: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-67xw9-764bc7c4b7"}
Jun  2 22:29:15.113: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.113: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-67xw9-764bc7c4b7"}
Jun  2 22:29:15.113: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  2 22:29:15.114: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.114: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-06-02 22:29:10 +0000 UTC 2022-06-02 22:29:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun  2 22:29:15.114: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:11 +0000 UTC 2022-06-02 22:29:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-67xw9-764bc7c4b7" is progressing.}
Jun  2 22:29:15.114: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.114: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  2 22:29:15.114: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-67xw9-764bc7c4b7" has successfully progressed.}
Jun  2 22:29:15.115: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.115: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun  2 22:29:15.115: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-06-02 22:29:13 +0000 UTC 2022-06-02 22:29:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-67xw9-764bc7c4b7" has successfully progressed.}
Jun  2 22:29:15.116: INFO: Observed deployment test-deployment-67xw9 in namespace deployment-538 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun  2 22:29:15.116: INFO: Observed &Deployment event: MODIFIED
Jun  2 22:29:15.116: INFO: Found deployment test-deployment-67xw9 in namespace deployment-538 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun  2 22:29:15.116: INFO: Deployment test-deployment-67xw9 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 22:29:15.127: INFO: Deployment "test-deployment-67xw9":
&Deployment{ObjectMeta:{test-deployment-67xw9  deployment-538  dbf9353f-9a85-4aa0-97af-e445e865be3e 42847 1 2022-06-02 22:29:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-06-02 22:29:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-06-02 22:29:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c6f278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  2 22:29:15.136: INFO: New ReplicaSet "test-deployment-67xw9-764bc7c4b7" of Deployment "test-deployment-67xw9":
&ReplicaSet{ObjectMeta:{test-deployment-67xw9-764bc7c4b7  deployment-538  8b3bc8c1-970c-4d86-aa36-86fd2f11631d 42844 1 2022-06-02 22:29:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-67xw9 dbf9353f-9a85-4aa0-97af-e445e865be3e 0xc0035f1887 0xc0035f1888}] []  [{kube-controller-manager Update apps/v1 2022-06-02 22:29:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbf9353f-9a85-4aa0-97af-e445e865be3e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:29:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035f1938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:29:15.147: INFO: Pod "test-deployment-67xw9-764bc7c4b7-k7g6n" is available:
&Pod{ObjectMeta:{test-deployment-67xw9-764bc7c4b7-k7g6n test-deployment-67xw9-764bc7c4b7- deployment-538  0f53af62-b337-4afc-b3d6-39f7516e04d6 42843 0 2022-06-02 22:29:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:a352a4e27cc482bdb22bc24a95806478ce0e0258308717108fa34f3c00c86a51 cni.projectcalico.org/podIP:172.30.220.227/32 cni.projectcalico.org/podIPs:172.30.220.227/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-67xw9-764bc7c4b7 8b3bc8c1-970c-4d86-aa36-86fd2f11631d 0xc0035f1cc0 0xc0035f1cc1}] []  [{kube-controller-manager Update v1 2022-06-02 22:29:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b3bc8c1-970c-4d86-aa36-86fd2f11631d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 22:29:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 22:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wtvgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wtvgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:29:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.227,StartTime:2022-06-02 22:29:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 22:29:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5fa4af4a484ae819a2ddffa16a6d5c24c1667546d79d1a9c43f7bb29ae91e8a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:29:15.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-538" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":242,"skipped":4907,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:29:15.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-755
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  2 22:29:15.464: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  2 22:30:15.580: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Jun  2 22:30:15.664: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  2 22:30:15.704: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  2 22:30:15.756: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  2 22:30:15.789: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  2 22:30:15.855: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  2 22:30:15.868: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:30:32.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-755" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:77.085 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":243,"skipped":4908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:30:32.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7254
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-7254
STEP: creating replication controller nodeport-test in namespace services-7254
I0602 22:30:32.585762      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7254, replica count: 2
I0602 22:30:35.637302      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 22:30:35.637: INFO: Creating new exec pod
Jun  2 22:30:40.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7254 exec execpodqqvhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun  2 22:30:41.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  2 22:30:41.089: INFO: stdout: "nodeport-test-wxgt7"
Jun  2 22:30:41.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7254 exec execpodqqvhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.10.141 80'
Jun  2 22:30:41.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.10.141 80\nConnection to 172.21.10.141 80 port [tcp/http] succeeded!\n"
Jun  2 22:30:41.400: INFO: stdout: "nodeport-test-k8j7r"
Jun  2 22:30:41.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7254 exec execpodqqvhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.253 30896'
Jun  2 22:30:41.648: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.253 30896\nConnection to 10.134.156.253 30896 port [tcp/*] succeeded!\n"
Jun  2 22:30:41.648: INFO: stdout: ""
Jun  2 22:30:42.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7254 exec execpodqqvhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.253 30896'
Jun  2 22:30:42.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.253 30896\nConnection to 10.134.156.253 30896 port [tcp/*] succeeded!\n"
Jun  2 22:30:42.986: INFO: stdout: "nodeport-test-wxgt7"
Jun  2 22:30:42.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-7254 exec execpodqqvhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.247 30896'
Jun  2 22:30:43.335: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.247 30896\nConnection to 10.134.156.247 30896 port [tcp/*] succeeded!\n"
Jun  2 22:30:43.335: INFO: stdout: "nodeport-test-wxgt7"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:30:43.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7254" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.117 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":244,"skipped":4933,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:30:43.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7326
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Jun  2 22:30:43.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 create -f -'
Jun  2 22:30:45.311: INFO: stderr: ""
Jun  2 22:30:45.311: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  2 22:30:45.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 22:30:45.410: INFO: stderr: ""
Jun  2 22:30:45.410: INFO: stdout: "update-demo-nautilus-4wvhm update-demo-nautilus-jc7n8 "
Jun  2 22:30:45.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-4wvhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:30:45.585: INFO: stderr: ""
Jun  2 22:30:45.585: INFO: stdout: ""
Jun  2 22:30:45.585: INFO: update-demo-nautilus-4wvhm is created but not running
Jun  2 22:30:50.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 22:30:50.712: INFO: stderr: ""
Jun  2 22:30:50.712: INFO: stdout: "update-demo-nautilus-4wvhm update-demo-nautilus-jc7n8 "
Jun  2 22:30:50.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-4wvhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:30:50.856: INFO: stderr: ""
Jun  2 22:30:50.856: INFO: stdout: "true"
Jun  2 22:30:50.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-4wvhm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 22:30:50.953: INFO: stderr: ""
Jun  2 22:30:50.953: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 22:30:50.953: INFO: validating pod update-demo-nautilus-4wvhm
Jun  2 22:30:51.026: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 22:30:51.026: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 22:30:51.026: INFO: update-demo-nautilus-4wvhm is verified up and running
Jun  2 22:30:51.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:30:51.131: INFO: stderr: ""
Jun  2 22:30:51.131: INFO: stdout: "true"
Jun  2 22:30:51.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 22:30:51.228: INFO: stderr: ""
Jun  2 22:30:51.228: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 22:30:51.228: INFO: validating pod update-demo-nautilus-jc7n8
Jun  2 22:30:51.278: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 22:30:51.279: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 22:30:51.279: INFO: update-demo-nautilus-jc7n8 is verified up and running
STEP: scaling down the replication controller
Jun  2 22:30:51.283: INFO: scanned /root for discovery docs: <nil>
Jun  2 22:30:51.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun  2 22:30:52.449: INFO: stderr: ""
Jun  2 22:30:52.449: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  2 22:30:52.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 22:30:52.597: INFO: stderr: ""
Jun  2 22:30:52.597: INFO: stdout: "update-demo-nautilus-4wvhm update-demo-nautilus-jc7n8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun  2 22:30:57.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 22:30:57.703: INFO: stderr: ""
Jun  2 22:30:57.703: INFO: stdout: "update-demo-nautilus-jc7n8 "
Jun  2 22:30:57.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:30:57.790: INFO: stderr: ""
Jun  2 22:30:57.790: INFO: stdout: "true"
Jun  2 22:30:57.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 22:30:57.882: INFO: stderr: ""
Jun  2 22:30:57.882: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 22:30:57.882: INFO: validating pod update-demo-nautilus-jc7n8
Jun  2 22:30:57.901: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 22:30:57.901: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 22:30:57.901: INFO: update-demo-nautilus-jc7n8 is verified up and running
STEP: scaling up the replication controller
Jun  2 22:30:57.905: INFO: scanned /root for discovery docs: <nil>
Jun  2 22:30:57.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun  2 22:30:59.107: INFO: stderr: ""
Jun  2 22:30:59.107: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  2 22:30:59.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 22:30:59.234: INFO: stderr: ""
Jun  2 22:30:59.234: INFO: stdout: "update-demo-nautilus-jc7n8 update-demo-nautilus-rfftw "
Jun  2 22:30:59.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:30:59.362: INFO: stderr: ""
Jun  2 22:30:59.362: INFO: stdout: "true"
Jun  2 22:30:59.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 22:30:59.474: INFO: stderr: ""
Jun  2 22:30:59.474: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 22:30:59.474: INFO: validating pod update-demo-nautilus-jc7n8
Jun  2 22:30:59.509: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 22:30:59.509: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 22:30:59.509: INFO: update-demo-nautilus-jc7n8 is verified up and running
Jun  2 22:30:59.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-rfftw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:30:59.624: INFO: stderr: ""
Jun  2 22:30:59.624: INFO: stdout: ""
Jun  2 22:30:59.624: INFO: update-demo-nautilus-rfftw is created but not running
Jun  2 22:31:04.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  2 22:31:04.757: INFO: stderr: ""
Jun  2 22:31:04.757: INFO: stdout: "update-demo-nautilus-jc7n8 update-demo-nautilus-rfftw "
Jun  2 22:31:04.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:31:04.865: INFO: stderr: ""
Jun  2 22:31:04.865: INFO: stdout: "true"
Jun  2 22:31:04.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-jc7n8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 22:31:04.990: INFO: stderr: ""
Jun  2 22:31:04.990: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 22:31:04.990: INFO: validating pod update-demo-nautilus-jc7n8
Jun  2 22:31:05.007: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 22:31:05.007: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 22:31:05.007: INFO: update-demo-nautilus-jc7n8 is verified up and running
Jun  2 22:31:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-rfftw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  2 22:31:05.106: INFO: stderr: ""
Jun  2 22:31:05.106: INFO: stdout: "true"
Jun  2 22:31:05.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods update-demo-nautilus-rfftw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  2 22:31:05.208: INFO: stderr: ""
Jun  2 22:31:05.208: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jun  2 22:31:05.208: INFO: validating pod update-demo-nautilus-rfftw
Jun  2 22:31:05.360: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  2 22:31:05.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  2 22:31:05.360: INFO: update-demo-nautilus-rfftw is verified up and running
STEP: using delete to clean up resources
Jun  2 22:31:05.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 delete --grace-period=0 --force -f -'
Jun  2 22:31:05.487: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:31:05.488: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  2 22:31:05.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get rc,svc -l name=update-demo --no-headers'
Jun  2 22:31:05.611: INFO: stderr: "No resources found in kubectl-7326 namespace.\n"
Jun  2 22:31:05.611: INFO: stdout: ""
Jun  2 22:31:05.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7326 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  2 22:31:05.714: INFO: stderr: ""
Jun  2 22:31:05.714: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:05.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7326" for this suite.

• [SLOW TEST:22.363 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":245,"skipped":4937,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:05.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9355
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:31:05.985: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  2 22:31:11.007: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  2 22:31:11.007: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  2 22:31:13.035: INFO: Creating deployment "test-rollover-deployment"
Jun  2 22:31:13.056: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  2 22:31:15.086: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  2 22:31:15.108: INFO: Ensure that both replica sets have 1 created replica
Jun  2 22:31:15.139: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  2 22:31:15.169: INFO: Updating deployment test-rollover-deployment
Jun  2 22:31:15.169: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  2 22:31:17.193: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  2 22:31:17.214: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  2 22:31:17.238: INFO: all replica sets need to contain the pod-template-hash label
Jun  2 22:31:17.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:31:19.263: INFO: all replica sets need to contain the pod-template-hash label
Jun  2 22:31:19.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:31:21.274: INFO: all replica sets need to contain the pod-template-hash label
Jun  2 22:31:21.274: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:31:23.303: INFO: all replica sets need to contain the pod-template-hash label
Jun  2 22:31:23.303: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:31:25.260: INFO: all replica sets need to contain the pod-template-hash label
Jun  2 22:31:25.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:31:27.261: INFO: all replica sets need to contain the pod-template-hash label
Jun  2 22:31:27.261: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 31, 18, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 31, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  2 22:31:29.280: INFO: 
Jun  2 22:31:29.280: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jun  2 22:31:29.309: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9355  6404eab2-9688-454f-aef3-e6a7e1a8301f 43548 2 2022-06-02 22:31:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-06-02 22:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:31:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030bf338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-06-02 22:31:13 +0000 UTC,LastTransitionTime:2022-06-02 22:31:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-06-02 22:31:28 +0000 UTC,LastTransitionTime:2022-06-02 22:31:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  2 22:31:29.321: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-9355  bdc097b7-d936-41a4-8b1e-82a1d2f32253 43538 2 2022-06-02 22:31:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 6404eab2-9688-454f-aef3-e6a7e1a8301f 0xc0030bf827 0xc0030bf828}] []  [{kube-controller-manager Update apps/v1 2022-06-02 22:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6404eab2-9688-454f-aef3-e6a7e1a8301f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:31:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030bf8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:31:29.321: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  2 22:31:29.321: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9355  69cac3b6-2558-403f-a635-818293ec7e73 43547 2 2022-06-02 22:31:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 6404eab2-9688-454f-aef3-e6a7e1a8301f 0xc0030bf6f7 0xc0030bf6f8}] []  [{e2e.test Update apps/v1 2022-06-02 22:31:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:31:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6404eab2-9688-454f-aef3-e6a7e1a8301f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:31:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0030bf7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:31:29.321: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-9355  66ca4628-29fd-4b1c-a072-4fd45cc41813 43506 2 2022-06-02 22:31:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 6404eab2-9688-454f-aef3-e6a7e1a8301f 0xc0030bf947 0xc0030bf948}] []  [{kube-controller-manager Update apps/v1 2022-06-02 22:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6404eab2-9688-454f-aef3-e6a7e1a8301f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-06-02 22:31:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030bf9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  2 22:31:29.333: INFO: Pod "test-rollover-deployment-668b7f667d-5tbbz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-5tbbz test-rollover-deployment-668b7f667d- deployment-9355  401763e4-275f-4681-807c-2cbfe5148b02 43524 0 2022-06-02 22:31:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/containerID:8b0a56d0911f3131404ed54c8e6582b1592d2f902f6b4d64dfd549e68c1d0b24 cni.projectcalico.org/podIP:172.30.220.197/32 cni.projectcalico.org/podIPs:172.30.220.197/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d bdc097b7-d936-41a4-8b1e-82a1d2f32253 0xc004a9cb17 0xc004a9cb18}] []  [{kube-controller-manager Update v1 2022-06-02 22:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bdc097b7-d936-41a4-8b1e-82a1d2f32253\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-06-02 22:31:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-06-02 22:31:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.220.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mxwnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mxwnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.134.156.253,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-06-02 22:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.134.156.253,PodIP:172.30.220.197,StartTime:2022-06-02 22:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-06-02 22:31:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://2a3a3a82bd039c6b7a90564021f0d55a8d8105b58adc8de5bb0837cd1f21ecd2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.220.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:29.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9355" for this suite.

• [SLOW TEST:23.615 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":246,"skipped":4944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:29.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9133
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:31:29.625: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a" in namespace "projected-9133" to be "Succeeded or Failed"
Jun  2 22:31:29.638: INFO: Pod "downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.503028ms
Jun  2 22:31:31.664: INFO: Pod "downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03882437s
Jun  2 22:31:33.690: INFO: Pod "downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065210929s
STEP: Saw pod success
Jun  2 22:31:33.690: INFO: Pod "downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a" satisfied condition "Succeeded or Failed"
Jun  2 22:31:33.700: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a container client-container: <nil>
STEP: delete the pod
Jun  2 22:31:33.810: INFO: Waiting for pod downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a to disappear
Jun  2 22:31:33.823: INFO: Pod downwardapi-volume-3178aec5-0826-41af-995e-705c39f3971a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:33.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9133" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":5071,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2910
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-2910/configmap-test-16a5f843-013e-435b-9280-7908f5d0941b
STEP: Creating a pod to test consume configMaps
Jun  2 22:31:34.097: INFO: Waiting up to 5m0s for pod "pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d" in namespace "configmap-2910" to be "Succeeded or Failed"
Jun  2 22:31:34.108: INFO: Pod "pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.550167ms
Jun  2 22:31:36.126: INFO: Pod "pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029561429s
Jun  2 22:31:38.149: INFO: Pod "pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052347233s
Jun  2 22:31:40.178: INFO: Pod "pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081605379s
STEP: Saw pod success
Jun  2 22:31:40.178: INFO: Pod "pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d" satisfied condition "Succeeded or Failed"
Jun  2 22:31:40.189: INFO: Trying to get logs from node 10.134.156.247 pod pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d container env-test: <nil>
STEP: delete the pod
Jun  2 22:31:40.244: INFO: Waiting for pod pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d to disappear
Jun  2 22:31:40.253: INFO: Pod pod-configmaps-946b72ca-1b40-459c-9328-2257b6e86d8d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:40.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2910" for this suite.

• [SLOW TEST:6.431 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":248,"skipped":5074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:40.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5389
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:43.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5389" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":249,"skipped":5132,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:43.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3638
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:31:43.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771" in namespace "projected-3638" to be "Succeeded or Failed"
Jun  2 22:31:43.593: INFO: Pod "downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771": Phase="Pending", Reason="", readiness=false. Elapsed: 9.199171ms
Jun  2 22:31:45.610: INFO: Pod "downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025720155s
Jun  2 22:31:47.624: INFO: Pod "downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0400726s
STEP: Saw pod success
Jun  2 22:31:47.624: INFO: Pod "downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771" satisfied condition "Succeeded or Failed"
Jun  2 22:31:47.632: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771 container client-container: <nil>
STEP: delete the pod
Jun  2 22:31:47.680: INFO: Waiting for pod downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771 to disappear
Jun  2 22:31:47.689: INFO: Pod downwardapi-volume-7445b413-f44e-4d96-99c9-58397e5b2771 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:47.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3638" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":5137,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:47.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3143
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:47.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3143" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":251,"skipped":5154,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:48.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-8160
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jun  2 22:31:50.318: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:31:52.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8160" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":252,"skipped":5164,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:31:52.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2654
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:31:52.803: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  2 22:31:57.820: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jun  2 22:31:57.850: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jun  2 22:31:57.872: INFO: observed ReplicaSet test-rs in namespace replicaset-2654 with ReadyReplicas 1, AvailableReplicas 1
Jun  2 22:31:57.938: INFO: observed ReplicaSet test-rs in namespace replicaset-2654 with ReadyReplicas 1, AvailableReplicas 1
Jun  2 22:31:57.966: INFO: observed ReplicaSet test-rs in namespace replicaset-2654 with ReadyReplicas 1, AvailableReplicas 1
Jun  2 22:31:57.982: INFO: observed ReplicaSet test-rs in namespace replicaset-2654 with ReadyReplicas 1, AvailableReplicas 1
Jun  2 22:32:00.297: INFO: observed ReplicaSet test-rs in namespace replicaset-2654 with ReadyReplicas 2, AvailableReplicas 2
Jun  2 22:32:00.389: INFO: observed Replicaset test-rs in namespace replicaset-2654 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:32:00.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2654" for this suite.

• [SLOW TEST:7.884 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":253,"skipped":5164,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:32:00.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3888
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-103e7ccd-2e7a-47ce-a914-7456282dfea2
STEP: Creating a pod to test consume configMaps
Jun  2 22:32:00.705: INFO: Waiting up to 5m0s for pod "pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774" in namespace "configmap-3888" to be "Succeeded or Failed"
Jun  2 22:32:00.718: INFO: Pod "pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774": Phase="Pending", Reason="", readiness=false. Elapsed: 13.601739ms
Jun  2 22:32:02.736: INFO: Pod "pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031394258s
Jun  2 22:32:04.750: INFO: Pod "pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045389779s
STEP: Saw pod success
Jun  2 22:32:04.750: INFO: Pod "pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774" satisfied condition "Succeeded or Failed"
Jun  2 22:32:04.759: INFO: Trying to get logs from node 10.134.156.247 pod pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:32:04.826: INFO: Waiting for pod pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774 to disappear
Jun  2 22:32:04.834: INFO: Pod pod-configmaps-39a3dbe3-85a4-41bf-aa22-a2bd6fa0e774 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:32:04.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3888" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":5174,"failed":0}

------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:32:04.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2082
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun  2 22:32:06.092: INFO: Pod name wrapped-volume-race-7768b341-1651-48c7-8f81-f3bb3ed678a6: Found 0 pods out of 5
Jun  2 22:32:11.144: INFO: Pod name wrapped-volume-race-7768b341-1651-48c7-8f81-f3bb3ed678a6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7768b341-1651-48c7-8f81-f3bb3ed678a6 in namespace emptydir-wrapper-2082, will wait for the garbage collector to delete the pods
Jun  2 22:32:11.399: INFO: Deleting ReplicationController wrapped-volume-race-7768b341-1651-48c7-8f81-f3bb3ed678a6 took: 16.724553ms
Jun  2 22:32:11.614: INFO: Terminating ReplicationController wrapped-volume-race-7768b341-1651-48c7-8f81-f3bb3ed678a6 pods took: 215.565671ms
STEP: Creating RC which spawns configmap-volume pods
Jun  2 22:32:13.980: INFO: Pod name wrapped-volume-race-20761efb-9d71-4394-9bb7-d0681c5005f7: Found 0 pods out of 5
Jun  2 22:32:19.018: INFO: Pod name wrapped-volume-race-20761efb-9d71-4394-9bb7-d0681c5005f7: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-20761efb-9d71-4394-9bb7-d0681c5005f7 in namespace emptydir-wrapper-2082, will wait for the garbage collector to delete the pods
Jun  2 22:32:19.174: INFO: Deleting ReplicationController wrapped-volume-race-20761efb-9d71-4394-9bb7-d0681c5005f7 took: 30.256542ms
Jun  2 22:32:19.274: INFO: Terminating ReplicationController wrapped-volume-race-20761efb-9d71-4394-9bb7-d0681c5005f7 pods took: 100.269467ms
STEP: Creating RC which spawns configmap-volume pods
Jun  2 22:32:22.039: INFO: Pod name wrapped-volume-race-265f07f1-dab0-4fe1-996d-0a68bfd7ff46: Found 0 pods out of 5
Jun  2 22:32:27.068: INFO: Pod name wrapped-volume-race-265f07f1-dab0-4fe1-996d-0a68bfd7ff46: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-265f07f1-dab0-4fe1-996d-0a68bfd7ff46 in namespace emptydir-wrapper-2082, will wait for the garbage collector to delete the pods
Jun  2 22:32:27.206: INFO: Deleting ReplicationController wrapped-volume-race-265f07f1-dab0-4fe1-996d-0a68bfd7ff46 took: 21.171069ms
Jun  2 22:32:27.306: INFO: Terminating ReplicationController wrapped-volume-race-265f07f1-dab0-4fe1-996d-0a68bfd7ff46 pods took: 100.397605ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:32:31.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2082" for this suite.

• [SLOW TEST:26.387 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":255,"skipped":5174,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:32:31.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5270
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-qf8v
STEP: Creating a pod to test atomic-volume-subpath
Jun  2 22:32:31.568: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qf8v" in namespace "subpath-5270" to be "Succeeded or Failed"
Jun  2 22:32:31.577: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Pending", Reason="", readiness=false. Elapsed: 9.362295ms
Jun  2 22:32:33.814: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.246256875s
Jun  2 22:32:35.828: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 4.26047375s
Jun  2 22:32:37.864: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 6.295678873s
Jun  2 22:32:39.878: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 8.310598784s
Jun  2 22:32:41.891: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 10.32329953s
Jun  2 22:32:43.917: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 12.34923034s
Jun  2 22:32:45.935: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 14.367626437s
Jun  2 22:32:47.957: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 16.389091333s
Jun  2 22:32:49.977: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 18.409161825s
Jun  2 22:32:52.000: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=true. Elapsed: 20.432090589s
Jun  2 22:32:54.020: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Running", Reason="", readiness=false. Elapsed: 22.452256585s
Jun  2 22:32:56.039: INFO: Pod "pod-subpath-test-downwardapi-qf8v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.47095892s
STEP: Saw pod success
Jun  2 22:32:56.039: INFO: Pod "pod-subpath-test-downwardapi-qf8v" satisfied condition "Succeeded or Failed"
Jun  2 22:32:56.049: INFO: Trying to get logs from node 10.134.156.247 pod pod-subpath-test-downwardapi-qf8v container test-container-subpath-downwardapi-qf8v: <nil>
STEP: delete the pod
Jun  2 22:32:56.123: INFO: Waiting for pod pod-subpath-test-downwardapi-qf8v to disappear
Jun  2 22:32:56.133: INFO: Pod pod-subpath-test-downwardapi-qf8v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qf8v
Jun  2 22:32:56.133: INFO: Deleting pod "pod-subpath-test-downwardapi-qf8v" in namespace "subpath-5270"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:32:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5270" for this suite.

• [SLOW TEST:24.913 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":256,"skipped":5176,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:32:56.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-7ad2a4f6-16c7-492d-8fa1-00bb158bca53
STEP: Creating a pod to test consume secrets
Jun  2 22:32:56.448: INFO: Waiting up to 5m0s for pod "pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1" in namespace "secrets-7" to be "Succeeded or Failed"
Jun  2 22:32:56.461: INFO: Pod "pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.584125ms
Jun  2 22:32:58.484: INFO: Pod "pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035927126s
Jun  2 22:33:00.517: INFO: Pod "pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069197643s
Jun  2 22:33:02.541: INFO: Pod "pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.093154419s
STEP: Saw pod success
Jun  2 22:33:02.541: INFO: Pod "pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1" satisfied condition "Succeeded or Failed"
Jun  2 22:33:02.549: INFO: Trying to get logs from node 10.134.156.247 pod pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1 container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 22:33:02.616: INFO: Waiting for pod pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1 to disappear
Jun  2 22:33:02.626: INFO: Pod pod-secrets-c5a4a5c2-f954-4d9f-98a7-aecdd493e7d1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:33:02.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7" for this suite.

• [SLOW TEST:6.544 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":5183,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:33:02.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8456
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 22:33:03.513: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 22:33:05.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 33, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 33, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 33, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 33, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 22:33:08.607: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:33:08.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8456" for this suite.
STEP: Destroying namespace "webhook-8456-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":258,"skipped":5192,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:33:09.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4674
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-3cb25dcd-c6ae-4691-8d90-f0986fc2ea23 in namespace container-probe-4674
Jun  2 22:33:13.441: INFO: Started pod test-webserver-3cb25dcd-c6ae-4691-8d90-f0986fc2ea23 in namespace container-probe-4674
STEP: checking the pod's current state and verifying that restartCount is present
Jun  2 22:33:13.453: INFO: Initial restart count of pod test-webserver-3cb25dcd-c6ae-4691-8d90-f0986fc2ea23 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:37:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4674" for this suite.

• [SLOW TEST:244.955 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":5197,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:37:14.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1774
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:38:14.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1774" for this suite.

• [SLOW TEST:60.349 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":5201,"failed":0}
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:38:14.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-548
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun  2 22:38:14.674: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  2 22:39:14.815: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:39:14.843: INFO: Starting informer...
STEP: Starting pod...
Jun  2 22:39:15.133: INFO: Pod is running on 10.134.156.247. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun  2 22:39:15.216: INFO: Pod wasn't evicted. Proceeding
Jun  2 22:39:15.216: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun  2 22:40:30.269: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:40:30.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-548" for this suite.

• [SLOW TEST:135.848 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":261,"skipped":5207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:40:30.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7655
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-jsfqv in namespace proxy-7655
I0602 22:40:30.655751      21 runners.go:193] Created replication controller with name: proxy-service-jsfqv, namespace: proxy-7655, replica count: 1
I0602 22:40:31.707615      21 runners.go:193] proxy-service-jsfqv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0602 22:40:32.708205      21 runners.go:193] proxy-service-jsfqv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0602 22:40:33.708485      21 runners.go:193] proxy-service-jsfqv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0602 22:40:34.709142      21 runners.go:193] proxy-service-jsfqv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 22:40:34.728: INFO: setup took 4.128770097s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun  2 22:40:34.788: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 58.542132ms)
Jun  2 22:40:34.852: INFO: (0) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 123.364948ms)
Jun  2 22:40:34.898: INFO: (0) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 170.177318ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 169.812572ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 169.377704ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 169.319836ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 169.944348ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 169.870693ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 170.503863ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 170.420953ms)
Jun  2 22:40:34.899: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 170.508903ms)
Jun  2 22:40:34.900: INFO: (0) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 169.4587ms)
Jun  2 22:40:34.912: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 183.684462ms)
Jun  2 22:40:34.913: INFO: (0) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 183.695178ms)
Jun  2 22:40:34.937: INFO: (0) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 209.120854ms)
Jun  2 22:40:34.955: INFO: (0) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 225.750677ms)
Jun  2 22:40:35.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 54.68314ms)
Jun  2 22:40:35.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 54.507636ms)
Jun  2 22:40:35.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 55.740619ms)
Jun  2 22:40:35.011: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 53.875041ms)
Jun  2 22:40:35.013: INFO: (1) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 55.896807ms)
Jun  2 22:40:35.020: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 62.526929ms)
Jun  2 22:40:35.020: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 63.248522ms)
Jun  2 22:40:35.021: INFO: (1) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 64.595052ms)
Jun  2 22:40:35.021: INFO: (1) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 65.035703ms)
Jun  2 22:40:35.023: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 66.569866ms)
Jun  2 22:40:35.023: INFO: (1) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 66.803238ms)
Jun  2 22:40:35.024: INFO: (1) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 68.252101ms)
Jun  2 22:40:35.025: INFO: (1) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 69.547794ms)
Jun  2 22:40:35.025: INFO: (1) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 68.924181ms)
Jun  2 22:40:35.026: INFO: (1) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 68.513812ms)
Jun  2 22:40:35.027: INFO: (1) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 70.835631ms)
Jun  2 22:40:35.047: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 20.00542ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 24.911422ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 25.158795ms)
Jun  2 22:40:35.054: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 25.874092ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 25.322669ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 25.197529ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 24.943727ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 25.580975ms)
Jun  2 22:40:35.053: INFO: (2) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 25.654341ms)
Jun  2 22:40:35.058: INFO: (2) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 29.586778ms)
Jun  2 22:40:35.073: INFO: (2) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 44.534602ms)
Jun  2 22:40:35.074: INFO: (2) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 45.708723ms)
Jun  2 22:40:35.073: INFO: (2) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 45.081118ms)
Jun  2 22:40:35.073: INFO: (2) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 44.982603ms)
Jun  2 22:40:35.073: INFO: (2) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 45.212809ms)
Jun  2 22:40:35.074: INFO: (2) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 45.347967ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 30.117347ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 28.590663ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 29.201083ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 29.180752ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 29.266724ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 29.468841ms)
Jun  2 22:40:35.104: INFO: (3) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 29.660486ms)
Jun  2 22:40:35.105: INFO: (3) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 29.62937ms)
Jun  2 22:40:35.105: INFO: (3) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 29.813391ms)
Jun  2 22:40:35.105: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 30.502661ms)
Jun  2 22:40:35.106: INFO: (3) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 30.639547ms)
Jun  2 22:40:35.106: INFO: (3) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 31.296017ms)
Jun  2 22:40:35.106: INFO: (3) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 31.194618ms)
Jun  2 22:40:35.106: INFO: (3) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 30.274392ms)
Jun  2 22:40:35.106: INFO: (3) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 30.707634ms)
Jun  2 22:40:35.106: INFO: (3) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 31.486401ms)
Jun  2 22:40:35.124: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 17.174309ms)
Jun  2 22:40:35.134: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 27.441954ms)
Jun  2 22:40:35.141: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 34.527573ms)
Jun  2 22:40:35.143: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 36.555176ms)
Jun  2 22:40:35.143: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 36.136744ms)
Jun  2 22:40:35.143: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 36.359977ms)
Jun  2 22:40:35.143: INFO: (4) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 36.64879ms)
Jun  2 22:40:35.143: INFO: (4) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 36.683405ms)
Jun  2 22:40:35.144: INFO: (4) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 37.026924ms)
Jun  2 22:40:35.144: INFO: (4) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 36.979771ms)
Jun  2 22:40:35.144: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 37.415458ms)
Jun  2 22:40:35.144: INFO: (4) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 38.051369ms)
Jun  2 22:40:35.145: INFO: (4) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 37.888936ms)
Jun  2 22:40:35.156: INFO: (4) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 49.382893ms)
Jun  2 22:40:35.156: INFO: (4) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 49.922458ms)
Jun  2 22:40:35.156: INFO: (4) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 49.512257ms)
Jun  2 22:40:35.173: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 15.82002ms)
Jun  2 22:40:35.183: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 24.193967ms)
Jun  2 22:40:35.183: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 24.044315ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 36.210865ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 35.624775ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 35.916102ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 36.189919ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 36.510274ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 36.405947ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 36.486766ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 35.886893ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 35.622898ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 35.896311ms)
Jun  2 22:40:35.194: INFO: (5) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 36.521455ms)
Jun  2 22:40:35.195: INFO: (5) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 36.911489ms)
Jun  2 22:40:35.203: INFO: (5) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 45.958265ms)
Jun  2 22:40:35.233: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 29.038484ms)
Jun  2 22:40:35.241: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 35.246551ms)
Jun  2 22:40:35.243: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 37.223898ms)
Jun  2 22:40:35.243: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 37.41251ms)
Jun  2 22:40:35.244: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 38.677809ms)
Jun  2 22:40:35.246: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 40.56069ms)
Jun  2 22:40:35.246: INFO: (6) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 40.397697ms)
Jun  2 22:40:35.246: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 40.346214ms)
Jun  2 22:40:35.246: INFO: (6) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 41.648869ms)
Jun  2 22:40:35.252: INFO: (6) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 47.947787ms)
Jun  2 22:40:35.253: INFO: (6) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 47.912446ms)
Jun  2 22:40:35.253: INFO: (6) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 47.322562ms)
Jun  2 22:40:35.253: INFO: (6) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 47.178297ms)
Jun  2 22:40:35.253: INFO: (6) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 47.868436ms)
Jun  2 22:40:35.254: INFO: (6) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 49.24052ms)
Jun  2 22:40:35.254: INFO: (6) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 49.429678ms)
Jun  2 22:40:35.272: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 17.581508ms)
Jun  2 22:40:35.274: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 19.436737ms)
Jun  2 22:40:35.274: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 18.540156ms)
Jun  2 22:40:35.278: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 21.663964ms)
Jun  2 22:40:35.294: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 38.277653ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 38.312024ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 38.967897ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 38.646209ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 38.670309ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 39.201715ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 38.121505ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 40.297885ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 39.374218ms)
Jun  2 22:40:35.295: INFO: (7) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 39.815793ms)
Jun  2 22:40:35.303: INFO: (7) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 46.984923ms)
Jun  2 22:40:35.304: INFO: (7) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 47.518107ms)
Jun  2 22:40:35.322: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 17.668717ms)
Jun  2 22:40:35.322: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 17.559454ms)
Jun  2 22:40:35.327: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 22.449566ms)
Jun  2 22:40:35.328: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 23.913678ms)
Jun  2 22:40:35.328: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 23.681768ms)
Jun  2 22:40:35.328: INFO: (8) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 23.669943ms)
Jun  2 22:40:35.337: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 32.046456ms)
Jun  2 22:40:35.337: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 32.523346ms)
Jun  2 22:40:35.337: INFO: (8) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 32.340507ms)
Jun  2 22:40:35.337: INFO: (8) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 32.824397ms)
Jun  2 22:40:35.337: INFO: (8) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 32.499564ms)
Jun  2 22:40:35.353: INFO: (8) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 48.064228ms)
Jun  2 22:40:35.353: INFO: (8) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 48.293019ms)
Jun  2 22:40:35.353: INFO: (8) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 48.961648ms)
Jun  2 22:40:35.353: INFO: (8) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 47.865179ms)
Jun  2 22:40:35.353: INFO: (8) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 48.85771ms)
Jun  2 22:40:35.371: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 16.056733ms)
Jun  2 22:40:35.371: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 16.42037ms)
Jun  2 22:40:35.371: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 16.171086ms)
Jun  2 22:40:35.371: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 15.975027ms)
Jun  2 22:40:35.373: INFO: (9) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 17.529574ms)
Jun  2 22:40:35.374: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 18.306877ms)
Jun  2 22:40:35.374: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 18.278498ms)
Jun  2 22:40:35.374: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 19.199337ms)
Jun  2 22:40:35.374: INFO: (9) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 19.35784ms)
Jun  2 22:40:35.375: INFO: (9) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 18.585795ms)
Jun  2 22:40:35.375: INFO: (9) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 19.790783ms)
Jun  2 22:40:35.377: INFO: (9) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 22.054905ms)
Jun  2 22:40:35.378: INFO: (9) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 23.156795ms)
Jun  2 22:40:35.379: INFO: (9) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 23.533098ms)
Jun  2 22:40:35.379: INFO: (9) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 24.180233ms)
Jun  2 22:40:35.403: INFO: (9) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 48.375518ms)
Jun  2 22:40:35.422: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 16.908857ms)
Jun  2 22:40:35.422: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 16.425899ms)
Jun  2 22:40:35.422: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 17.080876ms)
Jun  2 22:40:35.422: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 16.564641ms)
Jun  2 22:40:35.425: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 18.610957ms)
Jun  2 22:40:35.426: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 20.53877ms)
Jun  2 22:40:35.433: INFO: (10) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 27.884387ms)
Jun  2 22:40:35.433: INFO: (10) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 27.806711ms)
Jun  2 22:40:35.439: INFO: (10) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 35.253841ms)
Jun  2 22:40:35.439: INFO: (10) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 34.880067ms)
Jun  2 22:40:35.440: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 34.899075ms)
Jun  2 22:40:35.440: INFO: (10) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 35.0705ms)
Jun  2 22:40:35.440: INFO: (10) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 36.18646ms)
Jun  2 22:40:35.441: INFO: (10) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 34.904057ms)
Jun  2 22:40:35.443: INFO: (10) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 38.116136ms)
Jun  2 22:40:35.448: INFO: (10) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 43.279374ms)
Jun  2 22:40:35.478: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 29.480466ms)
Jun  2 22:40:35.478: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 29.705841ms)
Jun  2 22:40:35.480: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 31.189959ms)
Jun  2 22:40:35.481: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 32.106547ms)
Jun  2 22:40:35.481: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 31.377893ms)
Jun  2 22:40:35.481: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 32.652797ms)
Jun  2 22:40:35.481: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 32.218066ms)
Jun  2 22:40:35.481: INFO: (11) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 32.121278ms)
Jun  2 22:40:35.481: INFO: (11) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 32.056409ms)
Jun  2 22:40:35.482: INFO: (11) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 33.522571ms)
Jun  2 22:40:35.492: INFO: (11) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 42.877729ms)
Jun  2 22:40:35.494: INFO: (11) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 44.908901ms)
Jun  2 22:40:35.494: INFO: (11) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 45.690288ms)
Jun  2 22:40:35.500: INFO: (11) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 51.597186ms)
Jun  2 22:40:35.501: INFO: (11) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 51.668732ms)
Jun  2 22:40:35.512: INFO: (11) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 63.159215ms)
Jun  2 22:40:35.585: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 69.932732ms)
Jun  2 22:40:35.598: INFO: (12) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 81.154388ms)
Jun  2 22:40:35.598: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 84.581474ms)
Jun  2 22:40:35.598: INFO: (12) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 81.908741ms)
Jun  2 22:40:35.606: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 92.555178ms)
Jun  2 22:40:35.621: INFO: (12) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 106.781424ms)
Jun  2 22:40:35.628: INFO: (12) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 114.0508ms)
Jun  2 22:40:35.644: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 128.608902ms)
Jun  2 22:40:35.644: INFO: (12) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 130.010533ms)
Jun  2 22:40:35.644: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 128.42708ms)
Jun  2 22:40:35.644: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 131.537891ms)
Jun  2 22:40:35.644: INFO: (12) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 127.83014ms)
Jun  2 22:40:35.647: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 133.018289ms)
Jun  2 22:40:35.647: INFO: (12) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 131.719733ms)
Jun  2 22:40:35.656: INFO: (12) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 142.962981ms)
Jun  2 22:40:35.660: INFO: (12) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 144.742331ms)
Jun  2 22:40:35.677: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 16.462518ms)
Jun  2 22:40:35.683: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 21.376206ms)
Jun  2 22:40:35.683: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 22.12449ms)
Jun  2 22:40:35.683: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 22.208489ms)
Jun  2 22:40:35.684: INFO: (13) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 22.505584ms)
Jun  2 22:40:35.684: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 23.499137ms)
Jun  2 22:40:35.684: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 22.932921ms)
Jun  2 22:40:35.684: INFO: (13) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 22.67999ms)
Jun  2 22:40:35.685: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 25.081614ms)
Jun  2 22:40:35.685: INFO: (13) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 24.089542ms)
Jun  2 22:40:35.688: INFO: (13) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 27.19013ms)
Jun  2 22:40:35.689: INFO: (13) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 28.13911ms)
Jun  2 22:40:35.689: INFO: (13) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 28.861318ms)
Jun  2 22:40:35.691: INFO: (13) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 29.175274ms)
Jun  2 22:40:35.691: INFO: (13) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 29.761834ms)
Jun  2 22:40:35.696: INFO: (13) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 34.646673ms)
Jun  2 22:40:35.718: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 20.356161ms)
Jun  2 22:40:35.718: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 20.499512ms)
Jun  2 22:40:35.718: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 20.475397ms)
Jun  2 22:40:35.718: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 20.367869ms)
Jun  2 22:40:35.721: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 23.479854ms)
Jun  2 22:40:35.723: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 25.349062ms)
Jun  2 22:40:35.725: INFO: (14) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 27.655786ms)
Jun  2 22:40:35.725: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 27.844324ms)
Jun  2 22:40:35.725: INFO: (14) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 28.805028ms)
Jun  2 22:40:35.727: INFO: (14) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 28.953454ms)
Jun  2 22:40:35.727: INFO: (14) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 29.535781ms)
Jun  2 22:40:35.730: INFO: (14) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 33.234017ms)
Jun  2 22:40:35.741: INFO: (14) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 44.01485ms)
Jun  2 22:40:35.742: INFO: (14) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 44.631046ms)
Jun  2 22:40:35.743: INFO: (14) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 46.448403ms)
Jun  2 22:40:35.744: INFO: (14) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 46.751482ms)
Jun  2 22:40:35.785: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 40.730847ms)
Jun  2 22:40:35.785: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 39.811764ms)
Jun  2 22:40:35.786: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 40.87358ms)
Jun  2 22:40:35.786: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 41.129532ms)
Jun  2 22:40:35.786: INFO: (15) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 41.197063ms)
Jun  2 22:40:35.787: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 41.746977ms)
Jun  2 22:40:35.789: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 43.540741ms)
Jun  2 22:40:35.790: INFO: (15) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 44.792211ms)
Jun  2 22:40:35.791: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 46.706272ms)
Jun  2 22:40:35.791: INFO: (15) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 45.581883ms)
Jun  2 22:40:35.794: INFO: (15) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 49.46171ms)
Jun  2 22:40:35.794: INFO: (15) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 49.395761ms)
Jun  2 22:40:35.794: INFO: (15) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 48.811682ms)
Jun  2 22:40:35.795: INFO: (15) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 49.33762ms)
Jun  2 22:40:35.796: INFO: (15) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 51.259394ms)
Jun  2 22:40:35.804: INFO: (15) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 58.83779ms)
Jun  2 22:40:35.826: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 20.897338ms)
Jun  2 22:40:35.829: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 23.9229ms)
Jun  2 22:40:35.829: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 23.952711ms)
Jun  2 22:40:35.829: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 24.198609ms)
Jun  2 22:40:35.830: INFO: (16) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 25.307873ms)
Jun  2 22:40:35.831: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 26.313274ms)
Jun  2 22:40:35.832: INFO: (16) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 26.327032ms)
Jun  2 22:40:35.832: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 27.19417ms)
Jun  2 22:40:35.834: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 28.761004ms)
Jun  2 22:40:35.834: INFO: (16) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 28.904608ms)
Jun  2 22:40:35.835: INFO: (16) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 28.18706ms)
Jun  2 22:40:35.856: INFO: (16) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 51.173698ms)
Jun  2 22:40:35.857: INFO: (16) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 52.182448ms)
Jun  2 22:40:35.858: INFO: (16) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 52.914748ms)
Jun  2 22:40:35.858: INFO: (16) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 52.821721ms)
Jun  2 22:40:35.858: INFO: (16) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 53.014665ms)
Jun  2 22:40:35.882: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 23.478606ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 24.007948ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 24.431844ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 23.566991ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 24.305708ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 23.978435ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 23.955076ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 24.302178ms)
Jun  2 22:40:35.883: INFO: (17) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 24.877362ms)
Jun  2 22:40:35.884: INFO: (17) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 24.986082ms)
Jun  2 22:40:35.885: INFO: (17) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 26.84267ms)
Jun  2 22:40:35.888: INFO: (17) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 29.188494ms)
Jun  2 22:40:35.889: INFO: (17) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 30.551778ms)
Jun  2 22:40:35.889: INFO: (17) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 30.577759ms)
Jun  2 22:40:35.890: INFO: (17) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 31.221268ms)
Jun  2 22:40:35.891: INFO: (17) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 32.580662ms)
Jun  2 22:40:35.911: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 19.16434ms)
Jun  2 22:40:35.913: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 21.606211ms)
Jun  2 22:40:35.914: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 22.489228ms)
Jun  2 22:40:35.916: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 24.316799ms)
Jun  2 22:40:35.916: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 24.451669ms)
Jun  2 22:40:35.917: INFO: (18) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 24.768554ms)
Jun  2 22:40:35.917: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 24.79044ms)
Jun  2 22:40:35.918: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 25.777124ms)
Jun  2 22:40:35.918: INFO: (18) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 26.24812ms)
Jun  2 22:40:35.918: INFO: (18) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 25.781377ms)
Jun  2 22:40:35.920: INFO: (18) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 29.180983ms)
Jun  2 22:40:35.920: INFO: (18) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 28.799089ms)
Jun  2 22:40:35.920: INFO: (18) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 28.442337ms)
Jun  2 22:40:35.925: INFO: (18) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 33.174138ms)
Jun  2 22:40:35.925: INFO: (18) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 32.993837ms)
Jun  2 22:40:35.925: INFO: (18) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 33.390294ms)
Jun  2 22:40:35.943: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:443/proxy/tlsrewritem... (200; 17.077836ms)
Jun  2 22:40:35.950: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 23.835107ms)
Jun  2 22:40:35.950: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 24.144357ms)
Jun  2 22:40:35.951: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj/proxy/rewriteme">test</a> (200; 25.40393ms)
Jun  2 22:40:35.951: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:162/proxy/: bar (200; 25.752694ms)
Jun  2 22:40:35.952: INFO: (19) /api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/http:proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">... (200; 25.978074ms)
Jun  2 22:40:35.952: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/: <a href="/api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:1080/proxy/rewriteme">test<... (200; 26.390461ms)
Jun  2 22:40:35.952: INFO: (19) /api/v1/namespaces/proxy-7655/pods/proxy-service-jsfqv-w99qj:160/proxy/: foo (200; 26.218836ms)
Jun  2 22:40:35.953: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:462/proxy/: tls qux (200; 26.907376ms)
Jun  2 22:40:35.953: INFO: (19) /api/v1/namespaces/proxy-7655/pods/https:proxy-service-jsfqv-w99qj:460/proxy/: tls baz (200; 27.184772ms)
Jun  2 22:40:35.956: INFO: (19) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname1/proxy/: foo (200; 30.31775ms)
Jun  2 22:40:35.959: INFO: (19) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname1/proxy/: foo (200; 33.170541ms)
Jun  2 22:40:35.960: INFO: (19) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname1/proxy/: tls baz (200; 33.955339ms)
Jun  2 22:40:35.969: INFO: (19) /api/v1/namespaces/proxy-7655/services/http:proxy-service-jsfqv:portname2/proxy/: bar (200; 43.089279ms)
Jun  2 22:40:35.975: INFO: (19) /api/v1/namespaces/proxy-7655/services/proxy-service-jsfqv:portname2/proxy/: bar (200; 49.729681ms)
Jun  2 22:40:35.976: INFO: (19) /api/v1/namespaces/proxy-7655/services/https:proxy-service-jsfqv:tlsportname2/proxy/: tls qux (200; 50.360111ms)
STEP: deleting ReplicationController proxy-service-jsfqv in namespace proxy-7655, will wait for the garbage collector to delete the pods
Jun  2 22:40:36.073: INFO: Deleting ReplicationController proxy-service-jsfqv took: 24.179336ms
Jun  2 22:40:36.474: INFO: Terminating ReplicationController proxy-service-jsfqv pods took: 400.405205ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:40:37.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7655" for this suite.

• [SLOW TEST:7.493 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":262,"skipped":5246,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:40:37.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8119
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-bb4243dc-6462-418f-ae35-26487df1df93
STEP: Creating the pod
Jun  2 22:40:38.064: INFO: The status of Pod pod-configmaps-2124bc2e-7205-4c89-a5e0-147eaf625cfc is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:40:40.097: INFO: The status of Pod pod-configmaps-2124bc2e-7205-4c89-a5e0-147eaf625cfc is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-bb4243dc-6462-418f-ae35-26487df1df93
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:40:42.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8119" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":5264,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:40:42.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7445
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7445
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Jun  2 22:40:42.557: INFO: Found 0 stateful pods, waiting for 3
Jun  2 22:40:52.581: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:40:52.581: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:40:52.582: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jun  2 22:40:52.672: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun  2 22:41:02.786: INFO: Updating stateful set ss2
Jun  2 22:41:02.813: INFO: Waiting for Pod statefulset-7445/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jun  2 22:41:12.939: INFO: Found 2 stateful pods, waiting for 3
Jun  2 22:41:22.955: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:41:22.955: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:41:22.955: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun  2 22:41:23.024: INFO: Updating stateful set ss2
Jun  2 22:41:23.074: INFO: Waiting for Pod statefulset-7445/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jun  2 22:41:33.171: INFO: Updating stateful set ss2
Jun  2 22:41:33.195: INFO: Waiting for StatefulSet statefulset-7445/ss2 to complete update
Jun  2 22:41:33.195: INFO: Waiting for Pod statefulset-7445/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 22:41:43.224: INFO: Deleting all statefulset in ns statefulset-7445
Jun  2 22:41:43.237: INFO: Scaling statefulset ss2 to 0
Jun  2 22:41:53.292: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 22:41:53.306: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:41:53.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7445" for this suite.

• [SLOW TEST:71.071 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":264,"skipped":5278,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:41:53.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-7871
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun  2 22:41:53.592: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  2 22:42:53.680: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:42:53.705: INFO: Starting informer...
STEP: Starting pods...
Jun  2 22:42:53.784: INFO: Pod1 is running on 10.134.156.247. Tainting Node
Jun  2 22:42:58.080: INFO: Pod2 is running on 10.134.156.247. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun  2 22:43:04.147: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun  2 22:43:24.173: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:24.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7871" for this suite.

• [SLOW TEST:90.868 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":265,"skipped":5288,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:24.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3770
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun  2 22:43:24.599: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:24.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3770" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":266,"skipped":5308,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:24.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6612
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:36.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6612" for this suite.

• [SLOW TEST:11.356 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":267,"skipped":5314,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:36.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6031
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  2 22:43:36.405: INFO: Waiting up to 5m0s for pod "pod-333014ad-b526-4f0f-9d50-108bcf603be1" in namespace "emptydir-6031" to be "Succeeded or Failed"
Jun  2 22:43:36.416: INFO: Pod "pod-333014ad-b526-4f0f-9d50-108bcf603be1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.011529ms
Jun  2 22:43:38.432: INFO: Pod "pod-333014ad-b526-4f0f-9d50-108bcf603be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026711526s
Jun  2 22:43:40.455: INFO: Pod "pod-333014ad-b526-4f0f-9d50-108bcf603be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049870662s
STEP: Saw pod success
Jun  2 22:43:40.455: INFO: Pod "pod-333014ad-b526-4f0f-9d50-108bcf603be1" satisfied condition "Succeeded or Failed"
Jun  2 22:43:40.467: INFO: Trying to get logs from node 10.134.156.247 pod pod-333014ad-b526-4f0f-9d50-108bcf603be1 container test-container: <nil>
STEP: delete the pod
Jun  2 22:43:40.571: INFO: Waiting for pod pod-333014ad-b526-4f0f-9d50-108bcf603be1 to disappear
Jun  2 22:43:40.581: INFO: Pod pod-333014ad-b526-4f0f-9d50-108bcf603be1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:40.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6031" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":5318,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:40.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9628
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jun  2 22:43:40.831: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  2 22:43:45.863: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:46.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9628" for this suite.

• [SLOW TEST:5.426 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":269,"skipped":5322,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:46.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5213
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-bfb92012-969a-4bce-a9bf-4da993797a22
STEP: Creating a pod to test consume configMaps
Jun  2 22:43:46.334: INFO: Waiting up to 5m0s for pod "pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb" in namespace "configmap-5213" to be "Succeeded or Failed"
Jun  2 22:43:46.346: INFO: Pod "pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.501808ms
Jun  2 22:43:48.364: INFO: Pod "pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029193295s
Jun  2 22:43:50.387: INFO: Pod "pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05261406s
STEP: Saw pod success
Jun  2 22:43:50.387: INFO: Pod "pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb" satisfied condition "Succeeded or Failed"
Jun  2 22:43:50.398: INFO: Trying to get logs from node 10.134.156.247 pod pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:43:50.477: INFO: Waiting for pod pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb to disappear
Jun  2 22:43:50.489: INFO: Pod pod-configmaps-807366d4-bc79-4499-8fd3-be46b5b6eedb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:50.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5213" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":5338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:50.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7140
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
Jun  2 22:43:50.736: INFO: Waiting up to 5m0s for pod "var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c" in namespace "var-expansion-7140" to be "Succeeded or Failed"
Jun  2 22:43:50.745: INFO: Pod "var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.374914ms
Jun  2 22:43:52.761: INFO: Pod "var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024587946s
Jun  2 22:43:54.774: INFO: Pod "var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037982108s
Jun  2 22:43:56.799: INFO: Pod "var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063241887s
STEP: Saw pod success
Jun  2 22:43:56.800: INFO: Pod "var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c" satisfied condition "Succeeded or Failed"
Jun  2 22:43:56.817: INFO: Trying to get logs from node 10.134.156.247 pod var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c container dapi-container: <nil>
STEP: delete the pod
Jun  2 22:43:56.877: INFO: Waiting for pod var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c to disappear
Jun  2 22:43:56.888: INFO: Pod var-expansion-d13e512c-2a0a-41e8-a747-45a22c124e9c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:43:56.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7140" for this suite.

• [SLOW TEST:6.401 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":5364,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:43:56.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1351
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  2 22:43:57.142: INFO: Waiting up to 5m0s for pod "pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154" in namespace "emptydir-1351" to be "Succeeded or Failed"
Jun  2 22:43:57.151: INFO: Pod "pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154": Phase="Pending", Reason="", readiness=false. Elapsed: 9.519153ms
Jun  2 22:43:59.188: INFO: Pod "pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046419754s
Jun  2 22:44:01.205: INFO: Pod "pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063026702s
Jun  2 22:44:03.222: INFO: Pod "pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.080378683s
STEP: Saw pod success
Jun  2 22:44:03.222: INFO: Pod "pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154" satisfied condition "Succeeded or Failed"
Jun  2 22:44:03.233: INFO: Trying to get logs from node 10.134.156.247 pod pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154 container test-container: <nil>
STEP: delete the pod
Jun  2 22:44:03.309: INFO: Waiting for pod pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154 to disappear
Jun  2 22:44:03.319: INFO: Pod pod-cd583185-0d16-47a9-8ec3-ffe2d00c5154 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:03.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1351" for this suite.

• [SLOW TEST:6.436 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":5364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:03.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1056
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5679
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1740
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:20.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1056" for this suite.
STEP: Destroying namespace "nsdeletetest-5679" for this suite.
Jun  2 22:44:20.090: INFO: Namespace nsdeletetest-5679 was already deleted
STEP: Destroying namespace "nsdeletetest-1740" for this suite.

• [SLOW TEST:16.742 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":273,"skipped":5391,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:20.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7670
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun  2 22:44:22.911: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7670 pod-service-account-97328df1-1364-4732-b868-b6dfb94df9cc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun  2 22:44:23.438: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7670 pod-service-account-97328df1-1364-4732-b868-b6dfb94df9cc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun  2 22:44:23.775: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7670 pod-service-account-97328df1-1364-4732-b868-b6dfb94df9cc -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:24.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7670" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":274,"skipped":5393,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:24.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8874
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:44:24.409: INFO: The status of Pod busybox-host-aliases711777c6-d51e-4834-ae4d-56531ec3b231 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:44:26.426: INFO: The status of Pod busybox-host-aliases711777c6-d51e-4834-ae4d-56531ec3b231 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:26.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8874" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5402,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:26.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1325
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 22:44:27.055: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 22:44:29.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 44, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 44, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 44, 27, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 44, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 22:44:32.201: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:32.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1325" for this suite.
STEP: Destroying namespace "webhook-1325-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":276,"skipped":5404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:33.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-654
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun  2 22:44:33.294: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-654  9662268b-4ca0-491d-a27a-d3d23b5b5959 47071 0 2022-06-02 22:44:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-02 22:44:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:44:33.294: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-654  9662268b-4ca0-491d-a27a-d3d23b5b5959 47072 0 2022-06-02 22:44:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-02 22:44:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun  2 22:44:33.399: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-654  9662268b-4ca0-491d-a27a-d3d23b5b5959 47073 0 2022-06-02 22:44:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-02 22:44:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  2 22:44:33.401: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-654  9662268b-4ca0-491d-a27a-d3d23b5b5959 47074 0 2022-06-02 22:44:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-06-02 22:44:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:33.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-654" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":277,"skipped":5429,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:33.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-4947
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jun  2 22:44:33.645: INFO: Waiting up to 5m0s for pod "security-context-6283a160-6e04-4582-a75d-23c35a2d3762" in namespace "security-context-4947" to be "Succeeded or Failed"
Jun  2 22:44:33.656: INFO: Pod "security-context-6283a160-6e04-4582-a75d-23c35a2d3762": Phase="Pending", Reason="", readiness=false. Elapsed: 10.714861ms
Jun  2 22:44:35.670: INFO: Pod "security-context-6283a160-6e04-4582-a75d-23c35a2d3762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024352486s
Jun  2 22:44:37.691: INFO: Pod "security-context-6283a160-6e04-4582-a75d-23c35a2d3762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046006107s
STEP: Saw pod success
Jun  2 22:44:37.691: INFO: Pod "security-context-6283a160-6e04-4582-a75d-23c35a2d3762" satisfied condition "Succeeded or Failed"
Jun  2 22:44:37.701: INFO: Trying to get logs from node 10.134.156.247 pod security-context-6283a160-6e04-4582-a75d-23c35a2d3762 container test-container: <nil>
STEP: delete the pod
Jun  2 22:44:37.758: INFO: Waiting for pod security-context-6283a160-6e04-4582-a75d-23c35a2d3762 to disappear
Jun  2 22:44:37.767: INFO: Pod security-context-6283a160-6e04-4582-a75d-23c35a2d3762 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:37.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4947" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":278,"skipped":5438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:37.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9285
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:49.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9285" for this suite.

• [SLOW TEST:11.608 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":279,"skipped":5482,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:49.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6586
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-ad2389ca-d890-4f60-831f-3d97116f627f
STEP: Creating a pod to test consume configMaps
Jun  2 22:44:49.659: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b" in namespace "projected-6586" to be "Succeeded or Failed"
Jun  2 22:44:49.688: INFO: Pod "pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.031021ms
Jun  2 22:44:51.736: INFO: Pod "pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077698164s
Jun  2 22:44:53.764: INFO: Pod "pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10556731s
Jun  2 22:44:55.780: INFO: Pod "pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.121508468s
STEP: Saw pod success
Jun  2 22:44:55.781: INFO: Pod "pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b" satisfied condition "Succeeded or Failed"
Jun  2 22:44:55.796: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:44:55.850: INFO: Waiting for pod pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b to disappear
Jun  2 22:44:55.887: INFO: Pod pod-projected-configmaps-8b9d7c7b-2307-47c3-a00a-96f500d7b59b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:44:55.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6586" for this suite.

• [SLOW TEST:6.507 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":5493,"failed":0}
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:44:55.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-128
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:44:56.156: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4bffc926-7e02-4466-850e-f439817a3172" in namespace "security-context-test-128" to be "Succeeded or Failed"
Jun  2 22:44:56.190: INFO: Pod "alpine-nnp-false-4bffc926-7e02-4466-850e-f439817a3172": Phase="Pending", Reason="", readiness=false. Elapsed: 33.738849ms
Jun  2 22:44:58.206: INFO: Pod "alpine-nnp-false-4bffc926-7e02-4466-850e-f439817a3172": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050107437s
Jun  2 22:45:00.227: INFO: Pod "alpine-nnp-false-4bffc926-7e02-4466-850e-f439817a3172": Phase="Running", Reason="", readiness=false. Elapsed: 4.070815924s
Jun  2 22:45:02.245: INFO: Pod "alpine-nnp-false-4bffc926-7e02-4466-850e-f439817a3172": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088719196s
Jun  2 22:45:02.245: INFO: Pod "alpine-nnp-false-4bffc926-7e02-4466-850e-f439817a3172" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:45:02.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-128" for this suite.

• [SLOW TEST:6.380 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5493,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:45:02.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4438
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-608db9ea-305b-4912-8fcf-728dbffaeae1
STEP: Creating a pod to test consume secrets
Jun  2 22:45:02.530: INFO: Waiting up to 5m0s for pod "pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d" in namespace "secrets-4438" to be "Succeeded or Failed"
Jun  2 22:45:02.574: INFO: Pod "pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 44.06657ms
Jun  2 22:45:04.619: INFO: Pod "pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089219727s
Jun  2 22:45:06.633: INFO: Pod "pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.10301323s
STEP: Saw pod success
Jun  2 22:45:06.633: INFO: Pod "pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d" satisfied condition "Succeeded or Failed"
Jun  2 22:45:06.642: INFO: Trying to get logs from node 10.134.156.247 pod pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 22:45:06.706: INFO: Waiting for pod pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d to disappear
Jun  2 22:45:06.717: INFO: Pod pod-secrets-13581a01-9154-4369-9406-5385d8b70f0d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:45:06.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4438" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":5508,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:45:06.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5375
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5375
Jun  2 22:45:07.035: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:45:09.073: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jun  2 22:45:09.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun  2 22:45:09.449: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun  2 22:45:09.449: INFO: stdout: "iptables"
Jun  2 22:45:09.449: INFO: proxyMode: iptables
Jun  2 22:45:09.542: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun  2 22:45:09.552: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5375
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5375
I0602 22:45:09.597583      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5375, replica count: 3
I0602 22:45:12.648544      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  2 22:45:12.722: INFO: Creating new exec pod
Jun  2 22:45:15.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jun  2 22:45:16.120: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun  2 22:45:16.120: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:45:16.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.150.127 80'
Jun  2 22:45:16.427: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.150.127 80\nConnection to 172.21.150.127 80 port [tcp/http] succeeded!\n"
Jun  2 22:45:16.427: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:45:16.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.247 32256'
Jun  2 22:45:16.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.247 32256\nConnection to 10.134.156.247 32256 port [tcp/*] succeeded!\n"
Jun  2 22:45:16.732: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:45:16.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.134.156.209 32256'
Jun  2 22:45:17.009: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.134.156.209 32256\nConnection to 10.134.156.209 32256 port [tcp/*] succeeded!\n"
Jun  2 22:45:17.009: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun  2 22:45:17.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.134.156.209:32256/ ; done'
Jun  2 22:45:17.400: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n"
Jun  2 22:45:17.400: INFO: stdout: "\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8\naffinity-nodeport-timeout-g5jh8"
Jun  2 22:45:17.400: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.400: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.400: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.400: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.400: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.400: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.401: INFO: Received response from host: affinity-nodeport-timeout-g5jh8
Jun  2 22:45:17.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.134.156.209:32256/'
Jun  2 22:45:17.670: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n"
Jun  2 22:45:17.670: INFO: stdout: "affinity-nodeport-timeout-g5jh8"
Jun  2 22:45:37.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=services-5375 exec execpod-affinitydp5l6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.134.156.209:32256/'
Jun  2 22:45:37.987: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.134.156.209:32256/\n"
Jun  2 22:45:37.987: INFO: stdout: "affinity-nodeport-timeout-cd5cv"
Jun  2 22:45:37.987: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5375, will wait for the garbage collector to delete the pods
Jun  2 22:45:38.118: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 18.111854ms
Jun  2 22:45:38.319: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.630697ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:45:41.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5375" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:34.496 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":283,"skipped":5523,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:45:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3855
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-d249af02-f86d-476f-9145-2b06084b319f
STEP: Creating a pod to test consume configMaps
Jun  2 22:45:41.487: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc" in namespace "projected-3855" to be "Succeeded or Failed"
Jun  2 22:45:41.524: INFO: Pod "pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc": Phase="Pending", Reason="", readiness=false. Elapsed: 36.413378ms
Jun  2 22:45:43.541: INFO: Pod "pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05374591s
Jun  2 22:45:45.559: INFO: Pod "pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071190612s
Jun  2 22:45:47.593: INFO: Pod "pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.10584418s
STEP: Saw pod success
Jun  2 22:45:47.594: INFO: Pod "pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc" satisfied condition "Succeeded or Failed"
Jun  2 22:45:47.628: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:45:47.781: INFO: Waiting for pod pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc to disappear
Jun  2 22:45:47.792: INFO: Pod pod-projected-configmaps-92af5922-6134-46c5-bdd4-57cf49c7efbc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:45:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3855" for this suite.

• [SLOW TEST:6.564 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5527,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:45:47.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7452
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
Jun  2 22:47:48.717: INFO: Successfully updated pod "var-expansion-6e2edaad-cddd-411b-babc-929c077ddb0b"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun  2 22:47:50.764: INFO: Deleting pod "var-expansion-6e2edaad-cddd-411b-babc-929c077ddb0b" in namespace "var-expansion-7452"
Jun  2 22:47:50.827: INFO: Wait up to 5m0s for pod "var-expansion-6e2edaad-cddd-411b-babc-929c077ddb0b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:22.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7452" for this suite.

• [SLOW TEST:155.061 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":285,"skipped":5529,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:22.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8576
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:48:23.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e" in namespace "downward-api-8576" to be "Succeeded or Failed"
Jun  2 22:48:23.160: INFO: Pod "downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.473487ms
Jun  2 22:48:25.204: INFO: Pod "downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052067733s
Jun  2 22:48:27.227: INFO: Pod "downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075266774s
STEP: Saw pod success
Jun  2 22:48:27.227: INFO: Pod "downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e" satisfied condition "Succeeded or Failed"
Jun  2 22:48:27.237: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e container client-container: <nil>
STEP: delete the pod
Jun  2 22:48:27.419: INFO: Waiting for pod downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e to disappear
Jun  2 22:48:27.432: INFO: Pod downwardapi-volume-3ea9ae0f-33bb-4fa6-8c3e-a78ad6b0db0e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:27.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8576" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":5565,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:27.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7467
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:48:27.781: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938" in namespace "projected-7467" to be "Succeeded or Failed"
Jun  2 22:48:27.818: INFO: Pod "downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938": Phase="Pending", Reason="", readiness=false. Elapsed: 36.77929ms
Jun  2 22:48:29.865: INFO: Pod "downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083850435s
Jun  2 22:48:31.919: INFO: Pod "downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137465997s
Jun  2 22:48:33.945: INFO: Pod "downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.162924514s
STEP: Saw pod success
Jun  2 22:48:33.945: INFO: Pod "downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938" satisfied condition "Succeeded or Failed"
Jun  2 22:48:33.985: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938 container client-container: <nil>
STEP: delete the pod
Jun  2 22:48:34.043: INFO: Waiting for pod downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938 to disappear
Jun  2 22:48:34.074: INFO: Pod downwardapi-volume-a9331675-0808-4d78-b713-97da1fe0d938 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:34.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7467" for this suite.

• [SLOW TEST:6.599 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:34.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7895
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
Jun  2 22:48:34.383: INFO: Waiting up to 5m0s for pod "client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904" in namespace "containers-7895" to be "Succeeded or Failed"
Jun  2 22:48:34.392: INFO: Pod "client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200867ms
Jun  2 22:48:36.431: INFO: Pod "client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047136287s
Jun  2 22:48:38.445: INFO: Pod "client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061525093s
STEP: Saw pod success
Jun  2 22:48:38.445: INFO: Pod "client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904" satisfied condition "Succeeded or Failed"
Jun  2 22:48:38.455: INFO: Trying to get logs from node 10.134.156.247 pod client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:48:38.526: INFO: Waiting for pod client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904 to disappear
Jun  2 22:48:38.566: INFO: Pod client-containers-6ab509d2-10d7-4a71-b8aa-52f2cab9b904 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:38.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7895" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":5657,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:38.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7975
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  2 22:48:38.914: INFO: Waiting up to 5m0s for pod "pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0" in namespace "emptydir-7975" to be "Succeeded or Failed"
Jun  2 22:48:38.923: INFO: Pod "pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.147558ms
Jun  2 22:48:40.952: INFO: Pod "pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038471956s
Jun  2 22:48:42.965: INFO: Pod "pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051233672s
Jun  2 22:48:45.005: INFO: Pod "pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091269853s
STEP: Saw pod success
Jun  2 22:48:45.005: INFO: Pod "pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0" satisfied condition "Succeeded or Failed"
Jun  2 22:48:45.016: INFO: Trying to get logs from node 10.134.156.247 pod pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0 container test-container: <nil>
STEP: delete the pod
Jun  2 22:48:45.101: INFO: Waiting for pod pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0 to disappear
Jun  2 22:48:45.110: INFO: Pod pod-a8cf1027-8718-45f5-8b0f-df8d95b02da0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:45.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7975" for this suite.

• [SLOW TEST:6.541 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":289,"skipped":5665,"failed":0}
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:45.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-7457
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:49.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7457" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":290,"skipped":5666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:49.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1661
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-2c3afe00-775a-4fb7-9d66-f080e1dd3ada
STEP: Creating secret with name s-test-opt-upd-d5bb93c2-0228-42f9-ad0a-dc55afd6942e
STEP: Creating the pod
Jun  2 22:48:50.013: INFO: The status of Pod pod-secrets-a20b5a9f-121e-4bd3-abd4-e79722df6407 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:48:52.028: INFO: The status of Pod pod-secrets-a20b5a9f-121e-4bd3-abd4-e79722df6407 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-2c3afe00-775a-4fb7-9d66-f080e1dd3ada
STEP: Updating secret s-test-opt-upd-d5bb93c2-0228-42f9-ad0a-dc55afd6942e
STEP: Creating secret with name s-test-opt-create-a1c83ec9-d6a8-4834-bc47-104850e456ad
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:48:54.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1661" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:48:54.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4405
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  2 22:48:55.405: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 22:48:58.486: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:48:58.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:02.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4405" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.957 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":292,"skipped":5743,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:02.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8460
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jun  2 22:49:02.665: INFO: The status of Pod pod-update-217f3b99-a7c0-4c5a-aa72-26c3640aa607 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:49:04.680: INFO: The status of Pod pod-update-217f3b99-a7c0-4c5a-aa72-26c3640aa607 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:49:06.679: INFO: The status of Pod pod-update-217f3b99-a7c0-4c5a-aa72-26c3640aa607 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  2 22:49:07.252: INFO: Successfully updated pod "pod-update-217f3b99-a7c0-4c5a-aa72-26c3640aa607"
STEP: verifying the updated pod is in kubernetes
Jun  2 22:49:07.278: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:07.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8460" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5749,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8043
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:07.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8043" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5792,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:07.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6042
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-ff893b21-7cd3-48e9-adca-a3b07a0ec342
STEP: Creating a pod to test consume configMaps
Jun  2 22:49:08.073: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321" in namespace "configmap-6042" to be "Succeeded or Failed"
Jun  2 22:49:08.086: INFO: Pod "pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321": Phase="Pending", Reason="", readiness=false. Elapsed: 12.319892ms
Jun  2 22:49:10.100: INFO: Pod "pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026869048s
Jun  2 22:49:12.118: INFO: Pod "pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044430285s
STEP: Saw pod success
Jun  2 22:49:12.118: INFO: Pod "pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321" satisfied condition "Succeeded or Failed"
Jun  2 22:49:12.129: INFO: Trying to get logs from node 10.134.156.247 pod pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  2 22:49:12.263: INFO: Waiting for pod pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321 to disappear
Jun  2 22:49:12.274: INFO: Pod pod-configmaps-6c8c0ef1-6871-4ee5-a618-1e205d002321 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:12.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6042" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5800,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:12.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4457
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  2 22:49:17.690: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:17.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4457" for this suite.

• [SLOW TEST:5.462 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5820,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:17.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7042
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Jun  2 22:49:17.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 create -f -'
Jun  2 22:49:19.291: INFO: stderr: ""
Jun  2 22:49:19.291: INFO: stdout: "pod/pause created\n"
Jun  2 22:49:19.291: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  2 22:49:19.291: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7042" to be "running and ready"
Jun  2 22:49:19.323: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 32.250091ms
Jun  2 22:49:21.340: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.049430481s
Jun  2 22:49:21.340: INFO: Pod "pause" satisfied condition "running and ready"
Jun  2 22:49:21.340: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
Jun  2 22:49:21.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 label pods pause testing-label=testing-label-value'
Jun  2 22:49:21.488: INFO: stderr: ""
Jun  2 22:49:21.488: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun  2 22:49:21.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 get pod pause -L testing-label'
Jun  2 22:49:21.601: INFO: stderr: ""
Jun  2 22:49:21.601: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun  2 22:49:21.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 label pods pause testing-label-'
Jun  2 22:49:21.752: INFO: stderr: ""
Jun  2 22:49:21.752: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun  2 22:49:21.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 get pod pause -L testing-label'
Jun  2 22:49:21.853: INFO: stderr: ""
Jun  2 22:49:21.853: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
Jun  2 22:49:21.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 delete --grace-period=0 --force -f -'
Jun  2 22:49:22.015: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:49:22.015: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  2 22:49:22.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 get rc,svc -l name=pause --no-headers'
Jun  2 22:49:22.129: INFO: stderr: "No resources found in kubectl-7042 namespace.\n"
Jun  2 22:49:22.129: INFO: stdout: ""
Jun  2 22:49:22.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7042 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  2 22:49:22.224: INFO: stderr: ""
Jun  2 22:49:22.224: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:22.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7042" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":297,"skipped":5828,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:22.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6263
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Jun  2 22:49:22.450: INFO: namespace kubectl-6263
Jun  2 22:49:22.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6263 create -f -'
Jun  2 22:49:22.757: INFO: stderr: ""
Jun  2 22:49:22.757: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  2 22:49:23.773: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 22:49:23.773: INFO: Found 0 / 1
Jun  2 22:49:24.786: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 22:49:24.786: INFO: Found 0 / 1
Jun  2 22:49:25.772: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 22:49:25.773: INFO: Found 1 / 1
Jun  2 22:49:25.773: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  2 22:49:25.785: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 22:49:25.785: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  2 22:49:25.785: INFO: wait on agnhost-primary startup in kubectl-6263 
Jun  2 22:49:25.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6263 logs agnhost-primary-cnzw8 agnhost-primary'
Jun  2 22:49:25.969: INFO: stderr: ""
Jun  2 22:49:25.969: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun  2 22:49:25.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6263 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun  2 22:49:26.135: INFO: stderr: ""
Jun  2 22:49:26.135: INFO: stdout: "service/rm2 exposed\n"
Jun  2 22:49:26.148: INFO: Service rm2 in namespace kubectl-6263 found.
STEP: exposing service
Jun  2 22:49:28.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6263 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun  2 22:49:28.337: INFO: stderr: ""
Jun  2 22:49:28.337: INFO: stdout: "service/rm3 exposed\n"
Jun  2 22:49:28.350: INFO: Service rm3 in namespace kubectl-6263 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:30.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6263" for this suite.

• [SLOW TEST:8.153 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":298,"skipped":5831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:30.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9888
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  2 22:49:30.672: INFO: Waiting up to 5m0s for pod "pod-365d5799-2f53-45b0-8726-81b542403ae8" in namespace "emptydir-9888" to be "Succeeded or Failed"
Jun  2 22:49:30.683: INFO: Pod "pod-365d5799-2f53-45b0-8726-81b542403ae8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187077ms
Jun  2 22:49:32.700: INFO: Pod "pod-365d5799-2f53-45b0-8726-81b542403ae8": Phase="Running", Reason="", readiness=true. Elapsed: 2.027233206s
Jun  2 22:49:34.714: INFO: Pod "pod-365d5799-2f53-45b0-8726-81b542403ae8": Phase="Running", Reason="", readiness=false. Elapsed: 4.041644484s
Jun  2 22:49:36.728: INFO: Pod "pod-365d5799-2f53-45b0-8726-81b542403ae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055974293s
STEP: Saw pod success
Jun  2 22:49:36.729: INFO: Pod "pod-365d5799-2f53-45b0-8726-81b542403ae8" satisfied condition "Succeeded or Failed"
Jun  2 22:49:36.740: INFO: Trying to get logs from node 10.134.156.247 pod pod-365d5799-2f53-45b0-8726-81b542403ae8 container test-container: <nil>
STEP: delete the pod
Jun  2 22:49:36.834: INFO: Waiting for pod pod-365d5799-2f53-45b0-8726-81b542403ae8 to disappear
Jun  2 22:49:36.847: INFO: Pod pod-365d5799-2f53-45b0-8726-81b542403ae8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:49:36.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9888" for this suite.

• [SLOW TEST:6.461 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5855,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:49:36.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-8400
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  2 22:49:37.106: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  2 22:50:37.201: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Jun  2 22:50:37.289: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun  2 22:50:37.341: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun  2 22:50:37.387: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun  2 22:50:37.403: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun  2 22:50:37.480: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun  2 22:50:37.501: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:50:45.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8400" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:69.257 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":300,"skipped":5866,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:50:46.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1745
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  2 22:50:50.534: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:50:50.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1745" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5875,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:50:50.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3170
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:50:50.976: INFO: The status of Pod pod-secrets-09af6e5c-cc33-4619-9817-5d9965adcb4e is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:50:52.993: INFO: The status of Pod pod-secrets-09af6e5c-cc33-4619-9817-5d9965adcb4e is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:50:55.000: INFO: The status of Pod pod-secrets-09af6e5c-cc33-4619-9817-5d9965adcb4e is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:50:55.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3170" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":302,"skipped":5887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:50:55.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1036
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-ec41e659-ae44-4d7a-aff5-d896ecf5b4d9
STEP: Creating a pod to test consume secrets
Jun  2 22:50:55.423: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379" in namespace "projected-1036" to be "Succeeded or Failed"
Jun  2 22:50:55.435: INFO: Pod "pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379": Phase="Pending", Reason="", readiness=false. Elapsed: 11.192476ms
Jun  2 22:50:57.458: INFO: Pod "pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034979582s
Jun  2 22:50:59.487: INFO: Pod "pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063471397s
STEP: Saw pod success
Jun  2 22:50:59.487: INFO: Pod "pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379" satisfied condition "Succeeded or Failed"
Jun  2 22:50:59.497: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  2 22:50:59.752: INFO: Waiting for pod pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379 to disappear
Jun  2 22:50:59.777: INFO: Pod pod-projected-secrets-a8bce6c9-a7bb-4229-a336-a6cb52b9b379 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:50:59.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1036" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:50:59.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8298
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-cc64630c-1d30-4e7d-97ac-e961606e29af
STEP: Creating a pod to test consume secrets
Jun  2 22:51:00.149: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3" in namespace "projected-8298" to be "Succeeded or Failed"
Jun  2 22:51:00.183: INFO: Pod "pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.248323ms
Jun  2 22:51:02.221: INFO: Pod "pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071427826s
Jun  2 22:51:04.234: INFO: Pod "pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08443699s
Jun  2 22:51:06.256: INFO: Pod "pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.106584827s
STEP: Saw pod success
Jun  2 22:51:06.256: INFO: Pod "pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3" satisfied condition "Succeeded or Failed"
Jun  2 22:51:06.267: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  2 22:51:06.327: INFO: Waiting for pod pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3 to disappear
Jun  2 22:51:06.339: INFO: Pod pod-projected-secrets-3bee022d-52e2-484c-a23c-18e875e2cbc3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:51:06.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8298" for this suite.

• [SLOW TEST:6.504 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5958,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:51:06.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7991
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-4f34ee4f-cc36-4360-9db7-f2fd63576226
STEP: Creating a pod to test consume configMaps
Jun  2 22:51:06.659: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb" in namespace "projected-7991" to be "Succeeded or Failed"
Jun  2 22:51:06.669: INFO: Pod "pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.762363ms
Jun  2 22:51:08.690: INFO: Pod "pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031149334s
Jun  2 22:51:10.706: INFO: Pod "pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04701156s
STEP: Saw pod success
Jun  2 22:51:10.707: INFO: Pod "pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb" satisfied condition "Succeeded or Failed"
Jun  2 22:51:10.718: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:51:10.767: INFO: Waiting for pod pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb to disappear
Jun  2 22:51:10.778: INFO: Pod pod-projected-configmaps-182de1b0-4bdb-4640-a545-a76d40fa49bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:51:10.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7991" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":305,"skipped":5960,"failed":0}

------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:51:10.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6105
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:51:11.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6105" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":306,"skipped":5960,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:51:11.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7558
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7558
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7558
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7558
Jun  2 22:51:11.487: INFO: Found 0 stateful pods, waiting for 1
Jun  2 22:51:21.503: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun  2 22:51:21.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 22:51:21.829: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 22:51:21.829: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 22:51:21.829: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 22:51:21.842: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  2 22:51:31.882: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 22:51:31.882: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 22:51:31.925: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999571s
Jun  2 22:51:32.961: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99119463s
Jun  2 22:51:33.982: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.95582521s
Jun  2 22:51:35.006: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.934678214s
Jun  2 22:51:36.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.910712875s
Jun  2 22:51:37.042: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.895043925s
Jun  2 22:51:38.060: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.87464104s
Jun  2 22:51:39.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.856809501s
Jun  2 22:51:40.084: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.846954353s
Jun  2 22:51:41.099: INFO: Verifying statefulset ss doesn't scale past 1 for another 833.124191ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7558
Jun  2 22:51:42.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:51:42.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  2 22:51:42.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 22:51:42.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 22:51:42.414: INFO: Found 1 stateful pods, waiting for 3
Jun  2 22:51:52.469: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:51:52.469: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  2 22:51:52.469: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun  2 22:51:52.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 22:51:52.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 22:51:52.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 22:51:52.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 22:51:52.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 22:51:53.237: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 22:51:53.237: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 22:51:53.237: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 22:51:53.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  2 22:51:53.631: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  2 22:51:53.631: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  2 22:51:53.631: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  2 22:51:53.631: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 22:51:53.647: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  2 22:52:03.686: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 22:52:03.686: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 22:52:03.686: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  2 22:52:03.770: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996165s
Jun  2 22:52:04.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.957589571s
Jun  2 22:52:05.807: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.942141525s
Jun  2 22:52:06.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.920420147s
Jun  2 22:52:07.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.876894315s
Jun  2 22:52:08.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.857171089s
Jun  2 22:52:09.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.84396062s
Jun  2 22:52:10.913: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.828901331s
Jun  2 22:52:11.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.814943841s
Jun  2 22:52:12.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 795.858143ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7558
Jun  2 22:52:13.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:14.274: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  2 22:52:14.274: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 22:52:14.274: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 22:52:14.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:14.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  2 22:52:14.641: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  2 22:52:14.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  2 22:52:14.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:14.961: INFO: rc: 1
Jun  2 22:52:14.961: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: failed to exec in container: container is in CONTAINER_EXITED state

error:
exit status 1
Jun  2 22:52:24.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:25.082: INFO: rc: 1
Jun  2 22:52:25.082: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:52:35.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:35.181: INFO: rc: 1
Jun  2 22:52:35.181: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:52:45.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:45.300: INFO: rc: 1
Jun  2 22:52:45.300: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:52:55.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:52:55.403: INFO: rc: 1
Jun  2 22:52:55.403: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:53:05.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:53:05.523: INFO: rc: 1
Jun  2 22:53:05.523: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:53:15.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:53:15.633: INFO: rc: 1
Jun  2 22:53:15.633: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:53:25.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:53:25.765: INFO: rc: 1
Jun  2 22:53:25.765: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:53:35.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:53:35.877: INFO: rc: 1
Jun  2 22:53:35.877: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:53:45.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:53:45.989: INFO: rc: 1
Jun  2 22:53:45.990: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:53:55.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:53:56.100: INFO: rc: 1
Jun  2 22:53:56.100: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:54:06.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:54:06.235: INFO: rc: 1
Jun  2 22:54:06.235: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:54:16.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:54:16.390: INFO: rc: 1
Jun  2 22:54:16.390: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:54:26.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:54:26.642: INFO: rc: 1
Jun  2 22:54:26.642: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:54:36.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:54:36.745: INFO: rc: 1
Jun  2 22:54:36.746: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:54:46.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:54:46.876: INFO: rc: 1
Jun  2 22:54:46.876: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:54:56.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:54:57.011: INFO: rc: 1
Jun  2 22:54:57.012: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:55:07.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:55:07.115: INFO: rc: 1
Jun  2 22:55:07.116: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:55:17.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:55:17.233: INFO: rc: 1
Jun  2 22:55:17.233: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:55:27.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:55:27.342: INFO: rc: 1
Jun  2 22:55:27.342: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:55:37.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:55:37.451: INFO: rc: 1
Jun  2 22:55:37.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:55:47.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:55:47.563: INFO: rc: 1
Jun  2 22:55:47.563: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:55:57.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:55:57.680: INFO: rc: 1
Jun  2 22:55:57.680: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:56:07.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:56:07.800: INFO: rc: 1
Jun  2 22:56:07.800: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:56:17.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:56:17.921: INFO: rc: 1
Jun  2 22:56:17.921: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:56:27.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:56:28.045: INFO: rc: 1
Jun  2 22:56:28.045: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:56:38.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:56:38.178: INFO: rc: 1
Jun  2 22:56:38.178: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:56:48.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:56:48.309: INFO: rc: 1
Jun  2 22:56:48.309: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:56:58.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:56:58.416: INFO: rc: 1
Jun  2 22:56:58.416: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:57:08.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:57:08.540: INFO: rc: 1
Jun  2 22:57:08.541: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Jun  2 22:57:18.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=statefulset-7558 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  2 22:57:18.661: INFO: rc: 1
Jun  2 22:57:18.661: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Jun  2 22:57:18.661: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jun  2 22:57:18.723: INFO: Deleting all statefulset in ns statefulset-7558
Jun  2 22:57:18.766: INFO: Scaling statefulset ss to 0
Jun  2 22:57:18.806: INFO: Waiting for statefulset status.replicas updated to 0
Jun  2 22:57:18.817: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:18.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7558" for this suite.

• [SLOW TEST:367.668 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":307,"skipped":5960,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:18.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7668
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
Jun  2 22:57:19.104: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun  2 22:57:19.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 create -f -'
Jun  2 22:57:19.444: INFO: stderr: ""
Jun  2 22:57:19.444: INFO: stdout: "service/agnhost-replica created\n"
Jun  2 22:57:19.444: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun  2 22:57:19.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 create -f -'
Jun  2 22:57:19.745: INFO: stderr: ""
Jun  2 22:57:19.745: INFO: stdout: "service/agnhost-primary created\n"
Jun  2 22:57:19.745: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  2 22:57:19.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 create -f -'
Jun  2 22:57:20.026: INFO: stderr: ""
Jun  2 22:57:20.026: INFO: stdout: "service/frontend created\n"
Jun  2 22:57:20.026: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun  2 22:57:20.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 create -f -'
Jun  2 22:57:20.306: INFO: stderr: ""
Jun  2 22:57:20.306: INFO: stdout: "deployment.apps/frontend created\n"
Jun  2 22:57:20.307: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  2 22:57:20.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 create -f -'
Jun  2 22:57:20.569: INFO: stderr: ""
Jun  2 22:57:20.569: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun  2 22:57:20.570: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  2 22:57:20.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 create -f -'
Jun  2 22:57:20.809: INFO: stderr: ""
Jun  2 22:57:20.809: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun  2 22:57:20.809: INFO: Waiting for all frontend pods to be Running.
Jun  2 22:57:25.861: INFO: Waiting for frontend to serve content.
Jun  2 22:57:25.957: INFO: Trying to add a new entry to the guestbook.
Jun  2 22:57:26.018: INFO: Verifying that added entry can be retrieved.
Jun  2 22:57:26.115: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jun  2 22:57:31.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 delete --grace-period=0 --force -f -'
Jun  2 22:57:31.339: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:57:31.339: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun  2 22:57:31.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 delete --grace-period=0 --force -f -'
Jun  2 22:57:31.471: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:57:31.472: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun  2 22:57:31.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 delete --grace-period=0 --force -f -'
Jun  2 22:57:31.639: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:57:31.639: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  2 22:57:31.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 delete --grace-period=0 --force -f -'
Jun  2 22:57:31.754: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:57:31.754: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  2 22:57:31.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 delete --grace-period=0 --force -f -'
Jun  2 22:57:31.849: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:57:31.849: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun  2 22:57:31.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-7668 delete --grace-period=0 --force -f -'
Jun  2 22:57:32.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  2 22:57:32.011: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:32.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7668" for this suite.

• [SLOW TEST:13.193 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":308,"skipped":5974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:32.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5437
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  2 22:57:32.813: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun  2 22:57:34.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 22, 57, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 57, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 22, 57, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 22, 57, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 22:57:37.915: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:57:37.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:57:45.645: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=e2e-test-crd-webhook-3852-crd failed: Post "https://e2e-test-crd-conversion-webhook.crd-webhook-5437.svc:9443/crdconvert?timeout=30s": EOF
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5437" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:14.916 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":309,"skipped":6051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:47.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3743
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-75e4c723-5dad-4d68-bb62-69c49fe1f232-9505
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:47.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3743" for this suite.
STEP: Destroying namespace "nspatchtest-75e4c723-5dad-4d68-bb62-69c49fe1f232-9505" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":310,"skipped":6101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:47.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3440
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:47.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3440" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":311,"skipped":6145,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:47.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5576
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-5e9df92a-67a3-4660-bc69-d24174d58e81
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:48.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5576" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":312,"skipped":6165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:48.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2357
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jun  2 22:57:48.336: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun  2 22:57:53.373: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jun  2 22:57:53.382: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:53.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2357" for this suite.

• [SLOW TEST:5.390 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":313,"skipped":6199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:53.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7766
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jun  2 22:57:53.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c" in namespace "projected-7766" to be "Succeeded or Failed"
Jun  2 22:57:53.743: INFO: Pod "downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.002715ms
Jun  2 22:57:55.762: INFO: Pod "downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030166677s
Jun  2 22:57:57.788: INFO: Pod "downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055815135s
STEP: Saw pod success
Jun  2 22:57:57.788: INFO: Pod "downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c" satisfied condition "Succeeded or Failed"
Jun  2 22:57:57.797: INFO: Trying to get logs from node 10.134.156.247 pod downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c container client-container: <nil>
STEP: delete the pod
Jun  2 22:57:57.918: INFO: Waiting for pod downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c to disappear
Jun  2 22:57:57.929: INFO: Pod downwardapi-volume-871f63e8-1be7-4d99-89c9-f8460942c58c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:57:57.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7766" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":6225,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:57:57.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8313
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
Jun  2 22:57:58.199: INFO: created test-pod-1
Jun  2 22:58:02.225: INFO: running and ready test-pod-1
Jun  2 22:58:02.249: INFO: created test-pod-2
Jun  2 22:58:06.286: INFO: running and ready test-pod-2
Jun  2 22:58:06.310: INFO: created test-pod-3
Jun  2 22:58:10.347: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Jun  2 22:58:10.424: INFO: Pod quantity 3 is different from expected quantity 0
Jun  2 22:58:11.438: INFO: Pod quantity 2 is different from expected quantity 0
Jun  2 22:58:12.442: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:58:13.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8313" for this suite.

• [SLOW TEST:15.512 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":315,"skipped":6239,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:58:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9837
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jun  2 22:58:13.710: INFO: Waiting up to 5m0s for pod "downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d" in namespace "downward-api-9837" to be "Succeeded or Failed"
Jun  2 22:58:13.721: INFO: Pod "downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.100227ms
Jun  2 22:58:15.739: INFO: Pod "downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029064184s
Jun  2 22:58:17.752: INFO: Pod "downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042799232s
STEP: Saw pod success
Jun  2 22:58:17.752: INFO: Pod "downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d" satisfied condition "Succeeded or Failed"
Jun  2 22:58:17.762: INFO: Trying to get logs from node 10.134.156.247 pod downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d container dapi-container: <nil>
STEP: delete the pod
Jun  2 22:58:17.812: INFO: Waiting for pod downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d to disappear
Jun  2 22:58:17.822: INFO: Pod downward-api-0dbea15d-a5d8-4762-8388-38080da95f8d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:58:17.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9837" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":6256,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:58:17.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7741
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-2149275a-ffed-4df7-8daf-16ce03b5ca17
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:58:18.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7741" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":317,"skipped":6263,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:58:18.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8557
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-8557
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  2 22:58:18.318: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  2 22:58:18.420: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:58:20.443: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:58:22.457: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:24.434: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:26.438: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:28.450: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:30.443: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:32.447: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:34.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:36.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:38.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  2 22:58:40.450: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  2 22:58:40.470: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  2 22:58:40.491: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  2 22:58:42.603: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  2 22:58:42.603: INFO: Breadth first check of 172.30.170.190 on host 10.134.156.209...
Jun  2 22:58:42.611: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.118.5:9080/dial?request=hostname&protocol=udp&host=172.30.170.190&port=8081&tries=1'] Namespace:pod-network-test-8557 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:58:42.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:58:42.612: INFO: ExecWithOptions: Clientset creation
Jun  2 22:58:42.612: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8557/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.118.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.170.190%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:58:42.824: INFO: Waiting for responses: map[]
Jun  2 22:58:42.824: INFO: reached 172.30.170.190 after 0/1 tries
Jun  2 22:58:42.824: INFO: Breadth first check of 172.30.118.9 on host 10.134.156.247...
Jun  2 22:58:42.840: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.118.5:9080/dial?request=hostname&protocol=udp&host=172.30.118.9&port=8081&tries=1'] Namespace:pod-network-test-8557 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:58:42.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:58:42.842: INFO: ExecWithOptions: Clientset creation
Jun  2 22:58:42.843: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8557/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.118.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.118.9%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:58:43.013: INFO: Waiting for responses: map[]
Jun  2 22:58:43.013: INFO: reached 172.30.118.9 after 0/1 tries
Jun  2 22:58:43.013: INFO: Breadth first check of 172.30.220.225 on host 10.134.156.253...
Jun  2 22:58:43.024: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.118.5:9080/dial?request=hostname&protocol=udp&host=172.30.220.225&port=8081&tries=1'] Namespace:pod-network-test-8557 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:58:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:58:43.026: INFO: ExecWithOptions: Clientset creation
Jun  2 22:58:43.026: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8557/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.118.5%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.220.225%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jun  2 22:58:43.197: INFO: Waiting for responses: map[]
Jun  2 22:58:43.197: INFO: reached 172.30.220.225 after 0/1 tries
Jun  2 22:58:43.197: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:58:43.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8557" for this suite.

• [SLOW TEST:25.139 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":6265,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:58:43.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6696
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:58:48.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6696" for this suite.

• [SLOW TEST:5.088 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":319,"skipped":6273,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:58:48.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-104
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 22:58:48.610: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun  2 22:58:48.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 22:58:48.650: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 22:58:49.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 22:58:49.674: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 22:58:50.691: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 22:58:50.692: INFO: Node 10.134.156.253 is running 0 daemon pod, expected 1
Jun  2 22:58:51.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 22:58:51.693: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun  2 22:58:51.823: INFO: Wrong image for pod: daemon-set-5jx9v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:51.823: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:51.823: INFO: Wrong image for pod: daemon-set-xlnw9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:52.854: INFO: Wrong image for pod: daemon-set-5jx9v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:52.854: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:53.858: INFO: Wrong image for pod: daemon-set-5jx9v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:53.858: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:53.858: INFO: Pod daemon-set-x25gz is not available
Jun  2 22:58:54.855: INFO: Wrong image for pod: daemon-set-5jx9v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:54.855: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:54.855: INFO: Pod daemon-set-x25gz is not available
Jun  2 22:58:55.859: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:56.854: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:57.861: INFO: Pod daemon-set-bt6tq is not available
Jun  2 22:58:57.861: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:58:58.861: INFO: Pod daemon-set-bt6tq is not available
Jun  2 22:58:58.861: INFO: Wrong image for pod: daemon-set-jptx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jun  2 22:59:00.857: INFO: Pod daemon-set-fqfwv is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun  2 22:59:00.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 22:59:00.906: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 22:59:01.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 22:59:01.946: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 22:59:02.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 22:59:02.934: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-104, will wait for the garbage collector to delete the pods
Jun  2 22:59:03.130: INFO: Deleting DaemonSet.extensions daemon-set took: 29.600496ms
Jun  2 22:59:03.230: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.495525ms
Jun  2 22:59:06.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 22:59:06.160: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  2 22:59:06.172: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50635"},"items":null}

Jun  2 22:59:06.183: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50635"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:59:06.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-104" for this suite.

• [SLOW TEST:17.951 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":320,"skipped":6284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:59:06.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3926
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4fce98f4-1888-4197-a74a-f414eb5d7ca6
STEP: Creating the pod
Jun  2 22:59:06.564: INFO: The status of Pod pod-projected-configmaps-648ecdb9-a38c-48af-b68a-7ae210930316 is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:59:08.577: INFO: The status of Pod pod-projected-configmaps-648ecdb9-a38c-48af-b68a-7ae210930316 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-4fce98f4-1888-4197-a74a-f414eb5d7ca6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:59:10.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3926" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":6324,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:59:10.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3245
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jun  2 22:59:10.967: INFO: The status of Pod labelsupdatefb56d5d9-37a4-4113-98f7-5ed7e2c4ce1d is Pending, waiting for it to be Running (with Ready = true)
Jun  2 22:59:12.983: INFO: The status of Pod labelsupdatefb56d5d9-37a4-4113-98f7-5ed7e2c4ce1d is Running (Ready = true)
Jun  2 22:59:13.582: INFO: Successfully updated pod "labelsupdatefb56d5d9-37a4-4113-98f7-5ed7e2c4ce1d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:59:17.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3245" for this suite.

• [SLOW TEST:6.985 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":322,"skipped":6324,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:59:17.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2065
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-5e7b326b-e986-42f9-b37b-20e793a449ab
STEP: Creating a pod to test consume configMaps
Jun  2 22:59:17.972: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922" in namespace "projected-2065" to be "Succeeded or Failed"
Jun  2 22:59:17.982: INFO: Pod "pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922": Phase="Pending", Reason="", readiness=false. Elapsed: 9.400391ms
Jun  2 22:59:20.000: INFO: Pod "pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027332052s
Jun  2 22:59:22.026: INFO: Pod "pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052983316s
STEP: Saw pod success
Jun  2 22:59:22.026: INFO: Pod "pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922" satisfied condition "Succeeded or Failed"
Jun  2 22:59:22.036: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922 container agnhost-container: <nil>
STEP: delete the pod
Jun  2 22:59:22.080: INFO: Waiting for pod pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922 to disappear
Jun  2 22:59:22.090: INFO: Pod pod-projected-configmaps-76f333b5-cdf2-41ff-a308-bb56891ea922 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:59:22.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2065" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":6339,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:59:22.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-189
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun  2 22:59:26.410: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-189 PodName:var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:59:26.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:59:26.411: INFO: ExecWithOptions: Clientset creation
Jun  2 22:59:26.411: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-189/pods/var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Jun  2 22:59:26.618: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-189 PodName:var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  2 22:59:26.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
Jun  2 22:59:26.619: INFO: ExecWithOptions: Clientset creation
Jun  2 22:59:26.619: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-189/pods/var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Jun  2 22:59:27.332: INFO: Successfully updated pod "var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun  2 22:59:27.341: INFO: Deleting pod "var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d" in namespace "var-expansion-189"
Jun  2 22:59:27.358: INFO: Wait up to 5m0s for pod "var-expansion-0fb6f931-1fc7-492a-9c75-8882a48d276d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 22:59:59.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-189" for this suite.

• [SLOW TEST:37.294 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":324,"skipped":6345,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 22:59:59.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1322
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jun  2 22:59:59.670: INFO: The status of Pod annotationupdate3b8f00da-850d-4ad7-a8bf-cc6fa1e5872c is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:01.715: INFO: The status of Pod annotationupdate3b8f00da-850d-4ad7-a8bf-cc6fa1e5872c is Running (Ready = true)
Jun  2 23:00:02.297: INFO: Successfully updated pod "annotationupdate3b8f00da-850d-4ad7-a8bf-cc6fa1e5872c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:04.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1322" for this suite.

• [SLOW TEST:5.002 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":325,"skipped":6347,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:04.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-336
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun  2 23:00:04.743: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:09.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-336" for this suite.

• [SLOW TEST:5.356 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6378,"failed":0}
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:09.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6150
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jun  2 23:00:10.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6150 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jun  2 23:00:10.180: INFO: stderr: ""
Jun  2 23:00:10.180: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
Jun  2 23:00:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-6150 delete pods e2e-test-httpd-pod'
Jun  2 23:00:12.784: INFO: stderr: ""
Jun  2 23:00:12.784: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:12.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6150" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":327,"skipped":6378,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:12.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3864
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  2 23:00:13.049: INFO: Waiting up to 5m0s for pod "pod-00d17b71-daec-4f63-97ce-c24c43c3b614" in namespace "emptydir-3864" to be "Succeeded or Failed"
Jun  2 23:00:13.061: INFO: Pod "pod-00d17b71-daec-4f63-97ce-c24c43c3b614": Phase="Pending", Reason="", readiness=false. Elapsed: 11.59492ms
Jun  2 23:00:15.074: INFO: Pod "pod-00d17b71-daec-4f63-97ce-c24c43c3b614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024925723s
Jun  2 23:00:17.095: INFO: Pod "pod-00d17b71-daec-4f63-97ce-c24c43c3b614": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046323504s
STEP: Saw pod success
Jun  2 23:00:17.097: INFO: Pod "pod-00d17b71-daec-4f63-97ce-c24c43c3b614" satisfied condition "Succeeded or Failed"
Jun  2 23:00:17.108: INFO: Trying to get logs from node 10.134.156.247 pod pod-00d17b71-daec-4f63-97ce-c24c43c3b614 container test-container: <nil>
STEP: delete the pod
Jun  2 23:00:17.153: INFO: Waiting for pod pod-00d17b71-daec-4f63-97ce-c24c43c3b614 to disappear
Jun  2 23:00:17.162: INFO: Pod pod-00d17b71-daec-4f63-97ce-c24c43c3b614 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:17.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3864" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":328,"skipped":6391,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:17.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4024
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jun  2 23:00:17.450: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:19.485: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jun  2 23:00:19.528: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:21.555: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:23.552: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  2 23:00:23.711: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  2 23:00:23.725: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  2 23:00:25.726: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  2 23:00:25.745: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:25.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4024" for this suite.

• [SLOW TEST:8.616 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:25.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2743
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  2 23:00:26.070: INFO: Waiting up to 5m0s for pod "pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0" in namespace "emptydir-2743" to be "Succeeded or Failed"
Jun  2 23:00:26.110: INFO: Pod "pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0": Phase="Pending", Reason="", readiness=false. Elapsed: 39.472176ms
Jun  2 23:00:28.137: INFO: Pod "pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06711404s
Jun  2 23:00:30.149: INFO: Pod "pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079007667s
STEP: Saw pod success
Jun  2 23:00:30.149: INFO: Pod "pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0" satisfied condition "Succeeded or Failed"
Jun  2 23:00:30.164: INFO: Trying to get logs from node 10.134.156.247 pod pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0 container test-container: <nil>
STEP: delete the pod
Jun  2 23:00:30.211: INFO: Waiting for pod pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0 to disappear
Jun  2 23:00:30.234: INFO: Pod pod-28b559c6-7b30-4bd1-95d2-15a9df9216e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:30.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2743" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":6417,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:30.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4397
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 23:00:30.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-4397 version'
Jun  2 23:00:30.570: INFO: stderr: ""
Jun  2 23:00:30.570: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.7\", GitCommit:\"42c05a547468804b2053ecf60a3bd15560362fc2\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:30:55Z\", GoVersion:\"go1.17.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.7+IKS\", GitCommit:\"73c1dd7ab1fa86abf7af93755590704eac9f2793\", GitTreeState:\"clean\", BuildDate:\"2022-05-26T15:57:20Z\", GoVersion:\"go1.17.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:30.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4397" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":331,"skipped":6419,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:30.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6669
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-c30d4bc5-3277-4729-b6d7-78fedf5ba704
STEP: Creating a pod to test consume secrets
Jun  2 23:00:30.836: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200" in namespace "projected-6669" to be "Succeeded or Failed"
Jun  2 23:00:30.846: INFO: Pod "pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200": Phase="Pending", Reason="", readiness=false. Elapsed: 10.217435ms
Jun  2 23:00:32.864: INFO: Pod "pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028129348s
Jun  2 23:00:34.878: INFO: Pod "pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042114512s
STEP: Saw pod success
Jun  2 23:00:34.878: INFO: Pod "pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200" satisfied condition "Succeeded or Failed"
Jun  2 23:00:34.887: INFO: Trying to get logs from node 10.134.156.247 pod pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  2 23:00:34.937: INFO: Waiting for pod pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200 to disappear
Jun  2 23:00:34.946: INFO: Pod pod-projected-secrets-d07da0c7-4952-45af-b158-f46026efe200 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:34.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6669" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":6435,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:34.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9957
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-7c03b192-71d1-46f5-957b-dce47385aedb
STEP: Creating a pod to test consume secrets
Jun  2 23:00:35.241: INFO: Waiting up to 5m0s for pod "pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192" in namespace "secrets-9957" to be "Succeeded or Failed"
Jun  2 23:00:35.252: INFO: Pod "pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192": Phase="Pending", Reason="", readiness=false. Elapsed: 11.141874ms
Jun  2 23:00:37.283: INFO: Pod "pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041863392s
Jun  2 23:00:39.297: INFO: Pod "pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05618471s
STEP: Saw pod success
Jun  2 23:00:39.298: INFO: Pod "pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192" satisfied condition "Succeeded or Failed"
Jun  2 23:00:39.308: INFO: Trying to get logs from node 10.134.156.247 pod pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192 container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 23:00:39.379: INFO: Waiting for pod pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192 to disappear
Jun  2 23:00:39.389: INFO: Pod pod-secrets-9ced4282-9beb-4bc8-b29c-eb2231780192 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:39.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9957" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6439,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:39.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-611
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 23:00:39.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 create -f -'
Jun  2 23:00:39.947: INFO: stderr: ""
Jun  2 23:00:39.947: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun  2 23:00:39.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 create -f -'
Jun  2 23:00:40.423: INFO: stderr: ""
Jun  2 23:00:40.423: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  2 23:00:41.439: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 23:00:41.440: INFO: Found 0 / 1
Jun  2 23:00:42.440: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 23:00:42.440: INFO: Found 1 / 1
Jun  2 23:00:42.440: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  2 23:00:42.451: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  2 23:00:42.451: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  2 23:00:42.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 describe pod agnhost-primary-jkdjw'
Jun  2 23:00:42.596: INFO: stderr: ""
Jun  2 23:00:42.596: INFO: stdout: "Name:         agnhost-primary-jkdjw\nNamespace:    kubectl-611\nPriority:     0\nNode:         10.134.156.247/10.134.156.247\nStart Time:   Thu, 02 Jun 2022 23:00:40 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: e67e45db00c3ef7a7c25a8a7d1fff8f46da1865232264fed00a01f286344c3b1\n              cni.projectcalico.org/podIP: 172.30.118.38/32\n              cni.projectcalico.org/podIPs: 172.30.118.38/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.118.38\nIPs:\n  IP:           172.30.118.38\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6df4f8e9921cf17095c29816d2c3b25a6548e5e554ab09ede612a17133cb2d5b\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 02 Jun 2022 23:00:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ns48m (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ns48m:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-611/agnhost-primary-jkdjw to 10.134.156.247\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jun  2 23:00:42.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 describe rc agnhost-primary'
Jun  2 23:00:42.742: INFO: stderr: ""
Jun  2 23:00:42.742: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-611\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-jkdjw\n"
Jun  2 23:00:42.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 describe service agnhost-primary'
Jun  2 23:00:42.878: INFO: stderr: ""
Jun  2 23:00:42.878: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-611\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.61.9\nIPs:               172.21.61.9\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.118.38:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  2 23:00:42.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 describe node 10.134.156.209'
Jun  2 23:00:43.113: INFO: stderr: ""
Jun  2 23:00:43.113: INFO: stdout: "Name:               10.134.156.209\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=159.122.86.44\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.134.156.209\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cacghfuf0f4jkaafvrqg-kubee2epvgd-default-0000019b\n                    ibm-cloud.kubernetes.io/worker-pool-id=cacghfuf0f4jkaafvrqg-b8656b7\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.23.6_1530\n                    ibm-cloud.kubernetes.io/zone=fra02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.134.156.209\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722990\n                    publicVLAN=2722976\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra02\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.134.156.209/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.170.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 02 Jun 2022 19:16:11 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.134.156.209\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 02 Jun 2022 23:00:33 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 02 Jun 2022 19:16:50 +0000   Thu, 02 Jun 2022 19:16:50 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 02 Jun 2022 22:55:57 +0000   Thu, 02 Jun 2022 19:16:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 02 Jun 2022 22:55:57 +0000   Thu, 02 Jun 2022 19:16:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 02 Jun 2022 22:55:57 +0000   Thu, 02 Jun 2022 19:16:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 02 Jun 2022 22:55:57 +0000   Thu, 02 Jun 2022 19:16:21 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.134.156.209\n  ExternalIP:  159.122.86.44\n  Hostname:    10.134.156.209\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102685624Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16212368Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93986994917\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13440400Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 73a265a7b79d4b18bead7c4f105f37de\n  System UUID:                299B4534-A7A4-22F5-D043-CF4988292175\n  Boot ID:                    e7f1f6aa-06fc-41d8-bc89-89182e83179f\n  Kernel Version:             4.15.0-177-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.23.6+IKS\n  Kube-Proxy Version:         v1.23.6+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///cacghfuf0f4jkaafvrqg/kube-cacghfuf0f4jkaafvrqg-kubee2epvgd-default-0000019b\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-169-50-20-163-66f5ffb6c5-qxhvw       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h40m\n  kube-system                 calico-node-xc26s                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h44m\n  kube-system                 calico-typha-7684bb556f-zfmb4                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h49m\n  kube-system                 coredns-74bf9bd988-fpffg                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     17m\n  kube-system                 coredns-74bf9bd988-v6qcf                                   100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     3h34m\n  kube-system                 ibm-keepalived-watcher-5wwlj                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h44m\n  kube-system                 ibm-master-proxy-static-10.134.156.209                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h43m\n  kube-system                 konnectivity-agent-wmghg                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     3h35m\n  kube-system                 metrics-server-6bc784d6c-ztp8q                             126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     179m\n  kube-system                 public-crcacghfuf0f4jkaafvrqg-alb1-6dd9879ffd-j82hw        10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h40m\n  sonobuoy                    sonobuoy-e2e-job-d730fbd8ba3a4c8f                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c956ac0bb94a4c0e-kd2m7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         108m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                881m (22%)     566m (14%)\n  memory             667154Ki (4%)  2380064Ki (17%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
Jun  2 23:00:43.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-611 describe namespace kubectl-611'
Jun  2 23:00:43.238: INFO: stderr: ""
Jun  2 23:00:43.238: INFO: stdout: "Name:         kubectl-611\nLabels:       e2e-framework=kubectl\n              e2e-run=68dd39b1-7a69-44e6-8e74-b60df5359f38\n              kubernetes.io/metadata.name=kubectl-611\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:43.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-611" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":334,"skipped":6454,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:43.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5450
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jun  2 23:00:43.501: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:47.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5450" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":335,"skipped":6470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:47.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2021
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jun  2 23:00:48.133: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:50.145: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:52.163: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jun  2 23:00:52.207: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:00:54.235: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun  2 23:00:54.261: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  2 23:00:54.270: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  2 23:00:56.270: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  2 23:00:56.299: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  2 23:00:58.271: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  2 23:00:58.294: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:00:58.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2021" for this suite.

• [SLOW TEST:10.561 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6507,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:00:58.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2985
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
Jun  2 23:00:58.680: INFO: Waiting up to 5m0s for pod "test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e" in namespace "svcaccounts-2985" to be "Succeeded or Failed"
Jun  2 23:00:58.690: INFO: Pod "test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.260136ms
Jun  2 23:01:00.726: INFO: Pod "test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04639022s
Jun  2 23:01:02.755: INFO: Pod "test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074705572s
Jun  2 23:01:04.781: INFO: Pod "test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.101152335s
STEP: Saw pod success
Jun  2 23:01:04.781: INFO: Pod "test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e" satisfied condition "Succeeded or Failed"
Jun  2 23:01:04.790: INFO: Trying to get logs from node 10.134.156.247 pod test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e container agnhost-container: <nil>
STEP: delete the pod
Jun  2 23:01:04.843: INFO: Waiting for pod test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e to disappear
Jun  2 23:01:04.853: INFO: Pod test-pod-f014a2ac-3ecb-45ca-b5c0-33bc8d2f723e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:01:04.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2985" for this suite.

• [SLOW TEST:6.439 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":337,"skipped":6521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:01:04.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3857
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-5072c7a0-a693-4877-9bd8-310fa49f8483
STEP: Creating secret with name secret-projected-all-test-volume-49a86843-4784-4f9b-89e3-9fe1facb9947
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun  2 23:01:05.156: INFO: Waiting up to 5m0s for pod "projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d" in namespace "projected-3857" to be "Succeeded or Failed"
Jun  2 23:01:05.167: INFO: Pod "projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.528002ms
Jun  2 23:01:07.187: INFO: Pod "projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030159392s
Jun  2 23:01:09.210: INFO: Pod "projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053161146s
STEP: Saw pod success
Jun  2 23:01:09.210: INFO: Pod "projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d" satisfied condition "Succeeded or Failed"
Jun  2 23:01:09.220: INFO: Trying to get logs from node 10.134.156.247 pod projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d container projected-all-volume-test: <nil>
STEP: delete the pod
Jun  2 23:01:09.275: INFO: Waiting for pod projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d to disappear
Jun  2 23:01:09.284: INFO: Pod projected-volume-bf0ce2e6-c9ca-4a59-a3ef-bd0e273a482d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:01:09.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3857" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":338,"skipped":6548,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:01:09.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3516
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun  2 23:01:21.114: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0602 23:01:21.114858      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun  2 23:01:21.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-267dk" in namespace "gc-3516"
Jun  2 23:01:21.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-2djjk" in namespace "gc-3516"
Jun  2 23:01:21.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-2pkql" in namespace "gc-3516"
Jun  2 23:01:21.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mf5n" in namespace "gc-3516"
Jun  2 23:01:21.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q8q8" in namespace "gc-3516"
Jun  2 23:01:21.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sb4x" in namespace "gc-3516"
Jun  2 23:01:21.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c6j8" in namespace "gc-3516"
Jun  2 23:01:21.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hcrm" in namespace "gc-3516"
Jun  2 23:01:21.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lfj8" in namespace "gc-3516"
Jun  2 23:01:21.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-77sb9" in namespace "gc-3516"
Jun  2 23:01:21.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gsm9" in namespace "gc-3516"
Jun  2 23:01:21.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nbgg" in namespace "gc-3516"
Jun  2 23:01:21.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-7st7z" in namespace "gc-3516"
Jun  2 23:01:21.499: INFO: Deleting pod "simpletest-rc-to-be-deleted-7x479" in namespace "gc-3516"
Jun  2 23:01:21.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-88wsg" in namespace "gc-3516"
Jun  2 23:01:21.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l9xw" in namespace "gc-3516"
Jun  2 23:01:21.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mxdt" in namespace "gc-3516"
Jun  2 23:01:21.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bgwg" in namespace "gc-3516"
Jun  2 23:01:21.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bq8m" in namespace "gc-3516"
Jun  2 23:01:21.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dq4s" in namespace "gc-3516"
Jun  2 23:01:21.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fd8q" in namespace "gc-3516"
Jun  2 23:01:21.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j9nh" in namespace "gc-3516"
Jun  2 23:01:21.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nrwd" in namespace "gc-3516"
Jun  2 23:01:21.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6czt" in namespace "gc-3516"
Jun  2 23:01:21.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8np7" in namespace "gc-3516"
Jun  2 23:01:21.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdh4d" in namespace "gc-3516"
Jun  2 23:01:21.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-blllg" in namespace "gc-3516"
Jun  2 23:01:21.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpvvr" in namespace "gc-3516"
Jun  2 23:01:21.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv266" in namespace "gc-3516"
Jun  2 23:01:21.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4skx" in namespace "gc-3516"
Jun  2 23:01:22.056: INFO: Deleting pod "simpletest-rc-to-be-deleted-cccxv" in namespace "gc-3516"
Jun  2 23:01:22.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-d72fp" in namespace "gc-3516"
Jun  2 23:01:22.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7rwl" in namespace "gc-3516"
Jun  2 23:01:22.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcp8k" in namespace "gc-3516"
Jun  2 23:01:22.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwz4h" in namespace "gc-3516"
Jun  2 23:01:22.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5lsd" in namespace "gc-3516"
Jun  2 23:01:22.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9dbz" in namespace "gc-3516"
Jun  2 23:01:22.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdzcw" in namespace "gc-3516"
Jun  2 23:01:22.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5zkm" in namespace "gc-3516"
Jun  2 23:01:22.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-g96rb" in namespace "gc-3516"
Jun  2 23:01:22.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfgm8" in namespace "gc-3516"
Jun  2 23:01:22.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghcjg" in namespace "gc-3516"
Jun  2 23:01:22.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghppj" in namespace "gc-3516"
Jun  2 23:01:22.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtmq4" in namespace "gc-3516"
Jun  2 23:01:22.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7zpl" in namespace "gc-3516"
Jun  2 23:01:22.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj2j9" in namespace "gc-3516"
Jun  2 23:01:22.573: INFO: Deleting pod "simpletest-rc-to-be-deleted-jpxdr" in namespace "gc-3516"
Jun  2 23:01:22.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-jrtd7" in namespace "gc-3516"
Jun  2 23:01:22.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8vn6" in namespace "gc-3516"
Jun  2 23:01:22.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkpk2" in namespace "gc-3516"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:01:22.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3516" for this suite.

• [SLOW TEST:13.413 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":339,"skipped":6553,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:01:22.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8781
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-5d5ae36a-446f-4da3-abd1-1eb08cfecc61
STEP: Creating a pod to test consume secrets
Jun  2 23:01:22.978: INFO: Waiting up to 5m0s for pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5" in namespace "secrets-8781" to be "Succeeded or Failed"
Jun  2 23:01:22.988: INFO: Pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.424258ms
Jun  2 23:01:25.009: INFO: Pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030069199s
Jun  2 23:01:27.026: INFO: Pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047981631s
Jun  2 23:01:29.045: INFO: Pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066772696s
Jun  2 23:01:31.065: INFO: Pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.086607798s
STEP: Saw pod success
Jun  2 23:01:31.065: INFO: Pod "pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5" satisfied condition "Succeeded or Failed"
Jun  2 23:01:31.075: INFO: Trying to get logs from node 10.134.156.247 pod pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5 container secret-volume-test: <nil>
STEP: delete the pod
Jun  2 23:01:31.131: INFO: Waiting for pod pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5 to disappear
Jun  2 23:01:31.143: INFO: Pod pod-secrets-98310715-bb99-4535-adff-5e7c2eb6b5d5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:01:31.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8781" for this suite.

• [SLOW TEST:8.441 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6570,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:01:31.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6205
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  2 23:01:31.516: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 23:01:31.517: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:32.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 23:01:32.548: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:33.554: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 23:01:33.554: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:34.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 23:01:34.547: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun  2 23:01:34.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 23:01:34.619: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:35.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 23:01:35.652: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:36.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 23:01:36.652: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:37.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 23:01:37.647: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:38.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun  2 23:01:38.649: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:01:39.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 23:01:39.654: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6205, will wait for the garbage collector to delete the pods
Jun  2 23:01:39.749: INFO: Deleting DaemonSet.extensions daemon-set took: 22.115967ms
Jun  2 23:01:39.849: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.27118ms
Jun  2 23:01:42.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 23:01:42.472: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  2 23:01:42.482: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"53665"},"items":null}

Jun  2 23:01:42.491: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"53665"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:01:42.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6205" for this suite.

• [SLOW TEST:11.446 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":341,"skipped":6583,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:01:42.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5767
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  2 23:01:43.313: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  2 23:01:45.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.June, 2, 23, 1, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 23, 1, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.June, 2, 23, 1, 43, 0, time.Local), LastTransitionTime:time.Date(2022, time.June, 2, 23, 1, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  2 23:01:48.407: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 23:01:48.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5001-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:01:51.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5767" for this suite.
STEP: Destroying namespace "webhook-5767-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":342,"skipped":6587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:01:52.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9745
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jun  2 23:01:52.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  2 23:01:57.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-9745 --namespace=crd-publish-openapi-9745 create -f -'
Jun  2 23:01:58.674: INFO: stderr: ""
Jun  2 23:01:58.674: INFO: stdout: "e2e-test-crd-publish-openapi-5721-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  2 23:01:58.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-9745 --namespace=crd-publish-openapi-9745 delete e2e-test-crd-publish-openapi-5721-crds test-cr'
Jun  2 23:01:58.813: INFO: stderr: ""
Jun  2 23:01:58.813: INFO: stdout: "e2e-test-crd-publish-openapi-5721-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  2 23:01:58.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-9745 --namespace=crd-publish-openapi-9745 apply -f -'
Jun  2 23:01:59.067: INFO: stderr: ""
Jun  2 23:01:59.067: INFO: stdout: "e2e-test-crd-publish-openapi-5721-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  2 23:01:59.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-9745 --namespace=crd-publish-openapi-9745 delete e2e-test-crd-publish-openapi-5721-crds test-cr'
Jun  2 23:01:59.258: INFO: stderr: ""
Jun  2 23:01:59.258: INFO: stdout: "e2e-test-crd-publish-openapi-5721-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  2 23:01:59.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=crd-publish-openapi-9745 explain e2e-test-crd-publish-openapi-5721-crds'
Jun  2 23:02:00.196: INFO: stderr: ""
Jun  2 23:02:00.196: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5721-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:02:04.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9745" for this suite.

• [SLOW TEST:12.616 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":343,"skipped":6615,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:02:04.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9455
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
Jun  2 23:02:05.075: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3478443831 --namespace=kubectl-9455 proxy --unix-socket=/tmp/kubectl-proxy-unix1027684441/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:02:05.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9455" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":344,"skipped":6632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:02:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8484
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jun  2 23:02:05.386: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:02:07.400: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:02:09.400: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jun  2 23:02:09.444: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:02:11.461: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun  2 23:02:13.456: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jun  2 23:02:13.490: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  2 23:02:13.500: INFO: Pod pod-with-prestop-http-hook still exists
Jun  2 23:02:15.501: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  2 23:02:15.518: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:02:15.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8484" for this suite.

• [SLOW TEST:10.446 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jun  2 23:02:15.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3478443831
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9508
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  2 23:02:15.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 23:02:15.915: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:02:16.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 23:02:16.982: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:02:17.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun  2 23:02:17.941: INFO: Node 10.134.156.209 is running 0 daemon pod, expected 1
Jun  2 23:02:18.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 23:02:18.968: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun  2 23:02:19.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun  2 23:02:19.023: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9508, will wait for the garbage collector to delete the pods
Jun  2 23:02:20.159: INFO: Deleting DaemonSet.extensions daemon-set took: 27.041079ms
Jun  2 23:02:20.360: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.917978ms
Jun  2 23:02:22.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun  2 23:02:22.675: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun  2 23:02:22.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54025"},"items":null}

Jun  2 23:02:22.695: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54025"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jun  2 23:02:22.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9508" for this suite.

• [SLOW TEST:7.144 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":346,"skipped":6688,"failed":0}
SSSSSSSSSSJun  2 23:02:22.766: INFO: Running AfterSuite actions on all nodes
Jun  2 23:02:22.766: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Jun  2 23:02:22.766: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jun  2 23:02:22.766: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Jun  2 23:02:22.767: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun  2 23:02:22.767: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun  2 23:02:22.767: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun  2 23:02:22.767: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jun  2 23:02:22.768: INFO: Running AfterSuite actions on node 1
Jun  2 23:02:22.768: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6698,"failed":0}

Ran 346 of 7044 Specs in 6595.869 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6698 Skipped
PASS

Ginkgo ran 1 suite in 1h49m58.267344272s
Test Suite Passed

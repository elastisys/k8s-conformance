I0527 09:38:55.492048      15 e2e.go:132] Starting e2e run "efba5363-37f5-4917-b385-4844f9f674fa" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1653644334 - Will randomize all specs
Will run 346 of 7044 specs

May 27 09:38:59.056: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 09:38:59.067: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 27 09:38:59.122: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 27 09:38:59.179: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 27 09:38:59.179: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 27 09:38:59.179: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 27 09:38:59.198: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
May 27 09:38:59.198: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
May 27 09:38:59.198: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 27 09:38:59.198: INFO: e2e test version: v1.23.7
May 27 09:38:59.200: INFO: kube-apiserver version: v1.23.7
May 27 09:38:59.201: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 09:38:59.209: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:38:59.210: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename proxy
W0527 09:38:59.251609      15 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
May 27 09:38:59.252: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:38:59.262: INFO: Creating pod...
May 27 09:38:59.292: INFO: Pod Quantity: 1 Status: Pending
May 27 09:39:00.306: INFO: Pod Quantity: 1 Status: Pending
May 27 09:39:01.308: INFO: Pod Status: Running
May 27 09:39:01.309: INFO: Creating service...
May 27 09:39:01.330: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/DELETE
May 27 09:39:01.353: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 27 09:39:01.353: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/GET
May 27 09:39:01.362: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 27 09:39:01.362: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/HEAD
May 27 09:39:01.369: INFO: http.Client request:HEAD | StatusCode:200
May 27 09:39:01.369: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/OPTIONS
May 27 09:39:01.377: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 27 09:39:01.377: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/PATCH
May 27 09:39:01.382: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 27 09:39:01.383: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/POST
May 27 09:39:01.388: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 27 09:39:01.388: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/pods/agnhost/proxy/some/path/with/PUT
May 27 09:39:01.394: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 27 09:39:01.394: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/DELETE
May 27 09:39:01.404: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 27 09:39:01.404: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/GET
May 27 09:39:01.413: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 27 09:39:01.413: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/HEAD
May 27 09:39:01.422: INFO: http.Client request:HEAD | StatusCode:200
May 27 09:39:01.422: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/OPTIONS
May 27 09:39:01.434: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 27 09:39:01.434: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/PATCH
May 27 09:39:01.444: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 27 09:39:01.444: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/POST
May 27 09:39:01.454: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 27 09:39:01.455: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4159/services/test-service/proxy/some/path/with/PUT
May 27 09:39:01.466: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:39:01.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4159" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:39:01.513: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
May 27 09:39:01.579: INFO: created test-pod-1
May 27 09:39:03.596: INFO: running and ready test-pod-1
May 27 09:39:03.605: INFO: created test-pod-2
May 27 09:39:05.628: INFO: running and ready test-pod-2
May 27 09:39:05.636: INFO: created test-pod-3
May 27 09:39:07.658: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
May 27 09:39:07.756: INFO: Pod quantity 3 is different from expected quantity 0
May 27 09:39:08.770: INFO: Pod quantity 1 is different from expected quantity 0
May 27 09:39:09.769: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:39:10.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2297" for this suite.

• [SLOW TEST:9.307 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":2,"skipped":16,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:39:10.820: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-79mz8 in namespace proxy-3808
I0527 09:39:10.913423      15 runners.go:193] Created replication controller with name: proxy-service-79mz8, namespace: proxy-3808, replica count: 1
I0527 09:39:11.970931      15 runners.go:193] proxy-service-79mz8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0527 09:39:12.972319      15 runners.go:193] proxy-service-79mz8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 09:39:12.987: INFO: setup took 2.119599524s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 27 09:39:12.998: INFO: (0) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 10.032825ms)
May 27 09:39:13.004: INFO: (0) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 16.113824ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 30.416132ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 31.495581ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 30.870618ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 30.484217ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 30.150202ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 30.254617ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 30.392999ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 31.802728ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 31.457871ms)
May 27 09:39:13.019: INFO: (0) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 31.330986ms)
May 27 09:39:13.024: INFO: (0) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 35.139879ms)
May 27 09:39:13.025: INFO: (0) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 35.90084ms)
May 27 09:39:13.025: INFO: (0) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 37.098047ms)
May 27 09:39:13.025: INFO: (0) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 36.9369ms)
May 27 09:39:13.041: INFO: (1) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 14.026794ms)
May 27 09:39:13.042: INFO: (1) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 14.891649ms)
May 27 09:39:13.042: INFO: (1) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 15.224757ms)
May 27 09:39:13.043: INFO: (1) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 16.074725ms)
May 27 09:39:13.043: INFO: (1) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 15.902244ms)
May 27 09:39:13.043: INFO: (1) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 17.842802ms)
May 27 09:39:13.043: INFO: (1) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 17.630331ms)
May 27 09:39:13.045: INFO: (1) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 17.761291ms)
May 27 09:39:13.045: INFO: (1) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 17.647238ms)
May 27 09:39:13.045: INFO: (1) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 19.340216ms)
May 27 09:39:13.045: INFO: (1) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 18.992253ms)
May 27 09:39:13.049: INFO: (1) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 23.03147ms)
May 27 09:39:13.051: INFO: (1) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 23.908111ms)
May 27 09:39:13.052: INFO: (1) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 23.918265ms)
May 27 09:39:13.051: INFO: (1) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 24.549512ms)
May 27 09:39:13.054: INFO: (1) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 27.002186ms)
May 27 09:39:13.063: INFO: (2) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 8.854265ms)
May 27 09:39:13.065: INFO: (2) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 10.607028ms)
May 27 09:39:13.067: INFO: (2) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 12.108476ms)
May 27 09:39:13.068: INFO: (2) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 12.702685ms)
May 27 09:39:13.069: INFO: (2) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 14.96032ms)
May 27 09:39:13.069: INFO: (2) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 14.290446ms)
May 27 09:39:13.070: INFO: (2) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 15.389195ms)
May 27 09:39:13.072: INFO: (2) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 17.197615ms)
May 27 09:39:13.074: INFO: (2) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 18.945297ms)
May 27 09:39:13.075: INFO: (2) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 19.996464ms)
May 27 09:39:13.076: INFO: (2) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 21.393127ms)
May 27 09:39:13.076: INFO: (2) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 21.034615ms)
May 27 09:39:13.077: INFO: (2) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 22.293572ms)
May 27 09:39:13.077: INFO: (2) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 22.093839ms)
May 27 09:39:13.077: INFO: (2) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 22.655909ms)
May 27 09:39:13.080: INFO: (2) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 24.87363ms)
May 27 09:39:13.091: INFO: (3) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 10.756671ms)
May 27 09:39:13.092: INFO: (3) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 11.487764ms)
May 27 09:39:13.094: INFO: (3) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 13.718897ms)
May 27 09:39:13.095: INFO: (3) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 13.575178ms)
May 27 09:39:13.096: INFO: (3) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 14.152389ms)
May 27 09:39:13.096: INFO: (3) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 15.908768ms)
May 27 09:39:13.098: INFO: (3) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 15.935985ms)
May 27 09:39:13.098: INFO: (3) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 17.646271ms)
May 27 09:39:13.100: INFO: (3) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 18.930151ms)
May 27 09:39:13.100: INFO: (3) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 17.925783ms)
May 27 09:39:13.101: INFO: (3) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 19.609152ms)
May 27 09:39:13.103: INFO: (3) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 22.123175ms)
May 27 09:39:13.105: INFO: (3) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 23.048781ms)
May 27 09:39:13.106: INFO: (3) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 23.85966ms)
May 27 09:39:13.106: INFO: (3) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 23.699787ms)
May 27 09:39:13.107: INFO: (3) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 24.522397ms)
May 27 09:39:13.118: INFO: (4) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 11.237464ms)
May 27 09:39:13.120: INFO: (4) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 13.16546ms)
May 27 09:39:13.121: INFO: (4) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 13.938644ms)
May 27 09:39:13.123: INFO: (4) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 15.037142ms)
May 27 09:39:13.123: INFO: (4) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 15.579029ms)
May 27 09:39:13.123: INFO: (4) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 16.05229ms)
May 27 09:39:13.128: INFO: (4) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 19.34653ms)
May 27 09:39:13.128: INFO: (4) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 19.976797ms)
May 27 09:39:13.130: INFO: (4) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 22.524715ms)
May 27 09:39:13.130: INFO: (4) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 23.011472ms)
May 27 09:39:13.130: INFO: (4) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 22.443492ms)
May 27 09:39:13.130: INFO: (4) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 22.458485ms)
May 27 09:39:13.130: INFO: (4) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 22.94309ms)
May 27 09:39:13.132: INFO: (4) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 23.004054ms)
May 27 09:39:13.133: INFO: (4) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 25.414588ms)
May 27 09:39:13.133: INFO: (4) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 25.129572ms)
May 27 09:39:13.144: INFO: (5) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 8.77986ms)
May 27 09:39:13.144: INFO: (5) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 10.058898ms)
May 27 09:39:13.159: INFO: (5) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 25.523303ms)
May 27 09:39:13.159: INFO: (5) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 24.986564ms)
May 27 09:39:13.160: INFO: (5) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 25.477517ms)
May 27 09:39:13.160: INFO: (5) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 25.496814ms)
May 27 09:39:13.160: INFO: (5) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 26.128187ms)
May 27 09:39:13.160: INFO: (5) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 27.035508ms)
May 27 09:39:13.161: INFO: (5) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 26.789824ms)
May 27 09:39:13.162: INFO: (5) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 28.145122ms)
May 27 09:39:13.163: INFO: (5) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 28.376741ms)
May 27 09:39:13.165: INFO: (5) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 29.723653ms)
May 27 09:39:13.165: INFO: (5) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 29.932635ms)
May 27 09:39:13.165: INFO: (5) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 29.90232ms)
May 27 09:39:13.165: INFO: (5) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 29.670342ms)
May 27 09:39:13.169: INFO: (5) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 34.260803ms)
May 27 09:39:13.187: INFO: (6) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 17.623655ms)
May 27 09:39:13.188: INFO: (6) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 18.286913ms)
May 27 09:39:13.189: INFO: (6) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 19.570077ms)
May 27 09:39:13.189: INFO: (6) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 19.147649ms)
May 27 09:39:13.189: INFO: (6) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 18.89551ms)
May 27 09:39:13.190: INFO: (6) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 19.772239ms)
May 27 09:39:13.190: INFO: (6) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 20.346362ms)
May 27 09:39:13.190: INFO: (6) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 19.609791ms)
May 27 09:39:13.195: INFO: (6) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 25.62161ms)
May 27 09:39:13.195: INFO: (6) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 25.465065ms)
May 27 09:39:13.195: INFO: (6) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 25.32255ms)
May 27 09:39:13.198: INFO: (6) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 28.27112ms)
May 27 09:39:13.199: INFO: (6) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 28.593996ms)
May 27 09:39:13.199: INFO: (6) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 29.780631ms)
May 27 09:39:13.200: INFO: (6) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 29.486168ms)
May 27 09:39:13.203: INFO: (6) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 32.921819ms)
May 27 09:39:13.214: INFO: (7) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 10.730964ms)
May 27 09:39:13.214: INFO: (7) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 10.760289ms)
May 27 09:39:13.216: INFO: (7) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 11.01777ms)
May 27 09:39:13.221: INFO: (7) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 14.627358ms)
May 27 09:39:13.221: INFO: (7) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 17.351727ms)
May 27 09:39:13.221: INFO: (7) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 18.2931ms)
May 27 09:39:13.222: INFO: (7) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 18.19352ms)
May 27 09:39:13.222: INFO: (7) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 16.599076ms)
May 27 09:39:13.223: INFO: (7) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 18.152636ms)
May 27 09:39:13.224: INFO: (7) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 19.836911ms)
May 27 09:39:13.224: INFO: (7) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 18.779714ms)
May 27 09:39:13.224: INFO: (7) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 17.815939ms)
May 27 09:39:13.228: INFO: (7) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 22.436718ms)
May 27 09:39:13.228: INFO: (7) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 23.639701ms)
May 27 09:39:13.229: INFO: (7) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 24.888117ms)
May 27 09:39:13.229: INFO: (7) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 23.830469ms)
May 27 09:39:13.247: INFO: (8) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 17.111131ms)
May 27 09:39:13.248: INFO: (8) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 17.488526ms)
May 27 09:39:13.248: INFO: (8) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 18.432623ms)
May 27 09:39:13.248: INFO: (8) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 17.731451ms)
May 27 09:39:13.248: INFO: (8) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 18.791671ms)
May 27 09:39:13.249: INFO: (8) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 19.504686ms)
May 27 09:39:13.249: INFO: (8) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 19.129506ms)
May 27 09:39:13.249: INFO: (8) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 19.589398ms)
May 27 09:39:13.250: INFO: (8) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 19.929093ms)
May 27 09:39:13.250: INFO: (8) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 20.712719ms)
May 27 09:39:13.250: INFO: (8) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 20.392832ms)
May 27 09:39:13.251: INFO: (8) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 21.3973ms)
May 27 09:39:13.255: INFO: (8) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 24.908969ms)
May 27 09:39:13.255: INFO: (8) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 24.848718ms)
May 27 09:39:13.256: INFO: (8) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 26.420347ms)
May 27 09:39:13.257: INFO: (8) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 27.114833ms)
May 27 09:39:13.265: INFO: (9) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 7.897086ms)
May 27 09:39:13.269: INFO: (9) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 11.490527ms)
May 27 09:39:13.269: INFO: (9) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 11.732491ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 19.436811ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 18.730804ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 20.539106ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 20.44058ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 21.054688ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 19.823043ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 20.141746ms)
May 27 09:39:13.278: INFO: (9) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 19.303298ms)
May 27 09:39:13.282: INFO: (9) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 22.958946ms)
May 27 09:39:13.282: INFO: (9) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 23.584166ms)
May 27 09:39:13.282: INFO: (9) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 24.486099ms)
May 27 09:39:13.282: INFO: (9) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 24.398456ms)
May 27 09:39:13.283: INFO: (9) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 24.354881ms)
May 27 09:39:13.295: INFO: (10) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 12.47051ms)
May 27 09:39:13.297: INFO: (10) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 13.589324ms)
May 27 09:39:13.298: INFO: (10) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 14.483484ms)
May 27 09:39:13.299: INFO: (10) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 16.412067ms)
May 27 09:39:13.305: INFO: (10) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 21.337862ms)
May 27 09:39:13.305: INFO: (10) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 21.0232ms)
May 27 09:39:13.306: INFO: (10) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 22.12681ms)
May 27 09:39:13.306: INFO: (10) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 22.336156ms)
May 27 09:39:13.306: INFO: (10) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 22.130171ms)
May 27 09:39:13.306: INFO: (10) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 22.857978ms)
May 27 09:39:13.306: INFO: (10) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 22.3339ms)
May 27 09:39:13.306: INFO: (10) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 22.52998ms)
May 27 09:39:13.307: INFO: (10) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 23.240134ms)
May 27 09:39:13.318: INFO: (10) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 33.983628ms)
May 27 09:39:13.318: INFO: (10) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 34.282027ms)
May 27 09:39:13.319: INFO: (10) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 34.986695ms)
May 27 09:39:13.327: INFO: (11) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 8.237858ms)
May 27 09:39:13.329: INFO: (11) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 10.562538ms)
May 27 09:39:13.333: INFO: (11) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 12.918411ms)
May 27 09:39:13.334: INFO: (11) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 14.08076ms)
May 27 09:39:13.335: INFO: (11) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 14.691804ms)
May 27 09:39:13.336: INFO: (11) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 17.123414ms)
May 27 09:39:13.337: INFO: (11) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 17.386574ms)
May 27 09:39:13.337: INFO: (11) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 17.805233ms)
May 27 09:39:13.339: INFO: (11) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 19.616105ms)
May 27 09:39:13.339: INFO: (11) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 19.784401ms)
May 27 09:39:13.340: INFO: (11) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 19.894335ms)
May 27 09:39:13.340: INFO: (11) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 19.581851ms)
May 27 09:39:13.340: INFO: (11) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 20.584482ms)
May 27 09:39:13.341: INFO: (11) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 21.610482ms)
May 27 09:39:13.343: INFO: (11) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 22.160962ms)
May 27 09:39:13.344: INFO: (11) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 23.800711ms)
May 27 09:39:13.358: INFO: (12) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 12.675084ms)
May 27 09:39:13.359: INFO: (12) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 15.173908ms)
May 27 09:39:13.361: INFO: (12) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 17.085114ms)
May 27 09:39:13.362: INFO: (12) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 16.778853ms)
May 27 09:39:13.362: INFO: (12) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 16.395631ms)
May 27 09:39:13.362: INFO: (12) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 18.203144ms)
May 27 09:39:13.362: INFO: (12) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 17.366442ms)
May 27 09:39:13.363: INFO: (12) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 17.300641ms)
May 27 09:39:13.363: INFO: (12) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 18.93382ms)
May 27 09:39:13.363: INFO: (12) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 19.049092ms)
May 27 09:39:13.366: INFO: (12) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 20.200885ms)
May 27 09:39:13.366: INFO: (12) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 20.676293ms)
May 27 09:39:13.368: INFO: (12) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 23.603337ms)
May 27 09:39:13.369: INFO: (12) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 24.397129ms)
May 27 09:39:13.369: INFO: (12) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 23.763596ms)
May 27 09:39:13.370: INFO: (12) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 26.230824ms)
May 27 09:39:13.390: INFO: (13) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 19.702514ms)
May 27 09:39:13.390: INFO: (13) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 19.415609ms)
May 27 09:39:13.390: INFO: (13) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 18.976587ms)
May 27 09:39:13.396: INFO: (13) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 23.685021ms)
May 27 09:39:13.396: INFO: (13) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 24.899405ms)
May 27 09:39:13.396: INFO: (13) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 24.264515ms)
May 27 09:39:13.396: INFO: (13) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 25.260286ms)
May 27 09:39:13.400: INFO: (13) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 29.633418ms)
May 27 09:39:13.400: INFO: (13) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 28.924796ms)
May 27 09:39:13.401: INFO: (13) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 28.740275ms)
May 27 09:39:13.401: INFO: (13) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 29.100143ms)
May 27 09:39:13.401: INFO: (13) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 29.776648ms)
May 27 09:39:13.403: INFO: (13) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 31.933749ms)
May 27 09:39:13.403: INFO: (13) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 32.020217ms)
May 27 09:39:13.405: INFO: (13) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 32.932977ms)
May 27 09:39:13.406: INFO: (13) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 34.112999ms)
May 27 09:39:13.420: INFO: (14) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 14.121342ms)
May 27 09:39:13.420: INFO: (14) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 13.628529ms)
May 27 09:39:13.421: INFO: (14) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 15.437603ms)
May 27 09:39:13.426: INFO: (14) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 19.271839ms)
May 27 09:39:13.432: INFO: (14) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 25.181748ms)
May 27 09:39:13.434: INFO: (14) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 28.18802ms)
May 27 09:39:13.434: INFO: (14) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 28.08141ms)
May 27 09:39:13.435: INFO: (14) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 27.962635ms)
May 27 09:39:13.434: INFO: (14) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 27.930505ms)
May 27 09:39:13.435: INFO: (14) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 28.990304ms)
May 27 09:39:13.435: INFO: (14) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 29.176674ms)
May 27 09:39:13.436: INFO: (14) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 29.13866ms)
May 27 09:39:13.436: INFO: (14) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 29.204206ms)
May 27 09:39:13.436: INFO: (14) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 30.034014ms)
May 27 09:39:13.439: INFO: (14) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 32.086191ms)
May 27 09:39:13.439: INFO: (14) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 32.733867ms)
May 27 09:39:13.456: INFO: (15) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 16.478566ms)
May 27 09:39:13.460: INFO: (15) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 19.807236ms)
May 27 09:39:13.461: INFO: (15) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 20.39021ms)
May 27 09:39:13.461: INFO: (15) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 20.560158ms)
May 27 09:39:13.461: INFO: (15) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 21.459552ms)
May 27 09:39:13.461: INFO: (15) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 20.64998ms)
May 27 09:39:13.465: INFO: (15) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 25.515656ms)
May 27 09:39:13.465: INFO: (15) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 25.473878ms)
May 27 09:39:13.466: INFO: (15) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 25.13541ms)
May 27 09:39:13.466: INFO: (15) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 25.288765ms)
May 27 09:39:13.467: INFO: (15) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 26.876288ms)
May 27 09:39:13.470: INFO: (15) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 31.067316ms)
May 27 09:39:13.470: INFO: (15) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 29.893384ms)
May 27 09:39:13.471: INFO: (15) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 30.837262ms)
May 27 09:39:13.471: INFO: (15) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 30.083557ms)
May 27 09:39:13.472: INFO: (15) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 31.09448ms)
May 27 09:39:13.495: INFO: (16) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 23.03479ms)
May 27 09:39:13.496: INFO: (16) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 23.94132ms)
May 27 09:39:13.497: INFO: (16) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 24.473383ms)
May 27 09:39:13.507: INFO: (16) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 34.538332ms)
May 27 09:39:13.510: INFO: (16) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 36.509972ms)
May 27 09:39:13.510: INFO: (16) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 35.599111ms)
May 27 09:39:13.510: INFO: (16) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 36.379635ms)
May 27 09:39:13.510: INFO: (16) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 36.035112ms)
May 27 09:39:13.511: INFO: (16) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 38.235627ms)
May 27 09:39:13.512: INFO: (16) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 40.00547ms)
May 27 09:39:13.512: INFO: (16) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 38.617057ms)
May 27 09:39:13.514: INFO: (16) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 41.467417ms)
May 27 09:39:13.515: INFO: (16) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 42.545367ms)
May 27 09:39:13.515: INFO: (16) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 41.910695ms)
May 27 09:39:13.516: INFO: (16) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 43.828949ms)
May 27 09:39:13.520: INFO: (16) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 46.767018ms)
May 27 09:39:13.537: INFO: (17) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 16.486139ms)
May 27 09:39:13.537: INFO: (17) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 16.674816ms)
May 27 09:39:13.539: INFO: (17) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 18.424004ms)
May 27 09:39:13.552: INFO: (17) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 31.508513ms)
May 27 09:39:13.552: INFO: (17) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 31.233936ms)
May 27 09:39:13.553: INFO: (17) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 31.713073ms)
May 27 09:39:13.553: INFO: (17) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 31.241151ms)
May 27 09:39:13.553: INFO: (17) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 31.379892ms)
May 27 09:39:13.553: INFO: (17) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 31.643898ms)
May 27 09:39:13.553: INFO: (17) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 31.95486ms)
May 27 09:39:13.553: INFO: (17) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 31.26589ms)
May 27 09:39:13.558: INFO: (17) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 37.186271ms)
May 27 09:39:13.559: INFO: (17) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 38.232347ms)
May 27 09:39:13.559: INFO: (17) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 38.573036ms)
May 27 09:39:13.560: INFO: (17) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 38.208671ms)
May 27 09:39:13.563: INFO: (17) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 41.248226ms)
May 27 09:39:13.570: INFO: (18) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 7.645114ms)
May 27 09:39:13.573: INFO: (18) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 10.337821ms)
May 27 09:39:13.576: INFO: (18) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 13.192139ms)
May 27 09:39:13.576: INFO: (18) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 13.447079ms)
May 27 09:39:13.578: INFO: (18) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 15.194726ms)
May 27 09:39:13.578: INFO: (18) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 15.357564ms)
May 27 09:39:13.579: INFO: (18) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 16.161325ms)
May 27 09:39:13.580: INFO: (18) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 16.343552ms)
May 27 09:39:13.582: INFO: (18) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 18.234343ms)
May 27 09:39:13.583: INFO: (18) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 19.706705ms)
May 27 09:39:13.583: INFO: (18) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 20.049539ms)
May 27 09:39:13.589: INFO: (18) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 25.281786ms)
May 27 09:39:13.590: INFO: (18) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 26.821586ms)
May 27 09:39:13.594: INFO: (18) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 30.877755ms)
May 27 09:39:13.594: INFO: (18) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 30.822713ms)
May 27 09:39:13.596: INFO: (18) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 32.214693ms)
May 27 09:39:13.604: INFO: (19) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 7.979035ms)
May 27 09:39:13.623: INFO: (19) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">test<... (200; 25.85232ms)
May 27 09:39:13.623: INFO: (19) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf/proxy/rewriteme">test</a> (200; 26.70678ms)
May 27 09:39:13.628: INFO: (19) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:1080/proxy/rewriteme">... (200; 29.543398ms)
May 27 09:39:13.632: INFO: (19) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:160/proxy/: foo (200; 35.155237ms)
May 27 09:39:13.632: INFO: (19) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname2/proxy/: bar (200; 34.736932ms)
May 27 09:39:13.632: INFO: (19) /api/v1/namespaces/proxy-3808/pods/http:proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 34.532717ms)
May 27 09:39:13.632: INFO: (19) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:462/proxy/: tls qux (200; 35.332914ms)
May 27 09:39:13.633: INFO: (19) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:460/proxy/: tls baz (200; 35.796778ms)
May 27 09:39:13.633: INFO: (19) /api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/: <a href="/api/v1/namespaces/proxy-3808/pods/https:proxy-service-79mz8-hrgxf:443/proxy/tlsrewritem... (200; 35.303611ms)
May 27 09:39:13.633: INFO: (19) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname1/proxy/: foo (200; 35.924826ms)
May 27 09:39:13.634: INFO: (19) /api/v1/namespaces/proxy-3808/services/http:proxy-service-79mz8:portname1/proxy/: foo (200; 35.791996ms)
May 27 09:39:13.634: INFO: (19) /api/v1/namespaces/proxy-3808/services/proxy-service-79mz8:portname2/proxy/: bar (200; 35.403994ms)
May 27 09:39:13.634: INFO: (19) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname2/proxy/: tls qux (200; 35.766297ms)
May 27 09:39:13.634: INFO: (19) /api/v1/namespaces/proxy-3808/services/https:proxy-service-79mz8:tlsportname1/proxy/: tls baz (200; 37.328708ms)
May 27 09:39:13.634: INFO: (19) /api/v1/namespaces/proxy-3808/pods/proxy-service-79mz8-hrgxf:162/proxy/: bar (200; 35.673477ms)
STEP: deleting ReplicationController proxy-service-79mz8 in namespace proxy-3808, will wait for the garbage collector to delete the pods
May 27 09:39:13.705: INFO: Deleting ReplicationController proxy-service-79mz8 took: 12.169967ms
May 27 09:39:13.807: INFO: Terminating ReplicationController proxy-service-79mz8 pods took: 101.376155ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:39:16.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3808" for this suite.

• [SLOW TEST:5.629 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":3,"skipped":26,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:39:16.450: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:39:16.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9317" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":4,"skipped":35,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:39:16.542: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 13.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.13_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 13.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.13_udp@PTR;check="$$(dig +tcp +noall +answer +search 13.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.13_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 09:39:18.756: INFO: Unable to read wheezy_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.761: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.765: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.770: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.825: INFO: Unable to read jessie_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.831: INFO: Unable to read jessie_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:18.864: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_udp@dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_udp@dns-test-service.dns-5119.svc.cluster.local jessie_tcp@dns-test-service.dns-5119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:23.877: INFO: Unable to read wheezy_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.885: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.894: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.902: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.961: INFO: Unable to read jessie_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.972: INFO: Unable to read jessie_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.978: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:23.985: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:24.011: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_udp@dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_udp@dns-test-service.dns-5119.svc.cluster.local jessie_tcp@dns-test-service.dns-5119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:28.873: INFO: Unable to read wheezy_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.879: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.885: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.890: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.916: INFO: Unable to read jessie_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.921: INFO: Unable to read jessie_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.926: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.934: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:28.952: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_udp@dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_udp@dns-test-service.dns-5119.svc.cluster.local jessie_tcp@dns-test-service.dns-5119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:33.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.890: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.896: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.938: INFO: Unable to read jessie_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.951: INFO: Unable to read jessie_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.961: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:33.976: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:34.022: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_udp@dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_udp@dns-test-service.dns-5119.svc.cluster.local jessie_tcp@dns-test-service.dns-5119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:38.882: INFO: Unable to read wheezy_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.903: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.910: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.936: INFO: Unable to read jessie_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.941: INFO: Unable to read jessie_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.953: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:38.981: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_udp@dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_udp@dns-test-service.dns-5119.svc.cluster.local jessie_tcp@dns-test-service.dns-5119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:43.903: INFO: Unable to read wheezy_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:43.911: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:43.916: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:43.949: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:43.989: INFO: Unable to read jessie_udp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:43.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:44.003: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:44.008: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:44.032: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_udp@dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@dns-test-service.dns-5119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_udp@dns-test-service.dns-5119.svc.cluster.local jessie_tcp@dns-test-service.dns-5119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:48.909: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local from pod dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8: the server could not find the requested resource (get pods dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8)
May 27 09:39:48.988: INFO: Lookups using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-5119.svc.cluster.local]

May 27 09:39:54.000: INFO: DNS probes using dns-5119/dns-test-7790e74a-7a8c-47b1-95a1-e33def888db8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:39:54.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5119" for this suite.

• [SLOW TEST:37.714 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":5,"skipped":45,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:39:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:00.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8745" for this suite.

• [SLOW TEST:306.168 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":6,"skipped":52,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:00.427: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 09:45:01.934: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 09:45:04.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:45:05.001: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5567-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:08.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9285" for this suite.
STEP: Destroying namespace "webhook-9285-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.379 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":7,"skipped":58,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:08.807: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8873
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8873
I0527 09:45:08.995357      15 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8873, replica count: 2
I0527 09:45:12.047847      15 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 09:45:12.048: INFO: Creating new exec pod
May 27 09:45:15.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-8873 exec execpod8ltbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 27 09:45:15.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 27 09:45:15.607: INFO: stdout: "externalname-service-92nh5"
May 27 09:45:15.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-8873 exec execpod8ltbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.31.59 80'
May 27 09:45:15.790: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.31.59 80\nConnection to 10.233.31.59 80 port [tcp/http] succeeded!\n"
May 27 09:45:15.790: INFO: stdout: "externalname-service-92nh5"
May 27 09:45:15.790: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:15.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8873" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.063 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":8,"skipped":63,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:15.871: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
May 27 09:45:16.008: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 27 09:45:18.019: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 27 09:45:19.060: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:20.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-869" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":9,"skipped":66,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:20.115: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:45:20.157: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 09:45:20.187: INFO: The status of Pod pod-exec-websocket-7e1c169c-1c39-4757-ab8a-c0090a03c071 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:45:22.205: INFO: The status of Pod pod-exec-websocket-7e1c169c-1c39-4757-ab8a-c0090a03c071 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:45:24.209: INFO: The status of Pod pod-exec-websocket-7e1c169c-1c39-4757-ab8a-c0090a03c071 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:24.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6161" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":74,"failed":0}
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:24.446: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:28.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3986" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":11,"skipped":79,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:28.603: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
May 27 09:45:28.658: INFO: created test-event-1
May 27 09:45:28.665: INFO: created test-event-2
May 27 09:45:28.672: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 27 09:45:28.676: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 27 09:45:28.710: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:45:28.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1921" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":12,"skipped":94,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:45:28.731: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
May 27 09:46:09.237: INFO: The status of Pod kube-controller-manager-ohp4eith3vui-2 is Running (Ready = true)
May 27 09:46:09.361: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 27 09:46:09.362: INFO: Deleting pod "simpletest.rc-2c4nr" in namespace "gc-9901"
May 27 09:46:09.434: INFO: Deleting pod "simpletest.rc-2q7ws" in namespace "gc-9901"
May 27 09:46:09.485: INFO: Deleting pod "simpletest.rc-42vtx" in namespace "gc-9901"
May 27 09:46:09.557: INFO: Deleting pod "simpletest.rc-45qd5" in namespace "gc-9901"
May 27 09:46:09.620: INFO: Deleting pod "simpletest.rc-4p5k6" in namespace "gc-9901"
May 27 09:46:09.663: INFO: Deleting pod "simpletest.rc-4qszs" in namespace "gc-9901"
May 27 09:46:09.806: INFO: Deleting pod "simpletest.rc-4z5qk" in namespace "gc-9901"
May 27 09:46:09.897: INFO: Deleting pod "simpletest.rc-544w2" in namespace "gc-9901"
May 27 09:46:09.971: INFO: Deleting pod "simpletest.rc-5mn9m" in namespace "gc-9901"
May 27 09:46:10.061: INFO: Deleting pod "simpletest.rc-5pdfh" in namespace "gc-9901"
May 27 09:46:10.124: INFO: Deleting pod "simpletest.rc-5vzp4" in namespace "gc-9901"
May 27 09:46:10.234: INFO: Deleting pod "simpletest.rc-69tzq" in namespace "gc-9901"
May 27 09:46:10.306: INFO: Deleting pod "simpletest.rc-6mcr2" in namespace "gc-9901"
May 27 09:46:10.406: INFO: Deleting pod "simpletest.rc-72lxq" in namespace "gc-9901"
May 27 09:46:10.465: INFO: Deleting pod "simpletest.rc-76q6k" in namespace "gc-9901"
May 27 09:46:10.527: INFO: Deleting pod "simpletest.rc-79dwb" in namespace "gc-9901"
May 27 09:46:10.835: INFO: Deleting pod "simpletest.rc-7rprh" in namespace "gc-9901"
May 27 09:46:10.983: INFO: Deleting pod "simpletest.rc-7ztv9" in namespace "gc-9901"
May 27 09:46:11.174: INFO: Deleting pod "simpletest.rc-8bj4d" in namespace "gc-9901"
May 27 09:46:11.255: INFO: Deleting pod "simpletest.rc-8hx8d" in namespace "gc-9901"
May 27 09:46:11.419: INFO: Deleting pod "simpletest.rc-8lrxq" in namespace "gc-9901"
May 27 09:46:11.521: INFO: Deleting pod "simpletest.rc-8x9j6" in namespace "gc-9901"
May 27 09:46:11.790: INFO: Deleting pod "simpletest.rc-92bhv" in namespace "gc-9901"
May 27 09:46:11.915: INFO: Deleting pod "simpletest.rc-92zsv" in namespace "gc-9901"
May 27 09:46:12.144: INFO: Deleting pod "simpletest.rc-9gbzv" in namespace "gc-9901"
May 27 09:46:12.216: INFO: Deleting pod "simpletest.rc-9hdsg" in namespace "gc-9901"
May 27 09:46:12.291: INFO: Deleting pod "simpletest.rc-9x6sv" in namespace "gc-9901"
May 27 09:46:12.326: INFO: Deleting pod "simpletest.rc-bcz8c" in namespace "gc-9901"
May 27 09:46:12.397: INFO: Deleting pod "simpletest.rc-bld45" in namespace "gc-9901"
May 27 09:46:12.478: INFO: Deleting pod "simpletest.rc-bsnqm" in namespace "gc-9901"
May 27 09:46:12.506: INFO: Deleting pod "simpletest.rc-bwpm7" in namespace "gc-9901"
May 27 09:46:12.630: INFO: Deleting pod "simpletest.rc-cl76x" in namespace "gc-9901"
May 27 09:46:12.686: INFO: Deleting pod "simpletest.rc-df57j" in namespace "gc-9901"
May 27 09:46:12.762: INFO: Deleting pod "simpletest.rc-dvpln" in namespace "gc-9901"
May 27 09:46:12.834: INFO: Deleting pod "simpletest.rc-dw776" in namespace "gc-9901"
May 27 09:46:12.882: INFO: Deleting pod "simpletest.rc-fh2nn" in namespace "gc-9901"
May 27 09:46:12.934: INFO: Deleting pod "simpletest.rc-fm2vm" in namespace "gc-9901"
May 27 09:46:12.985: INFO: Deleting pod "simpletest.rc-fmtrf" in namespace "gc-9901"
May 27 09:46:13.016: INFO: Deleting pod "simpletest.rc-ft7hv" in namespace "gc-9901"
May 27 09:46:13.067: INFO: Deleting pod "simpletest.rc-fx92r" in namespace "gc-9901"
May 27 09:46:13.129: INFO: Deleting pod "simpletest.rc-fxbws" in namespace "gc-9901"
May 27 09:46:13.227: INFO: Deleting pod "simpletest.rc-g2nsn" in namespace "gc-9901"
May 27 09:46:13.291: INFO: Deleting pod "simpletest.rc-gdtx6" in namespace "gc-9901"
May 27 09:46:13.354: INFO: Deleting pod "simpletest.rc-ggp65" in namespace "gc-9901"
May 27 09:46:13.404: INFO: Deleting pod "simpletest.rc-gzxcq" in namespace "gc-9901"
May 27 09:46:13.517: INFO: Deleting pod "simpletest.rc-hh9d6" in namespace "gc-9901"
May 27 09:46:13.728: INFO: Deleting pod "simpletest.rc-hkpqs" in namespace "gc-9901"
May 27 09:46:13.894: INFO: Deleting pod "simpletest.rc-jlbvl" in namespace "gc-9901"
May 27 09:46:13.957: INFO: Deleting pod "simpletest.rc-jnfww" in namespace "gc-9901"
May 27 09:46:14.050: INFO: Deleting pod "simpletest.rc-jpmbc" in namespace "gc-9901"
May 27 09:46:14.140: INFO: Deleting pod "simpletest.rc-jrd4p" in namespace "gc-9901"
May 27 09:46:14.265: INFO: Deleting pod "simpletest.rc-jscfc" in namespace "gc-9901"
May 27 09:46:14.442: INFO: Deleting pod "simpletest.rc-jw8lq" in namespace "gc-9901"
May 27 09:46:14.505: INFO: Deleting pod "simpletest.rc-jzbfm" in namespace "gc-9901"
May 27 09:46:14.596: INFO: Deleting pod "simpletest.rc-k62zc" in namespace "gc-9901"
May 27 09:46:14.726: INFO: Deleting pod "simpletest.rc-kmnct" in namespace "gc-9901"
May 27 09:46:15.033: INFO: Deleting pod "simpletest.rc-kz7lt" in namespace "gc-9901"
May 27 09:46:15.125: INFO: Deleting pod "simpletest.rc-l6dmd" in namespace "gc-9901"
May 27 09:46:15.214: INFO: Deleting pod "simpletest.rc-ljlcx" in namespace "gc-9901"
May 27 09:46:15.274: INFO: Deleting pod "simpletest.rc-ln67w" in namespace "gc-9901"
May 27 09:46:15.363: INFO: Deleting pod "simpletest.rc-lq8v4" in namespace "gc-9901"
May 27 09:46:15.488: INFO: Deleting pod "simpletest.rc-lxlls" in namespace "gc-9901"
May 27 09:46:15.672: INFO: Deleting pod "simpletest.rc-mfqt9" in namespace "gc-9901"
May 27 09:46:16.007: INFO: Deleting pod "simpletest.rc-mrwfp" in namespace "gc-9901"
May 27 09:46:16.123: INFO: Deleting pod "simpletest.rc-ms9r7" in namespace "gc-9901"
May 27 09:46:16.248: INFO: Deleting pod "simpletest.rc-ndpdr" in namespace "gc-9901"
May 27 09:46:16.333: INFO: Deleting pod "simpletest.rc-nfrsj" in namespace "gc-9901"
May 27 09:46:16.447: INFO: Deleting pod "simpletest.rc-nks74" in namespace "gc-9901"
May 27 09:46:16.622: INFO: Deleting pod "simpletest.rc-nnlsb" in namespace "gc-9901"
May 27 09:46:16.931: INFO: Deleting pod "simpletest.rc-pp845" in namespace "gc-9901"
May 27 09:46:17.042: INFO: Deleting pod "simpletest.rc-pxzf2" in namespace "gc-9901"
May 27 09:46:17.095: INFO: Deleting pod "simpletest.rc-q45m7" in namespace "gc-9901"
May 27 09:46:17.146: INFO: Deleting pod "simpletest.rc-q479k" in namespace "gc-9901"
May 27 09:46:17.213: INFO: Deleting pod "simpletest.rc-q85x4" in namespace "gc-9901"
May 27 09:46:17.286: INFO: Deleting pod "simpletest.rc-qb6ns" in namespace "gc-9901"
May 27 09:46:17.438: INFO: Deleting pod "simpletest.rc-qf5fj" in namespace "gc-9901"
May 27 09:46:17.540: INFO: Deleting pod "simpletest.rc-qpd52" in namespace "gc-9901"
May 27 09:46:17.641: INFO: Deleting pod "simpletest.rc-qrtns" in namespace "gc-9901"
May 27 09:46:17.710: INFO: Deleting pod "simpletest.rc-qs24c" in namespace "gc-9901"
May 27 09:46:17.873: INFO: Deleting pod "simpletest.rc-r26sh" in namespace "gc-9901"
May 27 09:46:18.007: INFO: Deleting pod "simpletest.rc-r59m8" in namespace "gc-9901"
May 27 09:46:18.116: INFO: Deleting pod "simpletest.rc-rfp7c" in namespace "gc-9901"
May 27 09:46:18.226: INFO: Deleting pod "simpletest.rc-rjq4b" in namespace "gc-9901"
May 27 09:46:18.300: INFO: Deleting pod "simpletest.rc-rlzbm" in namespace "gc-9901"
May 27 09:46:18.348: INFO: Deleting pod "simpletest.rc-sj49f" in namespace "gc-9901"
May 27 09:46:18.388: INFO: Deleting pod "simpletest.rc-sr7lk" in namespace "gc-9901"
May 27 09:46:18.547: INFO: Deleting pod "simpletest.rc-szx2v" in namespace "gc-9901"
May 27 09:46:18.626: INFO: Deleting pod "simpletest.rc-v2sjb" in namespace "gc-9901"
May 27 09:46:18.686: INFO: Deleting pod "simpletest.rc-vqcn9" in namespace "gc-9901"
May 27 09:46:18.881: INFO: Deleting pod "simpletest.rc-vvqkf" in namespace "gc-9901"
May 27 09:46:19.001: INFO: Deleting pod "simpletest.rc-wlrzg" in namespace "gc-9901"
May 27 09:46:19.080: INFO: Deleting pod "simpletest.rc-wtxh2" in namespace "gc-9901"
May 27 09:46:19.206: INFO: Deleting pod "simpletest.rc-wvhwt" in namespace "gc-9901"
May 27 09:46:19.316: INFO: Deleting pod "simpletest.rc-xgfmq" in namespace "gc-9901"
May 27 09:46:19.360: INFO: Deleting pod "simpletest.rc-xmcg7" in namespace "gc-9901"
May 27 09:46:19.474: INFO: Deleting pod "simpletest.rc-xv4sd" in namespace "gc-9901"
May 27 09:46:19.675: INFO: Deleting pod "simpletest.rc-z6jdd" in namespace "gc-9901"
May 27 09:46:19.974: INFO: Deleting pod "simpletest.rc-zbmsf" in namespace "gc-9901"
May 27 09:46:20.049: INFO: Deleting pod "simpletest.rc-zt7ws" in namespace "gc-9901"
May 27 09:46:20.139: INFO: Deleting pod "simpletest.rc-zww2f" in namespace "gc-9901"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:20.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9901" for this suite.

• [SLOW TEST:51.527 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":13,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:20.266: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
May 27 09:46:26.908: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5155 pod-service-account-97da1242-e3af-4112-b05b-5b43b7b0f5c4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 27 09:46:27.206: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5155 pod-service-account-97da1242-e3af-4112-b05b-5b43b7b0f5c4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 27 09:46:27.427: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5155 pod-service-account-97da1242-e3af-4112-b05b-5b43b7b0f5c4 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:27.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5155" for this suite.

• [SLOW TEST:7.414 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":14,"skipped":178,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:27.680: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:46:27.733: INFO: Creating simple deployment test-new-deployment
May 27 09:46:27.763: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 09:46:29.845: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6848  611a607e-3df8-47a9-9380-7a162ca2ce7e 15506 3 2022-05-27 09:46:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-05-27 09:46:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 09:46:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00438cd08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 09:46:29 +0000 UTC,LastTransitionTime:2022-05-27 09:46:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-05-27 09:46:29 +0000 UTC,LastTransitionTime:2022-05-27 09:46:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 09:46:29.877: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-6848  54f3bbad-97c6-4e21-83fc-b7297a17dd86 15507 2 2022-05-27 09:46:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 611a607e-3df8-47a9-9380-7a162ca2ce7e 0xc0043319e7 0xc0043319e8}] []  [{kube-controller-manager Update apps/v1 2022-05-27 09:46:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"611a607e-3df8-47a9-9380-7a162ca2ce7e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 09:46:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004331a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 09:46:29.921: INFO: Pod "test-new-deployment-5d9fdcc779-kqwwf" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-kqwwf test-new-deployment-5d9fdcc779- deployment-6848  b1427bf3-7cb8-4418-8a3c-27df788093a9 15409 0 2022-05-27 09:46:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 54f3bbad-97c6-4e21-83fc-b7297a17dd86 0xc004331e47 0xc004331e48}] []  [{kube-controller-manager Update v1 2022-05-27 09:46:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54f3bbad-97c6-4e21-83fc-b7297a17dd86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 09:46:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gndnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gndnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.22,StartTime:2022-05-27 09:46:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 09:46:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://0587acc5cd5583466ef9285703cf280efdc3330d136cc5269471f2dffa18baec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 09:46:29.922: INFO: Pod "test-new-deployment-5d9fdcc779-wwdg6" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-wwdg6 test-new-deployment-5d9fdcc779- deployment-6848  1f8a1e1d-f923-46fe-b12e-ec7c98397a27 15517 0 2022-05-27 09:46:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 54f3bbad-97c6-4e21-83fc-b7297a17dd86 0xc00441c037 0xc00441c038}] []  [{kube-controller-manager Update v1 2022-05-27 09:46:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54f3bbad-97c6-4e21-83fc-b7297a17dd86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 09:46:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rm9sz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rm9sz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:46:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 09:46:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:29.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6848" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":15,"skipped":184,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-e5154145-01bc-48bf-96cb-746d332f933a
STEP: Creating configMap with name cm-test-opt-upd-76a0a72a-1712-41a8-bf45-0934db7bf727
STEP: Creating the pod
May 27 09:46:30.125: INFO: The status of Pod pod-configmaps-056920f6-b1ed-4fb7-b6e2-1cfa40da1891 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:46:32.137: INFO: The status of Pod pod-configmaps-056920f6-b1ed-4fb7-b6e2-1cfa40da1891 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-e5154145-01bc-48bf-96cb-746d332f933a
STEP: Updating configmap cm-test-opt-upd-76a0a72a-1712-41a8-bf45-0934db7bf727
STEP: Creating configMap with name cm-test-opt-create-5f32008f-1456-462b-8415-029ef02ba130
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:34.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9642" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:34.256: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:38.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2379" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":17,"skipped":210,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:38.434: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:46:38.471: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:39.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9203" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":18,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:39.080: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 09:46:39.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327" in namespace "projected-4538" to be "Succeeded or Failed"
May 27 09:46:39.169: INFO: Pod "downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327": Phase="Pending", Reason="", readiness=false. Elapsed: 15.969564ms
May 27 09:46:41.185: INFO: Pod "downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032025102s
May 27 09:46:43.201: INFO: Pod "downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047321192s
STEP: Saw pod success
May 27 09:46:43.201: INFO: Pod "downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327" satisfied condition "Succeeded or Failed"
May 27 09:46:43.206: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327 container client-container: <nil>
STEP: delete the pod
May 27 09:46:43.238: INFO: Waiting for pod downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327 to disappear
May 27 09:46:43.243: INFO: Pod downwardapi-volume-aa6e9cc6-b6b6-452b-bca3-122b476b1327 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:46:43.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4538" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":19,"skipped":268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:46:43.260: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4196
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
May 27 09:46:43.354: INFO: Found 0 stateful pods, waiting for 3
May 27 09:46:53.397: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 09:46:53.398: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 09:46:53.398: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May 27 09:46:53.464: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 27 09:47:03.553: INFO: Updating stateful set ss2
May 27 09:47:03.562: INFO: Waiting for Pod statefulset-4196/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
May 27 09:47:13.680: INFO: Found 1 stateful pods, waiting for 3
May 27 09:47:23.701: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 09:47:23.701: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 09:47:23.701: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 27 09:47:23.742: INFO: Updating stateful set ss2
May 27 09:47:23.769: INFO: Waiting for Pod statefulset-4196/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
May 27 09:47:33.833: INFO: Updating stateful set ss2
May 27 09:47:33.847: INFO: Waiting for StatefulSet statefulset-4196/ss2 to complete update
May 27 09:47:33.847: INFO: Waiting for Pod statefulset-4196/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
May 27 09:47:43.882: INFO: Waiting for StatefulSet statefulset-4196/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 09:47:53.896: INFO: Deleting all statefulset in ns statefulset-4196
May 27 09:47:53.904: INFO: Scaling statefulset ss2 to 0
May 27 09:48:03.970: INFO: Waiting for statefulset status.replicas updated to 0
May 27 09:48:03.977: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:48:04.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4196" for this suite.

• [SLOW TEST:80.786 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":20,"skipped":312,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:48:04.053: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-54691d92-32a7-4cea-a609-957b58918906
STEP: Creating a pod to test consume secrets
May 27 09:48:04.117: INFO: Waiting up to 5m0s for pod "pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da" in namespace "secrets-5414" to be "Succeeded or Failed"
May 27 09:48:04.123: INFO: Pod "pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479413ms
May 27 09:48:06.152: INFO: Pod "pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035094686s
May 27 09:48:08.168: INFO: Pod "pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051308922s
STEP: Saw pod success
May 27 09:48:08.169: INFO: Pod "pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da" satisfied condition "Succeeded or Failed"
May 27 09:48:08.174: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da container secret-volume-test: <nil>
STEP: delete the pod
May 27 09:48:08.208: INFO: Waiting for pod pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da to disappear
May 27 09:48:08.214: INFO: Pod pod-secrets-2939f1c6-1ab2-4623-9aaa-0f39b300d8da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:48:08.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5414" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":315,"failed":0}
SSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:48:08.242: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:48:08.318: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea" in namespace "security-context-test-2422" to be "Succeeded or Failed"
May 27 09:48:08.324: INFO: Pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.437409ms
May 27 09:48:10.337: INFO: Pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019075492s
May 27 09:48:12.357: INFO: Pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039828397s
May 27 09:48:14.373: INFO: Pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055572602s
May 27 09:48:16.393: INFO: Pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.075240488s
May 27 09:48:16.393: INFO: Pod "alpine-nnp-false-cb20cadb-3a12-4731-9023-e88bb77d09ea" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:48:16.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2422" for this suite.

• [SLOW TEST:8.194 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":321,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:48:16.438: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 09:48:16.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42" in namespace "downward-api-8966" to be "Succeeded or Failed"
May 27 09:48:16.512: INFO: Pod "downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42": Phase="Pending", Reason="", readiness=false. Elapsed: 7.526494ms
May 27 09:48:18.530: INFO: Pod "downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02610655s
May 27 09:48:20.544: INFO: Pod "downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040010953s
May 27 09:48:22.559: INFO: Pod "downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054617418s
STEP: Saw pod success
May 27 09:48:22.559: INFO: Pod "downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42" satisfied condition "Succeeded or Failed"
May 27 09:48:22.564: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42 container client-container: <nil>
STEP: delete the pod
May 27 09:48:22.595: INFO: Waiting for pod downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42 to disappear
May 27 09:48:22.601: INFO: Pod downwardapi-volume-b5ccc51d-708c-4bdb-8887-611bdc959d42 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:48:22.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8966" for this suite.

• [SLOW TEST:6.181 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":23,"skipped":341,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:48:22.619: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-562f
STEP: Creating a pod to test atomic-volume-subpath
May 27 09:48:22.696: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-562f" in namespace "subpath-9601" to be "Succeeded or Failed"
May 27 09:48:22.705: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.524173ms
May 27 09:48:24.727: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 2.030875858s
May 27 09:48:26.740: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 4.043914796s
May 27 09:48:28.761: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 6.064921072s
May 27 09:48:30.769: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 8.073414635s
May 27 09:48:32.781: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 10.085704011s
May 27 09:48:34.819: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 12.123295267s
May 27 09:48:36.832: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 14.136605807s
May 27 09:48:38.853: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 16.157806045s
May 27 09:48:40.865: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 18.16903565s
May 27 09:48:42.879: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=true. Elapsed: 20.1835655s
May 27 09:48:44.898: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Running", Reason="", readiness=false. Elapsed: 22.202531758s
May 27 09:48:46.918: INFO: Pod "pod-subpath-test-downwardapi-562f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.222286034s
STEP: Saw pod success
May 27 09:48:46.918: INFO: Pod "pod-subpath-test-downwardapi-562f" satisfied condition "Succeeded or Failed"
May 27 09:48:46.923: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-subpath-test-downwardapi-562f container test-container-subpath-downwardapi-562f: <nil>
STEP: delete the pod
May 27 09:48:46.961: INFO: Waiting for pod pod-subpath-test-downwardapi-562f to disappear
May 27 09:48:46.967: INFO: Pod pod-subpath-test-downwardapi-562f no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-562f
May 27 09:48:46.967: INFO: Deleting pod "pod-subpath-test-downwardapi-562f" in namespace "subpath-9601"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:48:46.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9601" for this suite.

• [SLOW TEST:24.373 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":24,"skipped":349,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:48:46.993: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e
May 27 09:48:47.069: INFO: Pod name my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e: Found 0 pods out of 1
May 27 09:48:52.085: INFO: Pod name my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e: Found 1 pods out of 1
May 27 09:48:52.085: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e" are running
May 27 09:48:52.093: INFO: Pod "my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e-s2xg9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 09:48:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 09:48:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 09:48:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 09:48:47 +0000 UTC Reason: Message:}])
May 27 09:48:52.093: INFO: Trying to dial the pod
May 27 09:48:57.134: INFO: Controller my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e: Got expected result from replica 1 [my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e-s2xg9]: "my-hostname-basic-73721866-1f3f-4305-ace9-a2fa403d278e-s2xg9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:48:57.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6440" for this suite.

• [SLOW TEST:10.163 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":25,"skipped":358,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:48:57.157: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
May 27 09:48:57.247: INFO: Waiting up to 5m0s for pod "client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22" in namespace "containers-516" to be "Succeeded or Failed"
May 27 09:48:57.263: INFO: Pod "client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22": Phase="Pending", Reason="", readiness=false. Elapsed: 15.661881ms
May 27 09:48:59.281: INFO: Pod "client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033990315s
May 27 09:49:01.304: INFO: Pod "client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05707547s
STEP: Saw pod success
May 27 09:49:01.304: INFO: Pod "client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22" satisfied condition "Succeeded or Failed"
May 27 09:49:01.309: INFO: Trying to get logs from node ohp4eith3vui-3 pod client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22 container agnhost-container: <nil>
STEP: delete the pod
May 27 09:49:01.338: INFO: Waiting for pod client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22 to disappear
May 27 09:49:01.345: INFO: Pod client-containers-f4e708b6-1e53-4bb0-9c96-23e6eb99ce22 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:49:01.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-516" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":26,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:49:01.369: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7174, will wait for the garbage collector to delete the pods
May 27 09:49:05.504: INFO: Deleting Job.batch foo took: 10.760232ms
May 27 09:49:05.604: INFO: Terminating Job.batch foo pods took: 100.632563ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:49:37.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7174" for this suite.

• [SLOW TEST:35.687 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":27,"skipped":392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:49:37.063: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 09:49:37.149: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf" in namespace "projected-796" to be "Succeeded or Failed"
May 27 09:49:37.159: INFO: Pod "downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.415797ms
May 27 09:49:39.174: INFO: Pod "downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025055221s
May 27 09:49:41.188: INFO: Pod "downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039106779s
STEP: Saw pod success
May 27 09:49:41.189: INFO: Pod "downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf" satisfied condition "Succeeded or Failed"
May 27 09:49:41.194: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf container client-container: <nil>
STEP: delete the pod
May 27 09:49:41.230: INFO: Waiting for pod downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf to disappear
May 27 09:49:41.235: INFO: Pod downwardapi-volume-7f325d26-f09c-44b0-b593-53e069ef8ecf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:49:41.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-796" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:49:41.256: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:49:41.301: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6b312e77-faf3-4274-9371-140049dcad5c" in namespace "security-context-test-5701" to be "Succeeded or Failed"
May 27 09:49:41.308: INFO: Pod "busybox-user-65534-6b312e77-faf3-4274-9371-140049dcad5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.217441ms
May 27 09:49:43.320: INFO: Pod "busybox-user-65534-6b312e77-faf3-4274-9371-140049dcad5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01830896s
May 27 09:49:45.339: INFO: Pod "busybox-user-65534-6b312e77-faf3-4274-9371-140049dcad5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038072289s
May 27 09:49:45.340: INFO: Pod "busybox-user-65534-6b312e77-faf3-4274-9371-140049dcad5c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:49:45.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5701" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":448,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:49:45.367: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-6cf6f19c-2782-4b32-8ba5-a3e6bc7a4455
STEP: Creating a pod to test consume configMaps
May 27 09:49:45.431: INFO: Waiting up to 5m0s for pod "pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07" in namespace "configmap-623" to be "Succeeded or Failed"
May 27 09:49:45.438: INFO: Pod "pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07": Phase="Pending", Reason="", readiness=false. Elapsed: 6.638555ms
May 27 09:49:47.455: INFO: Pod "pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023281579s
May 27 09:49:49.475: INFO: Pod "pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043200459s
STEP: Saw pod success
May 27 09:49:49.475: INFO: Pod "pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07" satisfied condition "Succeeded or Failed"
May 27 09:49:49.482: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07 container agnhost-container: <nil>
STEP: delete the pod
May 27 09:49:49.518: INFO: Waiting for pod pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07 to disappear
May 27 09:49:49.523: INFO: Pod pod-configmaps-b851ac81-7269-4eff-8d44-9c39420b7e07 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:49:49.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-623" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:49:49.549: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:49:49.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2926" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":31,"skipped":542,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:49:49.682: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:49:49.746: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7556
I0527 09:49:49.767212      15 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7556, replica count: 1
I0527 09:49:50.818005      15 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0527 09:49:51.818468      15 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 09:49:51.944: INFO: Created: latency-svc-79qwz
May 27 09:49:51.972: INFO: Got endpoints: latency-svc-79qwz [53.153811ms]
May 27 09:49:52.010: INFO: Created: latency-svc-wl94t
May 27 09:49:52.025: INFO: Got endpoints: latency-svc-wl94t [50.367172ms]
May 27 09:49:52.026: INFO: Created: latency-svc-ltd9d
May 27 09:49:52.035: INFO: Created: latency-svc-qx7xt
May 27 09:49:52.046: INFO: Got endpoints: latency-svc-ltd9d [73.185738ms]
May 27 09:49:52.049: INFO: Got endpoints: latency-svc-qx7xt [74.855768ms]
May 27 09:49:52.061: INFO: Created: latency-svc-6bwsf
May 27 09:49:52.084: INFO: Created: latency-svc-prqts
May 27 09:49:52.086: INFO: Got endpoints: latency-svc-6bwsf [111.495739ms]
May 27 09:49:52.104: INFO: Created: latency-svc-pd46p
May 27 09:49:52.108: INFO: Got endpoints: latency-svc-prqts [133.506036ms]
May 27 09:49:52.122: INFO: Got endpoints: latency-svc-pd46p [146.610771ms]
May 27 09:49:52.130: INFO: Created: latency-svc-rdpxn
May 27 09:49:52.148: INFO: Got endpoints: latency-svc-rdpxn [173.897819ms]
May 27 09:49:52.291: INFO: Created: latency-svc-75hhb
May 27 09:49:52.299: INFO: Created: latency-svc-qx4pc
May 27 09:49:52.299: INFO: Created: latency-svc-6dcf9
May 27 09:49:52.301: INFO: Created: latency-svc-mgwd8
May 27 09:49:52.301: INFO: Created: latency-svc-wq857
May 27 09:49:52.301: INFO: Created: latency-svc-ztbfr
May 27 09:49:52.302: INFO: Created: latency-svc-c8zdn
May 27 09:49:52.302: INFO: Created: latency-svc-hsnsb
May 27 09:49:52.303: INFO: Created: latency-svc-wbvd7
May 27 09:49:52.303: INFO: Created: latency-svc-sx98x
May 27 09:49:52.303: INFO: Created: latency-svc-qqqsr
May 27 09:49:52.304: INFO: Created: latency-svc-76w28
May 27 09:49:52.308: INFO: Created: latency-svc-972rn
May 27 09:49:52.308: INFO: Created: latency-svc-xbtfj
May 27 09:49:52.312: INFO: Got endpoints: latency-svc-75hhb [338.566289ms]
May 27 09:49:52.319: INFO: Created: latency-svc-4fxmb
May 27 09:49:52.333: INFO: Got endpoints: latency-svc-76w28 [225.444105ms]
May 27 09:49:52.359: INFO: Got endpoints: latency-svc-qqqsr [383.860465ms]
May 27 09:49:52.359: INFO: Got endpoints: latency-svc-972rn [385.00013ms]
May 27 09:49:52.371: INFO: Got endpoints: latency-svc-4fxmb [397.809535ms]
May 27 09:49:52.392: INFO: Got endpoints: latency-svc-wq857 [419.605884ms]
May 27 09:49:52.393: INFO: Got endpoints: latency-svc-c8zdn [270.835797ms]
May 27 09:49:52.393: INFO: Created: latency-svc-rlz8r
May 27 09:49:52.424: INFO: Got endpoints: latency-svc-qx4pc [375.236531ms]
May 27 09:49:52.453: INFO: Created: latency-svc-n24sj
May 27 09:49:52.454: INFO: Got endpoints: latency-svc-6dcf9 [480.553794ms]
May 27 09:49:52.455: INFO: Got endpoints: latency-svc-ztbfr [368.636641ms]
May 27 09:49:52.456: INFO: Got endpoints: latency-svc-sx98x [482.756782ms]
May 27 09:49:52.456: INFO: Got endpoints: latency-svc-hsnsb [308.197143ms]
May 27 09:49:52.476: INFO: Got endpoints: latency-svc-wbvd7 [450.650737ms]
May 27 09:49:52.482: INFO: Created: latency-svc-zmxm9
May 27 09:49:52.523: INFO: Got endpoints: latency-svc-mgwd8 [547.626311ms]
May 27 09:49:52.523: INFO: Got endpoints: latency-svc-rlz8r [211.016487ms]
May 27 09:49:52.523: INFO: Got endpoints: latency-svc-n24sj [189.882342ms]
May 27 09:49:52.525: INFO: Got endpoints: latency-svc-zmxm9 [165.520856ms]
May 27 09:49:52.525: INFO: Got endpoints: latency-svc-xbtfj [478.491544ms]
May 27 09:49:52.541: INFO: Created: latency-svc-n6c5f
May 27 09:49:52.542: INFO: Got endpoints: latency-svc-n6c5f [182.859173ms]
May 27 09:49:52.543: INFO: Created: latency-svc-sdkc6
May 27 09:49:52.555: INFO: Created: latency-svc-qdc9p
May 27 09:49:52.561: INFO: Got endpoints: latency-svc-sdkc6 [189.465564ms]
May 27 09:49:52.579: INFO: Got endpoints: latency-svc-qdc9p [186.533ms]
May 27 09:49:52.593: INFO: Created: latency-svc-z9t8t
May 27 09:49:52.613: INFO: Created: latency-svc-46r9r
May 27 09:49:52.623: INFO: Created: latency-svc-q2lvt
May 27 09:49:52.642: INFO: Created: latency-svc-4r2zn
May 27 09:49:52.669: INFO: Created: latency-svc-9wgc5
May 27 09:49:52.671: INFO: Got endpoints: latency-svc-46r9r [246.064909ms]
May 27 09:49:52.671: INFO: Got endpoints: latency-svc-q2lvt [216.71632ms]
May 27 09:49:52.671: INFO: Got endpoints: latency-svc-z9t8t [278.415203ms]
May 27 09:49:52.680: INFO: Got endpoints: latency-svc-4r2zn [223.765452ms]
May 27 09:49:52.705: INFO: Created: latency-svc-xckxr
May 27 09:49:52.705: INFO: Got endpoints: latency-svc-9wgc5 [250.409879ms]
May 27 09:49:52.714: INFO: Created: latency-svc-7mk28
May 27 09:49:52.721: INFO: Got endpoints: latency-svc-xckxr [264.184321ms]
May 27 09:49:52.737: INFO: Got endpoints: latency-svc-7mk28 [261.41364ms]
May 27 09:49:52.745: INFO: Created: latency-svc-hqtmf
May 27 09:49:52.763: INFO: Got endpoints: latency-svc-hqtmf [240.433255ms]
May 27 09:49:52.765: INFO: Created: latency-svc-bn962
May 27 09:49:52.833: INFO: Created: latency-svc-7v8nk
May 27 09:49:52.845: INFO: Got endpoints: latency-svc-bn962 [320.186171ms]
May 27 09:49:52.873: INFO: Created: latency-svc-4l5b5
May 27 09:49:52.876: INFO: Got endpoints: latency-svc-7v8nk [352.462777ms]
May 27 09:49:52.909: INFO: Created: latency-svc-rb654
May 27 09:49:52.909: INFO: Got endpoints: latency-svc-4l5b5 [385.098275ms]
May 27 09:49:52.944: INFO: Got endpoints: latency-svc-rb654 [418.940167ms]
May 27 09:49:53.094: INFO: Created: latency-svc-rhljq
May 27 09:49:53.095: INFO: Created: latency-svc-d4t26
May 27 09:49:53.095: INFO: Created: latency-svc-xs6c4
May 27 09:49:53.095: INFO: Created: latency-svc-gzqw2
May 27 09:49:53.095: INFO: Created: latency-svc-52c6j
May 27 09:49:53.096: INFO: Created: latency-svc-wfr26
May 27 09:49:53.109: INFO: Created: latency-svc-tmhhm
May 27 09:49:53.110: INFO: Created: latency-svc-9vpft
May 27 09:49:53.110: INFO: Created: latency-svc-pzkbt
May 27 09:49:53.113: INFO: Created: latency-svc-vwsxg
May 27 09:49:53.119: INFO: Got endpoints: latency-svc-52c6j [175.914821ms]
May 27 09:49:53.122: INFO: Created: latency-svc-qn5rd
May 27 09:49:53.122: INFO: Created: latency-svc-r55ch
May 27 09:49:53.123: INFO: Created: latency-svc-kwpnf
May 27 09:49:53.123: INFO: Created: latency-svc-nb9nx
May 27 09:49:53.123: INFO: Created: latency-svc-mmqzz
May 27 09:49:53.147: INFO: Got endpoints: latency-svc-xs6c4 [568.125775ms]
May 27 09:49:53.169: INFO: Got endpoints: latency-svc-gzqw2 [627.30972ms]
May 27 09:49:53.171: INFO: Got endpoints: latency-svc-qn5rd [610.309525ms]
May 27 09:49:53.188: INFO: Created: latency-svc-xt6l5
May 27 09:49:53.194: INFO: Got endpoints: latency-svc-pzkbt [488.760742ms]
May 27 09:49:53.194: INFO: Got endpoints: latency-svc-r55ch [318.424489ms]
May 27 09:49:53.196: INFO: Got endpoints: latency-svc-rhljq [524.837281ms]
May 27 09:49:53.209: INFO: Created: latency-svc-xqftt
May 27 09:49:53.234: INFO: Got endpoints: latency-svc-d4t26 [388.668216ms]
May 27 09:49:53.235: INFO: Got endpoints: latency-svc-tmhhm [563.551938ms]
May 27 09:49:53.235: INFO: Got endpoints: latency-svc-wfr26 [514.233937ms]
May 27 09:49:53.236: INFO: Got endpoints: latency-svc-9vpft [471.909587ms]
May 27 09:49:53.241: INFO: Got endpoints: latency-svc-kwpnf [569.983471ms]
May 27 09:49:53.258: INFO: Got endpoints: latency-svc-nb9nx [577.841825ms]
May 27 09:49:53.258: INFO: Got endpoints: latency-svc-mmqzz [520.358376ms]
May 27 09:49:53.277: INFO: Got endpoints: latency-svc-vwsxg [367.658232ms]
May 27 09:49:53.325: INFO: Got endpoints: latency-svc-xt6l5 [205.645847ms]
May 27 09:49:53.366: INFO: Got endpoints: latency-svc-xqftt [219.336518ms]
May 27 09:49:53.403: INFO: Created: latency-svc-sq5c2
May 27 09:49:53.425: INFO: Got endpoints: latency-svc-sq5c2 [230.295393ms]
May 27 09:49:53.440: INFO: Created: latency-svc-dc4ch
May 27 09:49:53.441: INFO: Created: latency-svc-2bvb9
May 27 09:49:53.441: INFO: Created: latency-svc-khhwn
May 27 09:49:53.441: INFO: Created: latency-svc-csfv4
May 27 09:49:53.443: INFO: Created: latency-svc-c8gjk
May 27 09:49:53.456: INFO: Created: latency-svc-4npgg
May 27 09:49:53.456: INFO: Created: latency-svc-fkgjp
May 27 09:49:53.457: INFO: Created: latency-svc-ps6q6
May 27 09:49:53.463: INFO: Created: latency-svc-twpn9
May 27 09:49:53.463: INFO: Created: latency-svc-x6n29
May 27 09:49:53.464: INFO: Created: latency-svc-mkbfl
May 27 09:49:53.465: INFO: Created: latency-svc-k9k6d
May 27 09:49:53.466: INFO: Created: latency-svc-rptnf
May 27 09:49:53.466: INFO: Created: latency-svc-578mp
May 27 09:49:53.468: INFO: Created: latency-svc-lfkz5
May 27 09:49:53.472: INFO: Got endpoints: latency-svc-twpn9 [301.654869ms]
May 27 09:49:53.490: INFO: Created: latency-svc-8rllz
May 27 09:49:53.516: INFO: Got endpoints: latency-svc-ps6q6 [321.40944ms]
May 27 09:49:53.539: INFO: Created: latency-svc-pj65j
May 27 09:49:53.566: INFO: Got endpoints: latency-svc-x6n29 [289.002504ms]
May 27 09:49:53.581: INFO: Created: latency-svc-7n259
May 27 09:49:53.621: INFO: Got endpoints: latency-svc-c8gjk [386.658417ms]
May 27 09:49:53.636: INFO: Created: latency-svc-w49g7
May 27 09:49:53.670: INFO: Got endpoints: latency-svc-rptnf [428.899137ms]
May 27 09:49:53.688: INFO: Created: latency-svc-t7hqv
May 27 09:49:53.720: INFO: Got endpoints: latency-svc-k9k6d [548.78664ms]
May 27 09:49:53.735: INFO: Created: latency-svc-cnfqz
May 27 09:49:53.767: INFO: Got endpoints: latency-svc-mkbfl [400.008266ms]
May 27 09:49:53.783: INFO: Created: latency-svc-wdvw9
May 27 09:49:53.818: INFO: Got endpoints: latency-svc-578mp [582.317837ms]
May 27 09:49:53.835: INFO: Created: latency-svc-59n8n
May 27 09:49:53.870: INFO: Got endpoints: latency-svc-2bvb9 [612.050897ms]
May 27 09:49:53.889: INFO: Created: latency-svc-82hpx
May 27 09:49:53.920: INFO: Got endpoints: latency-svc-csfv4 [662.226887ms]
May 27 09:49:53.941: INFO: Created: latency-svc-jbv8d
May 27 09:49:53.965: INFO: Got endpoints: latency-svc-khhwn [768.981851ms]
May 27 09:49:53.990: INFO: Created: latency-svc-2f6l6
May 27 09:49:54.018: INFO: Got endpoints: latency-svc-dc4ch [692.332403ms]
May 27 09:49:54.053: INFO: Created: latency-svc-dktcd
May 27 09:49:54.068: INFO: Got endpoints: latency-svc-fkgjp [833.168855ms]
May 27 09:49:54.090: INFO: Created: latency-svc-hnkg6
May 27 09:49:54.115: INFO: Got endpoints: latency-svc-4npgg [879.313518ms]
May 27 09:49:54.148: INFO: Created: latency-svc-cfm7r
May 27 09:49:54.175: INFO: Got endpoints: latency-svc-lfkz5 [750.068298ms]
May 27 09:49:54.205: INFO: Created: latency-svc-fbrdn
May 27 09:49:54.216: INFO: Got endpoints: latency-svc-8rllz [744.192782ms]
May 27 09:49:54.241: INFO: Created: latency-svc-8bkrh
May 27 09:49:54.269: INFO: Got endpoints: latency-svc-pj65j [752.891934ms]
May 27 09:49:54.297: INFO: Created: latency-svc-ztnms
May 27 09:49:54.316: INFO: Got endpoints: latency-svc-7n259 [750.23124ms]
May 27 09:49:54.341: INFO: Created: latency-svc-q2dgt
May 27 09:49:54.367: INFO: Got endpoints: latency-svc-w49g7 [746.298417ms]
May 27 09:49:54.385: INFO: Created: latency-svc-2v9xs
May 27 09:49:54.420: INFO: Got endpoints: latency-svc-t7hqv [749.8101ms]
May 27 09:49:54.436: INFO: Created: latency-svc-p9pc9
May 27 09:49:54.470: INFO: Got endpoints: latency-svc-cnfqz [749.634394ms]
May 27 09:49:54.498: INFO: Created: latency-svc-skk84
May 27 09:49:54.518: INFO: Got endpoints: latency-svc-wdvw9 [750.766949ms]
May 27 09:49:54.545: INFO: Created: latency-svc-dxht5
May 27 09:49:54.565: INFO: Got endpoints: latency-svc-59n8n [746.526784ms]
May 27 09:49:54.599: INFO: Created: latency-svc-6vvkv
May 27 09:49:54.616: INFO: Got endpoints: latency-svc-82hpx [745.700934ms]
May 27 09:49:54.634: INFO: Created: latency-svc-zwwkb
May 27 09:49:54.668: INFO: Got endpoints: latency-svc-jbv8d [747.784054ms]
May 27 09:49:54.689: INFO: Created: latency-svc-55pp6
May 27 09:49:54.718: INFO: Got endpoints: latency-svc-2f6l6 [752.446484ms]
May 27 09:49:54.739: INFO: Created: latency-svc-94ldj
May 27 09:49:54.776: INFO: Got endpoints: latency-svc-dktcd [758.705793ms]
May 27 09:49:54.812: INFO: Created: latency-svc-h95d7
May 27 09:49:54.820: INFO: Got endpoints: latency-svc-hnkg6 [751.74173ms]
May 27 09:49:54.846: INFO: Created: latency-svc-fskvg
May 27 09:49:54.867: INFO: Got endpoints: latency-svc-cfm7r [752.429223ms]
May 27 09:49:54.891: INFO: Created: latency-svc-srq8s
May 27 09:49:54.923: INFO: Got endpoints: latency-svc-fbrdn [748.122449ms]
May 27 09:49:54.951: INFO: Created: latency-svc-6qfpx
May 27 09:49:54.976: INFO: Got endpoints: latency-svc-8bkrh [759.669962ms]
May 27 09:49:54.998: INFO: Created: latency-svc-k4csr
May 27 09:49:55.029: INFO: Got endpoints: latency-svc-ztnms [760.391113ms]
May 27 09:49:55.064: INFO: Created: latency-svc-m6xkx
May 27 09:49:55.075: INFO: Got endpoints: latency-svc-q2dgt [758.57646ms]
May 27 09:49:55.095: INFO: Created: latency-svc-9hdpf
May 27 09:49:55.124: INFO: Got endpoints: latency-svc-2v9xs [756.984717ms]
May 27 09:49:55.148: INFO: Created: latency-svc-ttqdx
May 27 09:49:55.167: INFO: Got endpoints: latency-svc-p9pc9 [747.376187ms]
May 27 09:49:55.200: INFO: Created: latency-svc-89hcf
May 27 09:49:55.221: INFO: Got endpoints: latency-svc-skk84 [751.254926ms]
May 27 09:49:55.257: INFO: Created: latency-svc-pcktc
May 27 09:49:55.276: INFO: Got endpoints: latency-svc-dxht5 [758.128886ms]
May 27 09:49:55.328: INFO: Created: latency-svc-x9qlt
May 27 09:49:55.329: INFO: Got endpoints: latency-svc-6vvkv [764.196846ms]
May 27 09:49:55.349: INFO: Created: latency-svc-vm7gm
May 27 09:49:55.364: INFO: Got endpoints: latency-svc-zwwkb [747.826583ms]
May 27 09:49:55.385: INFO: Created: latency-svc-vbfbv
May 27 09:49:55.420: INFO: Got endpoints: latency-svc-55pp6 [751.726133ms]
May 27 09:49:55.472: INFO: Created: latency-svc-clb7d
May 27 09:49:55.473: INFO: Got endpoints: latency-svc-94ldj [755.13363ms]
May 27 09:49:55.507: INFO: Created: latency-svc-jj87b
May 27 09:49:55.515: INFO: Got endpoints: latency-svc-h95d7 [738.790358ms]
May 27 09:49:55.533: INFO: Created: latency-svc-49p26
May 27 09:49:55.569: INFO: Got endpoints: latency-svc-fskvg [748.752892ms]
May 27 09:49:55.586: INFO: Created: latency-svc-xnpx4
May 27 09:49:55.617: INFO: Got endpoints: latency-svc-srq8s [749.243637ms]
May 27 09:49:55.640: INFO: Created: latency-svc-xfccs
May 27 09:49:55.671: INFO: Got endpoints: latency-svc-6qfpx [747.699105ms]
May 27 09:49:55.688: INFO: Created: latency-svc-md4k9
May 27 09:49:55.717: INFO: Got endpoints: latency-svc-k4csr [740.887799ms]
May 27 09:49:55.778: INFO: Created: latency-svc-2p7q8
May 27 09:49:55.793: INFO: Got endpoints: latency-svc-m6xkx [763.680034ms]
May 27 09:49:55.819: INFO: Created: latency-svc-v2cpd
May 27 09:49:55.839: INFO: Got endpoints: latency-svc-9hdpf [764.22838ms]
May 27 09:49:55.857: INFO: Created: latency-svc-kthr2
May 27 09:49:55.866: INFO: Got endpoints: latency-svc-ttqdx [741.12443ms]
May 27 09:49:55.885: INFO: Created: latency-svc-lmgrq
May 27 09:49:55.923: INFO: Got endpoints: latency-svc-89hcf [755.139439ms]
May 27 09:49:55.958: INFO: Created: latency-svc-jkxb6
May 27 09:49:55.977: INFO: Got endpoints: latency-svc-pcktc [755.204749ms]
May 27 09:49:55.999: INFO: Created: latency-svc-77t6v
May 27 09:49:56.017: INFO: Got endpoints: latency-svc-x9qlt [740.643153ms]
May 27 09:49:56.043: INFO: Created: latency-svc-bhwcr
May 27 09:49:56.067: INFO: Got endpoints: latency-svc-vm7gm [738.008736ms]
May 27 09:49:56.095: INFO: Created: latency-svc-svhvt
May 27 09:49:56.128: INFO: Got endpoints: latency-svc-vbfbv [763.799448ms]
May 27 09:49:56.167: INFO: Created: latency-svc-nhxwz
May 27 09:49:56.172: INFO: Got endpoints: latency-svc-clb7d [752.266175ms]
May 27 09:49:56.193: INFO: Created: latency-svc-wpk67
May 27 09:49:56.222: INFO: Got endpoints: latency-svc-jj87b [748.413081ms]
May 27 09:49:56.243: INFO: Created: latency-svc-pxln8
May 27 09:49:56.274: INFO: Got endpoints: latency-svc-49p26 [758.014377ms]
May 27 09:49:56.292: INFO: Created: latency-svc-tqzsp
May 27 09:49:56.318: INFO: Got endpoints: latency-svc-xnpx4 [748.797175ms]
May 27 09:49:56.343: INFO: Created: latency-svc-dkw8k
May 27 09:49:56.369: INFO: Got endpoints: latency-svc-xfccs [752.012942ms]
May 27 09:49:56.385: INFO: Created: latency-svc-r9r4p
May 27 09:49:56.415: INFO: Got endpoints: latency-svc-md4k9 [743.677441ms]
May 27 09:49:56.432: INFO: Created: latency-svc-cpxrm
May 27 09:49:56.477: INFO: Got endpoints: latency-svc-2p7q8 [759.62365ms]
May 27 09:49:56.504: INFO: Created: latency-svc-srs9j
May 27 09:49:56.515: INFO: Got endpoints: latency-svc-v2cpd [721.414974ms]
May 27 09:49:56.534: INFO: Created: latency-svc-8tpbd
May 27 09:49:56.564: INFO: Got endpoints: latency-svc-kthr2 [724.022018ms]
May 27 09:49:56.582: INFO: Created: latency-svc-78scz
May 27 09:49:56.617: INFO: Got endpoints: latency-svc-lmgrq [750.899447ms]
May 27 09:49:56.632: INFO: Created: latency-svc-dhrmb
May 27 09:49:56.665: INFO: Got endpoints: latency-svc-jkxb6 [742.109284ms]
May 27 09:49:56.696: INFO: Created: latency-svc-hhrx6
May 27 09:49:56.719: INFO: Got endpoints: latency-svc-77t6v [742.137343ms]
May 27 09:49:56.733: INFO: Created: latency-svc-wbp9g
May 27 09:49:56.768: INFO: Got endpoints: latency-svc-bhwcr [751.140743ms]
May 27 09:49:56.784: INFO: Created: latency-svc-6hr4j
May 27 09:49:56.820: INFO: Got endpoints: latency-svc-svhvt [752.347666ms]
May 27 09:49:56.836: INFO: Created: latency-svc-v2w2w
May 27 09:49:56.865: INFO: Got endpoints: latency-svc-nhxwz [737.737133ms]
May 27 09:49:56.881: INFO: Created: latency-svc-77lcr
May 27 09:49:56.919: INFO: Got endpoints: latency-svc-wpk67 [746.436168ms]
May 27 09:49:56.933: INFO: Created: latency-svc-62772
May 27 09:49:56.981: INFO: Got endpoints: latency-svc-pxln8 [758.946809ms]
May 27 09:49:57.008: INFO: Created: latency-svc-jq55l
May 27 09:49:57.015: INFO: Got endpoints: latency-svc-tqzsp [741.361224ms]
May 27 09:49:57.034: INFO: Created: latency-svc-t4q48
May 27 09:49:57.070: INFO: Got endpoints: latency-svc-dkw8k [752.311823ms]
May 27 09:49:57.095: INFO: Created: latency-svc-dbs94
May 27 09:49:57.117: INFO: Got endpoints: latency-svc-r9r4p [748.443979ms]
May 27 09:49:57.156: INFO: Created: latency-svc-nxblm
May 27 09:49:57.175: INFO: Got endpoints: latency-svc-cpxrm [759.977507ms]
May 27 09:49:57.195: INFO: Created: latency-svc-5gpvm
May 27 09:49:57.217: INFO: Got endpoints: latency-svc-srs9j [740.478552ms]
May 27 09:49:57.241: INFO: Created: latency-svc-8tg6k
May 27 09:49:57.267: INFO: Got endpoints: latency-svc-8tpbd [751.56969ms]
May 27 09:49:57.286: INFO: Created: latency-svc-fl7j7
May 27 09:49:57.318: INFO: Got endpoints: latency-svc-78scz [754.593733ms]
May 27 09:49:57.337: INFO: Created: latency-svc-dhx7w
May 27 09:49:57.367: INFO: Got endpoints: latency-svc-dhrmb [750.609287ms]
May 27 09:49:57.412: INFO: Created: latency-svc-c6gmd
May 27 09:49:57.427: INFO: Got endpoints: latency-svc-hhrx6 [761.305993ms]
May 27 09:49:57.449: INFO: Created: latency-svc-swhcn
May 27 09:49:57.468: INFO: Got endpoints: latency-svc-wbp9g [749.122657ms]
May 27 09:49:57.499: INFO: Created: latency-svc-ln8lk
May 27 09:49:57.524: INFO: Got endpoints: latency-svc-6hr4j [756.091053ms]
May 27 09:49:57.547: INFO: Created: latency-svc-fsz84
May 27 09:49:57.570: INFO: Got endpoints: latency-svc-v2w2w [750.296193ms]
May 27 09:49:57.597: INFO: Created: latency-svc-gt94w
May 27 09:49:57.617: INFO: Got endpoints: latency-svc-77lcr [751.406421ms]
May 27 09:49:57.641: INFO: Created: latency-svc-vgphh
May 27 09:49:57.666: INFO: Got endpoints: latency-svc-62772 [747.304107ms]
May 27 09:49:57.697: INFO: Created: latency-svc-tx869
May 27 09:49:57.718: INFO: Got endpoints: latency-svc-jq55l [737.215234ms]
May 27 09:49:57.747: INFO: Created: latency-svc-l9vd5
May 27 09:49:57.769: INFO: Got endpoints: latency-svc-t4q48 [753.543129ms]
May 27 09:49:57.790: INFO: Created: latency-svc-cgw72
May 27 09:49:57.817: INFO: Got endpoints: latency-svc-dbs94 [746.233429ms]
May 27 09:49:57.838: INFO: Created: latency-svc-wlzqp
May 27 09:49:57.868: INFO: Got endpoints: latency-svc-nxblm [750.659579ms]
May 27 09:49:57.890: INFO: Created: latency-svc-p7jkm
May 27 09:49:57.920: INFO: Got endpoints: latency-svc-5gpvm [744.721595ms]
May 27 09:49:57.958: INFO: Created: latency-svc-4b6f8
May 27 09:49:57.969: INFO: Got endpoints: latency-svc-8tg6k [751.621693ms]
May 27 09:49:57.996: INFO: Created: latency-svc-c87r2
May 27 09:49:58.016: INFO: Got endpoints: latency-svc-fl7j7 [749.590298ms]
May 27 09:49:58.037: INFO: Created: latency-svc-mq8zh
May 27 09:49:58.068: INFO: Got endpoints: latency-svc-dhx7w [749.472308ms]
May 27 09:49:58.087: INFO: Created: latency-svc-v7t27
May 27 09:49:58.118: INFO: Got endpoints: latency-svc-c6gmd [750.646579ms]
May 27 09:49:58.136: INFO: Created: latency-svc-9sglb
May 27 09:49:58.169: INFO: Got endpoints: latency-svc-swhcn [742.397934ms]
May 27 09:49:58.184: INFO: Created: latency-svc-fbqzr
May 27 09:49:58.222: INFO: Got endpoints: latency-svc-ln8lk [754.260848ms]
May 27 09:49:58.244: INFO: Created: latency-svc-f9gqv
May 27 09:49:58.269: INFO: Got endpoints: latency-svc-fsz84 [745.100426ms]
May 27 09:49:58.288: INFO: Created: latency-svc-g7d6m
May 27 09:49:58.322: INFO: Got endpoints: latency-svc-gt94w [751.201251ms]
May 27 09:49:58.336: INFO: Created: latency-svc-hw742
May 27 09:49:58.366: INFO: Got endpoints: latency-svc-vgphh [748.911875ms]
May 27 09:49:58.386: INFO: Created: latency-svc-74pql
May 27 09:49:58.416: INFO: Got endpoints: latency-svc-tx869 [750.077284ms]
May 27 09:49:58.434: INFO: Created: latency-svc-dgnb2
May 27 09:49:58.466: INFO: Got endpoints: latency-svc-l9vd5 [747.62078ms]
May 27 09:49:58.484: INFO: Created: latency-svc-xqn59
May 27 09:49:58.516: INFO: Got endpoints: latency-svc-cgw72 [746.856932ms]
May 27 09:49:58.536: INFO: Created: latency-svc-sv6lp
May 27 09:49:58.566: INFO: Got endpoints: latency-svc-wlzqp [748.92352ms]
May 27 09:49:58.581: INFO: Created: latency-svc-qxsh4
May 27 09:49:58.623: INFO: Got endpoints: latency-svc-p7jkm [755.179294ms]
May 27 09:49:58.645: INFO: Created: latency-svc-pln4j
May 27 09:49:58.674: INFO: Got endpoints: latency-svc-4b6f8 [753.676295ms]
May 27 09:49:58.699: INFO: Created: latency-svc-tq5hm
May 27 09:49:58.723: INFO: Got endpoints: latency-svc-c87r2 [753.413608ms]
May 27 09:49:58.738: INFO: Created: latency-svc-q2kdh
May 27 09:49:58.784: INFO: Got endpoints: latency-svc-mq8zh [767.672467ms]
May 27 09:49:58.799: INFO: Created: latency-svc-2rn5l
May 27 09:49:58.816: INFO: Got endpoints: latency-svc-v7t27 [746.937228ms]
May 27 09:49:58.836: INFO: Created: latency-svc-849th
May 27 09:49:58.865: INFO: Got endpoints: latency-svc-9sglb [746.1089ms]
May 27 09:49:58.890: INFO: Created: latency-svc-4vxm9
May 27 09:49:58.916: INFO: Got endpoints: latency-svc-fbqzr [746.385793ms]
May 27 09:49:58.946: INFO: Created: latency-svc-9v8kb
May 27 09:49:58.965: INFO: Got endpoints: latency-svc-f9gqv [742.433673ms]
May 27 09:49:58.986: INFO: Created: latency-svc-k4wnf
May 27 09:49:59.020: INFO: Got endpoints: latency-svc-g7d6m [750.023805ms]
May 27 09:49:59.035: INFO: Created: latency-svc-p4nt7
May 27 09:49:59.068: INFO: Got endpoints: latency-svc-hw742 [746.602419ms]
May 27 09:49:59.091: INFO: Created: latency-svc-lhwcl
May 27 09:49:59.119: INFO: Got endpoints: latency-svc-74pql [752.717422ms]
May 27 09:49:59.140: INFO: Created: latency-svc-c9gcs
May 27 09:49:59.168: INFO: Got endpoints: latency-svc-dgnb2 [750.967975ms]
May 27 09:49:59.183: INFO: Created: latency-svc-crzg4
May 27 09:49:59.218: INFO: Got endpoints: latency-svc-xqn59 [751.644993ms]
May 27 09:49:59.235: INFO: Created: latency-svc-fphkh
May 27 09:49:59.272: INFO: Got endpoints: latency-svc-sv6lp [754.926389ms]
May 27 09:49:59.294: INFO: Created: latency-svc-krbs2
May 27 09:49:59.321: INFO: Got endpoints: latency-svc-qxsh4 [755.067647ms]
May 27 09:49:59.343: INFO: Created: latency-svc-g6ltd
May 27 09:49:59.368: INFO: Got endpoints: latency-svc-pln4j [744.210015ms]
May 27 09:49:59.382: INFO: Created: latency-svc-7g7rf
May 27 09:49:59.446: INFO: Got endpoints: latency-svc-tq5hm [772.359679ms]
May 27 09:49:59.467: INFO: Got endpoints: latency-svc-q2kdh [744.522771ms]
May 27 09:49:59.478: INFO: Created: latency-svc-j4mp4
May 27 09:49:59.485: INFO: Created: latency-svc-wmkrp
May 27 09:49:59.520: INFO: Got endpoints: latency-svc-2rn5l [735.900808ms]
May 27 09:49:59.540: INFO: Created: latency-svc-5zjmz
May 27 09:49:59.567: INFO: Got endpoints: latency-svc-849th [751.601441ms]
May 27 09:49:59.585: INFO: Created: latency-svc-44x27
May 27 09:49:59.621: INFO: Got endpoints: latency-svc-4vxm9 [756.312441ms]
May 27 09:49:59.642: INFO: Created: latency-svc-gwqz8
May 27 09:49:59.672: INFO: Got endpoints: latency-svc-9v8kb [755.924588ms]
May 27 09:49:59.687: INFO: Created: latency-svc-cwm76
May 27 09:49:59.715: INFO: Got endpoints: latency-svc-k4wnf [749.169041ms]
May 27 09:49:59.735: INFO: Created: latency-svc-gcsx7
May 27 09:49:59.765: INFO: Got endpoints: latency-svc-p4nt7 [745.613717ms]
May 27 09:49:59.789: INFO: Created: latency-svc-cjxmw
May 27 09:49:59.817: INFO: Got endpoints: latency-svc-lhwcl [748.531314ms]
May 27 09:49:59.867: INFO: Got endpoints: latency-svc-c9gcs [748.321161ms]
May 27 09:49:59.919: INFO: Got endpoints: latency-svc-crzg4 [751.674693ms]
May 27 09:49:59.995: INFO: Got endpoints: latency-svc-fphkh [777.565848ms]
May 27 09:50:00.019: INFO: Got endpoints: latency-svc-krbs2 [746.886572ms]
May 27 09:50:00.066: INFO: Got endpoints: latency-svc-g6ltd [744.830038ms]
May 27 09:50:00.118: INFO: Got endpoints: latency-svc-7g7rf [750.501585ms]
May 27 09:50:00.167: INFO: Got endpoints: latency-svc-j4mp4 [720.52556ms]
May 27 09:50:00.217: INFO: Got endpoints: latency-svc-wmkrp [749.401764ms]
May 27 09:50:00.265: INFO: Got endpoints: latency-svc-5zjmz [744.75664ms]
May 27 09:50:00.323: INFO: Got endpoints: latency-svc-44x27 [755.742705ms]
May 27 09:50:00.364: INFO: Got endpoints: latency-svc-gwqz8 [742.860305ms]
May 27 09:50:00.417: INFO: Got endpoints: latency-svc-cwm76 [745.065833ms]
May 27 09:50:00.465: INFO: Got endpoints: latency-svc-gcsx7 [750.222098ms]
May 27 09:50:00.518: INFO: Got endpoints: latency-svc-cjxmw [752.272478ms]
May 27 09:50:00.518: INFO: Latencies: [50.367172ms 73.185738ms 74.855768ms 111.495739ms 133.506036ms 146.610771ms 165.520856ms 173.897819ms 175.914821ms 182.859173ms 186.533ms 189.465564ms 189.882342ms 205.645847ms 211.016487ms 216.71632ms 219.336518ms 223.765452ms 225.444105ms 230.295393ms 240.433255ms 246.064909ms 250.409879ms 261.41364ms 264.184321ms 270.835797ms 278.415203ms 289.002504ms 301.654869ms 308.197143ms 318.424489ms 320.186171ms 321.40944ms 338.566289ms 352.462777ms 367.658232ms 368.636641ms 375.236531ms 383.860465ms 385.00013ms 385.098275ms 386.658417ms 388.668216ms 397.809535ms 400.008266ms 418.940167ms 419.605884ms 428.899137ms 450.650737ms 471.909587ms 478.491544ms 480.553794ms 482.756782ms 488.760742ms 514.233937ms 520.358376ms 524.837281ms 547.626311ms 548.78664ms 563.551938ms 568.125775ms 569.983471ms 577.841825ms 582.317837ms 610.309525ms 612.050897ms 627.30972ms 662.226887ms 692.332403ms 720.52556ms 721.414974ms 724.022018ms 735.900808ms 737.215234ms 737.737133ms 738.008736ms 738.790358ms 740.478552ms 740.643153ms 740.887799ms 741.12443ms 741.361224ms 742.109284ms 742.137343ms 742.397934ms 742.433673ms 742.860305ms 743.677441ms 744.192782ms 744.210015ms 744.522771ms 744.721595ms 744.75664ms 744.830038ms 745.065833ms 745.100426ms 745.613717ms 745.700934ms 746.1089ms 746.233429ms 746.298417ms 746.385793ms 746.436168ms 746.526784ms 746.602419ms 746.856932ms 746.886572ms 746.937228ms 747.304107ms 747.376187ms 747.62078ms 747.699105ms 747.784054ms 747.826583ms 748.122449ms 748.321161ms 748.413081ms 748.443979ms 748.531314ms 748.752892ms 748.797175ms 748.911875ms 748.92352ms 749.122657ms 749.169041ms 749.243637ms 749.401764ms 749.472308ms 749.590298ms 749.634394ms 749.8101ms 750.023805ms 750.068298ms 750.077284ms 750.222098ms 750.23124ms 750.296193ms 750.501585ms 750.609287ms 750.646579ms 750.659579ms 750.766949ms 750.899447ms 750.967975ms 751.140743ms 751.201251ms 751.254926ms 751.406421ms 751.56969ms 751.601441ms 751.621693ms 751.644993ms 751.674693ms 751.726133ms 751.74173ms 752.012942ms 752.266175ms 752.272478ms 752.311823ms 752.347666ms 752.429223ms 752.446484ms 752.717422ms 752.891934ms 753.413608ms 753.543129ms 753.676295ms 754.260848ms 754.593733ms 754.926389ms 755.067647ms 755.13363ms 755.139439ms 755.179294ms 755.204749ms 755.742705ms 755.924588ms 756.091053ms 756.312441ms 756.984717ms 758.014377ms 758.128886ms 758.57646ms 758.705793ms 758.946809ms 759.62365ms 759.669962ms 759.977507ms 760.391113ms 761.305993ms 763.680034ms 763.799448ms 764.196846ms 764.22838ms 767.672467ms 768.981851ms 772.359679ms 777.565848ms 833.168855ms 879.313518ms]
May 27 09:50:00.519: INFO: 50 %ile: 746.298417ms
May 27 09:50:00.519: INFO: 90 %ile: 758.014377ms
May 27 09:50:00.519: INFO: 99 %ile: 833.168855ms
May 27 09:50:00.519: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:50:00.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7556" for this suite.

• [SLOW TEST:10.868 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":32,"skipped":548,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:50:00.553: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 09:50:00.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee" in namespace "projected-7376" to be "Succeeded or Failed"
May 27 09:50:00.621: INFO: Pod "downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.947442ms
May 27 09:50:02.638: INFO: Pod "downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026073054s
May 27 09:50:04.654: INFO: Pod "downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042009449s
STEP: Saw pod success
May 27 09:50:04.655: INFO: Pod "downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee" satisfied condition "Succeeded or Failed"
May 27 09:50:04.661: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee container client-container: <nil>
STEP: delete the pod
May 27 09:50:04.689: INFO: Waiting for pod downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee to disappear
May 27 09:50:04.695: INFO: Pod downwardapi-volume-d9773377-76bb-4a4e-a20c-d00b9e41a7ee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:50:04.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7376" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:50:04.716: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 27 09:50:08.813: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1901 PodName:pod-sharedvolume-5cfac520-2a2b-41de-b321-b85f9aca4d2b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 09:50:08.813: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 09:50:08.815: INFO: ExecWithOptions: Clientset creation
May 27 09:50:08.815: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-1901/pods/pod-sharedvolume-5cfac520-2a2b-41de-b321-b85f9aca4d2b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
May 27 09:50:08.954: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:50:08.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1901" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":34,"skipped":593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:50:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:50:10.021: INFO: Checking APIGroup: apiregistration.k8s.io
May 27 09:50:10.027: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 27 09:50:10.027: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May 27 09:50:10.027: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 27 09:50:10.027: INFO: Checking APIGroup: apps
May 27 09:50:10.031: INFO: PreferredVersion.GroupVersion: apps/v1
May 27 09:50:10.031: INFO: Versions found [{apps/v1 v1}]
May 27 09:50:10.031: INFO: apps/v1 matches apps/v1
May 27 09:50:10.031: INFO: Checking APIGroup: events.k8s.io
May 27 09:50:10.034: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 27 09:50:10.034: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 27 09:50:10.034: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 27 09:50:10.034: INFO: Checking APIGroup: authentication.k8s.io
May 27 09:50:10.038: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 27 09:50:10.038: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May 27 09:50:10.038: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 27 09:50:10.038: INFO: Checking APIGroup: authorization.k8s.io
May 27 09:50:10.039: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 27 09:50:10.039: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May 27 09:50:10.039: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 27 09:50:10.039: INFO: Checking APIGroup: autoscaling
May 27 09:50:10.044: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May 27 09:50:10.044: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 27 09:50:10.044: INFO: autoscaling/v2 matches autoscaling/v2
May 27 09:50:10.044: INFO: Checking APIGroup: batch
May 27 09:50:10.048: INFO: PreferredVersion.GroupVersion: batch/v1
May 27 09:50:10.049: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 27 09:50:10.049: INFO: batch/v1 matches batch/v1
May 27 09:50:10.049: INFO: Checking APIGroup: certificates.k8s.io
May 27 09:50:10.050: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 27 09:50:10.050: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May 27 09:50:10.050: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 27 09:50:10.050: INFO: Checking APIGroup: networking.k8s.io
May 27 09:50:10.051: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 27 09:50:10.051: INFO: Versions found [{networking.k8s.io/v1 v1}]
May 27 09:50:10.052: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 27 09:50:10.052: INFO: Checking APIGroup: policy
May 27 09:50:10.052: INFO: PreferredVersion.GroupVersion: policy/v1
May 27 09:50:10.052: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
May 27 09:50:10.052: INFO: policy/v1 matches policy/v1
May 27 09:50:10.052: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 27 09:50:10.054: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 27 09:50:10.054: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May 27 09:50:10.054: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 27 09:50:10.054: INFO: Checking APIGroup: storage.k8s.io
May 27 09:50:10.056: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 27 09:50:10.056: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 27 09:50:10.056: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 27 09:50:10.056: INFO: Checking APIGroup: admissionregistration.k8s.io
May 27 09:50:10.057: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 27 09:50:10.057: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May 27 09:50:10.057: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 27 09:50:10.057: INFO: Checking APIGroup: apiextensions.k8s.io
May 27 09:50:10.059: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 27 09:50:10.059: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May 27 09:50:10.059: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 27 09:50:10.059: INFO: Checking APIGroup: scheduling.k8s.io
May 27 09:50:10.061: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 27 09:50:10.062: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May 27 09:50:10.062: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 27 09:50:10.062: INFO: Checking APIGroup: coordination.k8s.io
May 27 09:50:10.063: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 27 09:50:10.063: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May 27 09:50:10.063: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 27 09:50:10.063: INFO: Checking APIGroup: node.k8s.io
May 27 09:50:10.065: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 27 09:50:10.065: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 27 09:50:10.065: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 27 09:50:10.065: INFO: Checking APIGroup: discovery.k8s.io
May 27 09:50:10.068: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 27 09:50:10.068: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
May 27 09:50:10.068: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 27 09:50:10.068: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 27 09:50:10.071: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May 27 09:50:10.071: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 27 09:50:10.071: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May 27 09:50:10.071: INFO: Checking APIGroup: cilium.io
May 27 09:50:10.073: INFO: PreferredVersion.GroupVersion: cilium.io/v2
May 27 09:50:10.073: INFO: Versions found [{cilium.io/v2 v2}]
May 27 09:50:10.073: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:50:10.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4664" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":35,"skipped":639,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:50:10.120: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 09:50:10.296: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 09:51:10.385: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
May 27 09:51:10.440: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 27 09:51:10.450: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 27 09:51:10.481: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 27 09:51:10.504: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 27 09:51:10.549: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 27 09:51:10.564: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:51:24.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3229" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.739 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":36,"skipped":646,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:51:24.860: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-z2gv
STEP: Creating a pod to test atomic-volume-subpath
May 27 09:51:24.963: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-z2gv" in namespace "subpath-7364" to be "Succeeded or Failed"
May 27 09:51:24.972: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Pending", Reason="", readiness=false. Elapsed: 9.037991ms
May 27 09:51:26.986: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.022402591s
May 27 09:51:29.000: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 4.036597302s
May 27 09:51:31.015: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 6.051604796s
May 27 09:51:33.032: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 8.068734554s
May 27 09:51:35.053: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 10.089748186s
May 27 09:51:37.066: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 12.103218347s
May 27 09:51:39.082: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 14.118438938s
May 27 09:51:41.097: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 16.133826864s
May 27 09:51:43.116: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 18.152671491s
May 27 09:51:45.130: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=true. Elapsed: 20.167102064s
May 27 09:51:47.148: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Running", Reason="", readiness=false. Elapsed: 22.184489764s
May 27 09:51:49.161: INFO: Pod "pod-subpath-test-secret-z2gv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.198055s
STEP: Saw pod success
May 27 09:51:49.161: INFO: Pod "pod-subpath-test-secret-z2gv" satisfied condition "Succeeded or Failed"
May 27 09:51:49.167: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-subpath-test-secret-z2gv container test-container-subpath-secret-z2gv: <nil>
STEP: delete the pod
May 27 09:51:49.235: INFO: Waiting for pod pod-subpath-test-secret-z2gv to disappear
May 27 09:51:49.240: INFO: Pod pod-subpath-test-secret-z2gv no longer exists
STEP: Deleting pod pod-subpath-test-secret-z2gv
May 27 09:51:49.241: INFO: Deleting pod "pod-subpath-test-secret-z2gv" in namespace "subpath-7364"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:51:49.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7364" for this suite.

• [SLOW TEST:24.406 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":37,"skipped":656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:51:49.270: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
May 27 09:51:49.349: INFO: Creating e2e-svc-a-zc5nm
May 27 09:51:49.374: INFO: Creating e2e-svc-b-qs998
May 27 09:51:49.399: INFO: Creating e2e-svc-c-m5clw
STEP: deleting service collection
May 27 09:51:49.496: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:51:49.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2493" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":38,"skipped":695,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:51:49.519: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:51:49.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4794" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":39,"skipped":702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:51:49.597: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
May 27 09:52:09.969: INFO: EndpointSlice for Service endpointslice-2188/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:52:20.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2188" for this suite.

• [SLOW TEST:30.440 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":40,"skipped":731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:52:20.051: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:52:20.117: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 27 09:52:26.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 create -f -'
May 27 09:52:28.034: INFO: stderr: ""
May 27 09:52:28.034: INFO: stdout: "e2e-test-crd-publish-openapi-7556-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 27 09:52:28.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 delete e2e-test-crd-publish-openapi-7556-crds test-foo'
May 27 09:52:28.157: INFO: stderr: ""
May 27 09:52:28.157: INFO: stdout: "e2e-test-crd-publish-openapi-7556-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 27 09:52:28.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 apply -f -'
May 27 09:52:29.338: INFO: stderr: ""
May 27 09:52:29.338: INFO: stdout: "e2e-test-crd-publish-openapi-7556-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 27 09:52:29.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 delete e2e-test-crd-publish-openapi-7556-crds test-foo'
May 27 09:52:29.470: INFO: stderr: ""
May 27 09:52:29.470: INFO: stdout: "e2e-test-crd-publish-openapi-7556-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
May 27 09:52:29.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 create -f -'
May 27 09:52:29.715: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 27 09:52:29.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 create -f -'
May 27 09:52:29.923: INFO: rc: 1
May 27 09:52:29.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 apply -f -'
May 27 09:52:30.142: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 27 09:52:30.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 create -f -'
May 27 09:52:30.378: INFO: rc: 1
May 27 09:52:30.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 --namespace=crd-publish-openapi-8074 apply -f -'
May 27 09:52:30.596: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 27 09:52:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 explain e2e-test-crd-publish-openapi-7556-crds'
May 27 09:52:30.861: INFO: stderr: ""
May 27 09:52:30.861: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7556-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 27 09:52:30.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 explain e2e-test-crd-publish-openapi-7556-crds.metadata'
May 27 09:52:31.142: INFO: stderr: ""
May 27 09:52:31.142: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7556-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 27 09:52:31.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 explain e2e-test-crd-publish-openapi-7556-crds.spec'
May 27 09:52:31.437: INFO: stderr: ""
May 27 09:52:31.437: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7556-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 27 09:52:31.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 explain e2e-test-crd-publish-openapi-7556-crds.spec.bars'
May 27 09:52:31.682: INFO: stderr: ""
May 27 09:52:31.682: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7556-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 27 09:52:31.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-8074 explain e2e-test-crd-publish-openapi-7556-crds.spec.bars2'
May 27 09:52:31.991: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:52:38.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8074" for this suite.

• [SLOW TEST:18.108 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":41,"skipped":796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:52:38.166: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5332.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5332.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 09:52:40.314: INFO: DNS probes using dns-test-7e85a06c-d726-4ec6-b5a2-d8b893a7b280 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5332.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5332.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 09:52:42.421: INFO: File wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:42.429: INFO: File jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:42.429: INFO: Lookups using dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 failed for: [wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local]

May 27 09:52:47.457: INFO: File wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:47.463: INFO: File jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:47.463: INFO: Lookups using dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 failed for: [wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local]

May 27 09:52:52.440: INFO: File wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:52.447: INFO: File jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:52.447: INFO: Lookups using dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 failed for: [wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local]

May 27 09:52:57.440: INFO: File wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:57.448: INFO: File jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:52:57.448: INFO: Lookups using dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 failed for: [wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local]

May 27 09:53:02.442: INFO: File wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:53:02.449: INFO: File jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:53:02.449: INFO: Lookups using dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 failed for: [wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local]

May 27 09:53:07.438: INFO: File wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains '' instead of 'bar.example.com.'
May 27 09:53:07.444: INFO: File jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local from pod  dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 09:53:07.444: INFO: Lookups using dns-5332/dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 failed for: [wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local]

May 27 09:53:12.444: INFO: DNS probes using dns-test-fd09a7d4-635f-4774-98db-3e7c87e2d1c4 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5332.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5332.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5332.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5332.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 09:53:16.655: INFO: DNS probes using dns-test-7d5812b2-f614-4730-ae9b-0d9ca6208525 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:16.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5332" for this suite.

• [SLOW TEST:38.575 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":42,"skipped":827,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:16.745: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 09:53:18.411: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 09:53:21.455: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:31.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3413" for this suite.
STEP: Destroying namespace "webhook-3413-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.235 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":43,"skipped":844,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:31.982: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
May 27 09:53:32.102: INFO: Waiting up to 5m0s for pod "var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9" in namespace "var-expansion-5716" to be "Succeeded or Failed"
May 27 09:53:32.119: INFO: Pod "var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.850387ms
May 27 09:53:34.134: INFO: Pod "var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031549134s
May 27 09:53:36.158: INFO: Pod "var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055988079s
STEP: Saw pod success
May 27 09:53:36.159: INFO: Pod "var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9" satisfied condition "Succeeded or Failed"
May 27 09:53:36.166: INFO: Trying to get logs from node ohp4eith3vui-3 pod var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9 container dapi-container: <nil>
STEP: delete the pod
May 27 09:53:36.210: INFO: Waiting for pod var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9 to disappear
May 27 09:53:36.217: INFO: Pod var-expansion-47605661-8025-4e49-80b4-3255ef1ad0e9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:36.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5716" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":856,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:36.253: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-9c2b9f0e-9dd6-4f00-a4d1-074f62db5426
STEP: Creating a pod to test consume configMaps
May 27 09:53:36.344: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558" in namespace "projected-2387" to be "Succeeded or Failed"
May 27 09:53:36.355: INFO: Pod "pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558": Phase="Pending", Reason="", readiness=false. Elapsed: 11.190173ms
May 27 09:53:38.371: INFO: Pod "pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0270591s
May 27 09:53:40.384: INFO: Pod "pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04048471s
STEP: Saw pod success
May 27 09:53:40.385: INFO: Pod "pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558" satisfied condition "Succeeded or Failed"
May 27 09:53:40.390: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558 container agnhost-container: <nil>
STEP: delete the pod
May 27 09:53:40.429: INFO: Waiting for pod pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558 to disappear
May 27 09:53:40.434: INFO: Pod pod-projected-configmaps-79729844-7ecd-4da0-ac71-59332935c558 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:40.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2387" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":859,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:40.458: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
May 27 09:53:40.529: INFO: Pod name sample-pod: Found 0 pods out of 3
May 27 09:53:45.552: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
May 27 09:53:45.564: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:45.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2079" for this suite.

• [SLOW TEST:5.175 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":46,"skipped":875,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-3279cf7f-5a85-4c44-bf4e-f258101d730b
STEP: Creating a pod to test consume secrets
May 27 09:53:45.806: INFO: Waiting up to 5m0s for pod "pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04" in namespace "secrets-3733" to be "Succeeded or Failed"
May 27 09:53:45.839: INFO: Pod "pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04": Phase="Pending", Reason="", readiness=false. Elapsed: 32.963869ms
May 27 09:53:47.857: INFO: Pod "pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050896888s
May 27 09:53:49.876: INFO: Pod "pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07033922s
STEP: Saw pod success
May 27 09:53:49.877: INFO: Pod "pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04" satisfied condition "Succeeded or Failed"
May 27 09:53:49.881: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04 container secret-volume-test: <nil>
STEP: delete the pod
May 27 09:53:49.912: INFO: Waiting for pod pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04 to disappear
May 27 09:53:49.917: INFO: Pod pod-secrets-18a4a367-9a6a-4601-9624-1f4f92ec4f04 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:49.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3733" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:49.938: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:53:50.010: INFO: The status of Pod busybox-host-aliases3d1d9305-2db4-4457-8bf4-61450d2a5eb3 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:53:52.027: INFO: The status of Pod busybox-host-aliases3d1d9305-2db4-4457-8bf4-61450d2a5eb3 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:53:54.028: INFO: The status of Pod busybox-host-aliases3d1d9305-2db4-4457-8bf4-61450d2a5eb3 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:53:54.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3687" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":48,"skipped":926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:53:54.067: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3355.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 09:53:56.215: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.220: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.226: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.232: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.239: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.244: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.248: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.253: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:53:56.253: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:01.263: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.269: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.277: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.282: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.289: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.301: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.307: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.313: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:01.313: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:06.264: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.274: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.294: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.300: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.308: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.314: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.321: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.327: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:06.327: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:11.264: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.273: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.279: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.285: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.290: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.295: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.301: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.306: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:11.306: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:16.260: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.268: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.275: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.281: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.286: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.295: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.310: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.317: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:16.317: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:21.272: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.278: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.284: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.289: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.297: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.302: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.308: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.315: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:21.315: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local jessie_udp@dns-test-service-2.dns-3355.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:26.264: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:26.271: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:26.279: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:26.284: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local from pod dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef: the server could not find the requested resource (get pods dns-test-a30a27a6-6149-413f-b149-159402f9bbef)
May 27 09:54:26.313: INFO: Lookups using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3355.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3355.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3355.svc.cluster.local]

May 27 09:54:31.319: INFO: DNS probes using dns-3355/dns-test-a30a27a6-6149-413f-b149-159402f9bbef succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:54:31.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3355" for this suite.

• [SLOW TEST:37.426 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":49,"skipped":995,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:54:31.497: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 09:54:32.538: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 09:54:35.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 27 09:54:35.636: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:54:35.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8800" for this suite.
STEP: Destroying namespace "webhook-8800-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":50,"skipped":1001,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:54:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 09:54:35.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 09:54:35.956: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 09:54:36.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 09:54:36.991: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 09:54:37.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 09:54:37.985: INFO: Node ohp4eith3vui-2 is running 0 daemon pod, expected 1
May 27 09:54:38.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 09:54:38.979: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 27 09:54:39.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 09:54:39.023: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 09:54:40.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 09:54:40.052: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 09:54:41.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 09:54:41.048: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 09:54:42.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 09:54:42.041: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 09:54:43.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 09:54:43.046: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7868, will wait for the garbage collector to delete the pods
May 27 09:54:43.126: INFO: Deleting DaemonSet.extensions daemon-set took: 16.908514ms
May 27 09:54:43.227: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.648534ms
May 27 09:54:46.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 09:54:46.044: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 09:54:46.050: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20169"},"items":null}

May 27 09:54:46.055: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20169"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:54:46.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7868" for this suite.

• [SLOW TEST:10.327 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":51,"skipped":1001,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:54:46.103: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-2458
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 09:54:46.148: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 09:54:46.207: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:54:48.226: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:54:50.223: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:54:52.218: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:54:54.221: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:54:56.221: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:54:58.220: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:55:00.226: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:55:02.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:55:04.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:55:06.224: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 09:55:08.227: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 09:55:08.240: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 09:55:08.250: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 09:55:10.304: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 09:55:10.305: INFO: Breadth first check of 10.233.65.128 on host 192.168.121.5...
May 27 09:55:10.311: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.93:9080/dial?request=hostname&protocol=udp&host=10.233.65.128&port=8081&tries=1'] Namespace:pod-network-test-2458 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 09:55:10.311: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 09:55:10.314: INFO: ExecWithOptions: Clientset creation
May 27 09:55:10.314: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2458/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.128%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 09:55:10.468: INFO: Waiting for responses: map[]
May 27 09:55:10.468: INFO: reached 10.233.65.128 after 0/1 tries
May 27 09:55:10.468: INFO: Breadth first check of 10.233.64.183 on host 192.168.121.118...
May 27 09:55:10.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.93:9080/dial?request=hostname&protocol=udp&host=10.233.64.183&port=8081&tries=1'] Namespace:pod-network-test-2458 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 09:55:10.475: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 09:55:10.477: INFO: ExecWithOptions: Clientset creation
May 27 09:55:10.477: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2458/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.183%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 09:55:10.600: INFO: Waiting for responses: map[]
May 27 09:55:10.600: INFO: reached 10.233.64.183 after 0/1 tries
May 27 09:55:10.600: INFO: Breadth first check of 10.233.66.179 on host 192.168.121.192...
May 27 09:55:10.607: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.93:9080/dial?request=hostname&protocol=udp&host=10.233.66.179&port=8081&tries=1'] Namespace:pod-network-test-2458 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 09:55:10.607: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 09:55:10.609: INFO: ExecWithOptions: Clientset creation
May 27 09:55:10.609: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2458/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.93%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 09:55:10.698: INFO: Waiting for responses: map[]
May 27 09:55:10.699: INFO: reached 10.233.66.179 after 0/1 tries
May 27 09:55:10.699: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:10.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2458" for this suite.

• [SLOW TEST:24.622 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":1007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:10.730: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:55:10.783: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5d2303cf-7504-4281-bc05-385e1cc3e154" in namespace "security-context-test-1795" to be "Succeeded or Failed"
May 27 09:55:10.817: INFO: Pod "busybox-readonly-false-5d2303cf-7504-4281-bc05-385e1cc3e154": Phase="Pending", Reason="", readiness=false. Elapsed: 33.805939ms
May 27 09:55:12.835: INFO: Pod "busybox-readonly-false-5d2303cf-7504-4281-bc05-385e1cc3e154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05192318s
May 27 09:55:14.848: INFO: Pod "busybox-readonly-false-5d2303cf-7504-4281-bc05-385e1cc3e154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065056885s
May 27 09:55:14.848: INFO: Pod "busybox-readonly-false-5d2303cf-7504-4281-bc05-385e1cc3e154" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:14.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1795" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":1062,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:14.876: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 27 09:55:14.930: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 27 09:55:14.944: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 27 09:55:14.944: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 27 09:55:14.955: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 27 09:55:14.956: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 27 09:55:14.981: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 27 09:55:14.981: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 27 09:55:22.127: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:22.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8322" for this suite.

• [SLOW TEST:7.300 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":54,"skipped":1077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:22.180: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:55:22.215: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 09:55:22.261: INFO: The status of Pod pod-logs-websocket-681714ab-cbef-4522-99c9-913edd3afce1 is Pending, waiting for it to be Running (with Ready = true)
May 27 09:55:24.277: INFO: The status of Pod pod-logs-websocket-681714ab-cbef-4522-99c9-913edd3afce1 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:24.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3251" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":1130,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:24.345: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:35.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7297" for this suite.

• [SLOW TEST:11.223 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":56,"skipped":1146,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:35.571: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 09:55:36.469: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 09:55:38.506: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 9, 55, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 9, 55, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 9, 55, 36, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 9, 55, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 09:55:41.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:41.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5734" for this suite.
STEP: Destroying namespace "webhook-5734-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.440 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":57,"skipped":1151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:42.018: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-3388
STEP: creating replication controller nodeport-test in namespace services-3388
I0527 09:55:42.143463      15 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3388, replica count: 2
I0527 09:55:45.194252      15 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 09:55:45.194: INFO: Creating new exec pod
May 27 09:55:48.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3388 exec execpodxqx2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 27 09:55:48.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 27 09:55:48.473: INFO: stdout: "nodeport-test-bbhxp"
May 27 09:55:48.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3388 exec execpodxqx2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.30.116 80'
May 27 09:55:48.693: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.30.116 80\nConnection to 10.233.30.116 80 port [tcp/http] succeeded!\n"
May 27 09:55:48.693: INFO: stdout: "nodeport-test-d5x27"
May 27 09:55:48.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3388 exec execpodxqx2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.118 30510'
May 27 09:55:48.882: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.118 30510\nConnection to 192.168.121.118 30510 port [tcp/*] succeeded!\n"
May 27 09:55:48.882: INFO: stdout: ""
May 27 09:55:49.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3388 exec execpodxqx2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.118 30510'
May 27 09:55:50.090: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 192.168.121.118 30510\nConnection to 192.168.121.118 30510 port [tcp/*] succeeded!\n"
May 27 09:55:50.090: INFO: stdout: "nodeport-test-d5x27"
May 27 09:55:50.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3388 exec execpodxqx2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.192 30510'
May 27 09:55:50.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.192 30510\nConnection to 192.168.121.192 30510 port [tcp/*] succeeded!\n"
May 27 09:55:50.337: INFO: stdout: "nodeport-test-d5x27"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:55:50.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3388" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.340 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":58,"skipped":1176,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:55:50.360: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:56:06.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6028" for this suite.

• [SLOW TEST:16.344 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":59,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:56:06.709: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-7a285a64-ea5d-4706-a01c-524c943c9824
STEP: Creating a pod to test consume secrets
May 27 09:56:06.775: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798" in namespace "projected-4945" to be "Succeeded or Failed"
May 27 09:56:06.826: INFO: Pod "pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798": Phase="Pending", Reason="", readiness=false. Elapsed: 50.83337ms
May 27 09:56:08.842: INFO: Pod "pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06671273s
May 27 09:56:10.855: INFO: Pod "pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079775061s
STEP: Saw pod success
May 27 09:56:10.855: INFO: Pod "pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798" satisfied condition "Succeeded or Failed"
May 27 09:56:10.860: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 09:56:10.895: INFO: Waiting for pod pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798 to disappear
May 27 09:56:10.900: INFO: Pod pod-projected-secrets-4f6406ca-bb77-424f-bfbd-526a7ef9b798 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:56:10.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4945" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":1230,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:56:10.917: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
May 27 09:56:10.970: INFO: Creating simple deployment test-deployment-qp2w4
May 27 09:56:10.994: INFO: new replicaset for deployment "test-deployment-qp2w4" is yet to be created
STEP: Getting /status
May 27 09:56:13.032: INFO: Deployment test-deployment-qp2w4 has Conditions: [{Available True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qp2w4-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
May 27 09:56:13.047: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 9, 56, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 9, 56, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 9, 56, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 9, 56, 10, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qp2w4-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
May 27 09:56:13.052: INFO: Observed &Deployment event: ADDED
May 27 09:56:13.052: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:10 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qp2w4-764bc7c4b7"}
May 27 09:56:13.053: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.053: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:10 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qp2w4-764bc7c4b7"}
May 27 09:56:13.053: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 09:56:11 +0000 UTC 2022-05-27 09:56:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 09:56:13.053: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.053: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 09:56:11 +0000 UTC 2022-05-27 09:56:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 09:56:13.053: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:11 +0000 UTC 2022-05-27 09:56:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qp2w4-764bc7c4b7" is progressing.}
May 27 09:56:13.054: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.054: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 09:56:13.054: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qp2w4-764bc7c4b7" has successfully progressed.}
May 27 09:56:13.054: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.054: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 09:56:13.054: INFO: Observed Deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qp2w4-764bc7c4b7" has successfully progressed.}
May 27 09:56:13.055: INFO: Found Deployment test-deployment-qp2w4 in namespace deployment-8725 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 09:56:13.055: INFO: Deployment test-deployment-qp2w4 has an updated status
STEP: patching the Statefulset Status
May 27 09:56:13.055: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 27 09:56:13.071: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
May 27 09:56:13.077: INFO: Observed &Deployment event: ADDED
May 27 09:56:13.077: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:10 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qp2w4-764bc7c4b7"}
May 27 09:56:13.079: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.079: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:10 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qp2w4-764bc7c4b7"}
May 27 09:56:13.079: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 09:56:11 +0000 UTC 2022-05-27 09:56:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 09:56:13.079: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.079: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 09:56:11 +0000 UTC 2022-05-27 09:56:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 09:56:13.079: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:11 +0000 UTC 2022-05-27 09:56:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qp2w4-764bc7c4b7" is progressing.}
May 27 09:56:13.080: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.080: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 09:56:13.080: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qp2w4-764bc7c4b7" has successfully progressed.}
May 27 09:56:13.080: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.080: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 09:56:13.080: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 09:56:12 +0000 UTC 2022-05-27 09:56:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qp2w4-764bc7c4b7" has successfully progressed.}
May 27 09:56:13.080: INFO: Observed deployment test-deployment-qp2w4 in namespace deployment-8725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 09:56:13.080: INFO: Observed &Deployment event: MODIFIED
May 27 09:56:13.080: INFO: Found deployment test-deployment-qp2w4 in namespace deployment-8725 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May 27 09:56:13.080: INFO: Deployment test-deployment-qp2w4 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 09:56:13.092: INFO: Deployment "test-deployment-qp2w4":
&Deployment{ObjectMeta:{test-deployment-qp2w4  deployment-8725  23cde21f-3c8a-4f54-af1f-ac0c5f8720f1 20887 1 2022-05-27 09:56:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-05-27 09:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-05-27 09:56:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-05-27 09:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00438d238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-qp2w4-764bc7c4b7",LastUpdateTime:2022-05-27 09:56:13 +0000 UTC,LastTransitionTime:2022-05-27 09:56:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 09:56:13.098: INFO: New ReplicaSet "test-deployment-qp2w4-764bc7c4b7" of Deployment "test-deployment-qp2w4":
&ReplicaSet{ObjectMeta:{test-deployment-qp2w4-764bc7c4b7  deployment-8725  d21eeac4-773c-43cb-a874-143e1242a372 20883 1 2022-05-27 09:56:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-qp2w4 23cde21f-3c8a-4f54-af1f-ac0c5f8720f1 0xc00467e0e0 0xc00467e0e1}] []  [{kube-controller-manager Update apps/v1 2022-05-27 09:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"23cde21f-3c8a-4f54-af1f-ac0c5f8720f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 09:56:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467e188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 09:56:13.104: INFO: Pod "test-deployment-qp2w4-764bc7c4b7-nmbjn" is available:
&Pod{ObjectMeta:{test-deployment-qp2w4-764bc7c4b7-nmbjn test-deployment-qp2w4-764bc7c4b7- deployment-8725  a6977ada-29ea-465c-bcf4-275db41f3ca3 20882 0 2022-05-27 09:56:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [{apps/v1 ReplicaSet test-deployment-qp2w4-764bc7c4b7 d21eeac4-773c-43cb-a874-143e1242a372 0xc00438d630 0xc00438d631}] []  [{kube-controller-manager Update v1 2022-05-27 09:56:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d21eeac4-773c-43cb-a874-143e1242a372\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 09:56:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhmh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhmh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:56:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 09:56:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.162,StartTime:2022-05-27 09:56:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 09:56:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://cf47fc692e0e442c75dd85eb18cc86747bb048920f5f239bbb85e6ebeb06763d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:56:13.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8725" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":61,"skipped":1240,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:56:13.121: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 09:56:14.153: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 09:56:17.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
May 27 09:56:17.241: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:56:17.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2888" for this suite.
STEP: Destroying namespace "webhook-2888-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":62,"skipped":1242,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:56:17.556: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:56:34.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7748" for this suite.

• [SLOW TEST:17.179 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":63,"skipped":1253,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:56:34.736: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-0e3db677-3528-40a0-b5e6-bc63e5f280da
STEP: Creating a pod to test consume secrets
May 27 09:56:34.818: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318" in namespace "projected-2080" to be "Succeeded or Failed"
May 27 09:56:34.832: INFO: Pod "pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318": Phase="Pending", Reason="", readiness=false. Elapsed: 13.102312ms
May 27 09:56:36.844: INFO: Pod "pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025713526s
May 27 09:56:38.858: INFO: Pod "pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039219293s
STEP: Saw pod success
May 27 09:56:38.858: INFO: Pod "pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318" satisfied condition "Succeeded or Failed"
May 27 09:56:38.863: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 09:56:38.898: INFO: Waiting for pod pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318 to disappear
May 27 09:56:38.903: INFO: Pod pod-projected-secrets-0c59d628-8894-4561-ac04-cc1d178b3318 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:56:38.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2080" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1266,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:56:38.919: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:56:38.978: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Creating first CR 
May 27 09:56:41.699: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T09:56:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T09:56:41Z]] name:name1 resourceVersion:21108 uid:aba6b978-c7b2-47cb-927c-36da72ffecf3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 27 09:56:51.733: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T09:56:51Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T09:56:51Z]] name:name2 resourceVersion:21138 uid:856c4ba3-0193-4bf1-aa03-3fa0d00d2d0a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 27 09:57:01.765: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T09:56:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T09:57:01Z]] name:name1 resourceVersion:21159 uid:aba6b978-c7b2-47cb-927c-36da72ffecf3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 27 09:57:11.799: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T09:56:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T09:57:11Z]] name:name2 resourceVersion:21180 uid:856c4ba3-0193-4bf1-aa03-3fa0d00d2d0a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 27 09:57:21.829: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T09:56:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T09:57:01Z]] name:name1 resourceVersion:21201 uid:aba6b978-c7b2-47cb-927c-36da72ffecf3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 27 09:57:31.869: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T09:56:51Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T09:57:11Z]] name:name2 resourceVersion:21222 uid:856c4ba3-0193-4bf1-aa03-3fa0d00d2d0a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:57:42.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7797" for this suite.

• [SLOW TEST:63.529 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":65,"skipped":1267,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:57:42.454: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 27 09:57:43.682: INFO: The status of Pod kube-controller-manager-ohp4eith3vui-2 is Running (Ready = true)
May 27 09:57:43.785: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:57:43.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1321" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":66,"skipped":1280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:57:43.810: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-3780e848-5fec-4c5b-afdb-126348f94354
STEP: Creating a pod to test consume secrets
May 27 09:57:43.868: INFO: Waiting up to 5m0s for pod "pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e" in namespace "secrets-2718" to be "Succeeded or Failed"
May 27 09:57:43.875: INFO: Pod "pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.237527ms
May 27 09:57:45.883: INFO: Pod "pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014998388s
May 27 09:57:47.897: INFO: Pod "pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02829517s
STEP: Saw pod success
May 27 09:57:47.897: INFO: Pod "pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e" satisfied condition "Succeeded or Failed"
May 27 09:57:47.926: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e container secret-volume-test: <nil>
STEP: delete the pod
May 27 09:57:47.968: INFO: Waiting for pod pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e to disappear
May 27 09:57:47.978: INFO: Pod pod-secrets-de698bd4-d78a-4b72-8419-90ab5613023e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:57:47.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2718" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1326,"failed":0}
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:57:47.998: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 09:57:48.051: INFO: PodSpec: initContainers in spec.initContainers
May 27 09:58:34.141: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ec296172-34dc-4a7e-9dba-ac329545896f", GenerateName:"", Namespace:"init-container-7070", SelfLink:"", UID:"d18291b7-e807-4697-9f6b-6c9756ce06ae", ResourceVersion:"21484", Generation:0, CreationTimestamp:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"51182613"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00324fe30), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 27, 9, 57, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00324fe60), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-rv649", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004963da0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rv649", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rv649", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rv649", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003035c78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ohp4eith3vui-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002dc44d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003035d00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003035d20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003035d28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003035d2c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002127360), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.192", PodIP:"10.233.66.246", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.246"}}, StartTime:time.Date(2022, time.May, 27, 9, 57, 48, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002dc45b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002dc4690)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://0459ca5709167cac495ba71799e66acf9eb55a064fc8b604f27b7cec22ff06b5", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004963e20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004963e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc003035da4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 09:58:34.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7070" for this suite.

• [SLOW TEST:46.201 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":68,"skipped":1331,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 09:58:34.201: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 27 09:58:34.257: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 09:59:34.330: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 09:59:34.338: INFO: Starting informer...
STEP: Starting pods...
May 27 09:59:34.572: INFO: Pod1 is running on ohp4eith3vui-3. Tainting Node
May 27 09:59:36.821: INFO: Pod2 is running on ohp4eith3vui-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 27 09:59:42.387: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 27 10:00:02.445: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:02.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4306" for this suite.

• [SLOW TEST:88.312 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":69,"skipped":1344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:02.520: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:00:02.584: INFO: Endpoints addresses: [192.168.121.118 192.168.121.5] , ports: [6443]
May 27 10:00:02.584: INFO: EndpointSlices addresses: [192.168.121.118 192.168.121.5] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:02.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-598" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":70,"skipped":1392,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:02.617: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
May 27 10:00:02.681: INFO: Waiting up to 5m0s for pod "pod-44d43da5-5599-4201-948b-72f67d2e5b2f" in namespace "emptydir-9815" to be "Succeeded or Failed"
May 27 10:00:02.697: INFO: Pod "pod-44d43da5-5599-4201-948b-72f67d2e5b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.370315ms
May 27 10:00:04.715: INFO: Pod "pod-44d43da5-5599-4201-948b-72f67d2e5b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033349027s
May 27 10:00:06.727: INFO: Pod "pod-44d43da5-5599-4201-948b-72f67d2e5b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045369966s
STEP: Saw pod success
May 27 10:00:06.727: INFO: Pod "pod-44d43da5-5599-4201-948b-72f67d2e5b2f" satisfied condition "Succeeded or Failed"
May 27 10:00:06.732: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-44d43da5-5599-4201-948b-72f67d2e5b2f container test-container: <nil>
STEP: delete the pod
May 27 10:00:06.779: INFO: Waiting for pod pod-44d43da5-5599-4201-948b-72f67d2e5b2f to disappear
May 27 10:00:06.784: INFO: Pod pod-44d43da5-5599-4201-948b-72f67d2e5b2f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:06.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9815" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":71,"skipped":1396,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:06.839: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
May 27 10:00:06.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-8425 cluster-info'
May 27 10:00:07.069: INFO: stderr: ""
May 27 10:00:07.069: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:07.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8425" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":72,"skipped":1407,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:07.095: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-91e48344-e1f7-46ef-9352-fe49e7eda714
STEP: Creating a pod to test consume secrets
May 27 10:00:07.169: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411" in namespace "projected-2127" to be "Succeeded or Failed"
May 27 10:00:07.178: INFO: Pod "pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411": Phase="Pending", Reason="", readiness=false. Elapsed: 8.874667ms
May 27 10:00:09.190: INFO: Pod "pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411": Phase="Running", Reason="", readiness=true. Elapsed: 2.02116619s
May 27 10:00:11.209: INFO: Pod "pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411": Phase="Running", Reason="", readiness=false. Elapsed: 4.039594426s
May 27 10:00:13.222: INFO: Pod "pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052719123s
STEP: Saw pod success
May 27 10:00:13.222: INFO: Pod "pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411" satisfied condition "Succeeded or Failed"
May 27 10:00:13.229: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 10:00:13.268: INFO: Waiting for pod pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411 to disappear
May 27 10:00:13.274: INFO: Pod pod-projected-secrets-69637cc0-647b-421b-bbbd-835d292b2411 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:13.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2127" for this suite.

• [SLOW TEST:6.196 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:13.292: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 10:00:18.423: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:18.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4718" for this suite.

• [SLOW TEST:5.173 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1448,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:18.466: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-9aa086b1-a553-40a8-ae39-c678fde89468 in namespace container-probe-850
May 27 10:00:20.544: INFO: Started pod liveness-9aa086b1-a553-40a8-ae39-c678fde89468 in namespace container-probe-850
STEP: checking the pod's current state and verifying that restartCount is present
May 27 10:00:20.551: INFO: Initial restart count of pod liveness-9aa086b1-a553-40a8-ae39-c678fde89468 is 0
May 27 10:00:40.688: INFO: Restart count of pod container-probe-850/liveness-9aa086b1-a553-40a8-ae39-c678fde89468 is now 1 (20.137292866s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:40.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-850" for this suite.

• [SLOW TEST:22.266 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1450,"failed":0}
SSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:40.733: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 27 10:00:40.823: INFO: Waiting up to 5m0s for pod "security-context-4d98d72b-1936-473e-8a7e-887497828da6" in namespace "security-context-824" to be "Succeeded or Failed"
May 27 10:00:40.834: INFO: Pod "security-context-4d98d72b-1936-473e-8a7e-887497828da6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.13146ms
May 27 10:00:42.847: INFO: Pod "security-context-4d98d72b-1936-473e-8a7e-887497828da6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02427277s
May 27 10:00:44.863: INFO: Pod "security-context-4d98d72b-1936-473e-8a7e-887497828da6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040160414s
STEP: Saw pod success
May 27 10:00:44.863: INFO: Pod "security-context-4d98d72b-1936-473e-8a7e-887497828da6" satisfied condition "Succeeded or Failed"
May 27 10:00:44.869: INFO: Trying to get logs from node ohp4eith3vui-3 pod security-context-4d98d72b-1936-473e-8a7e-887497828da6 container test-container: <nil>
STEP: delete the pod
May 27 10:00:44.912: INFO: Waiting for pod security-context-4d98d72b-1936-473e-8a7e-887497828da6 to disappear
May 27 10:00:44.917: INFO: Pod security-context-4d98d72b-1936-473e-8a7e-887497828da6 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:44.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-824" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":76,"skipped":1454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:44.938: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:44.995: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-9959
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:51.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-9321" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:00:51.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9959" for this suite.

• [SLOW TEST:6.278 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":77,"skipped":1482,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:00:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 27 10:00:58.070: INFO: 84 pods remaining
May 27 10:00:58.081: INFO: 79 pods has nil DeletionTimestamp
May 27 10:00:58.081: INFO: 
May 27 10:00:59.218: INFO: 79 pods remaining
May 27 10:00:59.218: INFO: 69 pods has nil DeletionTimestamp
May 27 10:00:59.218: INFO: 
May 27 10:01:00.063: INFO: 74 pods remaining
May 27 10:01:00.063: INFO: 57 pods has nil DeletionTimestamp
May 27 10:01:00.063: INFO: 
May 27 10:01:00.962: INFO: 62 pods remaining
May 27 10:01:00.962: INFO: 42 pods has nil DeletionTimestamp
May 27 10:01:00.962: INFO: 
May 27 10:01:02.147: INFO: 56 pods remaining
May 27 10:01:02.147: INFO: 28 pods has nil DeletionTimestamp
May 27 10:01:02.147: INFO: 
May 27 10:01:02.784: INFO: 49 pods remaining
May 27 10:01:02.784: INFO: 18 pods has nil DeletionTimestamp
May 27 10:01:02.784: INFO: 
May 27 10:01:04.446: INFO: 44 pods remaining
May 27 10:01:04.446: INFO: 2 pods has nil DeletionTimestamp
May 27 10:01:04.446: INFO: 
May 27 10:01:05.158: INFO: 39 pods remaining
May 27 10:01:05.158: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:05.158: INFO: 
May 27 10:01:06.139: INFO: 33 pods remaining
May 27 10:01:06.145: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:06.145: INFO: 
May 27 10:01:06.820: INFO: 27 pods remaining
May 27 10:01:06.820: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:06.820: INFO: 
May 27 10:01:07.921: INFO: 18 pods remaining
May 27 10:01:07.921: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:07.921: INFO: 
May 27 10:01:08.820: INFO: 11 pods remaining
May 27 10:01:08.820: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:08.820: INFO: 
May 27 10:01:09.812: INFO: 3 pods remaining
May 27 10:01:09.812: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:09.812: INFO: 
May 27 10:01:10.744: INFO: 0 pods remaining
May 27 10:01:10.744: INFO: 0 pods has nil DeletionTimestamp
May 27 10:01:10.744: INFO: 
STEP: Gathering metrics
May 27 10:01:11.852: INFO: The status of Pod kube-controller-manager-ohp4eith3vui-2 is Running (Ready = true)
May 27 10:01:12.004: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:01:12.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-666" for this suite.

• [SLOW TEST:20.888 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":78,"skipped":1485,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:01:12.106: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-4980
May 27 10:01:12.286: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 10:01:14.299: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 10:01:16.309: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 10:01:18.295: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 10:01:20.297: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 27 10:01:20.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 27 10:01:20.625: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 27 10:01:20.625: INFO: stdout: "iptables"
May 27 10:01:20.625: INFO: proxyMode: iptables
May 27 10:01:20.676: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 27 10:01:20.684: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4980
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4980
I0527 10:01:20.805173      15 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4980, replica count: 3
I0527 10:01:23.857610      15 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0527 10:01:26.858836      15 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 10:01:26.892: INFO: Creating new exec pod
May 27 10:01:29.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May 27 10:01:30.162: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 27 10:01:30.162: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:01:30.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.203 80'
May 27 10:01:30.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.19.203 80\nConnection to 10.233.19.203 80 port [tcp/http] succeeded!\n"
May 27 10:01:30.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:01:30.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.118 31470'
May 27 10:01:30.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.118 31470\nConnection to 192.168.121.118 31470 port [tcp/*] succeeded!\n"
May 27 10:01:30.586: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:01:30.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.5 31470'
May 27 10:01:30.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.5 31470\nConnection to 192.168.121.5 31470 port [tcp/*] succeeded!\n"
May 27 10:01:30.812: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:01:30.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.5:31470/ ; done'
May 27 10:01:31.194: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n"
May 27 10:01:31.194: INFO: stdout: "\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2\naffinity-nodeport-timeout-q4lv2"
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.194: INFO: Received response from host: affinity-nodeport-timeout-q4lv2
May 27 10:01:31.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.5:31470/'
May 27 10:01:31.417: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n"
May 27 10:01:31.417: INFO: stdout: "affinity-nodeport-timeout-q4lv2"
May 27 10:01:51.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.5:31470/'
May 27 10:01:51.685: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n"
May 27 10:01:51.685: INFO: stdout: "affinity-nodeport-timeout-q4lv2"
May 27 10:02:11.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.5:31470/'
May 27 10:02:11.973: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n"
May 27 10:02:11.973: INFO: stdout: "affinity-nodeport-timeout-q4lv2"
May 27 10:02:31.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.5:31470/'
May 27 10:02:32.353: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n"
May 27 10:02:32.353: INFO: stdout: "affinity-nodeport-timeout-q4lv2"
May 27 10:02:52.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-4980 exec execpod-affinitys88pt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.5:31470/'
May 27 10:02:52.601: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.5:31470/\n"
May 27 10:02:52.601: INFO: stdout: "affinity-nodeport-timeout-55l4k"
May 27 10:02:52.601: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4980, will wait for the garbage collector to delete the pods
May 27 10:02:52.708: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.76988ms
May 27 10:02:52.809: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.04273ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:02:54.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4980" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:102.725 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":79,"skipped":1490,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:02:54.832: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:02:54.919: INFO: The status of Pod busybox-readonly-fsbb6b2cee-188e-482d-a169-f7b36a7e606f is Pending, waiting for it to be Running (with Ready = true)
May 27 10:02:56.932: INFO: The status of Pod busybox-readonly-fsbb6b2cee-188e-482d-a169-f7b36a7e606f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:02:56.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-531" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":80,"skipped":1494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:02:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:02:57.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639" in namespace "projected-6423" to be "Succeeded or Failed"
May 27 10:02:57.063: INFO: Pod "downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639": Phase="Pending", Reason="", readiness=false. Elapsed: 12.051389ms
May 27 10:02:59.080: INFO: Pod "downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028949597s
May 27 10:03:01.091: INFO: Pod "downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040492554s
STEP: Saw pod success
May 27 10:03:01.091: INFO: Pod "downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639" satisfied condition "Succeeded or Failed"
May 27 10:03:01.097: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639 container client-container: <nil>
STEP: delete the pod
May 27 10:03:01.126: INFO: Waiting for pod downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639 to disappear
May 27 10:03:01.131: INFO: Pod downwardapi-volume-9181c44b-247f-4571-8420-92665ea68639 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:01.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6423" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1524,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:01.157: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 10:03:01.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:03:01.269: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:03:02.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:03:02.305: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:03:03.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 10:03:03.290: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:03:04.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 10:03:04.290: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
May 27 10:03:04.303: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
May 27 10:03:04.322: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
May 27 10:03:04.326: INFO: Observed &DaemonSet event: ADDED
May 27 10:03:04.327: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.327: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.327: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.328: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.328: INFO: Found daemon set daemon-set in namespace daemonsets-1951 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 10:03:04.328: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
May 27 10:03:04.343: INFO: Observed &DaemonSet event: ADDED
May 27 10:03:04.343: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.344: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.344: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.344: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.344: INFO: Observed daemon set daemon-set in namespace daemonsets-1951 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 10:03:04.344: INFO: Observed &DaemonSet event: MODIFIED
May 27 10:03:04.345: INFO: Found daemon set daemon-set in namespace daemonsets-1951 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May 27 10:03:04.345: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1951, will wait for the garbage collector to delete the pods
May 27 10:03:04.419: INFO: Deleting DaemonSet.extensions daemon-set took: 11.127667ms
May 27 10:03:04.519: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.784343ms
May 27 10:03:06.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:03:06.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 10:03:06.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24012"},"items":null}

May 27 10:03:06.653: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24012"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:06.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1951" for this suite.

• [SLOW TEST:5.552 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":82,"skipped":1526,"failed":0}
SSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:06.709: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 10:03:09.542: INFO: starting watch
STEP: patching
STEP: updating
May 27 10:03:09.564: INFO: waiting for watch events with expected annotations
May 27 10:03:09.564: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:09.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4111" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":83,"skipped":1531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:09.679: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-c5b5c45a-abf2-4fbb-ab80-f2e2a74a7bed
STEP: Creating secret with name s-test-opt-upd-245b6f90-b17c-415d-903a-461f31c2565c
STEP: Creating the pod
May 27 10:03:09.772: INFO: The status of Pod pod-projected-secrets-d21bed27-88d9-4c24-b463-5f7724e504e6 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:03:11.790: INFO: The status of Pod pod-projected-secrets-d21bed27-88d9-4c24-b463-5f7724e504e6 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-c5b5c45a-abf2-4fbb-ab80-f2e2a74a7bed
STEP: Updating secret s-test-opt-upd-245b6f90-b17c-415d-903a-461f31c2565c
STEP: Creating secret with name s-test-opt-create-dbd0df71-c31b-4b75-9492-28a737edfa0b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:13.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4508" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1562,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:13.933: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 10:03:13.988: INFO: Waiting up to 5m0s for pod "downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d" in namespace "downward-api-4490" to be "Succeeded or Failed"
May 27 10:03:13.994: INFO: Pod "downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899091ms
May 27 10:03:16.007: INFO: Pod "downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018956209s
May 27 10:03:18.021: INFO: Pod "downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032740988s
STEP: Saw pod success
May 27 10:03:18.021: INFO: Pod "downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d" satisfied condition "Succeeded or Failed"
May 27 10:03:18.028: INFO: Trying to get logs from node ohp4eith3vui-3 pod downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d container dapi-container: <nil>
STEP: delete the pod
May 27 10:03:18.063: INFO: Waiting for pod downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d to disappear
May 27 10:03:18.069: INFO: Pod downward-api-683e0ec4-0d60-44d5-83bf-ee21f437cc2d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:18.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4490" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":85,"skipped":1565,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:18.086: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
May 27 10:03:18.133: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:45.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4667" for this suite.

• [SLOW TEST:27.261 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":86,"skipped":1570,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:03:45.419: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba" in namespace "downward-api-3223" to be "Succeeded or Failed"
May 27 10:03:45.423: INFO: Pod "downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184039ms
May 27 10:03:47.444: INFO: Pod "downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025671823s
May 27 10:03:49.473: INFO: Pod "downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054603403s
STEP: Saw pod success
May 27 10:03:49.474: INFO: Pod "downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba" satisfied condition "Succeeded or Failed"
May 27 10:03:49.481: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba container client-container: <nil>
STEP: delete the pod
May 27 10:03:49.554: INFO: Waiting for pod downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba to disappear
May 27 10:03:49.559: INFO: Pod downwardapi-volume-a77fd0fb-318b-4e37-aeb0-1512bf9046ba no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:49.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3223" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:49.584: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 10:03:49.703: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 27 10:03:49.711: INFO: starting watch
STEP: patching
STEP: updating
May 27 10:03:49.736: INFO: waiting for watch events with expected annotations
May 27 10:03:49.736: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:49.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4777" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":88,"skipped":1599,"failed":0}

------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:49.801: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:03:53.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-691" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:03:53.949: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:03:53.993: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 27 10:03:59.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-5782 --namespace=crd-publish-openapi-5782 create -f -'
May 27 10:04:00.734: INFO: stderr: ""
May 27 10:04:00.735: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 27 10:04:00.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-5782 --namespace=crd-publish-openapi-5782 delete e2e-test-crd-publish-openapi-9324-crds test-cr'
May 27 10:04:00.881: INFO: stderr: ""
May 27 10:04:00.881: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 27 10:04:00.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-5782 --namespace=crd-publish-openapi-5782 apply -f -'
May 27 10:04:01.194: INFO: stderr: ""
May 27 10:04:01.194: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 27 10:04:01.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-5782 --namespace=crd-publish-openapi-5782 delete e2e-test-crd-publish-openapi-9324-crds test-cr'
May 27 10:04:01.345: INFO: stderr: ""
May 27 10:04:01.345: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 27 10:04:01.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-5782 explain e2e-test-crd-publish-openapi-9324-crds'
May 27 10:04:01.617: INFO: stderr: ""
May 27 10:04:01.617: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9324-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:04:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5782" for this suite.

• [SLOW TEST:13.303 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":90,"skipped":1619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:04:07.277: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:04:07.761: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:04:10.804: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:04:10.817: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6618-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:04:14.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5391" for this suite.
STEP: Destroying namespace "webhook-5391-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.018 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":91,"skipped":1689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:04:14.296: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-782
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-782
STEP: creating replication controller externalsvc in namespace services-782
I0527 10:04:14.509392      15 runners.go:193] Created replication controller with name: externalsvc, namespace: services-782, replica count: 2
I0527 10:04:17.561569      15 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 27 10:04:17.604: INFO: Creating new exec pod
May 27 10:04:19.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-782 exec execpodsfcjr -- /bin/sh -x -c nslookup nodeport-service.services-782.svc.cluster.local'
May 27 10:04:20.076: INFO: stderr: "+ nslookup nodeport-service.services-782.svc.cluster.local\n"
May 27 10:04:20.077: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-782.svc.cluster.local\tcanonical name = externalsvc.services-782.svc.cluster.local.\nName:\texternalsvc.services-782.svc.cluster.local\nAddress: 10.233.9.56\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-782, will wait for the garbage collector to delete the pods
May 27 10:04:20.148: INFO: Deleting ReplicationController externalsvc took: 13.492968ms
May 27 10:04:20.253: INFO: Terminating ReplicationController externalsvc pods took: 104.42775ms
May 27 10:04:21.934: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:04:21.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-782" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.702 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":92,"skipped":1748,"failed":0}
SSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:04:21.999: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:02.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-644" for this suite.

• [SLOW TEST:100.121 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":93,"skipped":1754,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:02.121: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:02.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6413" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":94,"skipped":1766,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:02.294: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
May 27 10:06:04.377: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:10.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3984" for this suite.

• [SLOW TEST:8.390 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":95,"skipped":1781,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:10.684: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 27 10:06:15.328: INFO: Successfully updated pod "adopt-release-2cggr"
STEP: Checking that the Job readopts the Pod
May 27 10:06:15.329: INFO: Waiting up to 15m0s for pod "adopt-release-2cggr" in namespace "job-4300" to be "adopted"
May 27 10:06:15.335: INFO: Pod "adopt-release-2cggr": Phase="Running", Reason="", readiness=true. Elapsed: 5.836335ms
May 27 10:06:17.345: INFO: Pod "adopt-release-2cggr": Phase="Running", Reason="", readiness=true. Elapsed: 2.015805459s
May 27 10:06:17.345: INFO: Pod "adopt-release-2cggr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 27 10:06:17.865: INFO: Successfully updated pod "adopt-release-2cggr"
STEP: Checking that the Job releases the Pod
May 27 10:06:17.866: INFO: Waiting up to 15m0s for pod "adopt-release-2cggr" in namespace "job-4300" to be "released"
May 27 10:06:17.879: INFO: Pod "adopt-release-2cggr": Phase="Running", Reason="", readiness=true. Elapsed: 12.953265ms
May 27 10:06:19.898: INFO: Pod "adopt-release-2cggr": Phase="Running", Reason="", readiness=true. Elapsed: 2.031981034s
May 27 10:06:19.898: INFO: Pod "adopt-release-2cggr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:19.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4300" for this suite.

• [SLOW TEST:9.270 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":96,"skipped":1791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:19.955: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:20.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6440" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":97,"skipped":1815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:20.119: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-e27ed9cd-15b5-4a2a-950c-7918133d3de9
STEP: Creating a pod to test consume configMaps
May 27 10:06:20.186: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7" in namespace "projected-2383" to be "Succeeded or Failed"
May 27 10:06:20.195: INFO: Pod "pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.866099ms
May 27 10:06:22.206: INFO: Pod "pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02019916s
May 27 10:06:24.229: INFO: Pod "pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042844719s
STEP: Saw pod success
May 27 10:06:24.229: INFO: Pod "pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7" satisfied condition "Succeeded or Failed"
May 27 10:06:24.235: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:06:24.288: INFO: Waiting for pod pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7 to disappear
May 27 10:06:24.295: INFO: Pod pod-projected-configmaps-427631ca-4d5d-4b0a-a826-5c859aa685e7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:24.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2383" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":98,"skipped":1845,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:24.324: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:06:26.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5117" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":99,"skipped":1854,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:06:26.452: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 27 10:06:26.498: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 27 10:06:45.253: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:06:49.158: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:07:06.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5507" for this suite.

• [SLOW TEST:40.017 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":100,"skipped":1854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:07:06.470: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5578
STEP: creating service affinity-nodeport in namespace services-5578
STEP: creating replication controller affinity-nodeport in namespace services-5578
I0527 10:07:06.579356      15 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5578, replica count: 3
I0527 10:07:09.637860      15 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 10:07:09.666: INFO: Creating new exec pod
May 27 10:07:12.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5578 exec execpod-affinityvnlm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May 27 10:07:13.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 27 10:07:13.071: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:07:13.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5578 exec execpod-affinityvnlm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.8 80'
May 27 10:07:13.304: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.8 80\nConnection to 10.233.9.8 80 port [tcp/http] succeeded!\n"
May 27 10:07:13.304: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:07:13.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5578 exec execpod-affinityvnlm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.5 30305'
May 27 10:07:13.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.5 30305\nConnection to 192.168.121.5 30305 port [tcp/*] succeeded!\n"
May 27 10:07:13.491: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:07:13.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5578 exec execpod-affinityvnlm5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.192 30305'
May 27 10:07:13.688: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.192 30305\nConnection to 192.168.121.192 30305 port [tcp/*] succeeded!\n"
May 27 10:07:13.689: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:07:13.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5578 exec execpod-affinityvnlm5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.5:30305/ ; done'
May 27 10:07:14.054: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:30305/\n"
May 27 10:07:14.054: INFO: stdout: "\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw\naffinity-nodeport-wgtqw"
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.054: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Received response from host: affinity-nodeport-wgtqw
May 27 10:07:14.055: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5578, will wait for the garbage collector to delete the pods
May 27 10:07:14.168: INFO: Deleting ReplicationController affinity-nodeport took: 12.209219ms
May 27 10:07:14.269: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.963213ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:07:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5578" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.258 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":101,"skipped":1895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:07:16.732: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 27 10:07:16.791: INFO: Waiting up to 5m0s for pod "pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4" in namespace "emptydir-3523" to be "Succeeded or Failed"
May 27 10:07:16.797: INFO: Pod "pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037865ms
May 27 10:07:18.808: INFO: Pod "pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01713943s
May 27 10:07:20.820: INFO: Pod "pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028439142s
STEP: Saw pod success
May 27 10:07:20.820: INFO: Pod "pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4" satisfied condition "Succeeded or Failed"
May 27 10:07:20.825: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4 container test-container: <nil>
STEP: delete the pod
May 27 10:07:20.855: INFO: Waiting for pod pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4 to disappear
May 27 10:07:20.860: INFO: Pod pod-cc42936f-e6ca-4d7e-8ba4-fa929cae7fa4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:07:20.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3523" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":1919,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:07:20.879: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 27 10:07:20.917: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 10:08:20.976: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:08:20.982: INFO: Starting informer...
STEP: Starting pod...
May 27 10:08:21.205: INFO: Pod is running on ohp4eith3vui-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 27 10:08:21.236: INFO: Pod wasn't evicted. Proceeding
May 27 10:08:21.236: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 27 10:09:36.275: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:09:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5915" for this suite.

• [SLOW TEST:135.433 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":103,"skipped":1929,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:09:36.313: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2083
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2083
I0527 10:09:36.434189      15 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2083, replica count: 2
I0527 10:09:39.486266      15 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 10:09:39.486: INFO: Creating new exec pod
May 27 10:09:42.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-2083 exec execpodp88gc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 27 10:09:42.795: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 27 10:09:42.796: INFO: stdout: "externalname-service-g5l6f"
May 27 10:09:42.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-2083 exec execpodp88gc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.238 80'
May 27 10:09:43.091: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.238 80\nConnection to 10.233.2.238 80 port [tcp/http] succeeded!\n"
May 27 10:09:43.091: INFO: stdout: "externalname-service-mqf2r"
May 27 10:09:43.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-2083 exec execpodp88gc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.118 31648'
May 27 10:09:43.301: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.118 31648\nConnection to 192.168.121.118 31648 port [tcp/*] succeeded!\n"
May 27 10:09:43.301: INFO: stdout: "externalname-service-g5l6f"
May 27 10:09:43.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-2083 exec execpodp88gc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.192 31648'
May 27 10:09:43.509: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.192 31648\nConnection to 192.168.121.192 31648 port [tcp/*] succeeded!\n"
May 27 10:09:43.509: INFO: stdout: "externalname-service-g5l6f"
May 27 10:09:43.509: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:09:43.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2083" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.261 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":104,"skipped":1933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:09:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:09:43.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2643" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":105,"skipped":1955,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:09:43.652: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
May 27 10:09:43.699: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 27 10:09:43.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 create -f -'
May 27 10:09:44.248: INFO: stderr: ""
May 27 10:09:44.248: INFO: stdout: "service/agnhost-replica created\n"
May 27 10:09:44.248: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 27 10:09:44.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 create -f -'
May 27 10:09:45.184: INFO: stderr: ""
May 27 10:09:45.184: INFO: stdout: "service/agnhost-primary created\n"
May 27 10:09:45.184: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 27 10:09:45.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 create -f -'
May 27 10:09:45.498: INFO: stderr: ""
May 27 10:09:45.498: INFO: stdout: "service/frontend created\n"
May 27 10:09:45.500: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 27 10:09:45.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 create -f -'
May 27 10:09:45.745: INFO: stderr: ""
May 27 10:09:45.745: INFO: stdout: "deployment.apps/frontend created\n"
May 27 10:09:45.745: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 27 10:09:45.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 create -f -'
May 27 10:09:46.051: INFO: stderr: ""
May 27 10:09:46.051: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 27 10:09:46.052: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 27 10:09:46.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 create -f -'
May 27 10:09:46.746: INFO: stderr: ""
May 27 10:09:46.746: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 27 10:09:46.746: INFO: Waiting for all frontend pods to be Running.
May 27 10:09:51.799: INFO: Waiting for frontend to serve content.
May 27 10:09:51.824: INFO: Trying to add a new entry to the guestbook.
May 27 10:09:51.847: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 27 10:09:51.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 delete --grace-period=0 --force -f -'
May 27 10:09:52.091: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:09:52.092: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 27 10:09:52.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 delete --grace-period=0 --force -f -'
May 27 10:09:52.267: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:09:52.267: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 27 10:09:52.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 delete --grace-period=0 --force -f -'
May 27 10:09:52.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:09:52.467: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 27 10:09:52.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 delete --grace-period=0 --force -f -'
May 27 10:09:52.602: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:09:52.602: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 27 10:09:52.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 delete --grace-period=0 --force -f -'
May 27 10:09:52.814: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:09:52.814: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 27 10:09:52.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2705 delete --grace-period=0 --force -f -'
May 27 10:09:52.983: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:09:52.983: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:09:52.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2705" for this suite.

• [SLOW TEST:9.374 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":106,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:09:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:09:53.136: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a" in namespace "downward-api-8918" to be "Succeeded or Failed"
May 27 10:09:53.142: INFO: Pod "downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.650745ms
May 27 10:09:55.158: INFO: Pod "downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021135803s
May 27 10:09:57.168: INFO: Pod "downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031387463s
STEP: Saw pod success
May 27 10:09:57.168: INFO: Pod "downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a" satisfied condition "Succeeded or Failed"
May 27 10:09:57.175: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a container client-container: <nil>
STEP: delete the pod
May 27 10:09:57.225: INFO: Waiting for pod downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a to disappear
May 27 10:09:57.232: INFO: Pod downwardapi-volume-aa8a6a14-fe06-4d24-9b7d-52cc3739192a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:09:57.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8918" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":1992,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:09:57.256: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
May 27 10:09:57.859: INFO: created pod pod-service-account-defaultsa
May 27 10:09:57.859: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 27 10:09:57.876: INFO: created pod pod-service-account-mountsa
May 27 10:09:57.876: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 27 10:09:57.907: INFO: created pod pod-service-account-nomountsa
May 27 10:09:57.907: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 27 10:09:57.920: INFO: created pod pod-service-account-defaultsa-mountspec
May 27 10:09:57.921: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 27 10:09:57.939: INFO: created pod pod-service-account-mountsa-mountspec
May 27 10:09:57.939: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 27 10:09:57.951: INFO: created pod pod-service-account-nomountsa-mountspec
May 27 10:09:57.951: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 27 10:09:57.962: INFO: created pod pod-service-account-defaultsa-nomountspec
May 27 10:09:57.962: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 27 10:09:57.973: INFO: created pod pod-service-account-mountsa-nomountspec
May 27 10:09:57.973: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 27 10:09:57.993: INFO: created pod pod-service-account-nomountsa-nomountspec
May 27 10:09:57.994: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:09:57.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5279" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":108,"skipped":1997,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:09:58.061: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:10:26.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1119" for this suite.

• [SLOW TEST:28.331 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":109,"skipped":2013,"failed":0}
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:10:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 10:10:26.466: INFO: The status of Pod pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e is Pending, waiting for it to be Running (with Ready = true)
May 27 10:10:28.477: INFO: The status of Pod pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 27 10:10:29.010: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e"
May 27 10:10:29.011: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e" in namespace "pods-2638" to be "terminated due to deadline exceeded"
May 27 10:10:29.017: INFO: Pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e": Phase="Running", Reason="", readiness=true. Elapsed: 6.042556ms
May 27 10:10:31.028: INFO: Pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016849345s
May 27 10:10:33.042: INFO: Pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e": Phase="Running", Reason="", readiness=true. Elapsed: 4.031631282s
May 27 10:10:35.051: INFO: Pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.040240113s
May 27 10:10:35.051: INFO: Pod "pod-update-activedeadlineseconds-cd6a8557-01df-4c84-9d8b-59f35c3b1c4e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:10:35.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2638" for this suite.

• [SLOW TEST:8.692 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:10:35.101: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-34b4740b-ed6c-4119-84de-72e41a3311cb
STEP: Creating a pod to test consume configMaps
May 27 10:10:35.172: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9" in namespace "configmap-7362" to be "Succeeded or Failed"
May 27 10:10:35.182: INFO: Pod "pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.627662ms
May 27 10:10:37.188: INFO: Pod "pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015912537s
May 27 10:10:39.205: INFO: Pod "pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032674559s
May 27 10:10:41.216: INFO: Pod "pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043634616s
STEP: Saw pod success
May 27 10:10:41.216: INFO: Pod "pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9" satisfied condition "Succeeded or Failed"
May 27 10:10:41.223: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:10:41.253: INFO: Waiting for pod pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9 to disappear
May 27 10:10:41.258: INFO: Pod pod-configmaps-8bfb5c8f-a249-4c6c-bbe5-c074e34423e9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:10:41.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7362" for this suite.

• [SLOW TEST:6.182 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:10:41.294: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5732 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5732;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5732 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5732;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5732.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5732.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5732.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5732.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5732.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5732.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5732.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5732.svc;check="$$(dig +notcp +noall +answer +search 187.18.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.18.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.18.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.18.187_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5732 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5732;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5732 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5732;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5732.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5732.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5732.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5732.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5732.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5732.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5732.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5732.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5732.svc;check="$$(dig +notcp +noall +answer +search 187.18.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.18.187_udp@PTR;check="$$(dig +tcp +noall +answer +search 187.18.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.18.187_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 10:10:43.432: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.439: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.468: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.475: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.493: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.500: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.541: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.549: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.557: INFO: Unable to read jessie_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.563: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.568: INFO: Unable to read jessie_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.575: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.581: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.589: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:43.615: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732 wheezy_tcp@dns-test-service.dns-5732 wheezy_udp@dns-test-service.dns-5732.svc wheezy_tcp@dns-test-service.dns-5732.svc wheezy_udp@_http._tcp.dns-test-service.dns-5732.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5732.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5732 jessie_tcp@dns-test-service.dns-5732 jessie_udp@dns-test-service.dns-5732.svc jessie_tcp@dns-test-service.dns-5732.svc jessie_udp@_http._tcp.dns-test-service.dns-5732.svc jessie_tcp@_http._tcp.dns-test-service.dns-5732.svc]

May 27 10:10:48.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.632: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.638: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.644: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.649: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.656: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.697: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.702: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.707: INFO: Unable to read jessie_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.718: INFO: Unable to read jessie_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.723: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.728: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:48.757: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732 wheezy_tcp@dns-test-service.dns-5732 wheezy_udp@dns-test-service.dns-5732.svc wheezy_tcp@dns-test-service.dns-5732.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5732 jessie_tcp@dns-test-service.dns-5732 jessie_udp@dns-test-service.dns-5732.svc jessie_tcp@dns-test-service.dns-5732.svc jessie_udp@_http._tcp.dns-test-service.dns-5732.svc]

May 27 10:10:53.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.632: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.637: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.643: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.648: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.654: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.708: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.714: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.720: INFO: Unable to read jessie_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.728: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.734: INFO: Unable to read jessie_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.739: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:53.781: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732 wheezy_tcp@dns-test-service.dns-5732 wheezy_udp@dns-test-service.dns-5732.svc wheezy_tcp@dns-test-service.dns-5732.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5732 jessie_tcp@dns-test-service.dns-5732 jessie_udp@dns-test-service.dns-5732.svc jessie_tcp@dns-test-service.dns-5732.svc]

May 27 10:10:58.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.631: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.637: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.642: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.648: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.653: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.699: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.704: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.709: INFO: Unable to read jessie_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.715: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.720: INFO: Unable to read jessie_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:10:58.765: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732 wheezy_tcp@dns-test-service.dns-5732 wheezy_udp@dns-test-service.dns-5732.svc wheezy_tcp@dns-test-service.dns-5732.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5732 jessie_tcp@dns-test-service.dns-5732 jessie_udp@dns-test-service.dns-5732.svc jessie_tcp@dns-test-service.dns-5732.svc]

May 27 10:11:03.625: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.631: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.642: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.650: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.655: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.661: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.702: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.707: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.713: INFO: Unable to read jessie_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.724: INFO: Unable to read jessie_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:03.783: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732 wheezy_tcp@dns-test-service.dns-5732 wheezy_udp@dns-test-service.dns-5732.svc wheezy_tcp@dns-test-service.dns-5732.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5732 jessie_tcp@dns-test-service.dns-5732 jessie_udp@dns-test-service.dns-5732.svc jessie_tcp@dns-test-service.dns-5732.svc]

May 27 10:11:08.628: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.634: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.638: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.646: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.659: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.704: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.709: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.714: INFO: Unable to read jessie_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.720: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.725: INFO: Unable to read jessie_udp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-5732.svc from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:08.771: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732 wheezy_tcp@dns-test-service.dns-5732 wheezy_udp@dns-test-service.dns-5732.svc wheezy_tcp@dns-test-service.dns-5732.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5732 jessie_tcp@dns-test-service.dns-5732 jessie_udp@dns-test-service.dns-5732.svc jessie_tcp@dns-test-service.dns-5732.svc]

May 27 10:11:13.626: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:13.633: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:13.638: INFO: Unable to read wheezy_udp@dns-test-service.dns-5732 from pod dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838: the server could not find the requested resource (get pods dns-test-2394be61-f827-4639-b41c-c54546242838)
May 27 10:11:13.752: INFO: Lookups using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5732]

May 27 10:11:18.830: INFO: DNS probes using dns-5732/dns-test-2394be61-f827-4639-b41c-c54546242838 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:11:19.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5732" for this suite.

• [SLOW TEST:37.770 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":112,"skipped":2139,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:11:19.064: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
May 27 10:11:19.133: INFO: Waiting up to 5m0s for pod "var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674" in namespace "var-expansion-8076" to be "Succeeded or Failed"
May 27 10:11:19.158: INFO: Pod "var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674": Phase="Pending", Reason="", readiness=false. Elapsed: 25.283841ms
May 27 10:11:21.169: INFO: Pod "var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036035498s
May 27 10:11:23.181: INFO: Pod "var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04775514s
May 27 10:11:25.192: INFO: Pod "var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059124379s
STEP: Saw pod success
May 27 10:11:25.192: INFO: Pod "var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674" satisfied condition "Succeeded or Failed"
May 27 10:11:25.197: INFO: Trying to get logs from node ohp4eith3vui-3 pod var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674 container dapi-container: <nil>
STEP: delete the pod
May 27 10:11:25.239: INFO: Waiting for pod var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674 to disappear
May 27 10:11:25.243: INFO: Pod var-expansion-8d180c84-b10f-4a0e-b2e7-7c807763e674 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:11:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8076" for this suite.

• [SLOW TEST:6.208 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2140,"failed":0}
SSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:11:25.274: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:11:25.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-385" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":114,"skipped":2145,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:11:25.376: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-113.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-113.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-113.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-113.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 10:11:27.490: INFO: DNS probes using dns-113/dns-test-d30b6325-9c62-4046-8c84-3ce53510d3e9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:11:27.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-113" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":115,"skipped":2158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:11:27.540: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
May 27 10:11:27.602: INFO: Waiting up to 5m0s for pod "pod-a1b45624-2147-4bc5-b6a6-f0ce89141666" in namespace "emptydir-5736" to be "Succeeded or Failed"
May 27 10:11:27.607: INFO: Pod "pod-a1b45624-2147-4bc5-b6a6-f0ce89141666": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493979ms
May 27 10:11:29.619: INFO: Pod "pod-a1b45624-2147-4bc5-b6a6-f0ce89141666": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017474091s
May 27 10:11:31.633: INFO: Pod "pod-a1b45624-2147-4bc5-b6a6-f0ce89141666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031351135s
STEP: Saw pod success
May 27 10:11:31.633: INFO: Pod "pod-a1b45624-2147-4bc5-b6a6-f0ce89141666" satisfied condition "Succeeded or Failed"
May 27 10:11:31.639: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-a1b45624-2147-4bc5-b6a6-f0ce89141666 container test-container: <nil>
STEP: delete the pod
May 27 10:11:31.677: INFO: Waiting for pod pod-a1b45624-2147-4bc5-b6a6-f0ce89141666 to disappear
May 27 10:11:31.683: INFO: Pod pod-a1b45624-2147-4bc5-b6a6-f0ce89141666 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:11:31.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5736" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2247,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:11:31.706: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:11:31.761: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 27 10:11:37.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-3932 --namespace=crd-publish-openapi-3932 create -f -'
May 27 10:11:38.848: INFO: stderr: ""
May 27 10:11:38.848: INFO: stdout: "e2e-test-crd-publish-openapi-2065-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 27 10:11:38.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-3932 --namespace=crd-publish-openapi-3932 delete e2e-test-crd-publish-openapi-2065-crds test-cr'
May 27 10:11:38.965: INFO: stderr: ""
May 27 10:11:38.965: INFO: stdout: "e2e-test-crd-publish-openapi-2065-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 27 10:11:38.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-3932 --namespace=crd-publish-openapi-3932 apply -f -'
May 27 10:11:39.305: INFO: stderr: ""
May 27 10:11:39.305: INFO: stdout: "e2e-test-crd-publish-openapi-2065-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 27 10:11:39.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-3932 --namespace=crd-publish-openapi-3932 delete e2e-test-crd-publish-openapi-2065-crds test-cr'
May 27 10:11:39.535: INFO: stderr: ""
May 27 10:11:39.535: INFO: stdout: "e2e-test-crd-publish-openapi-2065-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 27 10:11:39.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-3932 explain e2e-test-crd-publish-openapi-2065-crds'
May 27 10:11:39.816: INFO: stderr: ""
May 27 10:11:39.816: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2065-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:11:45.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3932" for this suite.

• [SLOW TEST:13.453 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":117,"skipped":2255,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:11:45.162: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2118
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-2118
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2118
May 27 10:11:45.239: INFO: Found 0 stateful pods, waiting for 1
May 27 10:11:55.248: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 27 10:11:55.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:11:55.574: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:11:55.574: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:11:55.575: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:11:55.583: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 27 10:12:05.598: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:12:05.599: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:12:05.630: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 27 10:12:05.631: INFO: ss-0  ohp4eith3vui-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:45 +0000 UTC  }]
May 27 10:12:05.631: INFO: 
May 27 10:12:05.631: INFO: StatefulSet ss has not reached scale 3, at 1
May 27 10:12:06.643: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993826741s
May 27 10:12:07.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981803096s
May 27 10:12:08.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968205575s
May 27 10:12:09.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.956858927s
May 27 10:12:10.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946245397s
May 27 10:12:11.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.931644949s
May 27 10:12:12.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.918928841s
May 27 10:12:13.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.908099538s
May 27 10:12:14.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 896.391293ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2118
May 27 10:12:15.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:12:16.007: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:12:16.007: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:12:16.007: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:12:16.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:12:16.244: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 27 10:12:16.244: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:12:16.244: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:12:16.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:12:16.490: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 27 10:12:16.490: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:12:16.490: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:12:16.501: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:12:16.501: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:12:16.501: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 27 10:12:16.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:12:16.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:12:16.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:12:16.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:12:16.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:12:16.961: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:12:16.962: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:12:16.962: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:12:16.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-2118 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:12:17.151: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:12:17.151: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:12:17.151: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:12:17.151: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:12:17.160: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 27 10:12:27.182: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:12:27.183: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:12:27.184: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:12:27.213: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 27 10:12:27.213: INFO: ss-0  ohp4eith3vui-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:45 +0000 UTC  }]
May 27 10:12:27.213: INFO: ss-1  ohp4eith3vui-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:05 +0000 UTC  }]
May 27 10:12:27.213: INFO: ss-2  ohp4eith3vui-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:05 +0000 UTC  }]
May 27 10:12:27.214: INFO: 
May 27 10:12:27.214: INFO: StatefulSet ss has not reached scale 0, at 3
May 27 10:12:28.226: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 27 10:12:28.226: INFO: ss-0  ohp4eith3vui-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:11:45 +0000 UTC  }]
May 27 10:12:28.226: INFO: ss-1  ohp4eith3vui-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:12:05 +0000 UTC  }]
May 27 10:12:28.226: INFO: 
May 27 10:12:28.226: INFO: StatefulSet ss has not reached scale 0, at 2
May 27 10:12:29.238: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.978649962s
May 27 10:12:30.246: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.96608741s
May 27 10:12:31.263: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95781689s
May 27 10:12:32.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.941474988s
May 27 10:12:33.282: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.931010064s
May 27 10:12:34.294: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.922438236s
May 27 10:12:35.304: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.909849848s
May 27 10:12:36.313: INFO: Verifying statefulset ss doesn't scale past 0 for another 901.03049ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2118
May 27 10:12:37.323: INFO: Scaling statefulset ss to 0
May 27 10:12:37.347: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 10:12:37.352: INFO: Deleting all statefulset in ns statefulset-2118
May 27 10:12:37.357: INFO: Scaling statefulset ss to 0
May 27 10:12:37.373: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:12:37.377: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:12:37.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2118" for this suite.

• [SLOW TEST:52.248 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":118,"skipped":2275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:12:37.416: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7445
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7445
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7445
May 27 10:12:37.498: INFO: Found 0 stateful pods, waiting for 1
May 27 10:12:47.512: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 27 10:12:47.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:12:47.718: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:12:47.718: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:12:47.718: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:12:47.724: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 27 10:12:57.734: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:12:57.734: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:12:57.763: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999358s
May 27 10:12:58.772: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993996123s
May 27 10:12:59.784: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984367426s
May 27 10:13:00.795: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972674979s
May 27 10:13:01.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961963875s
May 27 10:13:02.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.951624674s
May 27 10:13:03.828: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.938876194s
May 27 10:13:04.843: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.929141692s
May 27 10:13:05.858: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912257053s
May 27 10:13:06.869: INFO: Verifying statefulset ss doesn't scale past 1 for another 898.435682ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7445
May 27 10:13:07.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:13:08.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:13:08.152: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:13:08.152: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:13:08.159: INFO: Found 1 stateful pods, waiting for 3
May 27 10:13:18.171: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:13:18.171: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:13:18.171: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 27 10:13:18.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:13:18.408: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:13:18.408: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:13:18.408: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:13:18.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:13:18.716: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:13:18.716: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:13:18.716: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:13:18.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:13:18.933: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:13:18.933: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:13:18.933: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:13:18.933: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:13:18.940: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 27 10:13:28.961: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:13:28.961: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:13:28.961: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 27 10:13:28.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999234s
May 27 10:13:29.996: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992326134s
May 27 10:13:31.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983979006s
May 27 10:13:32.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972394356s
May 27 10:13:33.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.957163119s
May 27 10:13:34.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.947009906s
May 27 10:13:35.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936599591s
May 27 10:13:36.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.927725587s
May 27 10:13:37.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.913416958s
May 27 10:13:38.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 897.140267ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7445
May 27 10:13:39.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:13:39.339: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:13:39.340: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:13:39.340: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:13:39.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:13:39.677: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:13:39.677: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:13:39.677: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:13:39.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-7445 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:13:39.912: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:13:39.913: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:13:39.913: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 10:13:39.913: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 10:13:49.979: INFO: Deleting all statefulset in ns statefulset-7445
May 27 10:13:49.984: INFO: Scaling statefulset ss to 0
May 27 10:13:50.005: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:13:50.010: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:13:50.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7445" for this suite.

• [SLOW TEST:72.649 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":119,"skipped":2308,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:13:50.068: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 27 10:13:50.499: INFO: Pod name wrapped-volume-race-f6bcac7b-213a-4899-86ec-140bd27f7f02: Found 0 pods out of 5
May 27 10:13:55.540: INFO: Pod name wrapped-volume-race-f6bcac7b-213a-4899-86ec-140bd27f7f02: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f6bcac7b-213a-4899-86ec-140bd27f7f02 in namespace emptydir-wrapper-8076, will wait for the garbage collector to delete the pods
May 27 10:14:07.693: INFO: Deleting ReplicationController wrapped-volume-race-f6bcac7b-213a-4899-86ec-140bd27f7f02 took: 22.390503ms
May 27 10:14:07.794: INFO: Terminating ReplicationController wrapped-volume-race-f6bcac7b-213a-4899-86ec-140bd27f7f02 pods took: 101.136869ms
STEP: Creating RC which spawns configmap-volume pods
May 27 10:14:10.731: INFO: Pod name wrapped-volume-race-f5c1b3b9-a804-4c3d-8c29-863f52ca5f10: Found 0 pods out of 5
May 27 10:14:15.746: INFO: Pod name wrapped-volume-race-f5c1b3b9-a804-4c3d-8c29-863f52ca5f10: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f5c1b3b9-a804-4c3d-8c29-863f52ca5f10 in namespace emptydir-wrapper-8076, will wait for the garbage collector to delete the pods
May 27 10:14:27.897: INFO: Deleting ReplicationController wrapped-volume-race-f5c1b3b9-a804-4c3d-8c29-863f52ca5f10 took: 29.467495ms
May 27 10:14:27.997: INFO: Terminating ReplicationController wrapped-volume-race-f5c1b3b9-a804-4c3d-8c29-863f52ca5f10 pods took: 100.414741ms
STEP: Creating RC which spawns configmap-volume pods
May 27 10:14:31.063: INFO: Pod name wrapped-volume-race-d502e664-c995-4ad3-8cc2-946c62aea649: Found 0 pods out of 5
May 27 10:14:36.089: INFO: Pod name wrapped-volume-race-d502e664-c995-4ad3-8cc2-946c62aea649: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d502e664-c995-4ad3-8cc2-946c62aea649 in namespace emptydir-wrapper-8076, will wait for the garbage collector to delete the pods
May 27 10:14:48.212: INFO: Deleting ReplicationController wrapped-volume-race-d502e664-c995-4ad3-8cc2-946c62aea649 took: 13.503942ms
May 27 10:14:48.413: INFO: Terminating ReplicationController wrapped-volume-race-d502e664-c995-4ad3-8cc2-946c62aea649 pods took: 200.822676ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:14:51.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8076" for this suite.

• [SLOW TEST:61.952 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":120,"skipped":2318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:14:52.035: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:14:52.097: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:14:53.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7304" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":121,"skipped":2403,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:14:53.169: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:14:53.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-666" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":122,"skipped":2418,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:14:53.275: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:14:53.331: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 27 10:14:58.345: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 27 10:14:58.345: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 10:14:58.388: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4224  60a8fdc9-3f8e-4611-a7f8-267a1a1ba96a 28607 1 2022-05-27 10:14:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-05-27 10:14:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c61c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 27 10:14:58.394: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:14:58.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4224" for this suite.

• [SLOW TEST:5.150 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":123,"skipped":2429,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:14:58.427: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
May 27 10:14:58.510: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 10:15:03.531: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
May 27 10:15:03.551: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
May 27 10:15:03.590: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
May 27 10:15:03.594: INFO: Observed &ReplicaSet event: ADDED
May 27 10:15:03.594: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.594: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.595: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.595: INFO: Found replicaset test-rs in namespace replicaset-9237 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 10:15:03.595: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
May 27 10:15:03.595: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 27 10:15:03.615: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
May 27 10:15:03.618: INFO: Observed &ReplicaSet event: ADDED
May 27 10:15:03.619: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.619: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.620: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.620: INFO: Observed replicaset test-rs in namespace replicaset-9237 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 10:15:03.620: INFO: Observed &ReplicaSet event: MODIFIED
May 27 10:15:03.620: INFO: Found replicaset test-rs in namespace replicaset-9237 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May 27 10:15:03.620: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:03.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9237" for this suite.

• [SLOW TEST:5.216 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":124,"skipped":2433,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:03.645: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
May 27 10:15:03.717: INFO: Waiting up to 5m0s for pod "client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70" in namespace "containers-1184" to be "Succeeded or Failed"
May 27 10:15:03.724: INFO: Pod "client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.772485ms
May 27 10:15:05.734: INFO: Pod "client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01641645s
May 27 10:15:07.745: INFO: Pod "client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027139737s
STEP: Saw pod success
May 27 10:15:07.745: INFO: Pod "client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70" satisfied condition "Succeeded or Failed"
May 27 10:15:07.753: INFO: Trying to get logs from node ohp4eith3vui-3 pod client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:15:07.805: INFO: Waiting for pod client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70 to disappear
May 27 10:15:07.810: INFO: Pod client-containers-4268ce11-abfc-4d06-953a-fe5a30389c70 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:07.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1184" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2448,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 10:15:07.902: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 10:15:09.919: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 10:15:11.927: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 10:15:11.949: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 10:15:13.960: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 27 10:15:14.015: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 27 10:15:14.021: INFO: Pod pod-with-poststart-http-hook still exists
May 27 10:15:16.022: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 27 10:15:16.032: INFO: Pod pod-with-poststart-http-hook still exists
May 27 10:15:18.022: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 27 10:15:18.031: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:18.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4393" for this suite.

• [SLOW TEST:10.221 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2451,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:18.059: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 10:15:18.114: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:24.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7295" for this suite.

• [SLOW TEST:6.216 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":127,"skipped":2465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:24.278: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:15:24.369: INFO: The status of Pod pod-secrets-9852baab-04ad-4126-8d74-c98ae2385b3f is Pending, waiting for it to be Running (with Ready = true)
May 27 10:15:26.381: INFO: The status of Pod pod-secrets-9852baab-04ad-4126-8d74-c98ae2385b3f is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:26.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9460" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":128,"skipped":2503,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:26.473: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:15:26.558: INFO: The status of Pod server-envvars-cc1e9904-9f91-4a19-9838-81a22e07efd5 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:15:28.567: INFO: The status of Pod server-envvars-cc1e9904-9f91-4a19-9838-81a22e07efd5 is Running (Ready = true)
May 27 10:15:28.601: INFO: Waiting up to 5m0s for pod "client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0" in namespace "pods-9424" to be "Succeeded or Failed"
May 27 10:15:28.613: INFO: Pod "client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.781844ms
May 27 10:15:30.622: INFO: Pod "client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020826803s
May 27 10:15:32.633: INFO: Pod "client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03195402s
STEP: Saw pod success
May 27 10:15:32.633: INFO: Pod "client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0" satisfied condition "Succeeded or Failed"
May 27 10:15:32.638: INFO: Trying to get logs from node ohp4eith3vui-3 pod client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0 container env3cont: <nil>
STEP: delete the pod
May 27 10:15:32.669: INFO: Waiting for pod client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0 to disappear
May 27 10:15:32.675: INFO: Pod client-envvars-e1cceae1-e7c8-41cd-b4aa-38f2053292f0 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9424" for this suite.

• [SLOW TEST:6.221 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2524,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:32.695: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:15:32.756: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-dd7139cd-12f2-437e-a95e-ce82bbb65252" in namespace "security-context-test-6037" to be "Succeeded or Failed"
May 27 10:15:32.761: INFO: Pod "busybox-privileged-false-dd7139cd-12f2-437e-a95e-ce82bbb65252": Phase="Pending", Reason="", readiness=false. Elapsed: 5.549506ms
May 27 10:15:34.779: INFO: Pod "busybox-privileged-false-dd7139cd-12f2-437e-a95e-ce82bbb65252": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022993572s
May 27 10:15:36.799: INFO: Pod "busybox-privileged-false-dd7139cd-12f2-437e-a95e-ce82bbb65252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043555752s
May 27 10:15:36.799: INFO: Pod "busybox-privileged-false-dd7139cd-12f2-437e-a95e-ce82bbb65252" satisfied condition "Succeeded or Failed"
May 27 10:15:36.814: INFO: Got logs for pod "busybox-privileged-false-dd7139cd-12f2-437e-a95e-ce82bbb65252": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:36.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6037" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2531,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:36.840: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-6760/secret-test-fefbe1d2-3dc6-4561-81a9-e18151ff3230
STEP: Creating a pod to test consume secrets
May 27 10:15:36.917: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a" in namespace "secrets-6760" to be "Succeeded or Failed"
May 27 10:15:36.926: INFO: Pod "pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.972855ms
May 27 10:15:38.936: INFO: Pod "pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019377027s
May 27 10:15:40.948: INFO: Pod "pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030579162s
STEP: Saw pod success
May 27 10:15:40.948: INFO: Pod "pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a" satisfied condition "Succeeded or Failed"
May 27 10:15:40.953: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a container env-test: <nil>
STEP: delete the pod
May 27 10:15:40.984: INFO: Waiting for pod pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a to disappear
May 27 10:15:40.990: INFO: Pod pod-configmaps-6e272097-0d01-4c80-9c7f-019eef6d9a9a no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:40.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6760" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2542,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:41.007: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 27 10:15:41.063: INFO: Pod name pod-release: Found 0 pods out of 1
May 27 10:15:46.074: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:47.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8723" for this suite.

• [SLOW TEST:6.148 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":132,"skipped":2546,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:47.158: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
May 27 10:15:47.247: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
May 27 10:15:49.276: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
May 27 10:15:51.305: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:15:53.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9631" for this suite.

• [SLOW TEST:6.184 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":133,"skipped":2559,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:15:53.341: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-4995
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 10:15:53.387: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 10:15:53.469: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:15:55.482: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:15:57.483: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:15:59.482: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:01.480: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:03.478: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:05.479: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:07.482: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:09.487: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:11.478: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:13.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:16:15.478: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 10:16:15.496: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 10:16:15.507: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 10:16:17.556: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 10:16:17.556: INFO: Breadth first check of 10.233.65.193 on host 192.168.121.5...
May 27 10:16:17.561: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.111:9080/dial?request=hostname&protocol=http&host=10.233.65.193&port=8083&tries=1'] Namespace:pod-network-test-4995 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:16:17.561: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:16:17.563: INFO: ExecWithOptions: Clientset creation
May 27 10:16:17.564: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4995/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.193%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 10:16:17.705: INFO: Waiting for responses: map[]
May 27 10:16:17.706: INFO: reached 10.233.65.193 after 0/1 tries
May 27 10:16:17.706: INFO: Breadth first check of 10.233.64.116 on host 192.168.121.118...
May 27 10:16:17.714: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.111:9080/dial?request=hostname&protocol=http&host=10.233.64.116&port=8083&tries=1'] Namespace:pod-network-test-4995 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:16:17.714: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:16:17.715: INFO: ExecWithOptions: Clientset creation
May 27 10:16:17.716: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4995/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.116%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 10:16:17.801: INFO: Waiting for responses: map[]
May 27 10:16:17.801: INFO: reached 10.233.64.116 after 0/1 tries
May 27 10:16:17.801: INFO: Breadth first check of 10.233.66.31 on host 192.168.121.192...
May 27 10:16:17.808: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.111:9080/dial?request=hostname&protocol=http&host=10.233.66.31&port=8083&tries=1'] Namespace:pod-network-test-4995 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:16:17.809: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:16:17.811: INFO: ExecWithOptions: Clientset creation
May 27 10:16:17.811: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4995/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.111%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.31%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 10:16:17.918: INFO: Waiting for responses: map[]
May 27 10:16:17.918: INFO: reached 10.233.66.31 after 0/1 tries
May 27 10:16:17.919: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:17.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4995" for this suite.

• [SLOW TEST:24.603 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":134,"skipped":2560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:17.951: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:16:18.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41" in namespace "projected-2228" to be "Succeeded or Failed"
May 27 10:16:18.029: INFO: Pod "downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41": Phase="Pending", Reason="", readiness=false. Elapsed: 9.141616ms
May 27 10:16:20.036: INFO: Pod "downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015827772s
May 27 10:16:22.047: INFO: Pod "downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027462309s
STEP: Saw pod success
May 27 10:16:22.047: INFO: Pod "downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41" satisfied condition "Succeeded or Failed"
May 27 10:16:22.054: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41 container client-container: <nil>
STEP: delete the pod
May 27 10:16:22.091: INFO: Waiting for pod downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41 to disappear
May 27 10:16:22.096: INFO: Pod downwardapi-volume-76d631c9-2146-49ce-b003-2725752bab41 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:22.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2228" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":135,"skipped":2594,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:22.114: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 10:16:22.189: INFO: Waiting up to 5m0s for pod "downward-api-4635cf41-c9ff-466e-9b05-b6067027767c" in namespace "downward-api-2092" to be "Succeeded or Failed"
May 27 10:16:22.198: INFO: Pod "downward-api-4635cf41-c9ff-466e-9b05-b6067027767c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.8901ms
May 27 10:16:24.215: INFO: Pod "downward-api-4635cf41-c9ff-466e-9b05-b6067027767c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026090831s
May 27 10:16:26.228: INFO: Pod "downward-api-4635cf41-c9ff-466e-9b05-b6067027767c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039022322s
STEP: Saw pod success
May 27 10:16:26.228: INFO: Pod "downward-api-4635cf41-c9ff-466e-9b05-b6067027767c" satisfied condition "Succeeded or Failed"
May 27 10:16:26.233: INFO: Trying to get logs from node ohp4eith3vui-3 pod downward-api-4635cf41-c9ff-466e-9b05-b6067027767c container dapi-container: <nil>
STEP: delete the pod
May 27 10:16:26.268: INFO: Waiting for pod downward-api-4635cf41-c9ff-466e-9b05-b6067027767c to disappear
May 27 10:16:26.274: INFO: Pod downward-api-4635cf41-c9ff-466e-9b05-b6067027767c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:26.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2092" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":136,"skipped":2597,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:26.309: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-6619
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6619 to expose endpoints map[]
May 27 10:16:26.399: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 27 10:16:27.420: INFO: successfully validated that service endpoint-test2 in namespace services-6619 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6619
May 27 10:16:27.439: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:16:29.453: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6619 to expose endpoints map[pod1:[80]]
May 27 10:16:29.477: INFO: successfully validated that service endpoint-test2 in namespace services-6619 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
May 27 10:16:29.478: INFO: Creating new exec pod
May 27 10:16:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6619 exec execpodk5kxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 27 10:16:32.750: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 27 10:16:32.750: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:16:32.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6619 exec execpodk5kxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.10.89 80'
May 27 10:16:32.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.10.89 80\nConnection to 10.233.10.89 80 port [tcp/http] succeeded!\n"
May 27 10:16:32.984: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6619
May 27 10:16:33.001: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:16:35.012: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6619 to expose endpoints map[pod1:[80] pod2:[80]]
May 27 10:16:35.044: INFO: successfully validated that service endpoint-test2 in namespace services-6619 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
May 27 10:16:36.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6619 exec execpodk5kxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 27 10:16:36.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 27 10:16:36.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:16:36.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6619 exec execpodk5kxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.10.89 80'
May 27 10:16:36.527: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.10.89 80\nConnection to 10.233.10.89 80 port [tcp/http] succeeded!\n"
May 27 10:16:36.528: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6619
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6619 to expose endpoints map[pod2:[80]]
May 27 10:16:37.639: INFO: successfully validated that service endpoint-test2 in namespace services-6619 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
May 27 10:16:38.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6619 exec execpodk5kxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 27 10:16:38.865: INFO: stderr: "+ nc -v -t -w+  2 endpoint-test2echo 80 hostName\n\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 27 10:16:38.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:16:38.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6619 exec execpodk5kxk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.10.89 80'
May 27 10:16:39.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.10.89 80\nConnection to 10.233.10.89 80 port [tcp/http] succeeded!\n"
May 27 10:16:39.104: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6619
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6619 to expose endpoints map[]
May 27 10:16:40.198: INFO: successfully validated that service endpoint-test2 in namespace services-6619 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:40.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6619" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.963 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":137,"skipped":2602,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:40.273: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 27 10:16:40.373: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3367  938d39d9-520a-4dc9-837e-e3df673706c2 29556 0 2022-05-27 10:16:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 10:16:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:16:40.376: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3367  938d39d9-520a-4dc9-837e-e3df673706c2 29558 0 2022-05-27 10:16:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 10:16:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:16:40.376: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3367  938d39d9-520a-4dc9-837e-e3df673706c2 29559 0 2022-05-27 10:16:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 10:16:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 27 10:16:50.435: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3367  938d39d9-520a-4dc9-837e-e3df673706c2 29606 0 2022-05-27 10:16:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 10:16:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:16:50.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3367  938d39d9-520a-4dc9-837e-e3df673706c2 29607 0 2022-05-27 10:16:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 10:16:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:16:50.436: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3367  938d39d9-520a-4dc9-837e-e3df673706c2 29608 0 2022-05-27 10:16:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 10:16:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:50.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3367" for this suite.

• [SLOW TEST:10.192 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":138,"skipped":2612,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:50.466: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 27 10:16:50.529: INFO: Waiting up to 5m0s for pod "pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7" in namespace "emptydir-6424" to be "Succeeded or Failed"
May 27 10:16:50.540: INFO: Pod "pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.08166ms
May 27 10:16:52.553: INFO: Pod "pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023727178s
May 27 10:16:54.569: INFO: Pod "pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039740209s
STEP: Saw pod success
May 27 10:16:54.569: INFO: Pod "pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7" satisfied condition "Succeeded or Failed"
May 27 10:16:54.575: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7 container test-container: <nil>
STEP: delete the pod
May 27 10:16:54.605: INFO: Waiting for pod pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7 to disappear
May 27 10:16:54.612: INFO: Pod pod-3361251a-5ffe-45d0-adf8-2ecc32675dc7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:54.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6424" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":139,"skipped":2621,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:54.629: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-f64deeba-c618-4baf-b3ba-f827436082ae
STEP: Creating the pod
May 27 10:16:54.724: INFO: The status of Pod pod-configmaps-f9c64b72-c6b6-4559-8a13-385977b38ba3 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:16:56.733: INFO: The status of Pod pod-configmaps-f9c64b72-c6b6-4559-8a13-385977b38ba3 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-f64deeba-c618-4baf-b3ba-f827436082ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:58.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8693" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":140,"skipped":2621,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:58.852: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:16:58.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-8730 version'
May 27 10:16:59.008: INFO: stderr: ""
May 27 10:16:59.008: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.7\", GitCommit:\"42c05a547468804b2053ecf60a3bd15560362fc2\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:30:55Z\", GoVersion:\"go1.17.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.7\", GitCommit:\"42c05a547468804b2053ecf60a3bd15560362fc2\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:24:41Z\", GoVersion:\"go1.17.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8730" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":141,"skipped":2629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:59.032: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:16:59.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8832" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":142,"skipped":2653,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:16:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:16:59.860: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:17:02.908: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:17:03.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5459" for this suite.
STEP: Destroying namespace "webhook-5459-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":143,"skipped":2666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:17:03.215: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:17:03.280: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 27 10:17:03.300: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 10:17:08.316: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 27 10:17:08.317: INFO: Creating deployment "test-rolling-update-deployment"
May 27 10:17:08.327: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 27 10:17:08.339: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 27 10:17:10.356: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 27 10:17:10.363: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 10:17:10.386: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-556  1ffbcbff-d2e9-44b8-a6e0-8c39661c7df0 29868 1 2022-05-27 10:17:08 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-05-27 10:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027c0288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 10:17:08 +0000 UTC,LastTransitionTime:2022-05-27 10:17:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-05-27 10:17:09 +0000 UTC,LastTransitionTime:2022-05-27 10:17:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 10:17:10.398: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-556  316d89ac-b3b6-4939-81f5-88b441e8ad6d 29858 1 2022-05-27 10:17:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1ffbcbff-d2e9-44b8-a6e0-8c39661c7df0 0xc0027c0757 0xc0027c0758}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ffbcbff-d2e9-44b8-a6e0-8c39661c7df0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:17:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027c0808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 10:17:10.398: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 27 10:17:10.398: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-556  5232d641-5e05-4081-b821-53bb603bb889 29867 2 2022-05-27 10:17:03 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1ffbcbff-d2e9-44b8-a6e0-8c39661c7df0 0xc0027c0627 0xc0027c0628}] []  [{e2e.test Update apps/v1 2022-05-27 10:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ffbcbff-d2e9-44b8-a6e0-8c39661c7df0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:17:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027c06e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 10:17:10.407: INFO: Pod "test-rolling-update-deployment-796dbc4547-lpxxw" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-lpxxw test-rolling-update-deployment-796dbc4547- deployment-556  28506e99-3df8-4fc8-b042-21ea07e93e1b 29857 0 2022-05-27 10:17:08 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 316d89ac-b3b6-4939-81f5-88b441e8ad6d 0xc0045b7147 0xc0045b7148}] []  [{kube-controller-manager Update v1 2022-05-27 10:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"316d89ac-b3b6-4939-81f5-88b441e8ad6d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 10:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgbhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgbhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:17:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:17:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.118,StartTime:2022-05-27 10:17:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 10:17:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:cri-o://361ed6298ebfe1d9ff817496ce19ab149f0888a5b696d1b6449fd9a5d1b68d2a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:17:10.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-556" for this suite.

• [SLOW TEST:7.219 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":144,"skipped":2697,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:17:10.437: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 10:17:14.577: INFO: DNS probes using dns-9618/dns-test-de93fd3a-274a-4eb3-90df-21227cf90361 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:17:14.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9618" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":145,"skipped":2703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:17:14.631: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:17:43.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4233" for this suite.

• [SLOW TEST:28.588 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2746,"failed":0}
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:17:43.220: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
May 27 10:17:43.310: INFO: Found Service test-service-824pw in namespace services-8144 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 27 10:17:43.311: INFO: Service test-service-824pw created
STEP: Getting /status
May 27 10:17:43.319: INFO: Service test-service-824pw has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
May 27 10:17:43.337: INFO: observed Service test-service-824pw in namespace services-8144 with annotations: map[] & LoadBalancer: {[]}
May 27 10:17:43.338: INFO: Found Service test-service-824pw in namespace services-8144 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 27 10:17:43.338: INFO: Service test-service-824pw has service status patched
STEP: updating the ServiceStatus
May 27 10:17:43.355: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
May 27 10:17:43.359: INFO: Observed Service test-service-824pw in namespace services-8144 with annotations: map[] & Conditions: {[]}
May 27 10:17:43.359: INFO: Observed event: &Service{ObjectMeta:{test-service-824pw  services-8144  efa91868-dbfd-4f00-a295-41602a891258 30080 0 2022-05-27 10:17:43 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-05-27 10:17:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-05-27 10:17:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.55.243,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.55.243],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 27 10:17:43.359: INFO: Found Service test-service-824pw in namespace services-8144 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 10:17:43.360: INFO: Service test-service-824pw has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
May 27 10:17:43.386: INFO: observed Service test-service-824pw in namespace services-8144 with labels: map[test-service-static:true]
May 27 10:17:43.386: INFO: observed Service test-service-824pw in namespace services-8144 with labels: map[test-service-static:true]
May 27 10:17:43.386: INFO: observed Service test-service-824pw in namespace services-8144 with labels: map[test-service-static:true]
May 27 10:17:43.387: INFO: Found Service test-service-824pw in namespace services-8144 with labels: map[test-service:patched test-service-static:true]
May 27 10:17:43.387: INFO: Service test-service-824pw patched
STEP: deleting the service
STEP: watching for the Service to be deleted
May 27 10:17:43.416: INFO: Observed event: ADDED
May 27 10:17:43.417: INFO: Observed event: MODIFIED
May 27 10:17:43.417: INFO: Observed event: MODIFIED
May 27 10:17:43.417: INFO: Observed event: MODIFIED
May 27 10:17:43.417: INFO: Found Service test-service-824pw in namespace services-8144 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 27 10:17:43.417: INFO: Service test-service-824pw deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:17:43.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8144" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":147,"skipped":2746,"failed":0}

------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:17:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:22:43.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2126" for this suite.

• [SLOW TEST:300.141 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":148,"skipped":2746,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:22:43.660: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:22:44.569: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:22:47.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:22:59.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1126" for this suite.
STEP: Destroying namespace "webhook-1126-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.384 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":149,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:23:00.048: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
May 27 10:23:02.171: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:23:04.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1869" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":150,"skipped":2774,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:23:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:23:05.345: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:23:08.392: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:23:08.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9179" for this suite.
STEP: Destroying namespace "webhook-9179-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":151,"skipped":2780,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:23:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
May 27 10:23:08.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-4526 create -f -'
May 27 10:23:10.815: INFO: stderr: ""
May 27 10:23:10.815: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 27 10:23:10.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-4526 diff -f -'
May 27 10:23:11.184: INFO: rc: 1
May 27 10:23:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-4526 delete -f -'
May 27 10:23:11.431: INFO: stderr: ""
May 27 10:23:11.431: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:23:11.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4526" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":152,"skipped":2796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:23:11.489: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
May 27 10:23:11.582: INFO: The status of Pod pod-hostip-3aa3fb73-7743-4171-9e37-58108e7ea778 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:23:13.603: INFO: The status of Pod pod-hostip-3aa3fb73-7743-4171-9e37-58108e7ea778 is Running (Ready = true)
May 27 10:23:13.615: INFO: Pod pod-hostip-3aa3fb73-7743-4171-9e37-58108e7ea778 has hostIP: 192.168.121.192
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:23:13.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-863" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:23:13.634: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 10:23:13.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1991 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
May 27 10:23:13.816: INFO: stderr: ""
May 27 10:23:13.816: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
May 27 10:23:13.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1991 delete pods e2e-test-httpd-pod'
May 27 10:23:16.093: INFO: stderr: ""
May 27 10:23:16.093: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:23:16.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1991" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":154,"skipped":2880,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:23:16.125: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 10:23:16.202: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 10:24:16.263: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
May 27 10:24:16.321: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 27 10:24:16.331: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 27 10:24:16.363: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 27 10:24:16.393: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 27 10:24:16.459: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 27 10:24:16.470: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:24:26.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9721" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:70.591 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":155,"skipped":2912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:24:26.724: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-e8b9afae-7ba0-4fb5-90de-b747cfe72c52
STEP: Creating a pod to test consume secrets
May 27 10:24:26.814: INFO: Waiting up to 5m0s for pod "pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731" in namespace "secrets-7047" to be "Succeeded or Failed"
May 27 10:24:26.827: INFO: Pod "pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731": Phase="Pending", Reason="", readiness=false. Elapsed: 12.138407ms
May 27 10:24:28.834: INFO: Pod "pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019717044s
May 27 10:24:30.847: INFO: Pod "pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032239131s
STEP: Saw pod success
May 27 10:24:30.847: INFO: Pod "pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731" satisfied condition "Succeeded or Failed"
May 27 10:24:30.854: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731 container secret-volume-test: <nil>
STEP: delete the pod
May 27 10:24:30.910: INFO: Waiting for pod pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731 to disappear
May 27 10:24:30.916: INFO: Pod pod-secrets-d8d82bb3-ca84-4cd4-bda8-1cc8cf561731 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:24:30.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7047" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":2983,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:24:30.938: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 10:24:31.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5640 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 27 10:24:31.137: INFO: stderr: ""
May 27 10:24:31.138: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 27 10:24:31.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5640 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May 27 10:24:31.686: INFO: stderr: ""
May 27 10:24:31.686: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 10:24:31.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5640 delete pods e2e-test-httpd-pod'
May 27 10:24:34.382: INFO: stderr: ""
May 27 10:24:34.382: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:24:34.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5640" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":157,"skipped":2988,"failed":0}
SSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:24:34.409: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
May 27 10:26:35.015: INFO: Successfully updated pod "var-expansion-12f933d3-ded6-4393-9317-62d9ba51e4c7"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 27 10:26:37.037: INFO: Deleting pod "var-expansion-12f933d3-ded6-4393-9317-62d9ba51e4c7" in namespace "var-expansion-7599"
May 27 10:26:37.055: INFO: Wait up to 5m0s for pod "var-expansion-12f933d3-ded6-4393-9317-62d9ba51e4c7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:09.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7599" for this suite.

• [SLOW TEST:154.694 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":158,"skipped":2995,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:09.107: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-953a21a6-2a79-4583-9411-d253c1854edd
STEP: Creating a pod to test consume secrets
May 27 10:27:09.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040" in namespace "projected-1238" to be "Succeeded or Failed"
May 27 10:27:09.207: INFO: Pod "pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040": Phase="Pending", Reason="", readiness=false. Elapsed: 4.444123ms
May 27 10:27:11.216: INFO: Pod "pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0138969s
May 27 10:27:13.230: INFO: Pod "pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027711365s
STEP: Saw pod success
May 27 10:27:13.230: INFO: Pod "pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040" satisfied condition "Succeeded or Failed"
May 27 10:27:13.235: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040 container secret-volume-test: <nil>
STEP: delete the pod
May 27 10:27:13.275: INFO: Waiting for pod pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040 to disappear
May 27 10:27:13.281: INFO: Pod pod-projected-secrets-de8282ed-9f78-467f-9b89-926ad5780040 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1238" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":159,"skipped":3003,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 10:27:13.404: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 10:27:15.416: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 10:27:15.439: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 10:27:17.450: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 27 10:27:17.477: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 27 10:27:17.484: INFO: Pod pod-with-prestop-http-hook still exists
May 27 10:27:19.485: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 27 10:27:19.497: INFO: Pod pod-with-prestop-http-hook still exists
May 27 10:27:21.484: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 27 10:27:21.499: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2596" for this suite.

• [SLOW TEST:8.222 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":3077,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:21.531: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-881b4508-94cd-4df0-b386-971b0b3d5d47
STEP: Creating a pod to test consume secrets
May 27 10:27:21.602: INFO: Waiting up to 5m0s for pod "pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2" in namespace "secrets-7083" to be "Succeeded or Failed"
May 27 10:27:21.608: INFO: Pod "pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512174ms
May 27 10:27:23.622: INFO: Pod "pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020241729s
May 27 10:27:25.632: INFO: Pod "pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03041805s
STEP: Saw pod success
May 27 10:27:25.633: INFO: Pod "pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2" satisfied condition "Succeeded or Failed"
May 27 10:27:25.638: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2 container secret-env-test: <nil>
STEP: delete the pod
May 27 10:27:25.673: INFO: Waiting for pod pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2 to disappear
May 27 10:27:25.679: INFO: Pod pod-secrets-e6d2957b-f4a6-4915-9d01-05f1467a21d2 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:25.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7083" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":3081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:25.708: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
May 27 10:27:25.767: INFO: Waiting up to 5m0s for pod "test-pod-1a314c8c-8832-4456-828a-6c93be86f42f" in namespace "svcaccounts-6861" to be "Succeeded or Failed"
May 27 10:27:25.773: INFO: Pod "test-pod-1a314c8c-8832-4456-828a-6c93be86f42f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082789ms
May 27 10:27:27.782: INFO: Pod "test-pod-1a314c8c-8832-4456-828a-6c93be86f42f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015293429s
May 27 10:27:29.792: INFO: Pod "test-pod-1a314c8c-8832-4456-828a-6c93be86f42f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025468242s
STEP: Saw pod success
May 27 10:27:29.792: INFO: Pod "test-pod-1a314c8c-8832-4456-828a-6c93be86f42f" satisfied condition "Succeeded or Failed"
May 27 10:27:29.804: INFO: Trying to get logs from node ohp4eith3vui-3 pod test-pod-1a314c8c-8832-4456-828a-6c93be86f42f container agnhost-container: <nil>
STEP: delete the pod
May 27 10:27:29.842: INFO: Waiting for pod test-pod-1a314c8c-8832-4456-828a-6c93be86f42f to disappear
May 27 10:27:29.845: INFO: Pod test-pod-1a314c8c-8832-4456-828a-6c93be86f42f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:29.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6861" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":162,"skipped":3126,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:29.859: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
May 27 10:27:29.944: INFO: Waiting up to 5m0s for pod "pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811" in namespace "emptydir-5399" to be "Succeeded or Failed"
May 27 10:27:29.962: INFO: Pod "pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811": Phase="Pending", Reason="", readiness=false. Elapsed: 18.692588ms
May 27 10:27:31.979: INFO: Pod "pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811": Phase="Running", Reason="", readiness=false. Elapsed: 2.035768559s
May 27 10:27:33.992: INFO: Pod "pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811": Phase="Running", Reason="", readiness=false. Elapsed: 4.047969041s
May 27 10:27:36.005: INFO: Pod "pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06131833s
STEP: Saw pod success
May 27 10:27:36.005: INFO: Pod "pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811" satisfied condition "Succeeded or Failed"
May 27 10:27:36.011: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811 container test-container: <nil>
STEP: delete the pod
May 27 10:27:36.045: INFO: Waiting for pod pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811 to disappear
May 27 10:27:36.050: INFO: Pod pod-b4c1b8ef-8ce0-40a8-b18b-e0c39a36f811 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:36.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5399" for this suite.

• [SLOW TEST:6.208 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":163,"skipped":3129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:36.077: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:27:36.126: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 27 10:27:38.199: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-11" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":164,"skipped":3216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:39.238: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
May 27 10:27:39.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-8824 api-versions'
May 27 10:27:39.405: INFO: stderr: ""
May 27 10:27:39.405: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:39.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8824" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":165,"skipped":3238,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:39.433: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:42.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3767" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":166,"skipped":3259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:42.349: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:27:42.412: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1" in namespace "downward-api-6786" to be "Succeeded or Failed"
May 27 10:27:42.419: INFO: Pod "downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.136967ms
May 27 10:27:44.433: INFO: Pod "downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020676318s
May 27 10:27:46.448: INFO: Pod "downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035190431s
STEP: Saw pod success
May 27 10:27:46.448: INFO: Pod "downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1" satisfied condition "Succeeded or Failed"
May 27 10:27:46.453: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1 container client-container: <nil>
STEP: delete the pod
May 27 10:27:46.488: INFO: Waiting for pod downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1 to disappear
May 27 10:27:46.494: INFO: Pod downwardapi-volume-f6251cc3-bc92-4af5-87e2-3742b8a321a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:46.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6786" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":167,"skipped":3289,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:46.518: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 10:27:46.573: INFO: Waiting up to 5m0s for pod "downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc" in namespace "downward-api-1465" to be "Succeeded or Failed"
May 27 10:27:46.586: INFO: Pod "downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.890552ms
May 27 10:27:48.598: INFO: Pod "downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02411365s
May 27 10:27:50.613: INFO: Pod "downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038825348s
STEP: Saw pod success
May 27 10:27:50.613: INFO: Pod "downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc" satisfied condition "Succeeded or Failed"
May 27 10:27:50.619: INFO: Trying to get logs from node ohp4eith3vui-3 pod downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc container dapi-container: <nil>
STEP: delete the pod
May 27 10:27:50.675: INFO: Waiting for pod downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc to disappear
May 27 10:27:50.681: INFO: Pod downward-api-26eb2656-7e22-444a-8b84-3301e9b491dc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:50.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1465" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":3302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:50.708: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:27:50.802: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 10:27:55.812: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
May 27 10:27:55.830: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
May 27 10:27:55.854: INFO: observed ReplicaSet test-rs in namespace replicaset-6076 with ReadyReplicas 1, AvailableReplicas 1
May 27 10:27:55.969: INFO: observed ReplicaSet test-rs in namespace replicaset-6076 with ReadyReplicas 1, AvailableReplicas 1
May 27 10:27:56.007: INFO: observed ReplicaSet test-rs in namespace replicaset-6076 with ReadyReplicas 1, AvailableReplicas 1
May 27 10:27:56.062: INFO: observed ReplicaSet test-rs in namespace replicaset-6076 with ReadyReplicas 1, AvailableReplicas 1
May 27 10:27:57.663: INFO: observed ReplicaSet test-rs in namespace replicaset-6076 with ReadyReplicas 2, AvailableReplicas 2
May 27 10:27:57.966: INFO: observed Replicaset test-rs in namespace replicaset-6076 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:27:57.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6076" for this suite.

• [SLOW TEST:7.280 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":169,"skipped":3348,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:27:57.989: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-9jdf
STEP: Creating a pod to test atomic-volume-subpath
May 27 10:27:58.053: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9jdf" in namespace "subpath-9346" to be "Succeeded or Failed"
May 27 10:27:58.058: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.090824ms
May 27 10:28:00.068: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015235972s
May 27 10:28:02.083: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 4.02941296s
May 27 10:28:04.096: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 6.042629771s
May 27 10:28:06.102: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 8.048973275s
May 27 10:28:08.114: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 10.060670287s
May 27 10:28:10.123: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 12.0695194s
May 27 10:28:12.137: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 14.08427461s
May 27 10:28:14.154: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 16.100447921s
May 27 10:28:16.165: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 18.112103352s
May 27 10:28:18.176: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=true. Elapsed: 20.123056892s
May 27 10:28:20.185: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Running", Reason="", readiness=false. Elapsed: 22.132236858s
May 27 10:28:22.200: INFO: Pod "pod-subpath-test-projected-9jdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.146743934s
STEP: Saw pod success
May 27 10:28:22.200: INFO: Pod "pod-subpath-test-projected-9jdf" satisfied condition "Succeeded or Failed"
May 27 10:28:22.208: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-subpath-test-projected-9jdf container test-container-subpath-projected-9jdf: <nil>
STEP: delete the pod
May 27 10:28:22.248: INFO: Waiting for pod pod-subpath-test-projected-9jdf to disappear
May 27 10:28:22.253: INFO: Pod pod-subpath-test-projected-9jdf no longer exists
STEP: Deleting pod pod-subpath-test-projected-9jdf
May 27 10:28:22.253: INFO: Deleting pod "pod-subpath-test-projected-9jdf" in namespace "subpath-9346"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:22.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9346" for this suite.

• [SLOW TEST:24.292 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":170,"skipped":3358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:22.287: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:22.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3377" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":171,"skipped":3401,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:22.422: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-6692
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6692 to expose endpoints map[]
May 27 10:28:22.499: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 27 10:28:23.522: INFO: successfully validated that service multi-endpoint-test in namespace services-6692 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6692
May 27 10:28:23.550: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:28:25.559: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6692 to expose endpoints map[pod1:[100]]
May 27 10:28:25.586: INFO: successfully validated that service multi-endpoint-test in namespace services-6692 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6692
May 27 10:28:25.603: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:28:27.611: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6692 to expose endpoints map[pod1:[100] pod2:[101]]
May 27 10:28:27.639: INFO: successfully validated that service multi-endpoint-test in namespace services-6692 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
May 27 10:28:27.639: INFO: Creating new exec pod
May 27 10:28:30.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6692 exec execpodbzffs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 27 10:28:30.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May 27 10:28:30.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:28:30.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6692 exec execpodbzffs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.46.242 80'
May 27 10:28:31.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.46.242 80\nConnection to 10.233.46.242 80 port [tcp/http] succeeded!\n"
May 27 10:28:31.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:28:31.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6692 exec execpodbzffs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May 27 10:28:31.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May 27 10:28:31.340: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:28:31.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-6692 exec execpodbzffs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.46.242 81'
May 27 10:28:31.547: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.46.242 81\nConnection to 10.233.46.242 81 port [tcp/*] succeeded!\n"
May 27 10:28:31.548: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6692
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6692 to expose endpoints map[pod2:[101]]
May 27 10:28:31.633: INFO: successfully validated that service multi-endpoint-test in namespace services-6692 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6692
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6692 to expose endpoints map[]
May 27 10:28:32.689: INFO: successfully validated that service multi-endpoint-test in namespace services-6692 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:32.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6692" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.339 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":172,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 27 10:28:32.832: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2522  f4ab9ef1-82f2-4238-9604-b75f15d41958 32747 0 2022-05-27 10:28:32 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-05-27 10:28:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wm9r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wm9r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 10:28:32.843: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 27 10:28:34.855: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 27 10:28:34.855: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2522 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:28:34.855: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:28:34.857: INFO: ExecWithOptions: Clientset creation
May 27 10:28:34.857: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2522/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
May 27 10:28:34.959: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2522 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:28:34.959: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:28:34.964: INFO: ExecWithOptions: Clientset creation
May 27 10:28:34.964: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2522/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 10:28:35.075: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:35.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2522" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":173,"skipped":3465,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:35.124: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:28:35.175: INFO: Creating deployment "test-recreate-deployment"
May 27 10:28:35.186: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 27 10:28:35.201: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 27 10:28:37.225: INFO: Waiting deployment "test-recreate-deployment" to complete
May 27 10:28:37.243: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 27 10:28:37.280: INFO: Updating deployment test-recreate-deployment
May 27 10:28:37.280: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 10:28:37.487: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5451  17f03189-3b5b-4b06-ac3f-150548dbdba9 32825 2 2022-05-27 10:28:35 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007394188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-27 10:28:37 +0000 UTC,LastTransitionTime:2022-05-27 10:28:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-05-27 10:28:37 +0000 UTC,LastTransitionTime:2022-05-27 10:28:35 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 27 10:28:37.493: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-5451  82329904-3ef0-4058-afc8-169586bbc380 32823 1 2022-05-27 10:28:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 17f03189-3b5b-4b06-ac3f-150548dbdba9 0xc007394527 0xc007394528}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17f03189-3b5b-4b06-ac3f-150548dbdba9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073945c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 10:28:37.493: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 27 10:28:37.494: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-5451  bcf29192-8f03-4ec6-b62f-8fb19f3a8e54 32812 2 2022-05-27 10:28:35 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 17f03189-3b5b-4b06-ac3f-150548dbdba9 0xc007394637 0xc007394638}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:28:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17f03189-3b5b-4b06-ac3f-150548dbdba9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073946e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 10:28:37.503: INFO: Pod "test-recreate-deployment-5b99bd5487-rjl9f" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-rjl9f test-recreate-deployment-5b99bd5487- deployment-5451  efd7edd9-b712-4bf3-ae3b-cbfbd7a94e0b 32824 0 2022-05-27 10:28:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 82329904-3ef0-4058-afc8-169586bbc380 0xc00734dfa7 0xc00734dfa8}] []  [{kube-controller-manager Update v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82329904-3ef0-4058-afc8-169586bbc380\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 10:28:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdbgv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdbgv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:28:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:28:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:28:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:28:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 10:28:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:37.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5451" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":174,"skipped":3468,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:37.527: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 27 10:28:37.604: INFO: observed Pod pod-test in namespace pods-8434 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 27 10:28:37.612: INFO: observed Pod pod-test in namespace pods-8434 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC  }]
May 27 10:28:37.647: INFO: observed Pod pod-test in namespace pods-8434 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC  }]
May 27 10:28:39.279: INFO: Found Pod pod-test in namespace pods-8434 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 10:28:37 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 27 10:28:39.321: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 27 10:28:39.393: INFO: observed event type ADDED
May 27 10:28:39.393: INFO: observed event type MODIFIED
May 27 10:28:39.393: INFO: observed event type MODIFIED
May 27 10:28:39.393: INFO: observed event type MODIFIED
May 27 10:28:39.393: INFO: observed event type MODIFIED
May 27 10:28:39.393: INFO: observed event type MODIFIED
May 27 10:28:39.393: INFO: observed event type MODIFIED
May 27 10:28:41.301: INFO: observed event type MODIFIED
May 27 10:28:42.315: INFO: observed event type MODIFIED
May 27 10:28:42.329: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:42.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8434" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":175,"skipped":3474,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:42.364: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 27 10:28:42.449: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3698  4abb9b06-743f-4ab2-bad0-369f083cb759 32900 0 2022-05-27 10:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 10:28:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:28:42.449: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3698  4abb9b06-743f-4ab2-bad0-369f083cb759 32902 0 2022-05-27 10:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 10:28:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 27 10:28:42.476: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3698  4abb9b06-743f-4ab2-bad0-369f083cb759 32903 0 2022-05-27 10:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 10:28:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:28:42.476: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3698  4abb9b06-743f-4ab2-bad0-369f083cb759 32904 0 2022-05-27 10:28:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 10:28:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:42.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3698" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":176,"skipped":3487,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:42.499: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-9b06deae-69e6-4d6a-88ba-8a93fae2ab62
STEP: Creating a pod to test consume secrets
May 27 10:28:42.573: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848" in namespace "projected-2736" to be "Succeeded or Failed"
May 27 10:28:42.578: INFO: Pod "pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180096ms
May 27 10:28:44.590: INFO: Pod "pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017086816s
May 27 10:28:46.601: INFO: Pod "pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027926635s
STEP: Saw pod success
May 27 10:28:46.601: INFO: Pod "pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848" satisfied condition "Succeeded or Failed"
May 27 10:28:46.607: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 10:28:46.634: INFO: Waiting for pod pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848 to disappear
May 27 10:28:46.638: INFO: Pod pod-projected-secrets-109f7a54-ef4d-4572-bcbd-65b29d1d7848 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:46.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2736" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:46.659: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-b003db1a-36f5-4a18-b26e-d30ca5893337
STEP: Creating a pod to test consume configMaps
May 27 10:28:46.735: INFO: Waiting up to 5m0s for pod "pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651" in namespace "configmap-5125" to be "Succeeded or Failed"
May 27 10:28:46.750: INFO: Pod "pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651": Phase="Pending", Reason="", readiness=false. Elapsed: 14.445302ms
May 27 10:28:48.758: INFO: Pod "pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022584182s
May 27 10:28:50.767: INFO: Pod "pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031897291s
STEP: Saw pod success
May 27 10:28:50.768: INFO: Pod "pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651" satisfied condition "Succeeded or Failed"
May 27 10:28:50.774: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:28:50.848: INFO: Waiting for pod pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651 to disappear
May 27 10:28:50.854: INFO: Pod pod-configmaps-602e48ec-8782-4201-8cf0-68e90c0a7651 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:28:50.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5125" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":178,"skipped":3517,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:28:50.875: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5280
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
May 27 10:28:50.967: INFO: Found 0 stateful pods, waiting for 3
May 27 10:29:00.983: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:29:00.984: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:29:00.984: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:29:01.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-5280 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:29:01.318: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:29:01.318: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:29:01.318: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May 27 10:29:11.389: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 27 10:29:21.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-5280 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:29:21.648: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:29:21.648: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:29:21.648: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
May 27 10:29:31.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-5280 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 10:29:31.928: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 10:29:31.928: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 10:29:31.928: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 10:29:42.004: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 27 10:29:52.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=statefulset-5280 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 10:29:52.418: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 10:29:52.418: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 10:29:52.418: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 10:30:02.481: INFO: Deleting all statefulset in ns statefulset-5280
May 27 10:30:02.489: INFO: Scaling statefulset ss2 to 0
May 27 10:30:12.533: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:30:12.544: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:30:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5280" for this suite.

• [SLOW TEST:81.714 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":179,"skipped":3537,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:30:12.589: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 27 10:30:12.660: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.660: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.681: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.681: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.719: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.719: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.835: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:12.835: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 10:30:14.486: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 27 10:30:14.486: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 27 10:30:14.655: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 27 10:30:14.684: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.687: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 0
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.688: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.715: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.715: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.765: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.765: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:14.825: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:14.825: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:14.862: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:14.863: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:16.699: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:16.699: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:16.833: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
STEP: listing Deployments
May 27 10:30:16.849: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 27 10:30:16.871: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 27 10:30:16.889: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:16.903: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:16.985: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:17.022: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:17.047: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:18.524: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:18.672: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:18.787: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:18.880: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 27 10:30:20.542: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 27 10:30:20.645: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:20.645: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:20.645: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:20.646: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:20.646: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 1
May 27 10:30:20.647: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:20.647: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 3
May 27 10:30:20.648: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:20.648: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 2
May 27 10:30:20.648: INFO: observed Deployment test-deployment in namespace deployment-9063 with ReadyReplicas 3
STEP: deleting the Deployment
May 27 10:30:20.665: INFO: observed event type MODIFIED
May 27 10:30:20.666: INFO: observed event type MODIFIED
May 27 10:30:20.667: INFO: observed event type MODIFIED
May 27 10:30:20.667: INFO: observed event type MODIFIED
May 27 10:30:20.668: INFO: observed event type MODIFIED
May 27 10:30:20.668: INFO: observed event type MODIFIED
May 27 10:30:20.668: INFO: observed event type MODIFIED
May 27 10:30:20.668: INFO: observed event type MODIFIED
May 27 10:30:20.669: INFO: observed event type MODIFIED
May 27 10:30:20.669: INFO: observed event type MODIFIED
May 27 10:30:20.669: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 10:30:20.685: INFO: Log out all the ReplicaSets if there is no deployment created
May 27 10:30:20.696: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-9063  10138f62-2c5d-4141-b287-86e08bc395b0 33796 4 2022-05-27 10:30:14 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment fb3d8ec9-a120-4b0d-9887-998f7d37dc79 0xc005c58aa7 0xc005c58aa8}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:30:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb3d8ec9-a120-4b0d-9887-998f7d37dc79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:30:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c58b30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 27 10:30:20.703: INFO: pod: "test-deployment-5ddd8b47d8-mfpnm":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-mfpnm test-deployment-5ddd8b47d8- deployment-9063  bf56550f-2862-402f-a7db-80cf14d20dac 33792 0 2022-05-27 10:30:16 +0000 UTC 2022-05-27 10:30:21 +0000 UTC 0xc004c39de8 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 10138f62-2c5d-4141-b287-86e08bc395b0 0xc004c39e17 0xc004c39e18}] []  [{kube-controller-manager Update v1 2022-05-27 10:30:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10138f62-2c5d-4141-b287-86e08bc395b0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 10:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6855g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6855g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.65.61,StartTime:2022-05-27 10:30:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 10:30:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:cri-o://7aa7de64620ab109d4c9fd507bfc6da075971c6afd481aff85362021f076a901,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 27 10:30:20.704: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-9063  6232b9e3-713c-4782-ac03-7bda3861f7f5 33643 3 2022-05-27 10:30:12 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment fb3d8ec9-a120-4b0d-9887-998f7d37dc79 0xc005c58b97 0xc005c58b98}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:30:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb3d8ec9-a120-4b0d-9887-998f7d37dc79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:30:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c58c40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 27 10:30:20.714: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-9063  6c7315a7-1671-4b5c-8d5a-131b62041dc0 33788 2 2022-05-27 10:30:16 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment fb3d8ec9-a120-4b0d-9887-998f7d37dc79 0xc005c58ca7 0xc005c58ca8}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:30:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb3d8ec9-a120-4b0d-9887-998f7d37dc79\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:30:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c58d30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 27 10:30:20.736: INFO: pod: "test-deployment-854fdc678-jdp46":
&Pod{ObjectMeta:{test-deployment-854fdc678-jdp46 test-deployment-854fdc678- deployment-9063  e54836dd-b6d6-4cbb-a414-adaa2af6d564 33740 0 2022-05-27 10:30:16 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 6c7315a7-1671-4b5c-8d5a-131b62041dc0 0xc005e44a37 0xc005e44a38}] []  [{kube-controller-manager Update v1 2022-05-27 10:30:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7315a7-1671-4b5c-8d5a-131b62041dc0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 10:30:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7tvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7tvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.202,StartTime:2022-05-27 10:30:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 10:30:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://326d53f370309d85471a200374e7a356b1e0ef31029e3504c8d6334e11023446,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 27 10:30:20.736: INFO: pod: "test-deployment-854fdc678-zg8q6":
&Pod{ObjectMeta:{test-deployment-854fdc678-zg8q6 test-deployment-854fdc678- deployment-9063  549c7f59-2b69-40e8-89a0-2f5f80076a6d 33787 0 2022-05-27 10:30:18 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 6c7315a7-1671-4b5c-8d5a-131b62041dc0 0xc005e44c27 0xc005e44c28}] []  [{kube-controller-manager Update v1 2022-05-27 10:30:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c7315a7-1671-4b5c-8d5a-131b62041dc0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 10:30:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xdhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xdhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:30:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.65.85,StartTime:2022-05-27 10:30:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 10:30:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c2d6caa4135ec23d08793ab369033a40e12090d54102b987e1367444061c241d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:30:20.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9063" for this suite.

• [SLOW TEST:8.203 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":180,"skipped":3540,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:30:20.792: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 27 10:30:20.843: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
May 27 10:30:21.307: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 27 10:30:23.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:25.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:27.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:29.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:31.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:33.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:35.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 30, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:30:43.494: INFO: Waited 6.053205099s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
May 27 10:30:43.639: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:30:44.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9140" for this suite.

• [SLOW TEST:23.367 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":181,"skipped":3548,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:30:44.161: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:00.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9997" for this suite.

• [SLOW TEST:76.196 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":182,"skipped":3566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:00.367: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-09804693-9811-48df-a8f4-243673b0c23a
STEP: Creating a pod to test consume secrets
May 27 10:32:00.454: INFO: Waiting up to 5m0s for pod "pod-secrets-8666e896-7288-4459-9e1b-265358b66016" in namespace "secrets-6937" to be "Succeeded or Failed"
May 27 10:32:00.461: INFO: Pod "pod-secrets-8666e896-7288-4459-9e1b-265358b66016": Phase="Pending", Reason="", readiness=false. Elapsed: 6.926599ms
May 27 10:32:02.472: INFO: Pod "pod-secrets-8666e896-7288-4459-9e1b-265358b66016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018732295s
May 27 10:32:04.500: INFO: Pod "pod-secrets-8666e896-7288-4459-9e1b-265358b66016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04648424s
STEP: Saw pod success
May 27 10:32:04.500: INFO: Pod "pod-secrets-8666e896-7288-4459-9e1b-265358b66016" satisfied condition "Succeeded or Failed"
May 27 10:32:04.512: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-8666e896-7288-4459-9e1b-265358b66016 container secret-volume-test: <nil>
STEP: delete the pod
May 27 10:32:04.571: INFO: Waiting for pod pod-secrets-8666e896-7288-4459-9e1b-265358b66016 to disappear
May 27 10:32:04.577: INFO: Pod pod-secrets-8666e896-7288-4459-9e1b-265358b66016 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:04.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6937" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3611,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:04.598: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:04.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-937" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":184,"skipped":3621,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:04.727: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
May 27 10:32:04.788: INFO: Waiting up to 5m0s for pod "pod-75a31853-f7f1-4b76-b4dd-edbffec47aef" in namespace "emptydir-8146" to be "Succeeded or Failed"
May 27 10:32:04.793: INFO: Pod "pod-75a31853-f7f1-4b76-b4dd-edbffec47aef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295225ms
May 27 10:32:06.810: INFO: Pod "pod-75a31853-f7f1-4b76-b4dd-edbffec47aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0217856s
May 27 10:32:08.820: INFO: Pod "pod-75a31853-f7f1-4b76-b4dd-edbffec47aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032590535s
STEP: Saw pod success
May 27 10:32:08.821: INFO: Pod "pod-75a31853-f7f1-4b76-b4dd-edbffec47aef" satisfied condition "Succeeded or Failed"
May 27 10:32:08.827: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-75a31853-f7f1-4b76-b4dd-edbffec47aef container test-container: <nil>
STEP: delete the pod
May 27 10:32:08.860: INFO: Waiting for pod pod-75a31853-f7f1-4b76-b4dd-edbffec47aef to disappear
May 27 10:32:08.865: INFO: Pod pod-75a31853-f7f1-4b76-b4dd-edbffec47aef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:08.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8146" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:08.892: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 27 10:32:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:32:16.198: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:33.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5091" for this suite.

• [SLOW TEST:24.526 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":186,"skipped":3672,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:33.418: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-d1770475-33ec-4e5f-afdf-a714306a66a3
STEP: Creating a pod to test consume configMaps
May 27 10:32:33.491: INFO: Waiting up to 5m0s for pod "pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5" in namespace "configmap-3890" to be "Succeeded or Failed"
May 27 10:32:33.496: INFO: Pod "pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144779ms
May 27 10:32:35.511: INFO: Pod "pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020072655s
May 27 10:32:37.528: INFO: Pod "pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037620067s
STEP: Saw pod success
May 27 10:32:37.529: INFO: Pod "pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5" satisfied condition "Succeeded or Failed"
May 27 10:32:37.534: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:32:37.591: INFO: Waiting for pod pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5 to disappear
May 27 10:32:37.597: INFO: Pod pod-configmaps-c993d6b1-4919-45f6-9d85-afbc805fd7a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:37.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3890" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3672,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
May 27 10:32:37.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 create -f -'
May 27 10:32:38.914: INFO: stderr: ""
May 27 10:32:38.914: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 10:32:38.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 10:32:39.092: INFO: stderr: ""
May 27 10:32:39.093: INFO: stdout: "update-demo-nautilus-6dkd8 update-demo-nautilus-blkp2 "
May 27 10:32:39.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods update-demo-nautilus-6dkd8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 10:32:39.223: INFO: stderr: ""
May 27 10:32:39.223: INFO: stdout: ""
May 27 10:32:39.223: INFO: update-demo-nautilus-6dkd8 is created but not running
May 27 10:32:44.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 10:32:44.403: INFO: stderr: ""
May 27 10:32:44.403: INFO: stdout: "update-demo-nautilus-6dkd8 update-demo-nautilus-blkp2 "
May 27 10:32:44.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods update-demo-nautilus-6dkd8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 10:32:44.536: INFO: stderr: ""
May 27 10:32:44.536: INFO: stdout: "true"
May 27 10:32:44.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods update-demo-nautilus-6dkd8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 10:32:44.668: INFO: stderr: ""
May 27 10:32:44.668: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 10:32:44.668: INFO: validating pod update-demo-nautilus-6dkd8
May 27 10:32:44.716: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 10:32:44.716: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 10:32:44.716: INFO: update-demo-nautilus-6dkd8 is verified up and running
May 27 10:32:44.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods update-demo-nautilus-blkp2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 10:32:44.842: INFO: stderr: ""
May 27 10:32:44.842: INFO: stdout: "true"
May 27 10:32:44.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods update-demo-nautilus-blkp2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 10:32:44.961: INFO: stderr: ""
May 27 10:32:44.961: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 10:32:44.961: INFO: validating pod update-demo-nautilus-blkp2
May 27 10:32:44.980: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 10:32:44.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 10:32:44.980: INFO: update-demo-nautilus-blkp2 is verified up and running
STEP: using delete to clean up resources
May 27 10:32:44.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 delete --grace-period=0 --force -f -'
May 27 10:32:45.107: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 10:32:45.107: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 27 10:32:45.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get rc,svc -l name=update-demo --no-headers'
May 27 10:32:45.279: INFO: stderr: "No resources found in kubectl-5102 namespace.\n"
May 27 10:32:45.280: INFO: stdout: ""
May 27 10:32:45.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-5102 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 27 10:32:45.504: INFO: stderr: ""
May 27 10:32:45.504: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:32:45.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5102" for this suite.

• [SLOW TEST:7.917 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":188,"skipped":3677,"failed":0}
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:32:45.534: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:32:45.617: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 27 10:32:50.655: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 27 10:32:50.656: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 27 10:32:52.666: INFO: Creating deployment "test-rollover-deployment"
May 27 10:32:52.683: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 27 10:32:54.709: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 27 10:32:54.727: INFO: Ensure that both replica sets have 1 created replica
May 27 10:32:54.738: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 27 10:32:54.753: INFO: Updating deployment test-rollover-deployment
May 27 10:32:54.754: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 27 10:32:56.793: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 27 10:32:56.804: INFO: Make sure deployment "test-rollover-deployment" is complete
May 27 10:32:56.819: INFO: all replica sets need to contain the pod-template-hash label
May 27 10:32:56.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:32:58.841: INFO: all replica sets need to contain the pod-template-hash label
May 27 10:32:58.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:33:00.835: INFO: all replica sets need to contain the pod-template-hash label
May 27 10:33:00.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:33:02.836: INFO: all replica sets need to contain the pod-template-hash label
May 27 10:33:02.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:33:04.843: INFO: all replica sets need to contain the pod-template-hash label
May 27 10:33:04.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 32, 56, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 32, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 10:33:06.846: INFO: 
May 27 10:33:06.846: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 10:33:06.871: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9393  1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6 34645 2 2022-05-27 10:32:52 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-27 10:32:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0ca18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 10:32:52 +0000 UTC,LastTransitionTime:2022-05-27 10:32:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-05-27 10:33:06 +0000 UTC,LastTransitionTime:2022-05-27 10:32:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 10:33:06.876: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-9393  a540a38f-2cfb-4689-93b5-9c4b8dea020c 34634 2 2022-05-27 10:32:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6 0xc004e0cf17 0xc004e0cf18}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:32:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:33:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0d028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 10:33:06.876: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 27 10:33:06.876: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9393  bebfa7a6-ab87-435a-aa34-9a31b0ba8f44 34643 2 2022-05-27 10:32:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6 0xc004e0cde7 0xc004e0cde8}] []  [{e2e.test Update apps/v1 2022-05-27 10:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:33:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:33:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e0cea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 10:33:06.877: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-9393  0123ec41-9818-4de8-ac53-4ae2c978fd88 34592 2 2022-05-27 10:32:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6 0xc004e0d097 0xc004e0d098}] []  [{kube-controller-manager Update apps/v1 2022-05-27 10:32:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b7d2b53-0a70-4f15-aa9e-e1c00d6df5f6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 10:32:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0d168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 10:33:06.886: INFO: Pod "test-rollover-deployment-668b7f667d-b2dxt" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-b2dxt test-rollover-deployment-668b7f667d- deployment-9393  9698da2a-4b33-437c-867f-b258a2ee0b32 34611 0 2022-05-27 10:32:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d a540a38f-2cfb-4689-93b5-9c4b8dea020c 0xc004e35087 0xc004e35088}] []  [{kube-controller-manager Update v1 2022-05-27 10:32:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a540a38f-2cfb-4689-93b5-9c4b8dea020c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 10:32:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zqqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zqqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:32:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:32:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:32:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 10:32:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.59,StartTime:2022-05-27 10:32:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 10:32:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:cri-o://c08222ffe76ffdd8c4db8c71cf7eadc8c74a989c8facaabf30fc8e44dadec20d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:33:06.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9393" for this suite.

• [SLOW TEST:21.376 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":189,"skipped":3677,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:33:06.911: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:33:13.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2173" for this suite.

• [SLOW TEST:7.111 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":190,"skipped":3687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:33:14.034: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 27 10:33:14.090: INFO: Waiting up to 5m0s for pod "pod-db08f4eb-77cd-4988-b528-a2d96589a9cd" in namespace "emptydir-2909" to be "Succeeded or Failed"
May 27 10:33:14.096: INFO: Pod "pod-db08f4eb-77cd-4988-b528-a2d96589a9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069942ms
May 27 10:33:16.110: INFO: Pod "pod-db08f4eb-77cd-4988-b528-a2d96589a9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019918679s
May 27 10:33:18.125: INFO: Pod "pod-db08f4eb-77cd-4988-b528-a2d96589a9cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034660492s
STEP: Saw pod success
May 27 10:33:18.125: INFO: Pod "pod-db08f4eb-77cd-4988-b528-a2d96589a9cd" satisfied condition "Succeeded or Failed"
May 27 10:33:18.132: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-db08f4eb-77cd-4988-b528-a2d96589a9cd container test-container: <nil>
STEP: delete the pod
May 27 10:33:18.166: INFO: Waiting for pod pod-db08f4eb-77cd-4988-b528-a2d96589a9cd to disappear
May 27 10:33:18.170: INFO: Pod pod-db08f4eb-77cd-4988-b528-a2d96589a9cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:33:18.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2909" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3733,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:33:18.193: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:33:18.250: INFO: created pod
May 27 10:33:18.250: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9160" to be "Succeeded or Failed"
May 27 10:33:18.263: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.502983ms
May 27 10:33:20.277: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.026894342s
May 27 10:33:22.291: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.040588656s
May 27 10:33:24.306: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055847478s
STEP: Saw pod success
May 27 10:33:24.307: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 27 10:33:54.307: INFO: polling logs
May 27 10:33:54.323: INFO: Pod logs: 
2022/05/27 10:33:19 OK: Got token
2022/05/27 10:33:19 validating with in-cluster discovery
2022/05/27 10:33:19 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/05/27 10:33:19 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9160:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1653648198, NotBefore:1653647598, IssuedAt:1653647598, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9160", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"931e3e3b-54dd-491a-872b-4876c2e430a2"}}}
2022/05/27 10:33:19 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/05/27 10:33:19 OK: Validated signature on JWT
2022/05/27 10:33:19 OK: Got valid claims from token!
2022/05/27 10:33:19 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9160:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1653648198, NotBefore:1653647598, IssuedAt:1653647598, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9160", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"931e3e3b-54dd-491a-872b-4876c2e430a2"}}}

May 27 10:33:54.323: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:33:54.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9160" for this suite.

• [SLOW TEST:36.174 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":192,"skipped":3734,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:33:54.367: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 10:33:54.439: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 10:34:54.509: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:34:54.515: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:34:54.591: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 27 10:34:54.597: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:34:54.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8128" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:34:54.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5844" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.438 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":193,"skipped":3747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:34:54.811: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-7d784303-188d-4077-8ebc-bc0b8b07d3e1 in namespace container-probe-5245
May 27 10:34:56.901: INFO: Started pod busybox-7d784303-188d-4077-8ebc-bc0b8b07d3e1 in namespace container-probe-5245
STEP: checking the pod's current state and verifying that restartCount is present
May 27 10:34:56.907: INFO: Initial restart count of pod busybox-7d784303-188d-4077-8ebc-bc0b8b07d3e1 is 0
May 27 10:35:47.277: INFO: Restart count of pod container-probe-5245/busybox-7d784303-188d-4077-8ebc-bc0b8b07d3e1 is now 1 (50.370064097s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:35:47.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5245" for this suite.

• [SLOW TEST:52.507 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:35:47.321: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:36:47.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2916" for this suite.

• [SLOW TEST:60.134 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":195,"skipped":3822,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:36:47.460: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-754f4483-22f6-49f1-9ff0-91877c4cae21
STEP: Creating a pod to test consume configMaps
May 27 10:36:47.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb" in namespace "projected-9194" to be "Succeeded or Failed"
May 27 10:36:47.556: INFO: Pod "pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.097591ms
May 27 10:36:49.571: INFO: Pod "pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018300829s
May 27 10:36:51.584: INFO: Pod "pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031322689s
STEP: Saw pod success
May 27 10:36:51.584: INFO: Pod "pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb" satisfied condition "Succeeded or Failed"
May 27 10:36:51.594: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb container agnhost-container: <nil>
STEP: delete the pod
May 27 10:36:51.645: INFO: Waiting for pod pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb to disappear
May 27 10:36:51.651: INFO: Pod pod-projected-configmaps-ccd880c3-2248-4a84-aba5-807a34cf7ddb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:36:51.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9194" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:36:51.683: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db in namespace container-probe-6000
May 27 10:36:53.765: INFO: Started pod liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db in namespace container-probe-6000
STEP: checking the pod's current state and verifying that restartCount is present
May 27 10:36:53.770: INFO: Initial restart count of pod liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db is 0
May 27 10:37:13.913: INFO: Restart count of pod container-probe-6000/liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db is now 1 (20.142575221s elapsed)
May 27 10:37:34.084: INFO: Restart count of pod container-probe-6000/liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db is now 2 (40.313531808s elapsed)
May 27 10:37:54.242: INFO: Restart count of pod container-probe-6000/liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db is now 3 (1m0.472226066s elapsed)
May 27 10:38:12.390: INFO: Restart count of pod container-probe-6000/liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db is now 4 (1m18.619920739s elapsed)
May 27 10:39:12.826: INFO: Restart count of pod container-probe-6000/liveness-4a69a6dd-37db-4dd6-8ec9-da53b86ab7db is now 5 (2m19.055913565s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:12.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6000" for this suite.

• [SLOW TEST:141.202 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":197,"skipped":3957,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:12.888: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:39:14.272: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:39:17.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:39:17.322: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5851-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:20.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8425" for this suite.
STEP: Destroying namespace "webhook-8425-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.861 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":198,"skipped":3972,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:20.751: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-zvw6
STEP: Creating a pod to test atomic-volume-subpath
May 27 10:39:20.976: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zvw6" in namespace "subpath-8867" to be "Succeeded or Failed"
May 27 10:39:20.986: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.591259ms
May 27 10:39:23.000: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 2.024105745s
May 27 10:39:25.013: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 4.036855236s
May 27 10:39:27.026: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 6.049475598s
May 27 10:39:29.040: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 8.063591615s
May 27 10:39:31.054: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 10.077893903s
May 27 10:39:33.066: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 12.089870612s
May 27 10:39:35.084: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 14.107801065s
May 27 10:39:37.095: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 16.118585858s
May 27 10:39:39.110: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 18.133378427s
May 27 10:39:41.123: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=true. Elapsed: 20.1471122s
May 27 10:39:43.163: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Running", Reason="", readiness=false. Elapsed: 22.186444764s
May 27 10:39:45.178: INFO: Pod "pod-subpath-test-configmap-zvw6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.20147815s
STEP: Saw pod success
May 27 10:39:45.178: INFO: Pod "pod-subpath-test-configmap-zvw6" satisfied condition "Succeeded or Failed"
May 27 10:39:45.186: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-subpath-test-configmap-zvw6 container test-container-subpath-configmap-zvw6: <nil>
STEP: delete the pod
May 27 10:39:45.243: INFO: Waiting for pod pod-subpath-test-configmap-zvw6 to disappear
May 27 10:39:45.249: INFO: Pod pod-subpath-test-configmap-zvw6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zvw6
May 27 10:39:45.249: INFO: Deleting pod "pod-subpath-test-configmap-zvw6" in namespace "subpath-8867"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:45.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8867" for this suite.

• [SLOW TEST:24.532 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":199,"skipped":3978,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:45.286: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 10:39:49.435: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:49.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8602" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3981,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:49.479: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
May 27 10:39:50.814: INFO: The status of Pod kube-controller-manager-ohp4eith3vui-2 is Running (Ready = true)
May 27 10:39:50.932: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:50.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7351" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":201,"skipped":3983,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:50.960: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:39:51.042: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5" in namespace "downward-api-4359" to be "Succeeded or Failed"
May 27 10:39:51.052: INFO: Pod "downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.179653ms
May 27 10:39:53.070: INFO: Pod "downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027560628s
May 27 10:39:55.094: INFO: Pod "downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051703985s
STEP: Saw pod success
May 27 10:39:55.094: INFO: Pod "downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5" satisfied condition "Succeeded or Failed"
May 27 10:39:55.106: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5 container client-container: <nil>
STEP: delete the pod
May 27 10:39:55.144: INFO: Waiting for pod downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5 to disappear
May 27 10:39:55.155: INFO: Pod downwardapi-volume-7bda4955-670d-4cc6-80fa-50a20fc8d8e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:55.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4359" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":202,"skipped":3999,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:55.183: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:39:55.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57" in namespace "downward-api-712" to be "Succeeded or Failed"
May 27 10:39:55.291: INFO: Pod "downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57": Phase="Pending", Reason="", readiness=false. Elapsed: 15.214325ms
May 27 10:39:57.308: INFO: Pod "downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032158661s
May 27 10:39:59.324: INFO: Pod "downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048127293s
STEP: Saw pod success
May 27 10:39:59.324: INFO: Pod "downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57" satisfied condition "Succeeded or Failed"
May 27 10:39:59.330: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57 container client-container: <nil>
STEP: delete the pod
May 27 10:39:59.362: INFO: Waiting for pod downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57 to disappear
May 27 10:39:59.375: INFO: Pod downwardapi-volume-ac8c07bd-82de-4ce3-b511-749c78662b57 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:39:59.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-712" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":203,"skipped":4017,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:39:59.416: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 27 10:39:59.527: INFO: Waiting up to 5m0s for pod "security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263" in namespace "security-context-5491" to be "Succeeded or Failed"
May 27 10:39:59.537: INFO: Pod "security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263": Phase="Pending", Reason="", readiness=false. Elapsed: 9.955564ms
May 27 10:40:01.552: INFO: Pod "security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024792944s
May 27 10:40:03.572: INFO: Pod "security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044618556s
STEP: Saw pod success
May 27 10:40:03.572: INFO: Pod "security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263" satisfied condition "Succeeded or Failed"
May 27 10:40:03.578: INFO: Trying to get logs from node ohp4eith3vui-3 pod security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263 container test-container: <nil>
STEP: delete the pod
May 27 10:40:03.613: INFO: Waiting for pod security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263 to disappear
May 27 10:40:03.619: INFO: Pod security-context-6a9c8cb0-6fd5-4fa5-918c-5df3380bf263 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:40:03.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5491" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":204,"skipped":4018,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:40:03.639: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:40:05.203: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:40:08.256: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:40:08.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2448" for this suite.
STEP: Destroying namespace "webhook-2448-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":205,"skipped":4031,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:40:08.477: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-cfd70736-94bf-45a2-b243-5d755276882e
STEP: Creating configMap with name cm-test-opt-upd-37f549f2-cd95-4627-b1e2-27a5a0690280
STEP: Creating the pod
May 27 10:40:08.600: INFO: The status of Pod pod-projected-configmaps-dc701ff0-7232-40df-8297-ab7be58e0e9f is Pending, waiting for it to be Running (with Ready = true)
May 27 10:40:10.617: INFO: The status of Pod pod-projected-configmaps-dc701ff0-7232-40df-8297-ab7be58e0e9f is Pending, waiting for it to be Running (with Ready = true)
May 27 10:40:12.617: INFO: The status of Pod pod-projected-configmaps-dc701ff0-7232-40df-8297-ab7be58e0e9f is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-cfd70736-94bf-45a2-b243-5d755276882e
STEP: Updating configmap cm-test-opt-upd-37f549f2-cd95-4627-b1e2-27a5a0690280
STEP: Creating configMap with name cm-test-opt-create-951e2402-a608-43d0-a1e4-31a707d01344
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:41:31.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8345" for this suite.

• [SLOW TEST:83.008 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":4053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:41:31.486: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
May 27 10:41:31.620: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:41:33.638: INFO: The status of Pod pod1 is Running (Ready = false)
May 27 10:41:35.638: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.192 on the node which pod1 resides and expect scheduled
May 27 10:41:35.659: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:41:37.673: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.192 but use UDP protocol on the node which pod2 resides
May 27 10:41:37.691: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:41:39.707: INFO: The status of Pod pod3 is Running (Ready = true)
May 27 10:41:39.724: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 27 10:41:41.739: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
May 27 10:41:41.745: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.192 http://127.0.0.1:54323/hostname] Namespace:hostport-3279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:41:41.745: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:41:41.749: INFO: ExecWithOptions: Clientset creation
May 27 10:41:41.749: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.192+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.192, port: 54323
May 27 10:41:41.911: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.192:54323/hostname] Namespace:hostport-3279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:41:41.911: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:41:41.913: INFO: ExecWithOptions: Clientset creation
May 27 10:41:41.914: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.192%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.192, port: 54323 UDP
May 27 10:41:41.997: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.121.192 54323] Namespace:hostport-3279 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:41:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:41:41.999: INFO: ExecWithOptions: Clientset creation
May 27 10:41:41.999: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3279/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+192.168.121.192+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:41:47.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3279" for this suite.

• [SLOW TEST:15.649 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":207,"skipped":4095,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:41:47.137: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:41:47.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2435" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":208,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:41:47.286: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 27 10:41:47.980: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:41:51.051: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:41:51.065: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:41:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1123" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.294 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":209,"skipped":4156,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:41:55.580: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:42:01.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9276" for this suite.
STEP: Destroying namespace "nsdeletetest-982" for this suite.
May 27 10:42:01.855: INFO: Namespace nsdeletetest-982 was already deleted
STEP: Destroying namespace "nsdeletetest-7704" for this suite.

• [SLOW TEST:6.290 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":210,"skipped":4157,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:42:01.873: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2411
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-2411
May 27 10:42:01.999: INFO: Found 0 stateful pods, waiting for 1
May 27 10:42:12.021: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
May 27 10:42:12.066: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
May 27 10:42:12.083: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
May 27 10:42:12.090: INFO: Observed &StatefulSet event: ADDED
May 27 10:42:12.090: INFO: Found Statefulset ss in namespace statefulset-2411 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 10:42:12.090: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
May 27 10:42:12.090: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 27 10:42:12.102: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
May 27 10:42:12.106: INFO: Observed &StatefulSet event: ADDED
May 27 10:42:12.106: INFO: Observed Statefulset ss in namespace statefulset-2411 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 10:42:12.106: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 10:42:12.106: INFO: Deleting all statefulset in ns statefulset-2411
May 27 10:42:12.113: INFO: Scaling statefulset ss to 0
May 27 10:42:22.161: INFO: Waiting for statefulset status.replicas updated to 0
May 27 10:42:22.167: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:42:22.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2411" for this suite.

• [SLOW TEST:20.345 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":211,"skipped":4167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:42:22.220: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
May 27 10:42:39.305: INFO: 87 pods remaining
May 27 10:42:39.305: INFO: 69 pods has nil DeletionTimestamp
May 27 10:42:39.305: INFO: 
May 27 10:42:44.258: INFO: 68 pods remaining
May 27 10:42:44.258: INFO: 50 pods has nil DeletionTimestamp
May 27 10:42:44.258: INFO: 
STEP: Gathering metrics
May 27 10:42:49.287: INFO: The status of Pod kube-controller-manager-ohp4eith3vui-2 is Running (Ready = true)
May 27 10:42:49.418: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 27 10:42:49.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ftl" in namespace "gc-1044"
May 27 10:42:49.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p8hx" in namespace "gc-1044"
May 27 10:42:49.531: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qn59" in namespace "gc-1044"
May 27 10:42:49.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-45kvn" in namespace "gc-1044"
May 27 10:42:49.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-465fp" in namespace "gc-1044"
May 27 10:42:49.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vknh" in namespace "gc-1044"
May 27 10:42:49.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-54tbz" in namespace "gc-1044"
May 27 10:42:49.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bgvs" in namespace "gc-1044"
May 27 10:42:49.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wmlh" in namespace "gc-1044"
May 27 10:42:49.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rjst" in namespace "gc-1044"
May 27 10:42:50.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-746hl" in namespace "gc-1044"
May 27 10:42:50.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c8v4" in namespace "gc-1044"
May 27 10:42:50.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-7cp82" in namespace "gc-1044"
May 27 10:42:50.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dmdj" in namespace "gc-1044"
May 27 10:42:50.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mj7p" in namespace "gc-1044"
May 27 10:42:50.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qs6t" in namespace "gc-1044"
May 27 10:42:50.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s9x2" in namespace "gc-1044"
May 27 10:42:50.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cjsb" in namespace "gc-1044"
May 27 10:42:50.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gxz2" in namespace "gc-1044"
May 27 10:42:50.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-8l5sm" in namespace "gc-1044"
May 27 10:42:51.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-8trnv" in namespace "gc-1044"
May 27 10:42:51.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-96b95" in namespace "gc-1044"
May 27 10:42:51.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b67v" in namespace "gc-1044"
May 27 10:42:51.414: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5rmr" in namespace "gc-1044"
May 27 10:42:51.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7dgn" in namespace "gc-1044"
May 27 10:42:51.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-c92bt" in namespace "gc-1044"
May 27 10:42:51.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg79p" in namespace "gc-1044"
May 27 10:42:51.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct2x8" in namespace "gc-1044"
May 27 10:42:52.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct6tf" in namespace "gc-1044"
May 27 10:42:52.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-d22rz" in namespace "gc-1044"
May 27 10:42:52.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-djpnw" in namespace "gc-1044"
May 27 10:42:52.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkjct" in namespace "gc-1044"
May 27 10:42:52.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlsb7" in namespace "gc-1044"
May 27 10:42:52.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4crh" in namespace "gc-1044"
May 27 10:42:52.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7fcr" in namespace "gc-1044"
May 27 10:42:52.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8kjh" in namespace "gc-1044"
May 27 10:42:52.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9w5m" in namespace "gc-1044"
May 27 10:42:52.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbjnr" in namespace "gc-1044"
May 27 10:42:52.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr6tm" in namespace "gc-1044"
May 27 10:42:52.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft2bj" in namespace "gc-1044"
May 27 10:42:53.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8kqc" in namespace "gc-1044"
May 27 10:42:53.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjqgx" in namespace "gc-1044"
May 27 10:42:53.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwtvm" in namespace "gc-1044"
May 27 10:42:53.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzlbm" in namespace "gc-1044"
May 27 10:42:53.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6hfw" in namespace "gc-1044"
May 27 10:42:53.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhr8j" in namespace "gc-1044"
May 27 10:42:53.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-hntv9" in namespace "gc-1044"
May 27 10:42:53.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwgtb" in namespace "gc-1044"
May 27 10:42:53.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2sst" in namespace "gc-1044"
May 27 10:42:53.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8qlb" in namespace "gc-1044"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:42:53.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1044" for this suite.

• [SLOW TEST:31.558 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":212,"skipped":4193,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:42:53.779: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 10:42:53.985: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3" in namespace "downward-api-6040" to be "Succeeded or Failed"
May 27 10:42:53.991: INFO: Pod "downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.710674ms
May 27 10:42:56.009: INFO: Pod "downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.024169387s
May 27 10:42:58.026: INFO: Pod "downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3": Phase="Running", Reason="", readiness=false. Elapsed: 4.041846679s
May 27 10:43:00.039: INFO: Pod "downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054142803s
STEP: Saw pod success
May 27 10:43:00.039: INFO: Pod "downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3" satisfied condition "Succeeded or Failed"
May 27 10:43:00.044: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3 container client-container: <nil>
STEP: delete the pod
May 27 10:43:00.084: INFO: Waiting for pod downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3 to disappear
May 27 10:43:00.090: INFO: Pod downwardapi-volume-6d99bfec-e1f8-436d-a548-313aed7ea3b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:00.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6040" for this suite.

• [SLOW TEST:6.338 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":213,"skipped":4197,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:00.118: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 27 10:43:00.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38461 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:00.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38461 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 27 10:43:00.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38465 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:00.206: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38465 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 27 10:43:00.221: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38467 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:00.221: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38467 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 27 10:43:00.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38469 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:00.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8781  3b7dc9e4-de1b-47ea-a869-a4cb32008f4e 38469 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 27 10:43:00.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8781  39b0b7c6-8c10-45c7-895d-af414f8d02f8 38471 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:00.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8781  39b0b7c6-8c10-45c7-895d-af414f8d02f8 38471 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 27 10:43:10.257: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8781  39b0b7c6-8c10-45c7-895d-af414f8d02f8 38868 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:10.257: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8781  39b0b7c6-8c10-45c7-895d-af414f8d02f8 38868 0 2022-05-27 10:43:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:20.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8781" for this suite.

• [SLOW TEST:20.170 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":214,"skipped":4197,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:20.292: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 10:43:20.371: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 27 10:43:20.377: INFO: starting watch
STEP: patching
STEP: updating
May 27 10:43:20.400: INFO: waiting for watch events with expected annotations
May 27 10:43:20.400: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:20.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6377" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":215,"skipped":4215,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:20.490: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-9911/configmap-test-70800a08-ec2f-48e4-82a7-96e2eae9315f
STEP: Creating a pod to test consume configMaps
May 27 10:43:20.552: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d" in namespace "configmap-9911" to be "Succeeded or Failed"
May 27 10:43:20.572: INFO: Pod "pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.534518ms
May 27 10:43:22.586: INFO: Pod "pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034310928s
May 27 10:43:24.601: INFO: Pod "pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049089712s
STEP: Saw pod success
May 27 10:43:24.601: INFO: Pod "pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d" satisfied condition "Succeeded or Failed"
May 27 10:43:24.607: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d container env-test: <nil>
STEP: delete the pod
May 27 10:43:24.646: INFO: Waiting for pod pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d to disappear
May 27 10:43:24.653: INFO: Pod pod-configmaps-ac285011-f1bd-4c36-98ad-c0ddf9b7c08d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:24.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9911" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":4217,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:24.670: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
May 27 10:43:24.765: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 27 10:43:26.780: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:27.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-312" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":217,"skipped":4225,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:27.851: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-69d929a9-7a70-410e-a31c-aaef506d601b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:30.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4681" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4242,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:30.030: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:43:30.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:43:33.988: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:43:34.003: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:37.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8237" for this suite.
STEP: Destroying namespace "webhook-8237-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":219,"skipped":4253,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:37.644: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-9626a9be-850c-4c85-b4fc-4ea3d8905f28
STEP: Creating a pod to test consume configMaps
May 27 10:43:37.758: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45" in namespace "projected-7500" to be "Succeeded or Failed"
May 27 10:43:37.769: INFO: Pod "pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45": Phase="Pending", Reason="", readiness=false. Elapsed: 10.366939ms
May 27 10:43:39.778: INFO: Pod "pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02011632s
May 27 10:43:41.793: INFO: Pod "pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034949694s
STEP: Saw pod success
May 27 10:43:41.793: INFO: Pod "pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45" satisfied condition "Succeeded or Failed"
May 27 10:43:41.799: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 27 10:43:41.835: INFO: Waiting for pod pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45 to disappear
May 27 10:43:41.840: INFO: Pod pod-projected-configmaps-c5c87b3b-8369-4b92-abb7-5f2be964cf45 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7500" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":220,"skipped":4260,"failed":0}
SSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:41.861: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
May 27 10:43:41.908: INFO: created test-podtemplate-1
May 27 10:43:41.918: INFO: created test-podtemplate-2
May 27 10:43:41.924: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 27 10:43:41.928: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 27 10:43:41.958: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:41.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5496" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":221,"skipped":4267,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:41.979: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:43:42.017: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 27 10:43:48.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-999 --namespace=crd-publish-openapi-999 create -f -'
May 27 10:43:49.796: INFO: stderr: ""
May 27 10:43:49.796: INFO: stdout: "e2e-test-crd-publish-openapi-612-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 27 10:43:49.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-999 --namespace=crd-publish-openapi-999 delete e2e-test-crd-publish-openapi-612-crds test-cr'
May 27 10:43:49.935: INFO: stderr: ""
May 27 10:43:49.935: INFO: stdout: "e2e-test-crd-publish-openapi-612-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 27 10:43:49.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-999 --namespace=crd-publish-openapi-999 apply -f -'
May 27 10:43:51.219: INFO: stderr: ""
May 27 10:43:51.219: INFO: stdout: "e2e-test-crd-publish-openapi-612-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 27 10:43:51.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-999 --namespace=crd-publish-openapi-999 delete e2e-test-crd-publish-openapi-612-crds test-cr'
May 27 10:43:51.388: INFO: stderr: ""
May 27 10:43:51.388: INFO: stdout: "e2e-test-crd-publish-openapi-612-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 27 10:43:51.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=crd-publish-openapi-999 explain e2e-test-crd-publish-openapi-612-crds'
May 27 10:43:51.658: INFO: stderr: ""
May 27 10:43:51.658: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-612-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:54.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-999" for this suite.

• [SLOW TEST:12.944 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":222,"skipped":4276,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:54.927: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 27 10:43:55.068: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3646  7f00624c-dba9-4ec4-954f-c1d94ce48b69 39247 0 2022-05-27 10:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 10:43:55.071: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3646  7f00624c-dba9-4ec4-954f-c1d94ce48b69 39250 0 2022-05-27 10:43:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-27 10:43:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:55.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3646" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":223,"skipped":4297,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:55.094: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:43:56.020: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:43:59.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:43:59.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1902" for this suite.
STEP: Destroying namespace "webhook-1902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":224,"skipped":4316,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:43:59.417: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2611
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:43:59.561: INFO: Found 0 stateful pods, waiting for 1
May 27 10:44:09.577: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
May 27 10:44:09.622: INFO: Found 1 stateful pods, waiting for 2
May 27 10:44:19.645: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 10:44:19.645: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 10:44:19.687: INFO: Deleting all statefulset in ns statefulset-2611
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:19.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2611" for this suite.

• [SLOW TEST:20.302 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":225,"skipped":4319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:19.721: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
May 27 10:44:19.806: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 10:44:24.824: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:24.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4200" for this suite.

• [SLOW TEST:5.168 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":226,"skipped":4342,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:24.893: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-5741/configmap-test-c36a58d5-f54e-40a9-b6b6-a4293161ba19
STEP: Creating a pod to test consume configMaps
May 27 10:44:24.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec" in namespace "configmap-5741" to be "Succeeded or Failed"
May 27 10:44:24.986: INFO: Pod "pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898356ms
May 27 10:44:26.997: INFO: Pod "pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016373502s
May 27 10:44:29.014: INFO: Pod "pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032933318s
STEP: Saw pod success
May 27 10:44:29.014: INFO: Pod "pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec" satisfied condition "Succeeded or Failed"
May 27 10:44:29.019: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec container env-test: <nil>
STEP: delete the pod
May 27 10:44:29.070: INFO: Waiting for pod pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec to disappear
May 27 10:44:29.076: INFO: Pod pod-configmaps-b7971d56-68a4-4331-8ff6-cfae717a56ec no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:29.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5741" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":227,"skipped":4355,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:29.101: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
May 27 10:44:29.152: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1332 proxy --unix-socket=/tmp/kubectl-proxy-unix335649814/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:29.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1332" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":228,"skipped":4370,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:29.251: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
May 27 10:44:29.300: INFO: Waiting up to 5m0s for pod "pod-69c86316-2dda-44f0-9225-7df0ea954e3d" in namespace "emptydir-8713" to be "Succeeded or Failed"
May 27 10:44:29.306: INFO: Pod "pod-69c86316-2dda-44f0-9225-7df0ea954e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.519774ms
May 27 10:44:31.315: INFO: Pod "pod-69c86316-2dda-44f0-9225-7df0ea954e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014245653s
May 27 10:44:33.328: INFO: Pod "pod-69c86316-2dda-44f0-9225-7df0ea954e3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027638816s
STEP: Saw pod success
May 27 10:44:33.328: INFO: Pod "pod-69c86316-2dda-44f0-9225-7df0ea954e3d" satisfied condition "Succeeded or Failed"
May 27 10:44:33.333: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-69c86316-2dda-44f0-9225-7df0ea954e3d container test-container: <nil>
STEP: delete the pod
May 27 10:44:33.362: INFO: Waiting for pod pod-69c86316-2dda-44f0-9225-7df0ea954e3d to disappear
May 27 10:44:33.366: INFO: Pod pod-69c86316-2dda-44f0-9225-7df0ea954e3d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:33.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8713" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4380,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:33.387: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:44:33.432: INFO: Creating ReplicaSet my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab
May 27 10:44:33.448: INFO: Pod name my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab: Found 0 pods out of 1
May 27 10:44:38.459: INFO: Pod name my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab: Found 1 pods out of 1
May 27 10:44:38.459: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab" is running
May 27 10:44:38.468: INFO: Pod "my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab-bghtk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 10:44:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 10:44:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 10:44:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 10:44:33 +0000 UTC Reason: Message:}])
May 27 10:44:38.468: INFO: Trying to dial the pod
May 27 10:44:43.506: INFO: Controller my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab: Got expected result from replica 1 [my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab-bghtk]: "my-hostname-basic-31f389e6-cbfe-4871-b19f-8b3e350aa9ab-bghtk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:43.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8753" for this suite.

• [SLOW TEST:10.144 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":230,"skipped":4392,"failed":0}
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:43.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5096" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":231,"skipped":4392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:43.636: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-3a1df701-c072-457f-99d9-15dd3deedb1f
STEP: Creating secret with name secret-projected-all-test-volume-fc1b4b8f-d288-49ff-a792-8dcaaf13b00d
STEP: Creating a pod to test Check all projections for projected volume plugin
May 27 10:44:43.711: INFO: Waiting up to 5m0s for pod "projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b" in namespace "projected-9478" to be "Succeeded or Failed"
May 27 10:44:43.717: INFO: Pod "projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.525541ms
May 27 10:44:45.726: INFO: Pod "projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014679591s
May 27 10:44:47.741: INFO: Pod "projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029531505s
STEP: Saw pod success
May 27 10:44:47.741: INFO: Pod "projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b" satisfied condition "Succeeded or Failed"
May 27 10:44:47.746: INFO: Trying to get logs from node ohp4eith3vui-3 pod projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b container projected-all-volume-test: <nil>
STEP: delete the pod
May 27 10:44:47.787: INFO: Waiting for pod projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b to disappear
May 27 10:44:47.792: INFO: Pod projected-volume-b123217f-8c50-403a-9bd4-859f2a5d367b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9478" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4444,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:47.807: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
May 27 10:44:47.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1406 create -f -'
May 27 10:44:48.273: INFO: stderr: ""
May 27 10:44:48.273: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 27 10:44:49.296: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 10:44:49.296: INFO: Found 0 / 1
May 27 10:44:50.282: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 10:44:50.282: INFO: Found 1 / 1
May 27 10:44:50.282: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 27 10:44:50.292: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 10:44:50.292: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 27 10:44:50.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1406 patch pod agnhost-primary-hv95t -p {"metadata":{"annotations":{"x":"y"}}}'
May 27 10:44:50.480: INFO: stderr: ""
May 27 10:44:50.480: INFO: stdout: "pod/agnhost-primary-hv95t patched\n"
STEP: checking annotations
May 27 10:44:50.487: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 10:44:50.487: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:50.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1406" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":233,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:50.512: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 10:44:50.621: INFO: The status of Pod annotationupdate7b1ec2ed-d23d-47c8-ab0a-9b57545d3f0a is Pending, waiting for it to be Running (with Ready = true)
May 27 10:44:52.639: INFO: The status of Pod annotationupdate7b1ec2ed-d23d-47c8-ab0a-9b57545d3f0a is Running (Ready = true)
May 27 10:44:53.182: INFO: Successfully updated pod "annotationupdate7b1ec2ed-d23d-47c8-ab0a-9b57545d3f0a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:57.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7309" for this suite.

• [SLOW TEST:6.740 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":234,"skipped":4485,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:57.253: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:44:57.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9587" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":235,"skipped":4496,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:44:57.419: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 10:44:57.468: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:00.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2413" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":236,"skipped":4507,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:00.984: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 10:45:01.912: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 10:45:03.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 10, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 45, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 10, 45, 1, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 10, 45, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 10:45:07.017: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4694" for this suite.
STEP: Destroying namespace "webhook-4694-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.175 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":237,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:07.159: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 10:45:07.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:45:07.357: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:45:08.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:45:08.397: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:45:09.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 10:45:09.373: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:45:10.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 10:45:10.372: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 27 10:45:10.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 10:45:10.417: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4947, will wait for the garbage collector to delete the pods
May 27 10:45:11.549: INFO: Deleting DaemonSet.extensions daemon-set took: 14.036893ms
May 27 10:45:11.649: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.680641ms
May 27 10:45:13.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:45:13.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 10:45:13.165: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"40107"},"items":null}

May 27 10:45:13.170: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"40107"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:13.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4947" for this suite.

• [SLOW TEST:6.053 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":238,"skipped":4537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8262.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8262.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8262.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8262.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 10:45:15.368: INFO: DNS probes using dns-8262/dns-test-1e1bf7b2-ada1-4d92-a30a-a9a8a944fcdd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:15.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8262" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":239,"skipped":4586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:15.446: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5406
STEP: creating service affinity-nodeport-transition in namespace services-5406
STEP: creating replication controller affinity-nodeport-transition in namespace services-5406
I0527 10:45:15.613728      15 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5406, replica count: 3
I0527 10:45:18.665180      15 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 10:45:18.691: INFO: Creating new exec pod
May 27 10:45:21.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5406 exec execpod-affinitykjl97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May 27 10:45:22.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 27 10:45:22.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:45:22.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5406 exec execpod-affinitykjl97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.251 80'
May 27 10:45:22.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.26.251 80\nConnection to 10.233.26.251 80 port [tcp/http] succeeded!\n"
May 27 10:45:22.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:45:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5406 exec execpod-affinitykjl97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.118 32342'
May 27 10:45:22.451: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.118 32342\nConnection to 192.168.121.118 32342 port [tcp/*] succeeded!\n"
May 27 10:45:22.451: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:45:22.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5406 exec execpod-affinitykjl97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.5 32342'
May 27 10:45:22.669: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 192.168.121.5 32342\nConnection to 192.168.121.5 32342 port [tcp/*] succeeded!\n"
May 27 10:45:22.669: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:45:22.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5406 exec execpod-affinitykjl97 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.5:32342/ ; done'
May 27 10:45:23.078: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n"
May 27 10:45:23.078: INFO: stdout: "\naffinity-nodeport-transition-dlhc6\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-dlhc6\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-nzcgq\naffinity-nodeport-transition-nzcgq\naffinity-nodeport-transition-dlhc6\naffinity-nodeport-transition-nzcgq\naffinity-nodeport-transition-nzcgq\naffinity-nodeport-transition-dlhc6\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-dlhc6\naffinity-nodeport-transition-nzcgq\naffinity-nodeport-transition-nzcgq\naffinity-nodeport-transition-nzcgq"
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-dlhc6
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-dlhc6
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-dlhc6
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-dlhc6
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-dlhc6
May 27 10:45:23.078: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.079: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.079: INFO: Received response from host: affinity-nodeport-transition-nzcgq
May 27 10:45:23.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-5406 exec execpod-affinitykjl97 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.5:32342/ ; done'
May 27 10:45:23.478: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.5:32342/\n"
May 27 10:45:23.478: INFO: stdout: "\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls\naffinity-nodeport-transition-zktls"
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Received response from host: affinity-nodeport-transition-zktls
May 27 10:45:23.478: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5406, will wait for the garbage collector to delete the pods
May 27 10:45:23.566: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.891296ms
May 27 10:45:23.667: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.569899ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:26.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5406" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.602 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":240,"skipped":4628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:26.051: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
May 27 10:45:26.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May 27 10:45:26.215: INFO: stderr: ""
May 27 10:45:26.215: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
May 27 10:45:26.215: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 27 10:45:26.216: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2412" to be "running and ready, or succeeded"
May 27 10:45:26.224: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.751618ms
May 27 10:45:28.236: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.020428217s
May 27 10:45:28.236: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 27 10:45:28.236: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 27 10:45:28.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator'
May 27 10:45:28.391: INFO: stderr: ""
May 27 10:45:28.391: INFO: stdout: "I0527 10:45:27.347871       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/wxg 341\nI0527 10:45:27.548011       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/h8nf 343\nI0527 10:45:27.748462       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qqk 432\nI0527 10:45:27.947726       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6vk4 295\nI0527 10:45:28.148202       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7bn9 423\nI0527 10:45:28.348628       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/szxq 548\n"
May 27 10:45:30.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator'
May 27 10:45:30.518: INFO: stderr: ""
May 27 10:45:30.518: INFO: stdout: "I0527 10:45:27.347871       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/wxg 341\nI0527 10:45:27.548011       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/h8nf 343\nI0527 10:45:27.748462       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qqk 432\nI0527 10:45:27.947726       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6vk4 295\nI0527 10:45:28.148202       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7bn9 423\nI0527 10:45:28.348628       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/szxq 548\nI0527 10:45:28.548236       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/qgs2 277\nI0527 10:45:28.748706       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/dcd2 254\nI0527 10:45:28.948138       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/fhv 346\nI0527 10:45:29.148593       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/s4rh 550\nI0527 10:45:29.347971       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/hx8f 587\nI0527 10:45:29.548371       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/rqqh 493\nI0527 10:45:29.747746       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/wsxq 320\nI0527 10:45:29.948215       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/qgmz 294\nI0527 10:45:30.148673       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/mdg 242\nI0527 10:45:30.348294       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/z97t 343\n"
STEP: limiting log lines
May 27 10:45:30.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator --tail=1'
May 27 10:45:30.646: INFO: stderr: ""
May 27 10:45:30.646: INFO: stdout: "I0527 10:45:30.548220       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/mbz2 459\n"
May 27 10:45:30.646: INFO: got output "I0527 10:45:30.548220       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/mbz2 459\n"
STEP: limiting log bytes
May 27 10:45:30.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator --limit-bytes=1'
May 27 10:45:30.774: INFO: stderr: ""
May 27 10:45:30.774: INFO: stdout: "I"
May 27 10:45:30.774: INFO: got output "I"
STEP: exposing timestamps
May 27 10:45:30.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator --tail=1 --timestamps'
May 27 10:45:30.901: INFO: stderr: ""
May 27 10:45:30.901: INFO: stdout: "2022-05-27T10:45:30.755819313Z I0527 10:45:30.755333       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/98n 433\n"
May 27 10:45:30.901: INFO: got output "2022-05-27T10:45:30.755819313Z I0527 10:45:30.755333       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/98n 433\n"
STEP: restricting to a time range
May 27 10:45:33.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator --since=1s'
May 27 10:45:33.537: INFO: stderr: ""
May 27 10:45:33.538: INFO: stdout: "I0527 10:45:32.548267       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/4mz 204\nI0527 10:45:32.748690       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/rcn9 336\nI0527 10:45:32.948223       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/tl5c 385\nI0527 10:45:33.148691       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/nn2 304\nI0527 10:45:33.348377       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/5lv 545\n"
May 27 10:45:33.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 logs logs-generator logs-generator --since=24h'
May 27 10:45:33.668: INFO: stderr: ""
May 27 10:45:33.668: INFO: stdout: "I0527 10:45:27.347871       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/wxg 341\nI0527 10:45:27.548011       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/h8nf 343\nI0527 10:45:27.748462       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/qqk 432\nI0527 10:45:27.947726       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/6vk4 295\nI0527 10:45:28.148202       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7bn9 423\nI0527 10:45:28.348628       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/szxq 548\nI0527 10:45:28.548236       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/qgs2 277\nI0527 10:45:28.748706       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/dcd2 254\nI0527 10:45:28.948138       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/fhv 346\nI0527 10:45:29.148593       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/s4rh 550\nI0527 10:45:29.347971       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/hx8f 587\nI0527 10:45:29.548371       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/rqqh 493\nI0527 10:45:29.747746       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/wsxq 320\nI0527 10:45:29.948215       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/qgmz 294\nI0527 10:45:30.148673       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/mdg 242\nI0527 10:45:30.348294       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/z97t 343\nI0527 10:45:30.548220       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/mbz2 459\nI0527 10:45:30.755333       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/98n 433\nI0527 10:45:30.947701       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/mtjl 578\nI0527 10:45:31.148134       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/zrc 283\nI0527 10:45:31.348488       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/k8n 516\nI0527 10:45:31.547810       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/5m2 496\nI0527 10:45:31.748213       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/vk2d 208\nI0527 10:45:31.948635       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/9zj 471\nI0527 10:45:32.148044       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/m7w 377\nI0527 10:45:32.347750       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/lqk 563\nI0527 10:45:32.548267       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/4mz 204\nI0527 10:45:32.748690       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/rcn9 336\nI0527 10:45:32.948223       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/tl5c 385\nI0527 10:45:33.148691       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/nn2 304\nI0527 10:45:33.348377       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/default/pods/5lv 545\nI0527 10:45:33.547742       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/c4h 596\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
May 27 10:45:33.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-2412 delete pod logs-generator'
May 27 10:45:34.232: INFO: stderr: ""
May 27 10:45:34.232: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:34.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2412" for this suite.

• [SLOW TEST:8.209 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":241,"skipped":4655,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:34.261: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
May 27 10:45:34.337: INFO: Waiting up to 5m0s for pod "pod-5c822303-ed59-49b2-92be-eb5c205030b7" in namespace "emptydir-8381" to be "Succeeded or Failed"
May 27 10:45:34.343: INFO: Pod "pod-5c822303-ed59-49b2-92be-eb5c205030b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.721276ms
May 27 10:45:36.356: INFO: Pod "pod-5c822303-ed59-49b2-92be-eb5c205030b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019023968s
May 27 10:45:38.374: INFO: Pod "pod-5c822303-ed59-49b2-92be-eb5c205030b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036849828s
STEP: Saw pod success
May 27 10:45:38.374: INFO: Pod "pod-5c822303-ed59-49b2-92be-eb5c205030b7" satisfied condition "Succeeded or Failed"
May 27 10:45:38.380: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-5c822303-ed59-49b2-92be-eb5c205030b7 container test-container: <nil>
STEP: delete the pod
May 27 10:45:38.409: INFO: Waiting for pod pod-5c822303-ed59-49b2-92be-eb5c205030b7 to disappear
May 27 10:45:38.415: INFO: Pod pod-5c822303-ed59-49b2-92be-eb5c205030b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:38.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8381" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:38.437: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-7729
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7729
STEP: Deleting pre-stop pod
May 27 10:45:47.613: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:47.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7729" for this suite.

• [SLOW TEST:9.270 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":243,"skipped":4704,"failed":0}
SS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:47.709: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
May 27 10:45:47.810: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 27 10:45:49.826: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
May 27 10:45:49.880: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 27 10:45:51.895: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 27 10:45:51.901: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:51.903: INFO: ExecWithOptions: Clientset creation
May 27 10:45:51.904: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.049: INFO: Exec stderr: ""
May 27 10:45:52.049: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.049: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.052: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.052: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.146: INFO: Exec stderr: ""
May 27 10:45:52.146: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.146: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.149: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.149: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.257: INFO: Exec stderr: ""
May 27 10:45:52.258: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.258: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.260: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.260: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.381: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 27 10:45:52.381: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.381: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.384: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.384: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.485: INFO: Exec stderr: ""
May 27 10:45:52.485: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.485: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.486: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.486: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.570: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 27 10:45:52.570: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.570: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.571: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.572: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.672: INFO: Exec stderr: ""
May 27 10:45:52.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.672: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.674: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.674: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.776: INFO: Exec stderr: ""
May 27 10:45:52.776: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.777: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.780: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.780: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:52.889: INFO: Exec stderr: ""
May 27 10:45:52.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-603 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:45:52.889: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:45:52.892: INFO: ExecWithOptions: Clientset creation
May 27 10:45:52.892: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-603/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 10:45:53.009: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:53.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-603" for this suite.

• [SLOW TEST:5.327 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":244,"skipped":4706,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:53.039: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:45:53.092: INFO: Got root ca configmap in namespace "svcaccounts-7023"
May 27 10:45:53.102: INFO: Deleted root ca configmap in namespace "svcaccounts-7023"
STEP: waiting for a new root ca configmap created
May 27 10:45:53.610: INFO: Recreated root ca configmap in namespace "svcaccounts-7023"
May 27 10:45:53.620: INFO: Updated root ca configmap in namespace "svcaccounts-7023"
STEP: waiting for the root ca configmap reconciled
May 27 10:45:54.128: INFO: Reconciled root ca configmap in namespace "svcaccounts-7023"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:45:54.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7023" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":245,"skipped":4718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:45:54.156: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May 27 10:46:04.345: INFO: The status of Pod kube-controller-manager-ohp4eith3vui-2 is Running (Ready = true)
May 27 10:46:04.446: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:46:04.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7272" for this suite.

• [SLOW TEST:10.314 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":246,"skipped":4740,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:46:04.474: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
May 27 10:46:04.538: INFO: Waiting up to 5m0s for pod "client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202" in namespace "containers-8255" to be "Succeeded or Failed"
May 27 10:46:04.552: INFO: Pod "client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202": Phase="Pending", Reason="", readiness=false. Elapsed: 13.912087ms
May 27 10:46:06.563: INFO: Pod "client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025429189s
May 27 10:46:08.577: INFO: Pod "client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039429024s
STEP: Saw pod success
May 27 10:46:08.577: INFO: Pod "client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202" satisfied condition "Succeeded or Failed"
May 27 10:46:08.582: INFO: Trying to get logs from node ohp4eith3vui-3 pod client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:46:08.611: INFO: Waiting for pod client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202 to disappear
May 27 10:46:08.619: INFO: Pod client-containers-34c4747a-86d9-4c4b-b8b9-aaea53cc5202 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:46:08.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8255" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4750,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:46:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:46:12.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-149" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":248,"skipped":4764,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:46:12.202: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:46:12.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1367" for this suite.
STEP: Destroying namespace "nspatchtest-262eba1d-b622-46af-a965-cdc3b51b6bac-2303" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":249,"skipped":4781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:46:12.313: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May 27 10:46:14.492: INFO: running pods: 0 < 3
May 27 10:46:16.504: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:46:18.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2830" for this suite.

• [SLOW TEST:6.228 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":250,"skipped":4823,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:46:18.545: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-334
May 27 10:46:18.618: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 10:46:20.628: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 27 10:46:20.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-334 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 27 10:46:20.888: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 27 10:46:20.888: INFO: stdout: "iptables"
May 27 10:46:20.888: INFO: proxyMode: iptables
May 27 10:46:20.918: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 27 10:46:20.924: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-334
STEP: creating replication controller affinity-clusterip-timeout in namespace services-334
I0527 10:46:20.956468      15 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-334, replica count: 3
I0527 10:46:24.008530      15 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 10:46:24.064: INFO: Creating new exec pod
May 27 10:46:27.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-334 exec execpod-affinitylq2g7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May 27 10:46:27.345: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 27 10:46:27.345: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:46:27.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-334 exec execpod-affinitylq2g7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.51.255 80'
May 27 10:46:27.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.51.255 80\nConnection to 10.233.51.255 80 port [tcp/http] succeeded!\n"
May 27 10:46:27.587: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 10:46:27.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-334 exec execpod-affinitylq2g7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.51.255:80/ ; done'
May 27 10:46:27.976: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n"
May 27 10:46:27.976: INFO: stdout: "\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4\naffinity-clusterip-timeout-5m7j4"
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Received response from host: affinity-clusterip-timeout-5m7j4
May 27 10:46:27.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-334 exec execpod-affinitylq2g7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.51.255:80/'
May 27 10:46:28.166: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n"
May 27 10:46:28.166: INFO: stdout: "affinity-clusterip-timeout-5m7j4"
May 27 10:46:48.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-334 exec execpod-affinitylq2g7 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.51.255:80/'
May 27 10:46:48.446: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.51.255:80/\n"
May 27 10:46:48.446: INFO: stdout: "affinity-clusterip-timeout-hqbmf"
May 27 10:46:48.446: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-334, will wait for the garbage collector to delete the pods
May 27 10:46:48.561: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 16.924451ms
May 27 10:46:48.662: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.339679ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:46:50.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-334" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:32.321 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":251,"skipped":4844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:46:50.874: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-e1760fde-246e-4094-84f3-12e29637e84c in namespace container-probe-452
May 27 10:46:52.967: INFO: Started pod liveness-e1760fde-246e-4094-84f3-12e29637e84c in namespace container-probe-452
STEP: checking the pod's current state and verifying that restartCount is present
May 27 10:46:52.973: INFO: Initial restart count of pod liveness-e1760fde-246e-4094-84f3-12e29637e84c is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:50:54.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-452" for this suite.

• [SLOW TEST:243.771 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":252,"skipped":4879,"failed":0}
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:50:54.645: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 10:50:54.765: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 10:50:56.783: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 10:50:56.806: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 10:50:58.822: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 27 10:50:58.872: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 27 10:50:58.878: INFO: Pod pod-with-poststart-exec-hook still exists
May 27 10:51:00.878: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 27 10:51:00.889: INFO: Pod pod-with-poststart-exec-hook still exists
May 27 10:51:02.880: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 27 10:51:02.897: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:02.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3100" for this suite.

• [SLOW TEST:8.277 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":253,"skipped":4881,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:02.923: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-c50f1be0-7dbc-4ccf-8bcf-38be134adff1
STEP: Creating a pod to test consume configMaps
May 27 10:51:03.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b" in namespace "configmap-1130" to be "Succeeded or Failed"
May 27 10:51:03.077: INFO: Pod "pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.695002ms
May 27 10:51:05.085: INFO: Pod "pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026095578s
May 27 10:51:07.101: INFO: Pod "pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041569295s
STEP: Saw pod success
May 27 10:51:07.101: INFO: Pod "pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b" satisfied condition "Succeeded or Failed"
May 27 10:51:07.107: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b container agnhost-container: <nil>
STEP: delete the pod
May 27 10:51:07.170: INFO: Waiting for pod pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b to disappear
May 27 10:51:07.176: INFO: Pod pod-configmaps-a9424ac7-a8ae-4c5e-a5a3-f39b791dab8b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:07.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1130" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4918,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:07.199: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-ca273176-32ca-4a71-b373-ebe487756385
STEP: Creating a pod to test consume configMaps
May 27 10:51:07.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1" in namespace "configmap-6875" to be "Succeeded or Failed"
May 27 10:51:07.299: INFO: Pod "pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.96037ms
May 27 10:51:09.311: INFO: Pod "pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022253087s
May 27 10:51:11.324: INFO: Pod "pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034849762s
STEP: Saw pod success
May 27 10:51:11.324: INFO: Pod "pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1" satisfied condition "Succeeded or Failed"
May 27 10:51:11.332: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1 container configmap-volume-test: <nil>
STEP: delete the pod
May 27 10:51:11.360: INFO: Waiting for pod pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1 to disappear
May 27 10:51:11.365: INFO: Pod pod-configmaps-8851e5ae-8d30-4fe7-8e0d-c2180236fee1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:11.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6875" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":255,"skipped":4933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:11.394: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
May 27 10:51:11.483: INFO: Waiting up to 5m0s for pod "pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51" in namespace "emptydir-7794" to be "Succeeded or Failed"
May 27 10:51:11.489: INFO: Pod "pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51": Phase="Pending", Reason="", readiness=false. Elapsed: 6.79698ms
May 27 10:51:13.505: INFO: Pod "pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51": Phase="Running", Reason="", readiness=true. Elapsed: 2.022677573s
May 27 10:51:15.515: INFO: Pod "pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51": Phase="Running", Reason="", readiness=false. Elapsed: 4.032228795s
May 27 10:51:17.530: INFO: Pod "pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047444738s
STEP: Saw pod success
May 27 10:51:17.530: INFO: Pod "pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51" satisfied condition "Succeeded or Failed"
May 27 10:51:17.535: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51 container test-container: <nil>
STEP: delete the pod
May 27 10:51:17.585: INFO: Waiting for pod pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51 to disappear
May 27 10:51:17.591: INFO: Pod pod-4f9d8f89-1161-4ca5-91d5-1ba9c29a7a51 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:17.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7794" for this suite.

• [SLOW TEST:6.216 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":256,"skipped":4967,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:17.611: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:33.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5650" for this suite.

• [SLOW TEST:16.273 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":257,"skipped":4972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:33.890: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:34.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2586" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":258,"skipped":4995,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:34.036: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 10:51:34.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:51:34.140: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:51:35.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 10:51:35.164: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:51:36.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 10:51:36.165: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 10:51:37.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 10:51:37.162: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
May 27 10:51:37.202: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41979"},"items":null}

May 27 10:51:37.211: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41979"},"items":[{"metadata":{"name":"daemon-set-9sz2q","generateName":"daemon-set-","namespace":"daemonsets-3717","uid":"f2762c55-6a6b-4a00-8dbb-a9b31411543a","resourceVersion":"41976","creationTimestamp":"2022-05-27T10:51:34Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"472793ac-5f4e-40d1-be60-6bd04e9fffef","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-27T10:51:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"472793ac-5f4e-40d1-be60-6bd04e9fffef\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-27T10:51:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5k9j2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5k9j2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ohp4eith3vui-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ohp4eith3vui-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:34Z"}],"hostIP":"192.168.121.118","podIP":"10.233.64.235","podIPs":[{"ip":"10.233.64.235"}],"startTime":"2022-05-27T10:51:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-27T10:51:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://16df8e3fbf8cecc910ab9a3e6fbb10b99013a9561c6616fc214d3f907b5cba31","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-h2gth","generateName":"daemon-set-","namespace":"daemonsets-3717","uid":"4cc5ca96-232d-4a98-82fe-f5f4796bdce5","resourceVersion":"41974","creationTimestamp":"2022-05-27T10:51:34Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"472793ac-5f4e-40d1-be60-6bd04e9fffef","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-27T10:51:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"472793ac-5f4e-40d1-be60-6bd04e9fffef\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-27T10:51:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-sfn92","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-sfn92","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ohp4eith3vui-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ohp4eith3vui-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:34Z"}],"hostIP":"192.168.121.5","podIP":"10.233.65.39","podIPs":[{"ip":"10.233.65.39"}],"startTime":"2022-05-27T10:51:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-27T10:51:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://514f28466ede465e00f7e81753b8a6d17da0b6edcfe13cfe1ad466df833891aa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vt4l8","generateName":"daemon-set-","namespace":"daemonsets-3717","uid":"b57eb2b3-3a44-4b0b-9094-295df338fdd8","resourceVersion":"41970","creationTimestamp":"2022-05-27T10:51:34Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"472793ac-5f4e-40d1-be60-6bd04e9fffef","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-27T10:51:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"472793ac-5f4e-40d1-be60-6bd04e9fffef\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-27T10:51:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-prhnl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-prhnl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ohp4eith3vui-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ohp4eith3vui-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T10:51:34Z"}],"hostIP":"192.168.121.192","podIP":"10.233.66.28","podIPs":[{"ip":"10.233.66.28"}],"startTime":"2022-05-27T10:51:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-27T10:51:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://da9831f0d5336576624d9bf56d4ef84af3c64dac68ae6e5631dc38c5ca51af72","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:51:37.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3717" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":259,"skipped":4997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:51:37.280: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5430
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 10:51:37.314: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 10:51:37.369: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 10:51:39.392: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:41.384: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:43.385: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:45.377: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:47.388: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:49.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:51.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:53.381: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:55.381: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:57.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 10:51:59.394: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 10:51:59.422: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 10:51:59.436: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 10:52:01.516: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 10:52:01.517: INFO: Going to poll 10.233.65.181 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 27 10:52:01.521: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:52:01.521: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:52:01.523: INFO: ExecWithOptions: Clientset creation
May 27 10:52:01.523: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.181+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 10:52:02.660: INFO: Found all 1 expected endpoints: [netserver-0]
May 27 10:52:02.660: INFO: Going to poll 10.233.64.54 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 27 10:52:02.671: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.54 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:52:02.671: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:52:02.672: INFO: ExecWithOptions: Clientset creation
May 27 10:52:02.672: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.54+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 10:52:03.786: INFO: Found all 1 expected endpoints: [netserver-1]
May 27 10:52:03.786: INFO: Going to poll 10.233.66.253 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 27 10:52:03.796: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.253 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 10:52:03.797: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 10:52:03.798: INFO: ExecWithOptions: Clientset creation
May 27 10:52:03.798: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.253+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 10:52:04.924: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:52:04.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5430" for this suite.

• [SLOW TEST:27.666 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":5036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:52:04.952: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-f7371464-bfa4-4092-8bc3-d0896280a314
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:52:05.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5677" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":261,"skipped":5103,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:52:05.023: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 10:52:05.078: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Pending, waiting for it to be Running (with Ready = true)
May 27 10:52:07.094: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:09.094: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:11.092: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:13.092: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:15.088: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:17.093: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:19.092: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:21.090: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:23.087: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:25.087: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = false)
May 27 10:52:27.103: INFO: The status of Pod test-webserver-ff0747a2-4d6f-4233-bf12-27f7d783f41f is Running (Ready = true)
May 27 10:52:27.108: INFO: Container started at 2022-05-27 10:52:06 +0000 UTC, pod became ready at 2022-05-27 10:52:25 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:52:27.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6930" for this suite.

• [SLOW TEST:22.120 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":5112,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:52:27.146: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-4c7f4ab2-39c6-48cf-86ca-b3b0b4e31d91
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:52:27.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8682" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":263,"skipped":5131,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:52:27.226: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-e853b0d7-06b4-463a-a388-747f2b614338 in namespace container-probe-6060
May 27 10:52:29.292: INFO: Started pod test-webserver-e853b0d7-06b4-463a-a388-747f2b614338 in namespace container-probe-6060
STEP: checking the pod's current state and verifying that restartCount is present
May 27 10:52:29.297: INFO: Initial restart count of pod test-webserver-e853b0d7-06b4-463a-a388-747f2b614338 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:56:30.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6060" for this suite.

• [SLOW TEST:243.797 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":5136,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:56:31.024: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:56:41.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9287" for this suite.

• [SLOW TEST:10.105 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":265,"skipped":5142,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:56:41.129: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
May 27 10:56:41.190: INFO: Waiting up to 5m0s for pod "var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b" in namespace "var-expansion-6501" to be "Succeeded or Failed"
May 27 10:56:41.200: INFO: Pod "var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.866901ms
May 27 10:56:43.211: INFO: Pod "var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020648088s
May 27 10:56:45.220: INFO: Pod "var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02944172s
STEP: Saw pod success
May 27 10:56:45.220: INFO: Pod "var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b" satisfied condition "Succeeded or Failed"
May 27 10:56:45.227: INFO: Trying to get logs from node ohp4eith3vui-3 pod var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b container dapi-container: <nil>
STEP: delete the pod
May 27 10:56:45.280: INFO: Waiting for pod var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b to disappear
May 27 10:56:45.286: INFO: Pod var-expansion-12fabcd0-1279-418f-827d-b4a5c076da0b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:56:45.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6501" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":266,"skipped":5146,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:56:45.308: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:56:45.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4754" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":267,"skipped":5150,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:56:45.462: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-905a29bd-149a-405d-a103-7b54049f0b38
STEP: Creating a pod to test consume configMaps
May 27 10:56:45.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01" in namespace "configmap-7615" to be "Succeeded or Failed"
May 27 10:56:45.522: INFO: Pod "pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478938ms
May 27 10:56:47.535: INFO: Pod "pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017793641s
May 27 10:56:49.549: INFO: Pod "pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030999434s
STEP: Saw pod success
May 27 10:56:49.549: INFO: Pod "pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01" satisfied condition "Succeeded or Failed"
May 27 10:56:49.554: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01 container agnhost-container: <nil>
STEP: delete the pod
May 27 10:56:49.582: INFO: Waiting for pod pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01 to disappear
May 27 10:56:49.586: INFO: Pod pod-configmaps-6bb86a67-04a4-48bf-81da-a4b85b288f01 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 10:56:49.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7615" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":5153,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 10:56:49.609: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 10:56:49.653: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 10:56:49.667: INFO: Waiting for terminating namespaces to be deleted...
May 27 10:56:49.674: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-1 before test
May 27 10:56:49.687: INFO: echo-other-node-59d779959c-8lmx4 from cilium-test started at 2022-05-27 09:01:23 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container echo-other-node ready: true, restart count 1
May 27 10:56:49.687: INFO: cilium-node-init-cmpq2 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container node-init ready: true, restart count 1
May 27 10:56:49.687: INFO: cilium-pb7c6 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container cilium-agent ready: true, restart count 3
May 27 10:56:49.687: INFO: coredns-64897985d-v76xc from kube-system started at 2022-05-27 08:54:45 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container coredns ready: true, restart count 1
May 27 10:56:49.687: INFO: kube-addon-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 10:56:49.687: INFO: kube-apiserver-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 10:56:49.687: INFO: kube-controller-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 10:56:49.687: INFO: kube-proxy-bt99l from kube-system started at 2022-05-27 08:20:34 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 10:56:49.687: INFO: kube-scheduler-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container kube-scheduler ready: true, restart count 1
May 27 10:56:49.687: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-6cftw from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 10:56:49.687: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 10:56:49.687: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 10:56:49.687: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-2 before test
May 27 10:56:49.707: INFO: client-7568bc7f86-h8tfj from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.707: INFO: 	Container client ready: true, restart count 0
May 27 10:56:49.707: INFO: client2-686d5f784b-b24x8 from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.707: INFO: 	Container client2 ready: true, restart count 0
May 27 10:56:49.707: INFO: echo-same-node-5767b7b99d-9t6kf from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.707: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 10:56:49.708: INFO: cilium-27t29 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.708: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 10:56:49.708: INFO: cilium-node-init-xkmdh from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.708: INFO: 	Container node-init ready: true, restart count 1
May 27 10:56:49.708: INFO: cilium-operator-59d6f769d4-gh6js from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.708: INFO: 	Container cilium-operator ready: true, restart count 1
May 27 10:56:49.709: INFO: coredns-64897985d-f7x88 from kube-system started at 2022-05-27 08:54:30 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.709: INFO: 	Container coredns ready: true, restart count 1
May 27 10:56:49.709: INFO: kube-addon-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.709: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 10:56:49.709: INFO: kube-apiserver-ohp4eith3vui-2 from kube-system started at 2022-05-27 08:56:11 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.709: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 10:56:49.709: INFO: kube-controller-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.709: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 10:56:49.710: INFO: kube-proxy-8qhjn from kube-system started at 2022-05-27 08:20:49 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.710: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 10:56:49.710: INFO: kube-scheduler-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.710: INFO: 	Container kube-scheduler ready: true, restart count 2
May 27 10:56:49.710: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-t4l8w from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 10:56:49.710: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 10:56:49.710: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 10:56:49.710: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-3 before test
May 27 10:56:49.722: INFO: cilium-mgkvh from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.722: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 10:56:49.722: INFO: cilium-node-init-q8q6q from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.722: INFO: 	Container node-init ready: true, restart count 1
May 27 10:56:49.722: INFO: kube-proxy-xvr6w from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.722: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 10:56:49.722: INFO: sonobuoy from sonobuoy started at 2022-05-27 09:38:51 +0000 UTC (1 container statuses recorded)
May 27 10:56:49.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 10:56:49.722: INFO: sonobuoy-e2e-job-3507e2c9ba704a39 from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 10:56:49.722: INFO: 	Container e2e ready: true, restart count 0
May 27 10:56:49.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 10:56:49.722: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-kllxz from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 10:56:49.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 10:56:49.722: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ca4c054a-cead-4474-ab3a-d6370e8ebe9b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.192 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-ca4c054a-cead-4474-ab3a-d6370e8ebe9b off the node ohp4eith3vui-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ca4c054a-cead-4474-ab3a-d6370e8ebe9b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:01:53.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2325" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.369 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":269,"skipped":5163,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:01:53.979: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:01:54.063: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:02:00.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2566" for this suite.

• [SLOW TEST:6.755 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":270,"skipped":5169,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:02:00.737: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
May 27 11:02:00.789: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:02:31.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7077" for this suite.

• [SLOW TEST:31.095 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":271,"skipped":5173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:02:31.849: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:02:31.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3365" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":272,"skipped":5253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:02:32.023: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 11:02:33.204: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 11:02:36.244: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:02:36.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4916" for this suite.
STEP: Destroying namespace "webhook-4916-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":273,"skipped":5277,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:02:36.488: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:02:49.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8470" for this suite.

• [SLOW TEST:13.244 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":274,"skipped":5295,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:02:49.733: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 27 11:02:49.830: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:02:49.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-123" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":275,"skipped":5296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:02:49.890: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-a6d0335e-25e6-4824-a437-877626b2231a
STEP: Creating secret with name s-test-opt-upd-fb67c483-3a46-4bdd-b9ef-b234da21ad13
STEP: Creating the pod
May 27 11:02:49.997: INFO: The status of Pod pod-secrets-571a9c9f-a375-46b2-afe3-ec28164ad205 is Pending, waiting for it to be Running (with Ready = true)
May 27 11:02:52.007: INFO: The status of Pod pod-secrets-571a9c9f-a375-46b2-afe3-ec28164ad205 is Pending, waiting for it to be Running (with Ready = true)
May 27 11:02:54.012: INFO: The status of Pod pod-secrets-571a9c9f-a375-46b2-afe3-ec28164ad205 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-a6d0335e-25e6-4824-a437-877626b2231a
STEP: Updating secret s-test-opt-upd-fb67c483-3a46-4bdd-b9ef-b234da21ad13
STEP: Creating secret with name s-test-opt-create-d5404077-299a-4e3a-9f05-d5fc1782931c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:03:58.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4631" for this suite.

• [SLOW TEST:68.899 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5320,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:03:58.791: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 11:03:58.899: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 11:04:58.990: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:04:59.006: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 27 11:05:01.151: INFO: found a healthy node: ohp4eith3vui-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:05:15.308: INFO: pods created so far: [1 1 1]
May 27 11:05:15.308: INFO: length of pods created so far: 3
May 27 11:05:17.330: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:24.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7009" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:24.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1000" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:85.747 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":277,"skipped":5321,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:24.544: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:05:24.623: INFO: The status of Pod busybox-scheduling-4e8de139-2c83-4432-8399-11dcb75b6f97 is Pending, waiting for it to be Running (with Ready = true)
May 27 11:05:26.640: INFO: The status of Pod busybox-scheduling-4e8de139-2c83-4432-8399-11dcb75b6f97 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:26.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6212" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":5337,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:26.690: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 27 11:05:26.762: INFO: Waiting up to 5m0s for pod "pod-b325670c-726d-428d-b89d-fce1a50d4191" in namespace "emptydir-475" to be "Succeeded or Failed"
May 27 11:05:26.770: INFO: Pod "pod-b325670c-726d-428d-b89d-fce1a50d4191": Phase="Pending", Reason="", readiness=false. Elapsed: 7.338429ms
May 27 11:05:28.781: INFO: Pod "pod-b325670c-726d-428d-b89d-fce1a50d4191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018708091s
May 27 11:05:30.798: INFO: Pod "pod-b325670c-726d-428d-b89d-fce1a50d4191": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035905205s
May 27 11:05:32.813: INFO: Pod "pod-b325670c-726d-428d-b89d-fce1a50d4191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050724477s
STEP: Saw pod success
May 27 11:05:32.813: INFO: Pod "pod-b325670c-726d-428d-b89d-fce1a50d4191" satisfied condition "Succeeded or Failed"
May 27 11:05:32.820: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-b325670c-726d-428d-b89d-fce1a50d4191 container test-container: <nil>
STEP: delete the pod
May 27 11:05:32.854: INFO: Waiting for pod pod-b325670c-726d-428d-b89d-fce1a50d4191 to disappear
May 27 11:05:32.861: INFO: Pod pod-b325670c-726d-428d-b89d-fce1a50d4191 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:32.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-475" for this suite.

• [SLOW TEST:6.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":5340,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:32.879: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 11:05:32.927: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 11:05:32.946: INFO: Waiting for terminating namespaces to be deleted...
May 27 11:05:32.954: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-1 before test
May 27 11:05:32.972: INFO: echo-other-node-59d779959c-8lmx4 from cilium-test started at 2022-05-27 09:01:23 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container echo-other-node ready: true, restart count 1
May 27 11:05:32.972: INFO: cilium-node-init-cmpq2 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container node-init ready: true, restart count 1
May 27 11:05:32.972: INFO: cilium-pb7c6 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container cilium-agent ready: true, restart count 3
May 27 11:05:32.972: INFO: coredns-64897985d-v76xc from kube-system started at 2022-05-27 08:54:45 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container coredns ready: true, restart count 1
May 27 11:05:32.972: INFO: kube-addon-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 11:05:32.972: INFO: kube-apiserver-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 11:05:32.972: INFO: kube-controller-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 11:05:32.972: INFO: kube-proxy-bt99l from kube-system started at 2022-05-27 08:20:34 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:05:32.972: INFO: kube-scheduler-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container kube-scheduler ready: true, restart count 1
May 27 11:05:32.972: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-6cftw from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:05:32.972: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 11:05:32.972: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 11:05:32.972: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-2 before test
May 27 11:05:33.001: INFO: client-7568bc7f86-h8tfj from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container client ready: true, restart count 0
May 27 11:05:33.001: INFO: client2-686d5f784b-b24x8 from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container client2 ready: true, restart count 0
May 27 11:05:33.001: INFO: echo-same-node-5767b7b99d-9t6kf from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 11:05:33.001: INFO: cilium-27t29 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 11:05:33.001: INFO: cilium-node-init-xkmdh from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container node-init ready: true, restart count 1
May 27 11:05:33.001: INFO: cilium-operator-59d6f769d4-gh6js from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container cilium-operator ready: true, restart count 1
May 27 11:05:33.001: INFO: coredns-64897985d-f7x88 from kube-system started at 2022-05-27 08:54:30 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container coredns ready: true, restart count 1
May 27 11:05:33.001: INFO: kube-addon-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 11:05:33.001: INFO: kube-apiserver-ohp4eith3vui-2 from kube-system started at 2022-05-27 08:56:11 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 11:05:33.001: INFO: kube-controller-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 11:05:33.001: INFO: kube-proxy-8qhjn from kube-system started at 2022-05-27 08:20:49 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:05:33.001: INFO: kube-scheduler-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container kube-scheduler ready: true, restart count 2
May 27 11:05:33.001: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-t4l8w from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:05:33.001: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 11:05:33.001: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 11:05:33.001: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-3 before test
May 27 11:05:33.024: INFO: cilium-mgkvh from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.024: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 11:05:33.024: INFO: cilium-node-init-q8q6q from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.024: INFO: 	Container node-init ready: true, restart count 1
May 27 11:05:33.024: INFO: kube-proxy-xvr6w from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.024: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:05:33.024: INFO: busybox-scheduling-4e8de139-2c83-4432-8399-11dcb75b6f97 from kubelet-test-6212 started at 2022-05-27 11:05:24 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.025: INFO: 	Container busybox-scheduling-4e8de139-2c83-4432-8399-11dcb75b6f97 ready: true, restart count 0
May 27 11:05:33.025: INFO: sonobuoy from sonobuoy started at 2022-05-27 09:38:51 +0000 UTC (1 container statuses recorded)
May 27 11:05:33.025: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 11:05:33.025: INFO: sonobuoy-e2e-job-3507e2c9ba704a39 from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:05:33.025: INFO: 	Container e2e ready: true, restart count 0
May 27 11:05:33.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 11:05:33.025: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-kllxz from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:05:33.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 11:05:33.025: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4a32a4ac-ee45-400a-914b-dc36beae9541 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-4a32a4ac-ee45-400a-914b-dc36beae9541 off the node ohp4eith3vui-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4a32a4ac-ee45-400a-914b-dc36beae9541
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:37.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":280,"skipped":5344,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:37.249: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
May 27 11:05:37.317: INFO: Major version: 1
STEP: Confirm minor version
May 27 11:05:37.317: INFO: cleanMinorVersion: 23
May 27 11:05:37.317: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:37.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9274" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":281,"skipped":5363,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:37.341: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 11:05:37.384: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:41.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5996" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":282,"skipped":5377,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:41.822: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-2f196f74-df23-40ae-a8fc-c78799d9b1f8
STEP: Creating the pod
May 27 11:05:41.905: INFO: The status of Pod pod-projected-configmaps-d2c4e69e-0da2-447c-8742-fefb0115ebea is Pending, waiting for it to be Running (with Ready = true)
May 27 11:05:43.937: INFO: The status of Pod pod-projected-configmaps-d2c4e69e-0da2-447c-8742-fefb0115ebea is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-2f196f74-df23-40ae-a8fc-c78799d9b1f8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:45.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5424" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5395,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:46.027: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:46.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7554" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5398,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:46.137: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 11:05:50.292: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:50.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-158" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5400,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:50.406: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 11:05:50.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1" in namespace "downward-api-5498" to be "Succeeded or Failed"
May 27 11:05:50.485: INFO: Pod "downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.119584ms
May 27 11:05:52.501: INFO: Pod "downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021716221s
May 27 11:05:54.515: INFO: Pod "downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035668515s
STEP: Saw pod success
May 27 11:05:54.515: INFO: Pod "downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1" satisfied condition "Succeeded or Failed"
May 27 11:05:54.521: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1 container client-container: <nil>
STEP: delete the pod
May 27 11:05:54.574: INFO: Waiting for pod downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1 to disappear
May 27 11:05:54.581: INFO: Pod downwardapi-volume-3536635d-37eb-42b6-9bce-4560736ceeb1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:05:54.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5498" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":5472,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:05:54.613: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 11:05:54.686: INFO: The status of Pod labelsupdate5600a2c3-b337-4665-bb6d-e44f4c64a30c is Pending, waiting for it to be Running (with Ready = true)
May 27 11:05:56.705: INFO: The status of Pod labelsupdate5600a2c3-b337-4665-bb6d-e44f4c64a30c is Running (Ready = true)
May 27 11:05:57.255: INFO: Successfully updated pod "labelsupdate5600a2c3-b337-4665-bb6d-e44f4c64a30c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:01.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2017" for this suite.

• [SLOW TEST:6.742 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5498,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-6492a282-56ec-4599-b00e-d2174794bba4
STEP: Creating a pod to test consume secrets
May 27 11:06:01.448: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a" in namespace "projected-7537" to be "Succeeded or Failed"
May 27 11:06:01.454: INFO: Pod "pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.24152ms
May 27 11:06:03.478: INFO: Pod "pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030453579s
May 27 11:06:05.492: INFO: Pod "pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043984591s
STEP: Saw pod success
May 27 11:06:05.492: INFO: Pod "pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a" satisfied condition "Succeeded or Failed"
May 27 11:06:05.501: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 11:06:05.536: INFO: Waiting for pod pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a to disappear
May 27 11:06:05.544: INFO: Pod pod-projected-secrets-46578199-ac50-4754-8464-2b7dc6e8f06a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:05.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7537" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":5509,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:05.566: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:09.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6599" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":289,"skipped":5512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:09.799: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:06:11.894: INFO: Deleting pod "var-expansion-b65ab75d-beda-4943-8802-d0a07ab4c13b" in namespace "var-expansion-3844"
May 27 11:06:11.948: INFO: Wait up to 5m0s for pod "var-expansion-b65ab75d-beda-4943-8802-d0a07ab4c13b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:13.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3844" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":290,"skipped":5552,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:06:14.071: INFO: Creating deployment "webserver-deployment"
May 27 11:06:14.082: INFO: Waiting for observed generation 1
May 27 11:06:16.169: INFO: Waiting for all required pods to come up
May 27 11:06:16.207: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 27 11:06:18.304: INFO: Waiting for deployment "webserver-deployment" to complete
May 27 11:06:18.328: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 27 11:06:18.369: INFO: Updating deployment webserver-deployment
May 27 11:06:18.369: INFO: Waiting for observed generation 2
May 27 11:06:20.389: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 27 11:06:20.396: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 27 11:06:20.401: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 27 11:06:20.426: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 27 11:06:20.426: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 27 11:06:20.435: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 27 11:06:20.449: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 27 11:06:20.449: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 27 11:06:20.469: INFO: Updating deployment webserver-deployment
May 27 11:06:20.469: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 27 11:06:20.482: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 27 11:06:22.611: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 11:06:22.936: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8465  22989a6e-e750-4e0a-b1ef-cf4696845963 45466 3 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005faf758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-27 11:06:20 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-05-27 11:06:21 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 27 11:06:22.960: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-8465  ab2f13fe-9cea-4108-a229-b3eaad53be00 45461 3 2022-05-27 11:06:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 22989a6e-e750-4e0a-b1ef-cf4696845963 0xc005f67697 0xc005f67698}] []  [{kube-controller-manager Update apps/v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22989a6e-e750-4e0a-b1ef-cf4696845963\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f67738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 11:06:22.971: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 27 11:06:22.971: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-8465  f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 45450 3 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 22989a6e-e750-4e0a-b1ef-cf4696845963 0xc005f67797 0xc005f67798}] []  [{kube-controller-manager Update apps/v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22989a6e-e750-4e0a-b1ef-cf4696845963\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 11:06:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f67828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 27 11:06:23.180: INFO: Pod "webserver-deployment-566f96c878-4g2kx" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-4g2kx webserver-deployment-566f96c878- deployment-8465  b505f9f5-7563-4317-b2f9-dd584c6d8377 45415 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc005fafb37 0xc005fafb38}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-srzxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-srzxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.181: INFO: Pod "webserver-deployment-566f96c878-7d547" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-7d547 webserver-deployment-566f96c878- deployment-8465  dc073341-1964-448c-9b4d-670d51c451ee 45438 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc005fafd27 0xc005fafd28}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79ffd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79ffd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.181: INFO: Pod "webserver-deployment-566f96c878-7nckt" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-7nckt webserver-deployment-566f96c878- deployment-8465  c3035181-de17-43d6-873b-df0dc8445d46 45444 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc005faff17 0xc005faff18}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzkfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzkfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.184: INFO: Pod "webserver-deployment-566f96c878-829pn" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-829pn webserver-deployment-566f96c878- deployment-8465  1d08ad53-5fc8-4a75-9bd9-5ac4ac3436b5 45333 0 2022-05-27 11:06:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006152090 0xc006152091}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-npr6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-npr6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.185: INFO: Pod "webserver-deployment-566f96c878-87cxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-87cxs webserver-deployment-566f96c878- deployment-8465  b5d53702-49e6-44d9-ac60-25147ffe0a4c 45506 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006152277 0xc006152278}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8hql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8hql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.185: INFO: Pod "webserver-deployment-566f96c878-8xr6j" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-8xr6j webserver-deployment-566f96c878- deployment-8465  f84f4ff0-ea13-4f1f-b07a-bcdc6fb02394 45452 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006152467 0xc006152468}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6pqxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6pqxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.186: INFO: Pod "webserver-deployment-566f96c878-9pzbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9pzbv webserver-deployment-566f96c878- deployment-8465  ff4e71a9-836f-48d1-aafb-402f5329a480 45437 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc0061525e0 0xc0061525e1}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8zxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8zxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.186: INFO: Pod "webserver-deployment-566f96c878-d5d5v" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-d5d5v webserver-deployment-566f96c878- deployment-8465  0e8a7a70-edbb-4b44-b683-2c16bc629cba 45348 0 2022-05-27 11:06:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc0061527c7 0xc0061527c8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69hlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69hlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.194: INFO: Pod "webserver-deployment-566f96c878-ksfzr" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-ksfzr webserver-deployment-566f96c878- deployment-8465  c24b5891-cf2d-4f29-b9b4-a60b00df7cff 45445 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc0061529b7 0xc0061529b8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bs7w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bs7w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.196: INFO: Pod "webserver-deployment-566f96c878-l2c5x" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-l2c5x webserver-deployment-566f96c878- deployment-8465  934e7657-26b9-46d7-9357-1a6ee0604099 45352 0 2022-05-27 11:06:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006152ba7 0xc006152ba8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8h6nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8h6nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.196: INFO: Pod "webserver-deployment-566f96c878-m9c49" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-m9c49 webserver-deployment-566f96c878- deployment-8465  72a52f0b-c8e2-43b7-a978-f2ea9c112c95 45507 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006152d97 0xc006152d98}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8xd5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8xd5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.198: INFO: Pod "webserver-deployment-566f96c878-qb7rb" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-qb7rb webserver-deployment-566f96c878- deployment-8465  d8a68e4f-7cfc-4baf-b9fa-1dbf27558141 45318 0 2022-05-27 11:06:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006152f87 0xc006152f88}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxh8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxh8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.198: INFO: Pod "webserver-deployment-566f96c878-ttfn4" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-ttfn4 webserver-deployment-566f96c878- deployment-8465  a918c5fb-e481-4201-b4d9-4cf3919c2a90 45329 0 2022-05-27 11:06:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 ab2f13fe-9cea-4108-a229-b3eaad53be00 0xc006153177 0xc006153178}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab2f13fe-9cea-4108-a229-b3eaad53be00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xc4t8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xc4t8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.212: INFO: Pod "webserver-deployment-5d9fdcc779-9q2nm" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9q2nm webserver-deployment-5d9fdcc779- deployment-8465  ddf34aa9-a6df-42ae-a61b-4f6613497d06 45423 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc006153367 0xc006153368}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2p9bm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2p9bm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.213: INFO: Pod "webserver-deployment-5d9fdcc779-ccbdw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ccbdw webserver-deployment-5d9fdcc779- deployment-8465  5ba49036-d481-43e8-9ab6-2f318f25cd9b 45261 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc006153537 0xc006153538}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sls2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sls2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:10.233.64.23,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8d939fe1f269c20fd8b5df8af80ca9a389ce799708516fcd98ac851dbcf5c64f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.214: INFO: Pod "webserver-deployment-5d9fdcc779-cndmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-cndmt webserver-deployment-5d9fdcc779- deployment-8465  82b929de-0c9d-4bb3-896a-5d84f55293b5 45436 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc006153727 0xc006153728}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xggkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xggkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.215: INFO: Pod "webserver-deployment-5d9fdcc779-d49xb" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-d49xb webserver-deployment-5d9fdcc779- deployment-8465  9ee7d448-509a-42a5-b154-c727ccea7c2a 45410 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061538f7 0xc0061538f8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gh4cd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gh4cd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.216: INFO: Pod "webserver-deployment-5d9fdcc779-dclf2" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-dclf2 webserver-deployment-5d9fdcc779- deployment-8465  2068048b-5bea-46cb-b3ea-c9f140c9f31b 45293 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc006153ac7 0xc006153ac8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rngk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rngk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.65.213,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fe5c0801d188d8c71a30ce56a8d7998bbf691443616c51b5dbf9578785d541de,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.243: INFO: Pod "webserver-deployment-5d9fdcc779-g62kh" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-g62kh webserver-deployment-5d9fdcc779- deployment-8465  af664378-7846-4fbf-bfcc-6e03cd4eb4bd 45402 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc006153cb7 0xc006153cb8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wk7l4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wk7l4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.244: INFO: Pod "webserver-deployment-5d9fdcc779-gfxt2" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gfxt2 webserver-deployment-5d9fdcc779- deployment-8465  1b981c2a-34a3-41a0-a4a5-5dd2f16692d7 45513 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc006153e87 0xc006153e88}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9zpqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9zpqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.271: INFO: Pod "webserver-deployment-5d9fdcc779-h2xpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-h2xpp webserver-deployment-5d9fdcc779- deployment-8465  41860314-98e5-4592-937e-7af368324b21 45439 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8057 0xc0061b8058}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dz5s7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dz5s7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.271: INFO: Pod "webserver-deployment-5d9fdcc779-jg2bl" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jg2bl webserver-deployment-5d9fdcc779- deployment-8465  d9c5bcb3-ed16-43f8-9a57-f6516cf8a776 45245 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8227 0xc0061b8228}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95pxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95pxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.132,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://82e2a6ee117c4dbfa358e2065b17977716aa30a93b0918489a153d4790cf5b6a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.272: INFO: Pod "webserver-deployment-5d9fdcc779-kv4l5" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-kv4l5 webserver-deployment-5d9fdcc779- deployment-8465  fcc6a0e6-ed2a-456f-9be9-c040d4196984 45265 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8417 0xc0061b8418}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jvlcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jvlcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:10.233.64.60,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d7fbbd070e9477c40d78a7c489689d486d2c348859785498fcde316abb343513,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.272: INFO: Pod "webserver-deployment-5d9fdcc779-p7djn" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-p7djn webserver-deployment-5d9fdcc779- deployment-8465  e114b039-6128-43af-bdd5-7b68fbb31592 45475 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8607 0xc0061b8608}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftbwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftbwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.275: INFO: Pod "webserver-deployment-5d9fdcc779-pwndc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-pwndc webserver-deployment-5d9fdcc779- deployment-8465  5ea5ae14-142a-4554-908f-35b65aa843ac 45394 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b87d7 0xc0061b87d8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4l9f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4l9f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.277: INFO: Pod "webserver-deployment-5d9fdcc779-q5pqp" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-q5pqp webserver-deployment-5d9fdcc779- deployment-8465  0fae6518-8ca6-4a90-8c10-0bd34b3eba06 45295 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b89a7 0xc0061b89a8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2tnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2tnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.65.16,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6ca1108087d47558b9bf7fbbe7ed7513ed822eafe8a9d96cc40458064db1b4cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.281: INFO: Pod "webserver-deployment-5d9fdcc779-rhmch" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rhmch webserver-deployment-5d9fdcc779- deployment-8465  f5b04a7a-1888-42e5-a63e-39592394d781 45465 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8b97 0xc0061b8b98}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5v2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5v2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.283: INFO: Pod "webserver-deployment-5d9fdcc779-snpx7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-snpx7 webserver-deployment-5d9fdcc779- deployment-8465  a43e963c-b749-402a-8181-7e1130d1eabd 45418 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8d67 0xc0061b8d68}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5p9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5p9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.295: INFO: Pod "webserver-deployment-5d9fdcc779-svhm4" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-svhm4 webserver-deployment-5d9fdcc779- deployment-8465  2f999fb6-9d10-4a75-995a-a5da726967a4 45298 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b8f37 0xc0061b8f38}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kg986,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kg986,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:10.233.65.209,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e6bc1cbbc0dbf9a3c83d9bede99070dc86a37ec3689be5b431e335f5c522481a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.298: INFO: Pod "webserver-deployment-5d9fdcc779-tcgb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-tcgb8 webserver-deployment-5d9fdcc779- deployment-8465  04ab980c-eac8-4ea2-a031-7b23711e95d4 45458 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b9127 0xc0061b9128}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25wv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25wv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.303: INFO: Pod "webserver-deployment-5d9fdcc779-ttd7f" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-ttd7f webserver-deployment-5d9fdcc779- deployment-8465  f59122f7-1bdb-4c8f-a2d7-c8d0cbc08fa0 45281 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b92f7 0xc0061b92f8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g65cc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g65cc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.192,PodIP:10.233.66.203,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://37213d249ea91c7262f8ca182c94119e2eb79e9c2b4c4bdd51b183e413e3aece,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.305: INFO: Pod "webserver-deployment-5d9fdcc779-vfhnl" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vfhnl webserver-deployment-5d9fdcc779- deployment-8465  6c846c67-0c95-42e9-9922-681ea3e95985 45432 0 2022-05-27 11:06:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b94e7 0xc0061b94e8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkhzr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkhzr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.5,PodIP:,StartTime:2022-05-27 11:06:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 11:06:23.349: INFO: Pod "webserver-deployment-5d9fdcc779-xtz4m" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xtz4m webserver-deployment-5d9fdcc779- deployment-8465  090b5e9b-35ce-44ed-9f3a-56f9ecae3167 45302 0 2022-05-27 11:06:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 f0c620a8-47eb-404d-9fb8-6d4a15a6c94d 0xc0061b96b7 0xc0061b96b8}] []  [{kube-controller-manager Update v1 2022-05-27 11:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0c620a8-47eb-404d-9fb8-6d4a15a6c94d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 11:06:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kkzs7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kkzs7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ohp4eith3vui-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 11:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.118,PodIP:10.233.64.150,StartTime:2022-05-27 11:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 11:06:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://45d3e5c745aabce342c2c26b80a4b4aa6497e168436a5eee974dbcb5cef9df39,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:23.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8465" for this suite.

• [SLOW TEST:9.448 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":291,"skipped":5560,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:23.468: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-e06aa384-4879-41bd-a3dc-18becaf82740
STEP: Creating a pod to test consume secrets
May 27 11:06:24.267: INFO: Waiting up to 5m0s for pod "pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc" in namespace "secrets-6188" to be "Succeeded or Failed"
May 27 11:06:24.279: INFO: Pod "pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.938211ms
May 27 11:06:26.288: INFO: Pod "pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021310201s
May 27 11:06:28.300: INFO: Pod "pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033315271s
May 27 11:06:30.337: INFO: Pod "pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069547784s
STEP: Saw pod success
May 27 11:06:30.337: INFO: Pod "pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc" satisfied condition "Succeeded or Failed"
May 27 11:06:30.360: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc container secret-volume-test: <nil>
STEP: delete the pod
May 27 11:06:30.486: INFO: Waiting for pod pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc to disappear
May 27 11:06:30.495: INFO: Pod pod-secrets-7b1d3ea3-77e7-4df7-afa5-899476d454fc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:30.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6188" for this suite.

• [SLOW TEST:7.259 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5565,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:30.729: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 11:06:31.154: INFO: Waiting up to 5m0s for pod "downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980" in namespace "downward-api-8708" to be "Succeeded or Failed"
May 27 11:06:31.178: INFO: Pod "downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980": Phase="Pending", Reason="", readiness=false. Elapsed: 23.141468ms
May 27 11:06:33.190: INFO: Pod "downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035885986s
May 27 11:06:35.206: INFO: Pod "downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05194189s
May 27 11:06:37.219: INFO: Pod "downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064141223s
STEP: Saw pod success
May 27 11:06:37.219: INFO: Pod "downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980" satisfied condition "Succeeded or Failed"
May 27 11:06:37.225: INFO: Trying to get logs from node ohp4eith3vui-3 pod downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980 container dapi-container: <nil>
STEP: delete the pod
May 27 11:06:37.262: INFO: Waiting for pod downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980 to disappear
May 27 11:06:37.267: INFO: Pod downward-api-ec267d59-f2ad-4a30-b1d6-58ba71ef9980 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:06:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8708" for this suite.

• [SLOW TEST:6.564 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5567,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:06:37.294: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-88g7
STEP: Creating a pod to test atomic-volume-subpath
May 27 11:06:37.392: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-88g7" in namespace "subpath-378" to be "Succeeded or Failed"
May 27 11:06:37.399: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.949822ms
May 27 11:06:39.412: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019553532s
May 27 11:06:41.426: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 4.033793366s
May 27 11:06:43.435: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 6.043350303s
May 27 11:06:45.447: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 8.054574725s
May 27 11:06:47.464: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 10.072244238s
May 27 11:06:49.484: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 12.09231166s
May 27 11:06:51.499: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 14.107206782s
May 27 11:06:53.509: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 16.11651802s
May 27 11:06:55.520: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 18.12790034s
May 27 11:06:57.537: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=true. Elapsed: 20.144732648s
May 27 11:06:59.558: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Running", Reason="", readiness=false. Elapsed: 22.165546975s
May 27 11:07:01.571: INFO: Pod "pod-subpath-test-configmap-88g7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.178536717s
STEP: Saw pod success
May 27 11:07:01.571: INFO: Pod "pod-subpath-test-configmap-88g7" satisfied condition "Succeeded or Failed"
May 27 11:07:01.579: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-subpath-test-configmap-88g7 container test-container-subpath-configmap-88g7: <nil>
STEP: delete the pod
May 27 11:07:01.618: INFO: Waiting for pod pod-subpath-test-configmap-88g7 to disappear
May 27 11:07:01.626: INFO: Pod pod-subpath-test-configmap-88g7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-88g7
May 27 11:07:01.626: INFO: Deleting pod "pod-subpath-test-configmap-88g7" in namespace "subpath-378"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:01.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-378" for this suite.

• [SLOW TEST:24.364 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":294,"skipped":5574,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:01.661: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 11:07:01.760: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 11:07:03.775: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 11:07:03.800: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 11:07:05.813: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 27 11:07:05.841: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 27 11:07:05.849: INFO: Pod pod-with-prestop-exec-hook still exists
May 27 11:07:07.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 27 11:07:07.861: INFO: Pod pod-with-prestop-exec-hook still exists
May 27 11:07:09.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 27 11:07:09.865: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:09.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3785" for this suite.

• [SLOW TEST:8.288 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5584,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:09.953: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 11:07:10.825: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 11:07:13.892: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 27 11:07:15.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=webhook-2397 attach --namespace=webhook-2397 to-be-attached-pod -i -c=container1'
May 27 11:07:16.309: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:16.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2397" for this suite.
STEP: Destroying namespace "webhook-2397-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":296,"skipped":5598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:16.462: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 11:07:16.601: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8" in namespace "projected-1762" to be "Succeeded or Failed"
May 27 11:07:16.614: INFO: Pod "downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.768657ms
May 27 11:07:18.623: INFO: Pod "downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02179068s
May 27 11:07:20.633: INFO: Pod "downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031885693s
STEP: Saw pod success
May 27 11:07:20.633: INFO: Pod "downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8" satisfied condition "Succeeded or Failed"
May 27 11:07:20.640: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8 container client-container: <nil>
STEP: delete the pod
May 27 11:07:20.687: INFO: Waiting for pod downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8 to disappear
May 27 11:07:20.694: INFO: Pod downwardapi-volume-4bd08a1c-6a0a-4937-8be1-4f774d96e3f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:20.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1762" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":297,"skipped":5621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:20.720: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:07:22.811: INFO: Deleting pod "var-expansion-ae63021e-af0f-4fea-8ad9-65d9b719d61d" in namespace "var-expansion-5304"
May 27 11:07:22.831: INFO: Wait up to 5m0s for pod "var-expansion-ae63021e-af0f-4fea-8ad9-65d9b719d61d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:24.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5304" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":298,"skipped":5645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:24.874: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 27 11:07:24.935: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:29.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1427" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5680,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:29.256: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4832
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4832
STEP: Waiting until pod test-pod will start running in namespace statefulset-4832
STEP: Creating statefulset with conflicting port in namespace statefulset-4832
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4832
May 27 11:07:31.444: INFO: Observed stateful pod in namespace: statefulset-4832, name: ss-0, uid: f3d6bf49-f87d-451d-b1d3-6283094e030e, status phase: Pending. Waiting for statefulset controller to delete.
May 27 11:07:31.471: INFO: Observed stateful pod in namespace: statefulset-4832, name: ss-0, uid: f3d6bf49-f87d-451d-b1d3-6283094e030e, status phase: Failed. Waiting for statefulset controller to delete.
May 27 11:07:31.483: INFO: Observed stateful pod in namespace: statefulset-4832, name: ss-0, uid: f3d6bf49-f87d-451d-b1d3-6283094e030e, status phase: Failed. Waiting for statefulset controller to delete.
May 27 11:07:31.493: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4832
STEP: Removing pod with conflicting port in namespace statefulset-4832
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4832 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 11:07:49.650: INFO: Deleting all statefulset in ns statefulset-4832
May 27 11:07:49.658: INFO: Scaling statefulset ss to 0
May 27 11:07:59.697: INFO: Waiting for statefulset status.replicas updated to 0
May 27 11:07:59.705: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:07:59.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4832" for this suite.

• [SLOW TEST:30.505 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":300,"skipped":5686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:07:59.766: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 27 11:07:59.816: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 11:08:04.039: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:23.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4355" for this suite.

• [SLOW TEST:24.207 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":301,"skipped":5710,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:23.974: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 11:08:24.066: INFO: The status of Pod annotationupdate9247c0e3-648f-461d-bd10-69298c66133b is Pending, waiting for it to be Running (with Ready = true)
May 27 11:08:26.074: INFO: The status of Pod annotationupdate9247c0e3-648f-461d-bd10-69298c66133b is Running (Ready = true)
May 27 11:08:26.618: INFO: Successfully updated pod "annotationupdate9247c0e3-648f-461d-bd10-69298c66133b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:30.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-719" for this suite.

• [SLOW TEST:6.716 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5726,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:30.691: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-8dbcc782-cc7f-4e35-af1a-c4cbc8cc54a4
STEP: Creating a pod to test consume configMaps
May 27 11:08:30.782: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f" in namespace "projected-3576" to be "Succeeded or Failed"
May 27 11:08:30.789: INFO: Pod "pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.017323ms
May 27 11:08:32.804: INFO: Pod "pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021929912s
May 27 11:08:34.817: INFO: Pod "pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03495707s
STEP: Saw pod success
May 27 11:08:34.817: INFO: Pod "pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f" satisfied condition "Succeeded or Failed"
May 27 11:08:34.823: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f container agnhost-container: <nil>
STEP: delete the pod
May 27 11:08:34.860: INFO: Waiting for pod pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f to disappear
May 27 11:08:34.865: INFO: Pod pod-projected-configmaps-510dc9e6-a813-4bee-8c48-a95f950b749f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:34.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3576" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5730,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:34.889: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
May 27 11:08:34.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 create -f -'
May 27 11:08:36.413: INFO: stderr: ""
May 27 11:08:36.413: INFO: stdout: "pod/pause created\n"
May 27 11:08:36.413: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 27 11:08:36.413: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-734" to be "running and ready"
May 27 11:08:36.443: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 30.679783ms
May 27 11:08:38.457: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.043927502s
May 27 11:08:38.457: INFO: Pod "pause" satisfied condition "running and ready"
May 27 11:08:38.457: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
May 27 11:08:38.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 label pods pause testing-label=testing-label-value'
May 27 11:08:38.598: INFO: stderr: ""
May 27 11:08:38.598: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 27 11:08:38.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 get pod pause -L testing-label'
May 27 11:08:38.726: INFO: stderr: ""
May 27 11:08:38.727: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 27 11:08:38.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 label pods pause testing-label-'
May 27 11:08:38.878: INFO: stderr: ""
May 27 11:08:38.878: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 27 11:08:38.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 get pod pause -L testing-label'
May 27 11:08:39.013: INFO: stderr: ""
May 27 11:08:39.013: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
May 27 11:08:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 delete --grace-period=0 --force -f -'
May 27 11:08:39.181: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 11:08:39.181: INFO: stdout: "pod \"pause\" force deleted\n"
May 27 11:08:39.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 get rc,svc -l name=pause --no-headers'
May 27 11:08:39.341: INFO: stderr: "No resources found in kubectl-734 namespace.\n"
May 27 11:08:39.341: INFO: stdout: ""
May 27 11:08:39.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-734 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 27 11:08:39.495: INFO: stderr: ""
May 27 11:08:39.495: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:39.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-734" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":304,"skipped":5748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:39.526: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 11:08:39.594: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 27 11:08:39.609: INFO: starting watch
STEP: patching
STEP: updating
May 27 11:08:39.638: INFO: waiting for watch events with expected annotations
May 27 11:08:39.639: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:39.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7300" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":305,"skipped":5800,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:39.747: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 11:08:39.797: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 11:08:39.818: INFO: Waiting for terminating namespaces to be deleted...
May 27 11:08:39.825: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-1 before test
May 27 11:08:39.839: INFO: echo-other-node-59d779959c-8lmx4 from cilium-test started at 2022-05-27 09:01:23 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.839: INFO: 	Container echo-other-node ready: true, restart count 1
May 27 11:08:39.839: INFO: cilium-node-init-cmpq2 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.839: INFO: 	Container node-init ready: true, restart count 1
May 27 11:08:39.839: INFO: cilium-pb7c6 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.839: INFO: 	Container cilium-agent ready: true, restart count 3
May 27 11:08:39.839: INFO: coredns-64897985d-v76xc from kube-system started at 2022-05-27 08:54:45 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.839: INFO: 	Container coredns ready: true, restart count 1
May 27 11:08:39.840: INFO: kube-addon-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.840: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 11:08:39.840: INFO: kube-apiserver-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.840: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 11:08:39.840: INFO: kube-controller-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.840: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 11:08:39.840: INFO: kube-proxy-bt99l from kube-system started at 2022-05-27 08:20:34 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.840: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:08:39.841: INFO: kube-scheduler-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.841: INFO: 	Container kube-scheduler ready: true, restart count 1
May 27 11:08:39.841: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-6cftw from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:08:39.841: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 11:08:39.841: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 11:08:39.841: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-2 before test
May 27 11:08:39.861: INFO: client-7568bc7f86-h8tfj from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.861: INFO: 	Container client ready: true, restart count 0
May 27 11:08:39.861: INFO: client2-686d5f784b-b24x8 from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.861: INFO: 	Container client2 ready: true, restart count 0
May 27 11:08:39.861: INFO: echo-same-node-5767b7b99d-9t6kf from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.861: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 11:08:39.861: INFO: cilium-27t29 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.861: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 11:08:39.862: INFO: cilium-node-init-xkmdh from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container node-init ready: true, restart count 1
May 27 11:08:39.862: INFO: cilium-operator-59d6f769d4-gh6js from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container cilium-operator ready: true, restart count 1
May 27 11:08:39.862: INFO: coredns-64897985d-f7x88 from kube-system started at 2022-05-27 08:54:30 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container coredns ready: true, restart count 1
May 27 11:08:39.862: INFO: kube-addon-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 11:08:39.862: INFO: kube-apiserver-ohp4eith3vui-2 from kube-system started at 2022-05-27 08:56:11 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 11:08:39.862: INFO: kube-controller-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 11:08:39.862: INFO: kube-proxy-8qhjn from kube-system started at 2022-05-27 08:20:49 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:08:39.862: INFO: kube-scheduler-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container kube-scheduler ready: true, restart count 2
May 27 11:08:39.862: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-t4l8w from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:08:39.862: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 11:08:39.862: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 11:08:39.862: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-3 before test
May 27 11:08:39.890: INFO: cilium-mgkvh from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.890: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 11:08:39.890: INFO: cilium-node-init-q8q6q from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.890: INFO: 	Container node-init ready: true, restart count 1
May 27 11:08:39.890: INFO: kube-proxy-xvr6w from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.890: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:08:39.890: INFO: sonobuoy from sonobuoy started at 2022-05-27 09:38:51 +0000 UTC (1 container statuses recorded)
May 27 11:08:39.890: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 11:08:39.890: INFO: sonobuoy-e2e-job-3507e2c9ba704a39 from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:08:39.890: INFO: 	Container e2e ready: true, restart count 0
May 27 11:08:39.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 11:08:39.890: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-kllxz from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:08:39.891: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 11:08:39.891: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node ohp4eith3vui-1
STEP: verifying the node has the label node ohp4eith3vui-2
STEP: verifying the node has the label node ohp4eith3vui-3
May 27 11:08:40.135: INFO: Pod client-7568bc7f86-h8tfj requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.135: INFO: Pod client2-686d5f784b-b24x8 requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.135: INFO: Pod echo-other-node-59d779959c-8lmx4 requesting resource cpu=0m on Node ohp4eith3vui-1
May 27 11:08:40.135: INFO: Pod echo-same-node-5767b7b99d-9t6kf requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.135: INFO: Pod cilium-27t29 requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.136: INFO: Pod cilium-mgkvh requesting resource cpu=0m on Node ohp4eith3vui-3
May 27 11:08:40.136: INFO: Pod cilium-node-init-cmpq2 requesting resource cpu=0m on Node ohp4eith3vui-1
May 27 11:08:40.136: INFO: Pod cilium-node-init-q8q6q requesting resource cpu=0m on Node ohp4eith3vui-3
May 27 11:08:40.137: INFO: Pod cilium-node-init-xkmdh requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod cilium-operator-59d6f769d4-gh6js requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod cilium-pb7c6 requesting resource cpu=0m on Node ohp4eith3vui-1
May 27 11:08:40.137: INFO: Pod coredns-64897985d-f7x88 requesting resource cpu=100m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod coredns-64897985d-v76xc requesting resource cpu=100m on Node ohp4eith3vui-1
May 27 11:08:40.137: INFO: Pod kube-addon-manager-ohp4eith3vui-1 requesting resource cpu=5m on Node ohp4eith3vui-1
May 27 11:08:40.137: INFO: Pod kube-addon-manager-ohp4eith3vui-2 requesting resource cpu=5m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod kube-apiserver-ohp4eith3vui-1 requesting resource cpu=250m on Node ohp4eith3vui-1
May 27 11:08:40.137: INFO: Pod kube-apiserver-ohp4eith3vui-2 requesting resource cpu=250m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod kube-controller-manager-ohp4eith3vui-1 requesting resource cpu=200m on Node ohp4eith3vui-1
May 27 11:08:40.137: INFO: Pod kube-controller-manager-ohp4eith3vui-2 requesting resource cpu=200m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod kube-proxy-8qhjn requesting resource cpu=0m on Node ohp4eith3vui-2
May 27 11:08:40.137: INFO: Pod kube-proxy-bt99l requesting resource cpu=0m on Node ohp4eith3vui-1
May 27 11:08:40.138: INFO: Pod kube-proxy-xvr6w requesting resource cpu=0m on Node ohp4eith3vui-3
May 27 11:08:40.138: INFO: Pod kube-scheduler-ohp4eith3vui-1 requesting resource cpu=100m on Node ohp4eith3vui-1
May 27 11:08:40.138: INFO: Pod kube-scheduler-ohp4eith3vui-2 requesting resource cpu=100m on Node ohp4eith3vui-2
May 27 11:08:40.138: INFO: Pod sonobuoy requesting resource cpu=0m on Node ohp4eith3vui-3
May 27 11:08:40.139: INFO: Pod sonobuoy-e2e-job-3507e2c9ba704a39 requesting resource cpu=0m on Node ohp4eith3vui-3
May 27 11:08:40.139: INFO: Pod sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-6cftw requesting resource cpu=0m on Node ohp4eith3vui-1
May 27 11:08:40.139: INFO: Pod sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-kllxz requesting resource cpu=0m on Node ohp4eith3vui-3
May 27 11:08:40.139: INFO: Pod sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-t4l8w requesting resource cpu=0m on Node ohp4eith3vui-2
STEP: Starting Pods to consume most of the cluster CPU.
May 27 11:08:40.140: INFO: Creating a pod which consumes cpu=661m on Node ohp4eith3vui-1
May 27 11:08:40.159: INFO: Creating a pod which consumes cpu=661m on Node ohp4eith3vui-2
May 27 11:08:40.191: INFO: Creating a pod which consumes cpu=1120m on Node ohp4eith3vui-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed42da9-768f-4c89-9728-a4778f361749.16f2f1a8bd1b4af3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7659/filler-pod-1ed42da9-768f-4c89-9728-a4778f361749 to ohp4eith3vui-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed42da9-768f-4c89-9728-a4778f361749.16f2f1a90b162802], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed42da9-768f-4c89-9728-a4778f361749.16f2f1a9171c376d], Reason = [Created], Message = [Created container filler-pod-1ed42da9-768f-4c89-9728-a4778f361749]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1ed42da9-768f-4c89-9728-a4778f361749.16f2f1a919adacc8], Reason = [Started], Message = [Started container filler-pod-1ed42da9-768f-4c89-9728-a4778f361749]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4.16f2f1a8bfcc49f0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7659/filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4 to ohp4eith3vui-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4.16f2f1a8faea2a60], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4.16f2f1a9023e4e13], Reason = [Created], Message = [Created container filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4.16f2f1a908898c9c], Reason = [Started], Message = [Started container filler-pod-3240eeb3-df0d-4e34-9155-d3fe10fa03d4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229.16f2f1a8bede1a4d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7659/filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229 to ohp4eith3vui-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229.16f2f1a90670e4cd], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229.16f2f1a91067436d], Reason = [Created], Message = [Created container filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229.16f2f1a9136c44a3], Reason = [Started], Message = [Started container filler-pod-ea41bf6e-f61e-4e92-8f0b-065b50b8e229]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f2f1a9acf51b97], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ohp4eith3vui-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ohp4eith3vui-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ohp4eith3vui-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:45.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7659" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.655 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":306,"skipped":5807,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:45.410: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:08:45.514: INFO: Create a RollingUpdate DaemonSet
May 27 11:08:45.526: INFO: Check that daemon pods launch on every node of the cluster
May 27 11:08:45.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:08:45.549: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:08:46.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:08:46.586: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:08:47.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 11:08:47.567: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May 27 11:08:47.567: INFO: Update the DaemonSet to trigger a rollout
May 27 11:08:47.582: INFO: Updating DaemonSet daemon-set
May 27 11:08:50.614: INFO: Roll back the DaemonSet before rollout is complete
May 27 11:08:50.629: INFO: Updating DaemonSet daemon-set
May 27 11:08:50.629: INFO: Make sure DaemonSet rollback is complete
May 27 11:08:50.634: INFO: Wrong image for pod: daemon-set-pqj2j. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May 27 11:08:50.634: INFO: Pod daemon-set-pqj2j is not available
May 27 11:08:56.654: INFO: Pod daemon-set-cx6nb is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4207, will wait for the garbage collector to delete the pods
May 27 11:08:56.740: INFO: Deleting DaemonSet.extensions daemon-set took: 10.671365ms
May 27 11:08:56.841: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.49881ms
May 27 11:08:59.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:08:59.650: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 11:08:59.654: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46956"},"items":null}

May 27 11:08:59.659: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46956"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:59.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4207" for this suite.

• [SLOW TEST:14.288 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":307,"skipped":5820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:59.700: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
May 27 11:08:59.749: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6657 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:08:59.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6657" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":308,"skipped":5866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:08:59.872: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 11:09:03.983: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:04.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6040" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":309,"skipped":5892,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:04.029: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 11:09:04.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1213 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 27 11:09:04.194: INFO: stderr: ""
May 27 11:09:04.194: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 27 11:09:09.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1213 get pod e2e-test-httpd-pod -o json'
May 27 11:09:09.405: INFO: stderr: ""
May 27 11:09:09.405: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-05-27T11:09:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1213\",\n        \"resourceVersion\": \"47055\",\n        \"uid\": \"d73ce4c7-7269-4bf5-9062-c604074aec92\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-sc8fg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ohp4eith3vui-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-sc8fg\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T11:09:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T11:09:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T11:09:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T11:09:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://74448091ec1bcb9c978540b6e42f477661715248d34e3bc3482fa9d7c3f6a9a5\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-05-27T11:09:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.192\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.144\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.144\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-05-27T11:09:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 27 11:09:09.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1213 replace -f -'
May 27 11:09:09.788: INFO: stderr: ""
May 27 11:09:09.788: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
May 27 11:09:09.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1213 delete pods e2e-test-httpd-pod'
May 27 11:09:11.624: INFO: stderr: ""
May 27 11:09:11.624: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:11.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1213" for this suite.

• [SLOW TEST:7.616 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":310,"skipped":5897,"failed":0}
S
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:11.645: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 11:09:11.726: INFO: starting watch
STEP: patching
STEP: updating
May 27 11:09:11.743: INFO: waiting for watch events with expected annotations
May 27 11:09:11.743: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:11.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2704" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":311,"skipped":5898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:11.790: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 11:09:11.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4" in namespace "downward-api-3515" to be "Succeeded or Failed"
May 27 11:09:11.844: INFO: Pod "downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.570616ms
May 27 11:09:13.852: INFO: Pod "downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012480443s
May 27 11:09:15.862: INFO: Pod "downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022083225s
STEP: Saw pod success
May 27 11:09:15.862: INFO: Pod "downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4" satisfied condition "Succeeded or Failed"
May 27 11:09:15.868: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4 container client-container: <nil>
STEP: delete the pod
May 27 11:09:15.960: INFO: Waiting for pod downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4 to disappear
May 27 11:09:15.966: INFO: Pod downwardapi-volume-0136c998-021a-4271-80af-b90af1b586f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:15.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3515" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":5965,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 27 11:09:17.313: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
May 27 11:09:19.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 11, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 11, 9, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 11, 9, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 11, 9, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-bb9577b7b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 11:09:22.378: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:09:22.385: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:25.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5865" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.819 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":313,"skipped":5983,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:25.808: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 11:09:25.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e" in namespace "projected-3573" to be "Succeeded or Failed"
May 27 11:09:25.891: INFO: Pod "downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07193ms
May 27 11:09:27.925: INFO: Pod "downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042366906s
May 27 11:09:29.936: INFO: Pod "downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05251175s
STEP: Saw pod success
May 27 11:09:29.936: INFO: Pod "downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e" satisfied condition "Succeeded or Failed"
May 27 11:09:29.948: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e container client-container: <nil>
STEP: delete the pod
May 27 11:09:29.984: INFO: Waiting for pod downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e to disappear
May 27 11:09:29.991: INFO: Pod downwardapi-volume-81376862-c9fc-48ad-9598-4bb8c8830f7e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:29.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3573" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":314,"skipped":5983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:30.011: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 11:09:31.074: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 11:09:34.117: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:34.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1663" for this suite.
STEP: Destroying namespace "webhook-1663-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":315,"skipped":6009,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:34.641: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
May 27 11:09:34.725: INFO: Waiting up to 5m0s for pod "pod-9febe92d-d800-4b6f-abd0-52c4975c332e" in namespace "emptydir-93" to be "Succeeded or Failed"
May 27 11:09:34.733: INFO: Pod "pod-9febe92d-d800-4b6f-abd0-52c4975c332e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.587545ms
May 27 11:09:36.746: INFO: Pod "pod-9febe92d-d800-4b6f-abd0-52c4975c332e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020878206s
May 27 11:09:38.765: INFO: Pod "pod-9febe92d-d800-4b6f-abd0-52c4975c332e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039504312s
STEP: Saw pod success
May 27 11:09:38.765: INFO: Pod "pod-9febe92d-d800-4b6f-abd0-52c4975c332e" satisfied condition "Succeeded or Failed"
May 27 11:09:38.769: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-9febe92d-d800-4b6f-abd0-52c4975c332e container test-container: <nil>
STEP: delete the pod
May 27 11:09:38.804: INFO: Waiting for pod pod-9febe92d-d800-4b6f-abd0-52c4975c332e to disappear
May 27 11:09:38.812: INFO: Pod pod-9febe92d-d800-4b6f-abd0-52c4975c332e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:38.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-93" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":316,"skipped":6016,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:38.832: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-2ece29e6-a9a4-4f3c-81f4-68d622b07f76
STEP: Creating a pod to test consume configMaps
May 27 11:09:38.890: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73" in namespace "projected-2077" to be "Succeeded or Failed"
May 27 11:09:38.898: INFO: Pod "pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73": Phase="Pending", Reason="", readiness=false. Elapsed: 7.363237ms
May 27 11:09:40.913: INFO: Pod "pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02207565s
May 27 11:09:42.932: INFO: Pod "pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041197347s
STEP: Saw pod success
May 27 11:09:42.932: INFO: Pod "pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73" satisfied condition "Succeeded or Failed"
May 27 11:09:42.938: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73 container agnhost-container: <nil>
STEP: delete the pod
May 27 11:09:42.966: INFO: Waiting for pod pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73 to disappear
May 27 11:09:42.971: INFO: Pod pod-projected-configmaps-985da7f6-920c-4f4d-992e-ae8646076f73 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:42.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2077" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":317,"skipped":6032,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:42.993: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-e7d12786-7b5e-4233-9d6d-75ac94fa5e64
STEP: Creating a pod to test consume configMaps
May 27 11:09:43.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4" in namespace "projected-8540" to be "Succeeded or Failed"
May 27 11:09:43.072: INFO: Pod "pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.807429ms
May 27 11:09:45.080: INFO: Pod "pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012903077s
May 27 11:09:47.093: INFO: Pod "pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025120205s
STEP: Saw pod success
May 27 11:09:47.093: INFO: Pod "pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4" satisfied condition "Succeeded or Failed"
May 27 11:09:47.098: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4 container agnhost-container: <nil>
STEP: delete the pod
May 27 11:09:47.130: INFO: Waiting for pod pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4 to disappear
May 27 11:09:47.134: INFO: Pod pod-projected-configmaps-d4a1e8c1-b006-4ac5-80fd-b30777b6e5f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:47.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8540" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":6047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:47.150: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
May 27 11:09:47.226: INFO: Waiting up to 5m0s for pod "var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c" in namespace "var-expansion-4319" to be "Succeeded or Failed"
May 27 11:09:47.245: INFO: Pod "var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.839217ms
May 27 11:09:49.254: INFO: Pod "var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02753791s
May 27 11:09:51.268: INFO: Pod "var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041699376s
STEP: Saw pod success
May 27 11:09:51.268: INFO: Pod "var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c" satisfied condition "Succeeded or Failed"
May 27 11:09:51.274: INFO: Trying to get logs from node ohp4eith3vui-3 pod var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c container dapi-container: <nil>
STEP: delete the pod
May 27 11:09:51.311: INFO: Waiting for pod var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c to disappear
May 27 11:09:51.316: INFO: Pod var-expansion-2bf74cc7-dc09-484e-9826-5a6229bf296c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:51.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4319" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":6071,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:51.339: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-42af5517-8b65-4f35-a942-6e4c1dd77241
STEP: Creating a pod to test consume secrets
May 27 11:09:51.474: INFO: Waiting up to 5m0s for pod "pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38" in namespace "secrets-7616" to be "Succeeded or Failed"
May 27 11:09:51.479: INFO: Pod "pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.417154ms
May 27 11:09:53.488: INFO: Pod "pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013649743s
May 27 11:09:55.496: INFO: Pod "pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02183463s
STEP: Saw pod success
May 27 11:09:55.497: INFO: Pod "pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38" satisfied condition "Succeeded or Failed"
May 27 11:09:55.502: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38 container secret-volume-test: <nil>
STEP: delete the pod
May 27 11:09:55.537: INFO: Waiting for pod pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38 to disappear
May 27 11:09:55.541: INFO: Pod pod-secrets-39a464a1-4522-4b37-9fdb-a2084e77ca38 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:09:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7616" for this suite.
STEP: Destroying namespace "secret-namespace-5235" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":6071,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:09:55.572: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5136
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 11:09:55.632: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 11:09:55.679: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 11:09:57.693: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:09:59.688: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:01.710: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:03.688: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:05.689: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:07.686: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:09.695: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:11.691: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:13.691: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:15.688: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 11:10:17.692: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 11:10:17.704: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 11:10:17.712: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 11:10:21.780: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 11:10:21.780: INFO: Going to poll 10.233.65.125 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 27 11:10:21.786: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.125:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5136 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 11:10:21.786: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 11:10:21.787: INFO: ExecWithOptions: Clientset creation
May 27 11:10:21.787: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5136/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.125%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 11:10:21.918: INFO: Found all 1 expected endpoints: [netserver-0]
May 27 11:10:21.919: INFO: Going to poll 10.233.64.58 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 27 11:10:21.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.58:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5136 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 11:10:21.927: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 11:10:21.933: INFO: ExecWithOptions: Clientset creation
May 27 11:10:21.934: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5136/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.58%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 11:10:22.040: INFO: Found all 1 expected endpoints: [netserver-1]
May 27 11:10:22.040: INFO: Going to poll 10.233.66.102 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 27 11:10:22.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.102:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5136 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 11:10:22.047: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 11:10:22.048: INFO: ExecWithOptions: Clientset creation
May 27 11:10:22.048: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5136/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.102%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 11:10:22.131: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:10:22.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5136" for this suite.

• [SLOW TEST:26.585 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":321,"skipped":6079,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:10:22.158: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 27 11:10:24.245: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-519 PodName:var-expansion-af524225-0a17-43ed-8d53-a95829813e79 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 11:10:24.245: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 11:10:24.247: INFO: ExecWithOptions: Clientset creation
May 27 11:10:24.247: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-519/pods/var-expansion-af524225-0a17-43ed-8d53-a95829813e79/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
May 27 11:10:24.350: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-519 PodName:var-expansion-af524225-0a17-43ed-8d53-a95829813e79 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 11:10:24.350: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
May 27 11:10:24.351: INFO: ExecWithOptions: Clientset creation
May 27 11:10:24.351: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-519/pods/var-expansion-af524225-0a17-43ed-8d53-a95829813e79/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
May 27 11:10:24.961: INFO: Successfully updated pod "var-expansion-af524225-0a17-43ed-8d53-a95829813e79"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 27 11:10:24.970: INFO: Deleting pod "var-expansion-af524225-0a17-43ed-8d53-a95829813e79" in namespace "var-expansion-519"
May 27 11:10:24.986: INFO: Wait up to 5m0s for pod "var-expansion-af524225-0a17-43ed-8d53-a95829813e79" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:10:59.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-519" for this suite.

• [SLOW TEST:36.872 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":322,"skipped":6082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:10:59.041: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:10:59.169: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"740924e0-8655-42ab-8654-36b88bf2cb5f", Controller:(*bool)(0xc00718a4d6), BlockOwnerDeletion:(*bool)(0xc00718a4d7)}}
May 27 11:10:59.205: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"79558f85-c079-4a92-b3ca-be90583a559f", Controller:(*bool)(0xc00718a756), BlockOwnerDeletion:(*bool)(0xc00718a757)}}
May 27 11:10:59.221: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f7b2d23a-2077-4653-b4dc-8c587de0ccd3", Controller:(*bool)(0xc00054cfd6), BlockOwnerDeletion:(*bool)(0xc00054cfd7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:04.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-611" for this suite.

• [SLOW TEST:5.234 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":323,"skipped":6134,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:04.277: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
May 27 11:11:04.338: INFO: namespace kubectl-7118
May 27 11:11:04.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-7118 create -f -'
May 27 11:11:06.386: INFO: stderr: ""
May 27 11:11:06.386: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 27 11:11:07.414: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 11:11:07.414: INFO: Found 0 / 1
May 27 11:11:08.398: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 11:11:08.398: INFO: Found 1 / 1
May 27 11:11:08.398: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 27 11:11:08.404: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 11:11:08.404: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 27 11:11:08.404: INFO: wait on agnhost-primary startup in kubectl-7118 
May 27 11:11:08.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-7118 logs agnhost-primary-9qgh5 agnhost-primary'
May 27 11:11:08.577: INFO: stderr: ""
May 27 11:11:08.577: INFO: stdout: "Paused\n"
STEP: exposing RC
May 27 11:11:08.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-7118 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 27 11:11:08.712: INFO: stderr: ""
May 27 11:11:08.712: INFO: stdout: "service/rm2 exposed\n"
May 27 11:11:08.724: INFO: Service rm2 in namespace kubectl-7118 found.
STEP: exposing service
May 27 11:11:10.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-7118 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 27 11:11:10.880: INFO: stderr: ""
May 27 11:11:10.880: INFO: stdout: "service/rm3 exposed\n"
May 27 11:11:10.891: INFO: Service rm3 in namespace kubectl-7118 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:12.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7118" for this suite.

• [SLOW TEST:8.665 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":324,"skipped":6136,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:12.947: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:24.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4462" for this suite.

• [SLOW TEST:11.334 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":325,"skipped":6150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:24.286: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1510
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1510
STEP: creating replication controller externalsvc in namespace services-1510
I0527 11:11:24.407293      15 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1510, replica count: 2
I0527 11:11:27.459322      15 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 27 11:11:27.498: INFO: Creating new exec pod
May 27 11:11:29.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-1510 exec execpodphbfs -- /bin/sh -x -c nslookup clusterip-service.services-1510.svc.cluster.local'
May 27 11:11:29.872: INFO: stderr: "+ nslookup clusterip-service.services-1510.svc.cluster.local\n"
May 27 11:11:29.872: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-1510.svc.cluster.local\tcanonical name = externalsvc.services-1510.svc.cluster.local.\nName:\texternalsvc.services-1510.svc.cluster.local\nAddress: 10.233.8.65\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1510, will wait for the garbage collector to delete the pods
May 27 11:11:29.943: INFO: Deleting ReplicationController externalsvc took: 11.536357ms
May 27 11:11:30.044: INFO: Terminating ReplicationController externalsvc pods took: 100.351024ms
May 27 11:11:32.282: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:32.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1510" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.047 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":326,"skipped":6185,"failed":0}
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:32.334: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 11:11:32.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33" in namespace "projected-7408" to be "Succeeded or Failed"
May 27 11:11:32.408: INFO: Pod "downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97534ms
May 27 11:11:34.423: INFO: Pod "downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020444128s
May 27 11:11:36.435: INFO: Pod "downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033050582s
STEP: Saw pod success
May 27 11:11:36.435: INFO: Pod "downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33" satisfied condition "Succeeded or Failed"
May 27 11:11:36.442: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33 container client-container: <nil>
STEP: delete the pod
May 27 11:11:36.486: INFO: Waiting for pod downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33 to disappear
May 27 11:11:36.495: INFO: Pod downwardapi-volume-d3771a9d-3d45-4887-aa77-6a77bcb9db33 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:36.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7408" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6185,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:36.516: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 11:11:36.569: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 11:11:36.583: INFO: Waiting for terminating namespaces to be deleted...
May 27 11:11:36.590: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-1 before test
May 27 11:11:36.606: INFO: echo-other-node-59d779959c-8lmx4 from cilium-test started at 2022-05-27 09:01:23 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container echo-other-node ready: true, restart count 1
May 27 11:11:36.606: INFO: cilium-node-init-cmpq2 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container node-init ready: true, restart count 1
May 27 11:11:36.606: INFO: cilium-pb7c6 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container cilium-agent ready: true, restart count 3
May 27 11:11:36.606: INFO: coredns-64897985d-v76xc from kube-system started at 2022-05-27 08:54:45 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container coredns ready: true, restart count 1
May 27 11:11:36.606: INFO: kube-addon-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 11:11:36.606: INFO: kube-apiserver-ohp4eith3vui-1 from kube-system started at 2022-05-27 09:24:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 11:11:36.606: INFO: kube-controller-manager-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 11:11:36.606: INFO: kube-proxy-bt99l from kube-system started at 2022-05-27 08:20:34 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:11:36.606: INFO: kube-scheduler-ohp4eith3vui-1 from kube-system started at 2022-05-27 08:55:54 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container kube-scheduler ready: true, restart count 1
May 27 11:11:36.606: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-6cftw from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:11:36.606: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 11:11:36.606: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 11:11:36.606: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-2 before test
May 27 11:11:36.622: INFO: client-7568bc7f86-h8tfj from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container client ready: true, restart count 0
May 27 11:11:36.622: INFO: client2-686d5f784b-b24x8 from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container client2 ready: true, restart count 0
May 27 11:11:36.622: INFO: echo-same-node-5767b7b99d-9t6kf from cilium-test started at 2022-05-27 09:59:37 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 11:11:36.622: INFO: cilium-27t29 from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 11:11:36.622: INFO: cilium-node-init-xkmdh from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container node-init ready: true, restart count 1
May 27 11:11:36.622: INFO: cilium-operator-59d6f769d4-gh6js from kube-system started at 2022-05-27 08:53:32 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container cilium-operator ready: true, restart count 1
May 27 11:11:36.622: INFO: coredns-64897985d-f7x88 from kube-system started at 2022-05-27 08:54:30 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container coredns ready: true, restart count 1
May 27 11:11:36.622: INFO: kube-addon-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container kube-addon-manager ready: true, restart count 1
May 27 11:11:36.622: INFO: kube-apiserver-ohp4eith3vui-2 from kube-system started at 2022-05-27 08:56:11 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container kube-apiserver ready: true, restart count 1
May 27 11:11:36.622: INFO: kube-controller-manager-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container kube-controller-manager ready: true, restart count 1
May 27 11:11:36.622: INFO: kube-proxy-8qhjn from kube-system started at 2022-05-27 08:20:49 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:11:36.622: INFO: kube-scheduler-ohp4eith3vui-2 from kube-system started at 2022-05-27 09:26:12 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container kube-scheduler ready: true, restart count 2
May 27 11:11:36.622: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-t4l8w from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:11:36.622: INFO: 	Container sonobuoy-worker ready: false, restart count 0
May 27 11:11:36.622: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 11:11:36.622: INFO: 
Logging pods the apiserver thinks is on node ohp4eith3vui-3 before test
May 27 11:11:36.637: INFO: cilium-mgkvh from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container cilium-agent ready: true, restart count 1
May 27 11:11:36.637: INFO: cilium-node-init-q8q6q from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container node-init ready: true, restart count 1
May 27 11:11:36.637: INFO: kube-proxy-xvr6w from kube-system started at 2022-05-27 08:56:19 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container kube-proxy ready: true, restart count 1
May 27 11:11:36.637: INFO: execpodphbfs from services-1510 started at 2022-05-27 11:11:27 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container agnhost-container ready: true, restart count 0
May 27 11:11:36.637: INFO: sonobuoy from sonobuoy started at 2022-05-27 09:38:51 +0000 UTC (1 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 11:11:36.637: INFO: sonobuoy-e2e-job-3507e2c9ba704a39 from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container e2e ready: true, restart count 0
May 27 11:11:36.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 11:11:36.637: INFO: sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-kllxz from sonobuoy started at 2022-05-27 09:38:53 +0000 UTC (2 container statuses recorded)
May 27 11:11:36.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 11:11:36.637: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f2f1d1d2730a82], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:37.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7093" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":328,"skipped":6203,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:37.714: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 27 11:11:37.801: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 27 11:11:37.846: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:37.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3332" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":329,"skipped":6212,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:37.978: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 27 11:11:38.051: INFO: Waiting up to 5m0s for pod "pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da" in namespace "emptydir-3993" to be "Succeeded or Failed"
May 27 11:11:38.056: INFO: Pod "pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.938061ms
May 27 11:11:40.065: INFO: Pod "pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013844951s
May 27 11:11:42.078: INFO: Pod "pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027424805s
STEP: Saw pod success
May 27 11:11:42.079: INFO: Pod "pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da" satisfied condition "Succeeded or Failed"
May 27 11:11:42.085: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da container test-container: <nil>
STEP: delete the pod
May 27 11:11:42.122: INFO: Waiting for pod pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da to disappear
May 27 11:11:42.130: INFO: Pod pod-55bdf5ea-55e7-4ab1-962d-cef0cb5a28da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3993" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":330,"skipped":6246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 11:11:42.221: INFO: The status of Pod labelsupdatef1691701-8803-44c7-80cc-e2a16c1bfc1d is Pending, waiting for it to be Running (with Ready = true)
May 27 11:11:44.231: INFO: The status of Pod labelsupdatef1691701-8803-44c7-80cc-e2a16c1bfc1d is Running (Ready = true)
May 27 11:11:44.780: INFO: Successfully updated pod "labelsupdatef1691701-8803-44c7-80cc-e2a16c1bfc1d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:46.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8304" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":331,"skipped":6268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:46.840: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
May 27 11:11:46.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 create -f -'
May 27 11:11:47.250: INFO: stderr: ""
May 27 11:11:47.250: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 11:11:47.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 11:11:47.436: INFO: stderr: ""
May 27 11:11:47.436: INFO: stdout: "update-demo-nautilus-49zdr update-demo-nautilus-x7mcv "
May 27 11:11:47.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-49zdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 11:11:47.580: INFO: stderr: ""
May 27 11:11:47.580: INFO: stdout: ""
May 27 11:11:47.580: INFO: update-demo-nautilus-49zdr is created but not running
May 27 11:11:52.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 11:11:52.725: INFO: stderr: ""
May 27 11:11:52.725: INFO: stdout: "update-demo-nautilus-49zdr update-demo-nautilus-x7mcv "
May 27 11:11:52.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-49zdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 11:11:52.848: INFO: stderr: ""
May 27 11:11:52.848: INFO: stdout: "true"
May 27 11:11:52.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-49zdr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 11:11:52.991: INFO: stderr: ""
May 27 11:11:52.991: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 11:11:52.991: INFO: validating pod update-demo-nautilus-49zdr
May 27 11:11:53.004: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 11:11:53.004: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 11:11:53.004: INFO: update-demo-nautilus-49zdr is verified up and running
May 27 11:11:53.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-x7mcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 11:11:53.140: INFO: stderr: ""
May 27 11:11:53.140: INFO: stdout: "true"
May 27 11:11:53.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-x7mcv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 11:11:53.326: INFO: stderr: ""
May 27 11:11:53.326: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 11:11:53.326: INFO: validating pod update-demo-nautilus-x7mcv
May 27 11:11:53.350: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 11:11:53.350: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 11:11:53.350: INFO: update-demo-nautilus-x7mcv is verified up and running
STEP: scaling down the replication controller
May 27 11:11:53.379: INFO: scanned /root for discovery docs: <nil>
May 27 11:11:53.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 27 11:11:54.575: INFO: stderr: ""
May 27 11:11:54.575: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 11:11:54.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 11:11:54.712: INFO: stderr: ""
May 27 11:11:54.712: INFO: stdout: "update-demo-nautilus-x7mcv "
May 27 11:11:54.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-x7mcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 11:11:54.856: INFO: stderr: ""
May 27 11:11:54.856: INFO: stdout: "true"
May 27 11:11:54.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-x7mcv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 11:11:54.987: INFO: stderr: ""
May 27 11:11:54.987: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 11:11:54.987: INFO: validating pod update-demo-nautilus-x7mcv
May 27 11:11:55.001: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 11:11:55.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 11:11:55.002: INFO: update-demo-nautilus-x7mcv is verified up and running
STEP: scaling up the replication controller
May 27 11:11:55.014: INFO: scanned /root for discovery docs: <nil>
May 27 11:11:55.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 27 11:11:56.230: INFO: stderr: ""
May 27 11:11:56.230: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 11:11:56.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 11:11:56.375: INFO: stderr: ""
May 27 11:11:56.375: INFO: stdout: "update-demo-nautilus-5jlzm update-demo-nautilus-x7mcv "
May 27 11:11:56.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-5jlzm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 11:11:56.491: INFO: stderr: ""
May 27 11:11:56.491: INFO: stdout: "true"
May 27 11:11:56.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-5jlzm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 11:11:56.605: INFO: stderr: ""
May 27 11:11:56.605: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 11:11:56.605: INFO: validating pod update-demo-nautilus-5jlzm
May 27 11:11:56.616: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 11:11:56.616: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 11:11:56.616: INFO: update-demo-nautilus-5jlzm is verified up and running
May 27 11:11:56.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-x7mcv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 11:11:56.719: INFO: stderr: ""
May 27 11:11:56.719: INFO: stdout: "true"
May 27 11:11:56.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods update-demo-nautilus-x7mcv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 11:11:56.837: INFO: stderr: ""
May 27 11:11:56.837: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 11:11:56.837: INFO: validating pod update-demo-nautilus-x7mcv
May 27 11:11:56.843: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 11:11:56.843: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 11:11:56.843: INFO: update-demo-nautilus-x7mcv is verified up and running
STEP: using delete to clean up resources
May 27 11:11:56.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 delete --grace-period=0 --force -f -'
May 27 11:11:56.965: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 11:11:56.965: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 27 11:11:56.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get rc,svc -l name=update-demo --no-headers'
May 27 11:11:57.125: INFO: stderr: "No resources found in kubectl-1657 namespace.\n"
May 27 11:11:57.125: INFO: stdout: ""
May 27 11:11:57.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-1657 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 27 11:11:57.248: INFO: stderr: ""
May 27 11:11:57.248: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:11:57.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1657" for this suite.

• [SLOW TEST:10.431 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":332,"skipped":6306,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:11:57.274: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 11:11:57.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b" in namespace "projected-1141" to be "Succeeded or Failed"
May 27 11:11:57.372: INFO: Pod "downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.739613ms
May 27 11:11:59.380: INFO: Pod "downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025944687s
May 27 11:12:01.394: INFO: Pod "downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040209363s
STEP: Saw pod success
May 27 11:12:01.394: INFO: Pod "downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b" satisfied condition "Succeeded or Failed"
May 27 11:12:01.400: INFO: Trying to get logs from node ohp4eith3vui-3 pod downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b container client-container: <nil>
STEP: delete the pod
May 27 11:12:01.430: INFO: Waiting for pod downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b to disappear
May 27 11:12:01.435: INFO: Pod downwardapi-volume-504aa8f2-b412-4ef4-a7f2-2db88245f31b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:12:01.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1141" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:12:01.457: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:12:01.551: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 27 11:12:01.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:12:01.576: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:12:02.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:12:02.618: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:12:03.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 11:12:03.596: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:12:04.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 11:12:04.601: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 27 11:12:04.677: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:04.677: INFO: Wrong image for pod: daemon-set-rgqhp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:04.677: INFO: Wrong image for pod: daemon-set-wt86c. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:05.717: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:05.717: INFO: Wrong image for pod: daemon-set-rgqhp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:06.716: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:06.716: INFO: Pod daemon-set-k4fff is not available
May 27 11:12:06.716: INFO: Wrong image for pod: daemon-set-rgqhp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:07.714: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:07.714: INFO: Pod daemon-set-k4fff is not available
May 27 11:12:07.714: INFO: Wrong image for pod: daemon-set-rgqhp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:08.718: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:09.717: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:09.717: INFO: Pod daemon-set-lhf89 is not available
May 27 11:12:10.716: INFO: Wrong image for pod: daemon-set-4bmc6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 11:12:10.716: INFO: Pod daemon-set-lhf89 is not available
May 27 11:12:12.717: INFO: Pod daemon-set-t7hbk is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 27 11:12:12.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 11:12:12.748: INFO: Node ohp4eith3vui-3 is running 0 daemon pod, expected 1
May 27 11:12:13.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 11:12:13.762: INFO: Node ohp4eith3vui-3 is running 0 daemon pod, expected 1
May 27 11:12:14.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 11:12:14.775: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7528, will wait for the garbage collector to delete the pods
May 27 11:12:14.906: INFO: Deleting DaemonSet.extensions daemon-set took: 10.306366ms
May 27 11:12:15.006: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.620143ms
May 27 11:12:17.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:12:17.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 11:12:17.419: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48680"},"items":null}

May 27 11:12:17.424: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48680"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:12:17.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7528" for this suite.

• [SLOW TEST:16.029 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":334,"skipped":6365,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:12:17.489: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-f045006c-81af-4ff6-80e8-a3e31849433a in namespace container-probe-9176
May 27 11:12:19.576: INFO: Started pod busybox-f045006c-81af-4ff6-80e8-a3e31849433a in namespace container-probe-9176
STEP: checking the pod's current state and verifying that restartCount is present
May 27 11:12:19.584: INFO: Initial restart count of pod busybox-f045006c-81af-4ff6-80e8-a3e31849433a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:16:21.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9176" for this suite.

• [SLOW TEST:244.086 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":6407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:16:21.588: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:16:32.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7491" for this suite.

• [SLOW TEST:11.237 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":336,"skipped":6475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:16:32.826: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-3734
STEP: creating service affinity-clusterip-transition in namespace services-3734
STEP: creating replication controller affinity-clusterip-transition in namespace services-3734
I0527 11:16:32.946038      15 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3734, replica count: 3
I0527 11:16:35.998705      15 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 11:16:36.018: INFO: Creating new exec pod
May 27 11:16:39.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3734 exec execpod-affinityx7zrb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May 27 11:16:39.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 27 11:16:39.490: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 11:16:39.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3734 exec execpod-affinityx7zrb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.53.57 80'
May 27 11:16:39.725: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.53.57 80\nConnection to 10.233.53.57 80 port [tcp/http] succeeded!\n"
May 27 11:16:39.725: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 11:16:39.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3734 exec execpod-affinityx7zrb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.53.57:80/ ; done'
May 27 11:16:40.235: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n"
May 27 11:16:40.235: INFO: stdout: "\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-z2gkr\naffinity-clusterip-transition-gg7bv\naffinity-clusterip-transition-z2gkr\naffinity-clusterip-transition-gg7bv\naffinity-clusterip-transition-gg7bv\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-z2gkr\naffinity-clusterip-transition-z2gkr\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-z2gkr\naffinity-clusterip-transition-gg7bv\naffinity-clusterip-transition-gg7bv\naffinity-clusterip-transition-gg7bv\naffinity-clusterip-transition-gg7bv"
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-z2gkr
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-z2gkr
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-z2gkr
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-z2gkr
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-z2gkr
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.235: INFO: Received response from host: affinity-clusterip-transition-gg7bv
May 27 11:16:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-3734 exec execpod-affinityx7zrb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.53.57:80/ ; done'
May 27 11:16:40.764: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.53.57:80/\n"
May 27 11:16:40.764: INFO: stdout: "\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94\naffinity-clusterip-transition-brp94"
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Received response from host: affinity-clusterip-transition-brp94
May 27 11:16:40.764: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3734, will wait for the garbage collector to delete the pods
May 27 11:16:40.875: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.881595ms
May 27 11:16:40.976: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.186587ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:16:43.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3734" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.709 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":337,"skipped":6501,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:16:43.537: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3683
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-3683
May 27 11:16:43.641: INFO: Found 0 stateful pods, waiting for 1
May 27 11:16:53.660: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 11:16:53.739: INFO: Deleting all statefulset in ns statefulset-3683
May 27 11:16:53.746: INFO: Scaling statefulset ss to 0
May 27 11:17:03.836: INFO: Waiting for statefulset status.replicas updated to 0
May 27 11:17:03.843: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:03.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3683" for this suite.

• [SLOW TEST:20.381 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":338,"skipped":6508,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:03.920: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:17:04.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 create -f -'
May 27 11:17:04.607: INFO: stderr: ""
May 27 11:17:04.608: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 27 11:17:04.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 create -f -'
May 27 11:17:05.080: INFO: stderr: ""
May 27 11:17:05.080: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 27 11:17:06.091: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 11:17:06.091: INFO: Found 0 / 1
May 27 11:17:07.093: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 11:17:07.093: INFO: Found 1 / 1
May 27 11:17:07.093: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 27 11:17:07.099: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 11:17:07.099: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 27 11:17:07.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 describe pod agnhost-primary-5wrnk'
May 27 11:17:07.288: INFO: stderr: ""
May 27 11:17:07.288: INFO: stdout: "Name:         agnhost-primary-5wrnk\nNamespace:    kubectl-6987\nPriority:     0\nNode:         ohp4eith3vui-3/192.168.121.192\nStart Time:   Fri, 27 May 2022 11:17:04 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.233.66.169\nIPs:\n  IP:           10.233.66.169\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://f06fdb3f4bb3b944220b2743f32b6509e297d0e1bd7673a1d4e3a74e65122b09\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 27 May 2022 11:17:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64bzj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64bzj:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-6987/agnhost-primary-5wrnk to ohp4eith3vui-3\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
May 27 11:17:07.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 describe rc agnhost-primary'
May 27 11:17:07.531: INFO: stderr: ""
May 27 11:17:07.531: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6987\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-5wrnk\n"
May 27 11:17:07.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 describe service agnhost-primary'
May 27 11:17:07.726: INFO: stderr: ""
May 27 11:17:07.726: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6987\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.13.209\nIPs:               10.233.13.209\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.169:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 27 11:17:07.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 describe node ohp4eith3vui-1'
May 27 11:17:07.970: INFO: stderr: ""
May 27 11:17:07.970: INFO: stdout: "Name:               ohp4eith3vui-1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ohp4eith3vui-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        io.cilium.network.ipv4-cilium-host: 10.233.65.177\n                    io.cilium.network.ipv4-health-ip: 10.233.65.59\n                    io.cilium.network.ipv4-pod-cidr: 10.233.65.0/24\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 27 May 2022 08:20:14 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ohp4eith3vui-1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 27 May 2022 11:17:06 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 27 May 2022 08:54:58 +0000   Fri, 27 May 2022 08:54:58 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 27 May 2022 11:14:02 +0000   Fri, 27 May 2022 08:20:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 27 May 2022 11:14:02 +0000   Fri, 27 May 2022 08:20:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 27 May 2022 11:14:02 +0000   Fri, 27 May 2022 08:20:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 27 May 2022 11:14:02 +0000   Fri, 27 May 2022 08:55:53 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.5\n  Hostname:    ohp4eith3vui-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  122749536Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8142496Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  119410748528\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3292832Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9cabc6e55a8642a3ab578f9ae5537ee4\n  System UUID:                9cabc6e5-5a86-42a3-ab57-8f9ae5537ee4\n  Boot ID:                    34562ad3-7dbc-487a-800b-fdf95de52dff\n  Kernel Version:             5.15.0-33-generic\n  OS Image:                   Ubuntu 22.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.23.2\n  Kubelet Version:            v1.23.7\n  Kube-Proxy Version:         v1.23.7\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cilium-test                 echo-other-node-59d779959c-8lmx4                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         135m\n  kube-system                 cilium-node-init-cmpq2                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         143m\n  kube-system                 cilium-pb7c6                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         143m\n  kube-system                 coredns-64897985d-v76xc                                    100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     142m\n  kube-system                 kube-addon-manager-ohp4eith3vui-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         143m\n  kube-system                 kube-apiserver-ohp4eith3vui-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         176m\n  kube-system                 kube-controller-manager-ohp4eith3vui-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         176m\n  kube-system                 kube-proxy-bt99l                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         176m\n  kube-system                 kube-scheduler-ohp4eith3vui-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         176m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-aedc7445910247f3-6cftw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                755m (47%)  0 (0%)\n  memory             220Mi (6%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
May 27 11:17:07.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=kubectl-6987 describe namespace kubectl-6987'
May 27 11:17:08.138: INFO: stderr: ""
May 27 11:17:08.138: INFO: stdout: "Name:         kubectl-6987\nLabels:       e2e-framework=kubectl\n              e2e-run=efba5363-37f5-4917-b385-4844f9f674fa\n              kubernetes.io/metadata.name=kubectl-6987\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:08.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6987" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":339,"skipped":6528,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:08.170: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:22.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6423" for this suite.
STEP: Destroying namespace "nsdeletetest-2884" for this suite.
May 27 11:17:22.436: INFO: Namespace nsdeletetest-2884 was already deleted
STEP: Destroying namespace "nsdeletetest-1208" for this suite.

• [SLOW TEST:14.286 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":340,"skipped":6541,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:22.457: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:17:22.514: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:26.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3716" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":341,"skipped":6555,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:26.170: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-221
STEP: creating service affinity-clusterip in namespace services-221
STEP: creating replication controller affinity-clusterip in namespace services-221
I0527 11:17:26.356698      15 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-221, replica count: 3
I0527 11:17:29.412244      15 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 11:17:29.444: INFO: Creating new exec pod
May 27 11:17:34.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-221 exec execpod-affinitys5zds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May 27 11:17:35.008: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 27 11:17:35.008: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 11:17:35.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-221 exec execpod-affinitys5zds -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.35 80'
May 27 11:17:35.250: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.35 80\nConnection to 10.233.27.35 80 port [tcp/http] succeeded!\n"
May 27 11:17:35.250: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 11:17:35.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-574262213 --namespace=services-221 exec execpod-affinitys5zds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.27.35:80/ ; done'
May 27 11:17:35.716: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.35:80/\n"
May 27 11:17:35.716: INFO: stdout: "\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc\naffinity-clusterip-ccgzc"
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Received response from host: affinity-clusterip-ccgzc
May 27 11:17:35.716: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-221, will wait for the garbage collector to delete the pods
May 27 11:17:35.829: INFO: Deleting ReplicationController affinity-clusterip took: 12.780016ms
May 27 11:17:35.930: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.753555ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:37.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-221" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.748 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":342,"skipped":6575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:37.921: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 11:17:38.044: INFO: Waiting up to 5m0s for pod "downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866" in namespace "downward-api-252" to be "Succeeded or Failed"
May 27 11:17:38.051: INFO: Pod "downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866": Phase="Pending", Reason="", readiness=false. Elapsed: 6.624714ms
May 27 11:17:40.058: INFO: Pod "downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013972394s
May 27 11:17:42.089: INFO: Pod "downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044508194s
STEP: Saw pod success
May 27 11:17:42.089: INFO: Pod "downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866" satisfied condition "Succeeded or Failed"
May 27 11:17:42.099: INFO: Trying to get logs from node ohp4eith3vui-3 pod downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866 container dapi-container: <nil>
STEP: delete the pod
May 27 11:17:42.159: INFO: Waiting for pod downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866 to disappear
May 27 11:17:42.170: INFO: Pod downward-api-e0f400af-5e1a-4602-9878-e2e9df55b866 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:42.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-252" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":343,"skipped":6618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:42.207: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 11:17:42.318: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 27 11:17:42.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:42.345: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
May 27 11:17:42.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:42.408: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:43.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:43.429: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:44.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:44.417: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:45.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:45.429: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:46.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 11:17:46.425: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 27 11:17:46.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 11:17:46.477: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May 27 11:17:47.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:47.537: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 27 11:17:47.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:47.575: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:48.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:48.588: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:49.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:49.590: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:50.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:50.587: INFO: Node ohp4eith3vui-1 is running 0 daemon pod, expected 1
May 27 11:17:51.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 11:17:51.587: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5416, will wait for the garbage collector to delete the pods
May 27 11:17:51.675: INFO: Deleting DaemonSet.extensions daemon-set took: 19.373211ms
May 27 11:17:51.776: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.598983ms
May 27 11:17:53.784: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 11:17:53.784: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 11:17:53.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50082"},"items":null}

May 27 11:17:53.795: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50082"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:53.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5416" for this suite.

• [SLOW TEST:11.706 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":344,"skipped":6643,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:53.915: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 27 11:17:53.995: INFO: Waiting up to 5m0s for pod "pod-aabfb264-4879-4071-b612-7bd1db7e03a0" in namespace "emptydir-8718" to be "Succeeded or Failed"
May 27 11:17:54.007: INFO: Pod "pod-aabfb264-4879-4071-b612-7bd1db7e03a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.327964ms
May 27 11:17:56.021: INFO: Pod "pod-aabfb264-4879-4071-b612-7bd1db7e03a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025907743s
May 27 11:17:58.044: INFO: Pod "pod-aabfb264-4879-4071-b612-7bd1db7e03a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04899396s
STEP: Saw pod success
May 27 11:17:58.044: INFO: Pod "pod-aabfb264-4879-4071-b612-7bd1db7e03a0" satisfied condition "Succeeded or Failed"
May 27 11:17:58.052: INFO: Trying to get logs from node ohp4eith3vui-3 pod pod-aabfb264-4879-4071-b612-7bd1db7e03a0 container test-container: <nil>
STEP: delete the pod
May 27 11:17:58.099: INFO: Waiting for pod pod-aabfb264-4879-4071-b612-7bd1db7e03a0 to disappear
May 27 11:17:58.106: INFO: Pod pod-aabfb264-4879-4071-b612-7bd1db7e03a0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:17:58.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8718" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":345,"skipped":6670,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 11:17:58.132: INFO: >>> kubeConfig: /tmp/kubeconfig-574262213
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 11:17:58.226: INFO: The status of Pod pod-update-8dca356a-70bf-4bca-a8ce-18702e5a4dcb is Pending, waiting for it to be Running (with Ready = true)
May 27 11:18:00.235: INFO: The status of Pod pod-update-8dca356a-70bf-4bca-a8ce-18702e5a4dcb is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 27 11:18:00.769: INFO: Successfully updated pod "pod-update-8dca356a-70bf-4bca-a8ce-18702e5a4dcb"
STEP: verifying the updated pod is in kubernetes
May 27 11:18:00.785: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 11:18:00.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6458" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":346,"skipped":6673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSMay 27 11:18:00.821: INFO: Running AfterSuite actions on all nodes
May 27 11:18:00.821: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
May 27 11:18:00.821: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
May 27 11:18:00.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
May 27 11:18:00.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May 27 11:18:00.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May 27 11:18:00.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May 27 11:18:00.822: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
May 27 11:18:00.822: INFO: Running AfterSuite actions on node 1
May 27 11:18:00.822: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6698,"failed":0}

Ran 346 of 7044 Specs in 5941.775 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6698 Skipped
PASS

Ginkgo ran 1 suite in 1h39m6.310644353s
Test Suite Passed

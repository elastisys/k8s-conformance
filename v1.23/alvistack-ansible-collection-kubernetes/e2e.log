I0527 05:16:55.147938      14 e2e.go:132] Starting e2e run "da49959c-2be8-47a9-96c5-d742768aac68" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1653628614 - Will randomize all specs
Will run 346 of 7044 specs

May 27 05:16:58.822: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:16:58.835: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 27 05:16:58.890: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 27 05:16:58.952: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 27 05:16:58.952: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 27 05:16:58.952: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 27 05:16:58.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
May 27 05:16:58.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
May 27 05:16:58.965: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 27 05:16:58.965: INFO: e2e test version: v1.23.7
May 27 05:16:58.971: INFO: kube-apiserver version: v1.23.7
May 27 05:16:58.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:16:58.983: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:16:58.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
May 27 05:16:59.046: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
W0527 05:16:59.046128      14 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-87121b8c-266e-426c-b918-3f5dc0246b38
STEP: Creating secret with name secret-projected-all-test-volume-b3290e39-3398-4908-9f4b-a120908e53c5
STEP: Creating a pod to test Check all projections for projected volume plugin
May 27 05:16:59.096: INFO: Waiting up to 5m0s for pod "projected-volume-0277d5f1-9248-4210-93a9-515968005cfb" in namespace "projected-2882" to be "Succeeded or Failed"
May 27 05:16:59.101: INFO: Pod "projected-volume-0277d5f1-9248-4210-93a9-515968005cfb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.128272ms
May 27 05:17:01.117: INFO: Pod "projected-volume-0277d5f1-9248-4210-93a9-515968005cfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020986388s
May 27 05:17:03.129: INFO: Pod "projected-volume-0277d5f1-9248-4210-93a9-515968005cfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033294614s
May 27 05:17:05.193: INFO: Pod "projected-volume-0277d5f1-9248-4210-93a9-515968005cfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096878314s
STEP: Saw pod success
May 27 05:17:05.193: INFO: Pod "projected-volume-0277d5f1-9248-4210-93a9-515968005cfb" satisfied condition "Succeeded or Failed"
May 27 05:17:05.202: INFO: Trying to get logs from node ha9zeyohpei4-3 pod projected-volume-0277d5f1-9248-4210-93a9-515968005cfb container projected-all-volume-test: <nil>
STEP: delete the pod
May 27 05:17:05.270: INFO: Waiting for pod projected-volume-0277d5f1-9248-4210-93a9-515968005cfb to disappear
May 27 05:17:05.275: INFO: Pod projected-volume-0277d5f1-9248-4210-93a9-515968005cfb no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:17:05.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2882" for this suite.

• [SLOW TEST:6.313 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":1,"skipped":11,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:17:05.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-ef554b9a-9db1-4955-9e47-e875e836329a
STEP: Creating a pod to test consume configMaps
May 27 05:17:05.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9" in namespace "configmap-1324" to be "Succeeded or Failed"
May 27 05:17:05.378: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.953803ms
May 27 05:17:07.395: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024161698s
May 27 05:17:09.442: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070848686s
May 27 05:17:11.461: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089688423s
May 27 05:17:13.476: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105048269s
May 27 05:17:15.487: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.115670343s
May 27 05:17:17.505: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.134312984s
STEP: Saw pod success
May 27 05:17:17.505: INFO: Pod "pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9" satisfied condition "Succeeded or Failed"
May 27 05:17:17.512: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9 container agnhost-container: <nil>
STEP: delete the pod
May 27 05:17:17.626: INFO: Waiting for pod pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9 to disappear
May 27 05:17:17.634: INFO: Pod pod-configmaps-0cccab12-bdc2-485e-8853-02f8aa1440a9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:17:17.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1324" for this suite.

• [SLOW TEST:12.359 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":30,"failed":0}
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:17:17.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:17:17.770: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 27 05:17:22.789: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 27 05:17:28.811: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 27 05:17:30.825: INFO: Creating deployment "test-rollover-deployment"
May 27 05:17:30.848: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 27 05:17:32.870: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 27 05:17:32.886: INFO: Ensure that both replica sets have 1 created replica
May 27 05:17:32.897: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 27 05:17:32.924: INFO: Updating deployment test-rollover-deployment
May 27 05:17:32.924: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 27 05:17:34.955: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 27 05:17:34.971: INFO: Make sure deployment "test-rollover-deployment" is complete
May 27 05:17:34.984: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:34.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:37.011: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:37.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:39.005: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:39.006: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:41.013: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:41.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:43.004: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:43.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:45.007: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:45.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:47.004: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:47.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:49.010: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:49.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:51.017: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:51.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:53.010: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:53.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:55.007: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:55.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:57.001: INFO: all replica sets need to contain the pod-template-hash label
May 27 05:17:57.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:17:59.013: INFO: 
May 27 05:17:59.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 17, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 17, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:18:01.010: INFO: 
May 27 05:18:01.010: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 05:18:01.041: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8526  b676c580-2bd0-484d-848b-6292cd6b7b37 3492 2 2022-05-27 05:17:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-27 05:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:17:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00297aa98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 05:17:30 +0000 UTC,LastTransitionTime:2022-05-27 05:17:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-05-27 05:17:59 +0000 UTC,LastTransitionTime:2022-05-27 05:17:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 05:18:01.047: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-8526  d2af50bd-0533-40bb-8890-68fba89ae271 3482 2 2022-05-27 05:17:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b676c580-2bd0-484d-848b-6292cd6b7b37 0xc0028d2a97 0xc0028d2a98}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:17:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b676c580-2bd0-484d-848b-6292cd6b7b37\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:17:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028d2b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 05:18:01.047: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 27 05:18:01.048: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8526  afe090d4-9c7a-4c7a-9027-868dcd641421 3491 2 2022-05-27 05:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b676c580-2bd0-484d-848b-6292cd6b7b37 0xc0028d296f 0xc0028d2980}] []  [{e2e.test Update apps/v1 2022-05-27 05:17:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:17:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b676c580-2bd0-484d-848b-6292cd6b7b37\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:17:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028d2a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 05:18:01.048: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-8526  7528b78b-d63a-4086-861c-f3002c9a3638 3415 2 2022-05-27 05:17:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b676c580-2bd0-484d-848b-6292cd6b7b37 0xc0028d2ba7 0xc0028d2ba8}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:17:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b676c580-2bd0-484d-848b-6292cd6b7b37\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:17:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028d2c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 05:18:01.058: INFO: Pod "test-rollover-deployment-668b7f667d-l4w8h" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-l4w8h test-rollover-deployment-668b7f667d- deployment-8526  df115543-c22e-40ee-8077-fb4674a9338b 3458 0 2022-05-27 05:17:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d d2af50bd-0533-40bb-8890-68fba89ae271 0xc00297ae27 0xc00297ae28}] []  [{kube-controller-manager Update v1 2022-05-27 05:17:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2af50bd-0533-40bb-8890-68fba89ae271\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:17:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x5xqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x5xqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:17:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:17:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:17:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:17:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.143,StartTime:2022-05-27 05:17:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:17:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:cri-o://aba76661511e5813a075382153574470a1041235c909f91f257ac235fc9e81db,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:01.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8526" for this suite.

• [SLOW TEST:43.429 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":3,"skipped":30,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
May 27 05:18:01.236: INFO: Waiting up to 5m0s for pod "pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445" in namespace "emptydir-8189" to be "Succeeded or Failed"
May 27 05:18:01.258: INFO: Pod "pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445": Phase="Pending", Reason="", readiness=false. Elapsed: 21.449446ms
May 27 05:18:03.267: INFO: Pod "pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030498366s
May 27 05:18:05.283: INFO: Pod "pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046592797s
STEP: Saw pod success
May 27 05:18:05.283: INFO: Pod "pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445" satisfied condition "Succeeded or Failed"
May 27 05:18:05.289: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445 container test-container: <nil>
STEP: delete the pod
May 27 05:18:05.333: INFO: Waiting for pod pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445 to disappear
May 27 05:18:05.339: INFO: Pod pod-261bab20-96e5-42e7-a8fd-a9cbe6c4d445 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:05.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8189" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":4,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:05.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 05:18:05.472: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 05:18:07.486: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 05:18:09.484: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 05:18:09.513: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 05:18:11.527: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 05:18:13.531: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 27 05:18:13.593: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 27 05:18:13.600: INFO: Pod pod-with-poststart-http-hook still exists
May 27 05:18:15.601: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 27 05:18:15.619: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:15.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9488" for this suite.

• [SLOW TEST:10.282 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":76,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:15.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 05:18:15.698: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:21.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6248" for this suite.

• [SLOW TEST:5.610 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":6,"skipped":78,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:21.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:18:21.367: INFO: Creating deployment "test-recreate-deployment"
May 27 05:18:21.377: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 27 05:18:21.402: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 27 05:18:23.426: INFO: Waiting deployment "test-recreate-deployment" to complete
May 27 05:18:23.431: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 27 05:18:23.452: INFO: Updating deployment test-recreate-deployment
May 27 05:18:23.452: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 05:18:23.643: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4255  d7118e9f-a763-4b80-865c-9b14eb4cd14e 3735 2 2022-05-27 05:18:21 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003246088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-27 05:18:23 +0000 UTC,LastTransitionTime:2022-05-27 05:18:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-05-27 05:18:23 +0000 UTC,LastTransitionTime:2022-05-27 05:18:21 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 27 05:18:23.651: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-4255  d647a4b0-14e0-4a11-ae97-35bc2ae3abb4 3731 1 2022-05-27 05:18:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d7118e9f-a763-4b80-865c-9b14eb4cd14e 0xc0030b9f77 0xc0030b9f78}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7118e9f-a763-4b80-865c-9b14eb4cd14e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002cea018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 05:18:23.651: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 27 05:18:23.651: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-4255  07d1783c-6796-4119-a0bf-d66f09a10125 3722 2 2022-05-27 05:18:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d7118e9f-a763-4b80-865c-9b14eb4cd14e 0xc002cea077 0xc002cea078}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7118e9f-a763-4b80-865c-9b14eb4cd14e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002cea128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 05:18:23.659: INFO: Pod "test-recreate-deployment-5b99bd5487-96wkj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-96wkj test-recreate-deployment-5b99bd5487- deployment-4255  a8386426-ee81-4079-9983-34b3f2e1217d 3734 0 2022-05-27 05:18:23 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 d647a4b0-14e0-4a11-ae97-35bc2ae3abb4 0xc003246407 0xc003246408}] []  [{kube-controller-manager Update v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d647a4b0-14e0-4a11-ae97-35bc2ae3abb4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:18:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2mzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2mzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:18:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:18:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:,StartTime:2022-05-27 05:18:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:23.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4255" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":7,"skipped":83,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:23.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 27 05:18:23.755: INFO: Waiting up to 5m0s for pod "pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5" in namespace "emptydir-5571" to be "Succeeded or Failed"
May 27 05:18:23.768: INFO: Pod "pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.407563ms
May 27 05:18:25.780: INFO: Pod "pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024979488s
May 27 05:18:27.794: INFO: Pod "pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038660643s
STEP: Saw pod success
May 27 05:18:27.794: INFO: Pod "pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5" satisfied condition "Succeeded or Failed"
May 27 05:18:27.801: INFO: Trying to get logs from node ha9zeyohpei4-1 pod pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5 container test-container: <nil>
STEP: delete the pod
May 27 05:18:27.837: INFO: Waiting for pod pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5 to disappear
May 27 05:18:27.843: INFO: Pod pod-c13b744a-0275-4476-a9ad-f3b8fd940fd5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:27.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5571" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":89,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:27.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:18:29.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:18:32.693: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:32.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9892" for this suite.
STEP: Destroying namespace "webhook-9892-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":9,"skipped":103,"failed":0}
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:32.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 27 05:18:41.538: INFO: Successfully updated pod "adopt-release-f5rbc"
STEP: Checking that the Job readopts the Pod
May 27 05:18:41.538: INFO: Waiting up to 15m0s for pod "adopt-release-f5rbc" in namespace "job-7831" to be "adopted"
May 27 05:18:41.545: INFO: Pod "adopt-release-f5rbc": Phase="Running", Reason="", readiness=true. Elapsed: 7.209246ms
May 27 05:18:43.560: INFO: Pod "adopt-release-f5rbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.021840248s
May 27 05:18:43.560: INFO: Pod "adopt-release-f5rbc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 27 05:18:44.085: INFO: Successfully updated pod "adopt-release-f5rbc"
STEP: Checking that the Job releases the Pod
May 27 05:18:44.085: INFO: Waiting up to 15m0s for pod "adopt-release-f5rbc" in namespace "job-7831" to be "released"
May 27 05:18:44.096: INFO: Pod "adopt-release-f5rbc": Phase="Running", Reason="", readiness=true. Elapsed: 10.282722ms
May 27 05:18:46.114: INFO: Pod "adopt-release-f5rbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.028922357s
May 27 05:18:46.114: INFO: Pod "adopt-release-f5rbc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:46.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7831" for this suite.

• [SLOW TEST:13.262 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":10,"skipped":103,"failed":0}
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:46.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-a26d9ccd-0f1c-4baf-ae02-7ed476fa1ead
STEP: Creating a pod to test consume secrets
May 27 05:18:46.264: INFO: Waiting up to 5m0s for pod "pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959" in namespace "secrets-8817" to be "Succeeded or Failed"
May 27 05:18:46.271: INFO: Pod "pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959": Phase="Pending", Reason="", readiness=false. Elapsed: 6.577053ms
May 27 05:18:48.279: INFO: Pod "pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0149013s
May 27 05:18:50.297: INFO: Pod "pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032763685s
May 27 05:18:52.307: INFO: Pod "pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042321433s
STEP: Saw pod success
May 27 05:18:52.307: INFO: Pod "pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959" satisfied condition "Succeeded or Failed"
May 27 05:18:52.317: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959 container secret-env-test: <nil>
STEP: delete the pod
May 27 05:18:52.363: INFO: Waiting for pod pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959 to disappear
May 27 05:18:52.369: INFO: Pod pod-secrets-ef9815a4-98f9-496f-9b8d-72d813554959 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:52.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8817" for this suite.

• [SLOW TEST:6.239 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":103,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:18:52.483: INFO: The status of Pod busybox-host-aliases9af5479b-3cca-47a9-86a7-13237e53473a is Pending, waiting for it to be Running (with Ready = true)
May 27 05:18:54.506: INFO: The status of Pod busybox-host-aliases9af5479b-3cca-47a9-86a7-13237e53473a is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:54.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1683" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":12,"skipped":163,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:54.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-a78cc8e1-e424-4f5d-85bd-6ada91174ba0
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6127" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":13,"skipped":176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:54.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:54.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9022" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":14,"skipped":203,"failed":0}
SSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:54.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:54.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7910" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":15,"skipped":207,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:18:55.702: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:18:58.763: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:18:59.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5022" for this suite.
STEP: Destroying namespace "webhook-5022-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":16,"skipped":211,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:18:59.220: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5134
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:18:59.318: INFO: Found 0 stateful pods, waiting for 1
May 27 05:19:09.338: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 27 05:19:19.333: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
May 27 05:19:19.390: INFO: Found 1 stateful pods, waiting for 2
May 27 05:19:29.403: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 05:19:29.403: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
May 27 05:19:39.402: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 05:19:39.402: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 05:19:39.454: INFO: Deleting all statefulset in ns statefulset-5134
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:19:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5134" for this suite.

• [SLOW TEST:40.276 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":17,"skipped":227,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:19:39.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
May 27 05:19:56.926: INFO: 89 pods remaining
May 27 05:19:56.926: INFO: 70 pods has nil DeletionTimestamp
May 27 05:19:56.926: INFO: 
May 27 05:20:02.703: INFO: 64 pods remaining
May 27 05:20:02.707: INFO: 50 pods has nil DeletionTimestamp
May 27 05:20:02.707: INFO: 
STEP: Gathering metrics
May 27 05:20:07.015: INFO: The status of Pod kube-controller-manager-ha9zeyohpei4-2 is Running (Ready = true)
May 27 05:20:07.284: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 27 05:20:07.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-22w6f" in namespace "gc-7040"
May 27 05:20:07.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mpqr" in namespace "gc-7040"
May 27 05:20:07.438: INFO: Deleting pod "simpletest-rc-to-be-deleted-48xvs" in namespace "gc-7040"
May 27 05:20:07.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kvb5" in namespace "gc-7040"
May 27 05:20:07.540: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wphx" in namespace "gc-7040"
May 27 05:20:07.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-586rt" in namespace "gc-7040"
May 27 05:20:07.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d4q5" in namespace "gc-7040"
May 27 05:20:07.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s8gr" in namespace "gc-7040"
May 27 05:20:07.920: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mngx" in namespace "gc-7040"
May 27 05:20:07.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wv9x" in namespace "gc-7040"
May 27 05:20:08.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xjcx" in namespace "gc-7040"
May 27 05:20:08.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-74l95" in namespace "gc-7040"
May 27 05:20:08.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d7b7" in namespace "gc-7040"
May 27 05:20:08.493: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jgpd" in namespace "gc-7040"
May 27 05:20:08.603: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v9lq" in namespace "gc-7040"
May 27 05:20:08.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zdms" in namespace "gc-7040"
May 27 05:20:08.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-84dt4" in namespace "gc-7040"
May 27 05:20:08.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-92nzx" in namespace "gc-7040"
May 27 05:20:08.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-95xrj" in namespace "gc-7040"
May 27 05:20:09.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dl2g" in namespace "gc-7040"
May 27 05:20:09.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jhmm" in namespace "gc-7040"
May 27 05:20:09.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w7zt" in namespace "gc-7040"
May 27 05:20:09.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9bhp" in namespace "gc-7040"
May 27 05:20:09.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf9g9" in namespace "gc-7040"
May 27 05:20:10.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgbnm" in namespace "gc-7040"
May 27 05:20:10.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkw92" in namespace "gc-7040"
May 27 05:20:10.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsn4t" in namespace "gc-7040"
May 27 05:20:10.487: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw58j" in namespace "gc-7040"
May 27 05:20:10.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw6n7" in namespace "gc-7040"
May 27 05:20:10.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-c29jl" in namespace "gc-7040"
May 27 05:20:10.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5l5q" in namespace "gc-7040"
May 27 05:20:11.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-c675v" in namespace "gc-7040"
May 27 05:20:11.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-chzsh" in namespace "gc-7040"
May 27 05:20:11.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj978" in namespace "gc-7040"
May 27 05:20:11.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-cppgz" in namespace "gc-7040"
May 27 05:20:11.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-csnw9" in namespace "gc-7040"
May 27 05:20:11.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-dct2j" in namespace "gc-7040"
May 27 05:20:11.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddqkh" in namespace "gc-7040"
May 27 05:20:11.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-f27pg" in namespace "gc-7040"
May 27 05:20:11.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8wbq" in namespace "gc-7040"
May 27 05:20:11.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbkbv" in namespace "gc-7040"
May 27 05:20:12.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-flcdz" in namespace "gc-7040"
May 27 05:20:12.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnbj6" in namespace "gc-7040"
May 27 05:20:12.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnjsp" in namespace "gc-7040"
May 27 05:20:12.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6skd" in namespace "gc-7040"
May 27 05:20:12.527: INFO: Deleting pod "simpletest-rc-to-be-deleted-htkdv" in namespace "gc-7040"
May 27 05:20:12.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6jwz" in namespace "gc-7040"
May 27 05:20:12.788: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmgz4" in namespace "gc-7040"
May 27 05:20:12.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-kddtq" in namespace "gc-7040"
May 27 05:20:12.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-kf8zp" in namespace "gc-7040"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:20:13.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7040" for this suite.

• [SLOW TEST:33.777 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":18,"skipped":228,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:20:13.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-g7pf
STEP: Creating a pod to test atomic-volume-subpath
May 27 05:20:13.620: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g7pf" in namespace "subpath-6591" to be "Succeeded or Failed"
May 27 05:20:13.637: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.296019ms
May 27 05:20:15.652: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031312718s
May 27 05:20:17.676: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 4.05509562s
May 27 05:20:19.687: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 6.066411207s
May 27 05:20:21.707: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 8.08614146s
May 27 05:20:23.720: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 10.099747148s
May 27 05:20:25.737: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 12.116058122s
May 27 05:20:27.753: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 14.132024077s
May 27 05:20:29.768: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 16.147787634s
May 27 05:20:31.784: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 18.163209618s
May 27 05:20:33.798: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 20.177593139s
May 27 05:20:35.811: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=true. Elapsed: 22.190489941s
May 27 05:20:37.830: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Running", Reason="", readiness=false. Elapsed: 24.20949063s
May 27 05:20:39.839: INFO: Pod "pod-subpath-test-secret-g7pf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.218907977s
STEP: Saw pod success
May 27 05:20:39.840: INFO: Pod "pod-subpath-test-secret-g7pf" satisfied condition "Succeeded or Failed"
May 27 05:20:39.846: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-subpath-test-secret-g7pf container test-container-subpath-secret-g7pf: <nil>
STEP: delete the pod
May 27 05:20:39.901: INFO: Waiting for pod pod-subpath-test-secret-g7pf to disappear
May 27 05:20:39.907: INFO: Pod pod-subpath-test-secret-g7pf no longer exists
STEP: Deleting pod pod-subpath-test-secret-g7pf
May 27 05:20:39.907: INFO: Deleting pod "pod-subpath-test-secret-g7pf" in namespace "subpath-6591"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:20:39.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6591" for this suite.

• [SLOW TEST:26.660 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":19,"skipped":238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:20:39.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:20:40.860: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:20:43.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:20:44.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3927" for this suite.
STEP: Destroying namespace "webhook-3927-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":20,"skipped":297,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:20:44.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:20:44.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f" in namespace "projected-333" to be "Succeeded or Failed"
May 27 05:20:44.272: INFO: Pod "downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.154456ms
May 27 05:20:46.285: INFO: Pod "downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019666622s
May 27 05:20:48.303: INFO: Pod "downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037233392s
May 27 05:20:50.315: INFO: Pod "downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049547785s
STEP: Saw pod success
May 27 05:20:50.315: INFO: Pod "downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f" satisfied condition "Succeeded or Failed"
May 27 05:20:50.321: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f container client-container: <nil>
STEP: delete the pod
May 27 05:20:50.367: INFO: Waiting for pod downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f to disappear
May 27 05:20:50.380: INFO: Pod downwardapi-volume-fd88c494-3eb4-4340-a4ed-1381b96b7e3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:20:50.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-333" for this suite.

• [SLOW TEST:6.222 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":303,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:20:50.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 05:20:54.534: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:20:54.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4644" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":22,"skipped":310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:20:54.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:20:56.141: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:20:59.273: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:20:59.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2639" for this suite.
STEP: Destroying namespace "webhook-2639-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.150 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":23,"skipped":393,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:20:59.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
May 27 05:21:01.884: INFO: pods: 0 < 3
May 27 05:21:03.898: INFO: running pods: 2 < 3
May 27 05:21:05.901: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:12.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-316" for this suite.

• [SLOW TEST:12.522 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":24,"skipped":408,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:12.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
May 27 05:21:14.934: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2543 pod-service-account-e0b238c2-9f03-4e25-b4db-2376f21b311f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 27 05:21:15.477: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2543 pod-service-account-e0b238c2-9f03-4e25-b4db-2376f21b311f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 27 05:21:15.725: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2543 pod-service-account-e0b238c2-9f03-4e25-b4db-2376f21b311f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2543" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":25,"skipped":423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:15.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:18.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9313" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":26,"skipped":469,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:18.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:22.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1323" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":27,"skipped":474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:22.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:25.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-298" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":28,"skipped":499,"failed":0}
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:25.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:21:25.692: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa" in namespace "security-context-test-1573" to be "Succeeded or Failed"
May 27 05:21:25.706: INFO: Pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.253699ms
May 27 05:21:27.722: INFO: Pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029922175s
May 27 05:21:29.738: INFO: Pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046278169s
May 27 05:21:31.755: INFO: Pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063240669s
May 27 05:21:33.768: INFO: Pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.076105959s
May 27 05:21:33.768: INFO: Pod "alpine-nnp-false-6f4d23ab-859b-4a4e-afb9-5249f63910aa" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:33.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1573" for this suite.

• [SLOW TEST:8.178 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":29,"skipped":501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:33.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:21:34.865: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:21:37.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:21:48.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-589" for this suite.
STEP: Destroying namespace "webhook-589-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.463 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":30,"skipped":530,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:21:48.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-2152
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 05:21:48.330: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 05:21:48.466: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:21:50.478: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:21:52.481: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:21:54.478: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:21:56.479: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:21:58.482: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:22:00.477: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:22:02.482: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:22:04.483: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:22:06.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:22:08.478: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:22:10.476: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 05:22:10.487: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 05:22:10.497: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 05:22:12.578: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 05:22:12.578: INFO: Going to poll 10.233.65.236 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 27 05:22:12.587: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.236 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2152 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:22:12.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:22:12.588: INFO: ExecWithOptions: Clientset creation
May 27 05:22:12.589: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2152/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.236+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 05:22:13.781: INFO: Found all 1 expected endpoints: [netserver-0]
May 27 05:22:13.782: INFO: Going to poll 10.233.64.243 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 27 05:22:13.790: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.243 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2152 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:22:13.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:22:13.794: INFO: ExecWithOptions: Clientset creation
May 27 05:22:13.795: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2152/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.243+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 05:22:14.928: INFO: Found all 1 expected endpoints: [netserver-1]
May 27 05:22:14.928: INFO: Going to poll 10.233.66.140 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 27 05:22:14.943: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2152 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:22:14.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:22:14.944: INFO: ExecWithOptions: Clientset creation
May 27 05:22:14.944: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2152/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.140+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 05:22:16.056: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:22:16.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2152" for this suite.

• [SLOW TEST:27.808 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":531,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:22:16.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5448
STEP: creating service affinity-clusterip-transition in namespace services-5448
STEP: creating replication controller affinity-clusterip-transition in namespace services-5448
I0527 05:22:16.179488      14 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5448, replica count: 3
I0527 05:22:19.233940      14 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 05:22:19.278: INFO: Creating new exec pod
May 27 05:22:22.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-5448 exec execpod-affinity6vp9l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May 27 05:22:22.633: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 27 05:22:22.633: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:22:22.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-5448 exec execpod-affinity6vp9l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.60 80'
May 27 05:22:22.885: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.60 80\nConnection to 10.233.27.60 80 port [tcp/http] succeeded!\n"
May 27 05:22:22.885: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:22:22.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-5448 exec execpod-affinity6vp9l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.27.60:80/ ; done'
May 27 05:22:23.554: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n"
May 27 05:22:23.554: INFO: stdout: "\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-76k5x\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-tlcj7\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-tlcj7"
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-76k5x
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:23.555: INFO: Received response from host: affinity-clusterip-transition-tlcj7
May 27 05:22:23.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-5448 exec execpod-affinity6vp9l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.27.60:80/ ; done'
May 27 05:22:24.055: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.27.60:80/\n"
May 27 05:22:24.055: INFO: stdout: "\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5\naffinity-clusterip-transition-hlrx5"
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Received response from host: affinity-clusterip-transition-hlrx5
May 27 05:22:24.055: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5448, will wait for the garbage collector to delete the pods
May 27 05:22:24.154: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.953197ms
May 27 05:22:24.256: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.144278ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:22:26.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5448" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.168 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":32,"skipped":539,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:22:26.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 05:22:26.311: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 05:22:26.329: INFO: Waiting for terminating namespaces to be deleted...
May 27 05:22:26.338: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-1 before test
May 27 05:22:26.371: INFO: echo-other-node-59d779959c-4vckc from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container echo-other-node ready: true, restart count 0
May 27 05:22:26.372: INFO: rs-qxbzv from disruption-316 started at 2022-05-27 05:21:12 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container donothing ready: false, restart count 0
May 27 05:22:26.372: INFO: cilium-node-init-ph2ps from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container node-init ready: true, restart count 0
May 27 05:22:26.372: INFO: cilium-wjfhd from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 05:22:26.372: INFO: kube-addon-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 05:22:26.372: INFO: kube-apiserver-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 05:22:26.372: INFO: kube-controller-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 05:22:26.372: INFO: kube-proxy-4t9kh from kube-system started at 2022-05-27 04:57:26 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 05:22:26.372: INFO: kube-scheduler-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 05:22:26.372: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-lcdqs from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:22:26.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:22:26.372: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 05:22:26.372: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-2 before test
May 27 05:22:26.391: INFO: cilium-dfq7b from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 05:22:26.391: INFO: cilium-node-init-942c6 from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container node-init ready: true, restart count 0
May 27 05:22:26.391: INFO: coredns-64897985d-5hvb2 from kube-system started at 2022-05-27 05:10:55 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container coredns ready: true, restart count 0
May 27 05:22:26.391: INFO: kube-addon-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 05:22:26.391: INFO: kube-apiserver-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 05:22:26.391: INFO: kube-controller-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:57:59 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 05:22:26.391: INFO: kube-proxy-lpjdp from kube-system started at 2022-05-27 04:58:12 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 05:22:26.391: INFO: kube-scheduler-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 05:22:26.391: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-sgkpw from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:22:26.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:22:26.391: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 05:22:26.391: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-3 before test
May 27 05:22:26.413: INFO: client-7568bc7f86-94lb8 from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container client ready: true, restart count 0
May 27 05:22:26.413: INFO: client2-686d5f784b-xpnfm from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container client2 ready: true, restart count 0
May 27 05:22:26.413: INFO: echo-same-node-5767b7b99d-bwpk2 from cilium-test started at 2022-05-27 05:12:20 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 05:22:26.413: INFO: cilium-7dvhf from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 05:22:26.413: INFO: cilium-node-init-zxkdp from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container node-init ready: true, restart count 0
May 27 05:22:26.413: INFO: cilium-operator-59d6f769d4-nlxfs from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container cilium-operator ready: true, restart count 0
May 27 05:22:26.413: INFO: coredns-64897985d-484hx from kube-system started at 2022-05-27 05:11:10 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container coredns ready: true, restart count 0
May 27 05:22:26.413: INFO: kube-proxy-hzzmk from kube-system started at 2022-05-27 04:58:38 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 05:22:26.413: INFO: sonobuoy from sonobuoy started at 2022-05-27 05:16:07 +0000 UTC (1 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 05:22:26.413: INFO: sonobuoy-e2e-job-7e88fd63f0e849eb from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container e2e ready: true, restart count 0
May 27 05:22:26.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:22:26.413: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-tz8zz from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:22:26.413: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:22:26.413: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a360a4c5-9e3f-44fd-8523-58e992f23a12 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.191 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-a360a4c5-9e3f-44fd-8523-58e992f23a12 off the node ha9zeyohpei4-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a360a4c5-9e3f-44fd-8523-58e992f23a12
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:27:30.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3614" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.463 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":33,"skipped":559,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:27:30.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
May 27 05:27:30.808: INFO: Waiting up to 5m0s for pod "pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6" in namespace "emptydir-4510" to be "Succeeded or Failed"
May 27 05:27:30.814: INFO: Pod "pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.744975ms
May 27 05:27:32.827: INFO: Pod "pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.018482887s
May 27 05:27:34.843: INFO: Pod "pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6": Phase="Running", Reason="", readiness=false. Elapsed: 4.034042191s
May 27 05:27:36.857: INFO: Pod "pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04869385s
STEP: Saw pod success
May 27 05:27:36.857: INFO: Pod "pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6" satisfied condition "Succeeded or Failed"
May 27 05:27:36.863: INFO: Trying to get logs from node ha9zeyohpei4-2 pod pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6 container test-container: <nil>
STEP: delete the pod
May 27 05:27:36.913: INFO: Waiting for pod pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6 to disappear
May 27 05:27:36.918: INFO: Pod pod-cdbd1b70-3d51-49fd-a71f-1c4091a630f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:27:36.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4510" for this suite.

• [SLOW TEST:6.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":581,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:27:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-abc855cf-0e11-4952-ba48-862a392ce50f
STEP: Creating a pod to test consume secrets
May 27 05:27:37.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493" in namespace "projected-3529" to be "Succeeded or Failed"
May 27 05:27:37.047: INFO: Pod "pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493": Phase="Pending", Reason="", readiness=false. Elapsed: 7.895377ms
May 27 05:27:39.060: INFO: Pod "pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493": Phase="Running", Reason="", readiness=true. Elapsed: 2.020881297s
May 27 05:27:41.075: INFO: Pod "pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493": Phase="Running", Reason="", readiness=false. Elapsed: 4.035176739s
May 27 05:27:43.088: INFO: Pod "pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048708441s
STEP: Saw pod success
May 27 05:27:43.089: INFO: Pod "pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493" satisfied condition "Succeeded or Failed"
May 27 05:27:43.094: INFO: Trying to get logs from node ha9zeyohpei4-2 pod pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 05:27:43.129: INFO: Waiting for pod pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493 to disappear
May 27 05:27:43.134: INFO: Pod pod-projected-secrets-935c6185-a4b7-47ce-bcad-64c85f76f493 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:27:43.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3529" for this suite.

• [SLOW TEST:6.204 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":35,"skipped":600,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:27:43.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
May 27 05:27:43.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-866 cluster-info'
May 27 05:27:43.412: INFO: stderr: ""
May 27 05:27:43.412: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:27:43.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-866" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":36,"skipped":613,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:27:43.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-92cd3489-0400-466e-9d75-3309354dba62
STEP: Creating a pod to test consume configMaps
May 27 05:27:43.523: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a" in namespace "projected-5439" to be "Succeeded or Failed"
May 27 05:27:43.528: INFO: Pod "pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.237898ms
May 27 05:27:45.542: INFO: Pod "pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01912478s
May 27 05:27:47.564: INFO: Pod "pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04133488s
STEP: Saw pod success
May 27 05:27:47.564: INFO: Pod "pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a" satisfied condition "Succeeded or Failed"
May 27 05:27:47.573: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a container agnhost-container: <nil>
STEP: delete the pod
May 27 05:27:47.685: INFO: Waiting for pod pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a to disappear
May 27 05:27:47.692: INFO: Pod pod-projected-configmaps-aeae208e-0895-4b0e-b5b4-8fa4d825489a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:27:47.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5439" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":37,"skipped":621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:27:47.728: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 27 05:27:47.781: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
May 27 05:27:48.991: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 27 05:27:51.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:27:53.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:27:55.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:27:57.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:27:59.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:28:01.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:28:03.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:28:05.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 27, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 27, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 27 05:28:10.739: INFO: Waited 3.627759859s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
May 27 05:28:10.879: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:28:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2290" for this suite.

• [SLOW TEST:23.588 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":38,"skipped":712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:28:11.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:28:11.401: INFO: Got root ca configmap in namespace "svcaccounts-318"
May 27 05:28:11.415: INFO: Deleted root ca configmap in namespace "svcaccounts-318"
STEP: waiting for a new root ca configmap created
May 27 05:28:11.927: INFO: Recreated root ca configmap in namespace "svcaccounts-318"
May 27 05:28:11.936: INFO: Updated root ca configmap in namespace "svcaccounts-318"
STEP: waiting for the root ca configmap reconciled
May 27 05:28:12.458: INFO: Reconciled root ca configmap in namespace "svcaccounts-318"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:28:12.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-318" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":39,"skipped":738,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:28:12.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 27 05:28:19.755: INFO: 90 pods remaining
May 27 05:28:19.756: INFO: 83 pods has nil DeletionTimestamp
May 27 05:28:19.756: INFO: 
May 27 05:28:20.448: INFO: 80 pods remaining
May 27 05:28:20.448: INFO: 72 pods has nil DeletionTimestamp
May 27 05:28:20.448: INFO: 
May 27 05:28:21.826: INFO: 73 pods remaining
May 27 05:28:21.826: INFO: 55 pods has nil DeletionTimestamp
May 27 05:28:21.826: INFO: 
May 27 05:28:23.355: INFO: 67 pods remaining
May 27 05:28:23.355: INFO: 42 pods has nil DeletionTimestamp
May 27 05:28:23.355: INFO: 
May 27 05:28:24.848: INFO: 50 pods remaining
May 27 05:28:24.848: INFO: 15 pods has nil DeletionTimestamp
May 27 05:28:24.848: INFO: 
May 27 05:28:25.842: INFO: 43 pods remaining
May 27 05:28:25.842: INFO: 3 pods has nil DeletionTimestamp
May 27 05:28:25.842: INFO: 
May 27 05:28:26.735: INFO: 36 pods remaining
May 27 05:28:26.735: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:26.735: INFO: 
May 27 05:28:27.475: INFO: 32 pods remaining
May 27 05:28:27.507: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:27.511: INFO: 
May 27 05:28:28.326: INFO: 26 pods remaining
May 27 05:28:28.326: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:28.326: INFO: 
May 27 05:28:29.345: INFO: 17 pods remaining
May 27 05:28:29.346: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:29.346: INFO: 
May 27 05:28:30.527: INFO: 11 pods remaining
May 27 05:28:30.527: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:30.527: INFO: 
May 27 05:28:31.332: INFO: 4 pods remaining
May 27 05:28:31.332: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:31.333: INFO: 
May 27 05:28:32.314: INFO: 2 pods remaining
May 27 05:28:32.314: INFO: 0 pods has nil DeletionTimestamp
May 27 05:28:32.314: INFO: 
STEP: Gathering metrics
May 27 05:28:33.507: INFO: The status of Pod kube-controller-manager-ha9zeyohpei4-2 is Running (Ready = true)
May 27 05:28:34.022: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:28:34.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3103" for this suite.

• [SLOW TEST:21.861 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":40,"skipped":755,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:28:34.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
May 27 05:28:34.551: INFO: created test-podtemplate-1
May 27 05:28:34.576: INFO: created test-podtemplate-2
May 27 05:28:34.607: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 27 05:28:34.623: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 27 05:28:34.741: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:28:34.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1575" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":41,"skipped":790,"failed":0}
S
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:28:34.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
May 27 05:28:35.032: INFO: created test-pod-1
May 27 05:28:45.085: INFO: running and ready test-pod-1
May 27 05:28:45.096: INFO: created test-pod-2
May 27 05:28:47.148: INFO: running and ready test-pod-2
May 27 05:28:47.160: INFO: created test-pod-3
May 27 05:28:51.179: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
May 27 05:28:51.275: INFO: Pod quantity 3 is different from expected quantity 0
May 27 05:28:52.289: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:28:53.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7490" for this suite.

• [SLOW TEST:18.474 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":42,"skipped":791,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:28:53.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-4hzb
STEP: Creating a pod to test atomic-volume-subpath
May 27 05:28:53.392: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4hzb" in namespace "subpath-1143" to be "Succeeded or Failed"
May 27 05:28:53.399: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.289131ms
May 27 05:28:55.412: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01920308s
May 27 05:28:57.427: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 4.034188925s
May 27 05:28:59.455: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 6.062599414s
May 27 05:29:01.480: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 8.08777405s
May 27 05:29:03.498: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 10.105211455s
May 27 05:29:05.517: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 12.124521666s
May 27 05:29:07.529: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 14.136891268s
May 27 05:29:09.543: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 16.150806184s
May 27 05:29:11.564: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 18.171888637s
May 27 05:29:13.575: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 20.182127209s
May 27 05:29:15.590: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=true. Elapsed: 22.197238942s
May 27 05:29:17.605: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Running", Reason="", readiness=false. Elapsed: 24.212396176s
May 27 05:29:19.618: INFO: Pod "pod-subpath-test-configmap-4hzb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.225316495s
STEP: Saw pod success
May 27 05:29:19.618: INFO: Pod "pod-subpath-test-configmap-4hzb" satisfied condition "Succeeded or Failed"
May 27 05:29:19.623: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-subpath-test-configmap-4hzb container test-container-subpath-configmap-4hzb: <nil>
STEP: delete the pod
May 27 05:29:19.673: INFO: Waiting for pod pod-subpath-test-configmap-4hzb to disappear
May 27 05:29:19.682: INFO: Pod pod-subpath-test-configmap-4hzb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4hzb
May 27 05:29:19.682: INFO: Deleting pod "pod-subpath-test-configmap-4hzb" in namespace "subpath-1143"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:29:19.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1143" for this suite.

• [SLOW TEST:26.407 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":43,"skipped":806,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:29:19.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 05:29:19.793: INFO: Waiting up to 5m0s for pod "downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8" in namespace "downward-api-1935" to be "Succeeded or Failed"
May 27 05:29:19.802: INFO: Pod "downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.68033ms
May 27 05:29:21.817: INFO: Pod "downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023316724s
May 27 05:29:23.832: INFO: Pod "downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038712565s
STEP: Saw pod success
May 27 05:29:23.832: INFO: Pod "downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8" satisfied condition "Succeeded or Failed"
May 27 05:29:23.844: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8 container dapi-container: <nil>
STEP: delete the pod
May 27 05:29:23.882: INFO: Waiting for pod downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8 to disappear
May 27 05:29:23.895: INFO: Pod downward-api-dc50918a-2ad9-4b16-befe-41f4139949f8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:29:23.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1935" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:29:23.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:29:26.023: INFO: Deleting pod "var-expansion-6204ecd8-6b49-492a-a877-950313d50b8d" in namespace "var-expansion-5311"
May 27 05:29:26.039: INFO: Wait up to 5m0s for pod "var-expansion-6204ecd8-6b49-492a-a877-950313d50b8d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:29:28.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5311" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":45,"skipped":833,"failed":0}

------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:29:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 05:29:28.179: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 05:29:30.194: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 05:29:32.196: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 05:29:32.227: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 05:29:34.240: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 27 05:29:34.260: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 27 05:29:34.268: INFO: Pod pod-with-prestop-http-hook still exists
May 27 05:29:36.269: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 27 05:29:36.284: INFO: Pod pod-with-prestop-http-hook still exists
May 27 05:29:38.269: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 27 05:29:38.284: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:29:38.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7426" for this suite.

• [SLOW TEST:10.273 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":833,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:29:38.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:29:38.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 27 05:29:44.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-2369 --namespace=crd-publish-openapi-2369 create -f -'
May 27 05:29:46.374: INFO: stderr: ""
May 27 05:29:46.374: INFO: stdout: "e2e-test-crd-publish-openapi-8220-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 27 05:29:46.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-2369 --namespace=crd-publish-openapi-2369 delete e2e-test-crd-publish-openapi-8220-crds test-cr'
May 27 05:29:46.510: INFO: stderr: ""
May 27 05:29:46.510: INFO: stdout: "e2e-test-crd-publish-openapi-8220-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 27 05:29:46.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-2369 --namespace=crd-publish-openapi-2369 apply -f -'
May 27 05:29:47.697: INFO: stderr: ""
May 27 05:29:47.697: INFO: stdout: "e2e-test-crd-publish-openapi-8220-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 27 05:29:47.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-2369 --namespace=crd-publish-openapi-2369 delete e2e-test-crd-publish-openapi-8220-crds test-cr'
May 27 05:29:47.834: INFO: stderr: ""
May 27 05:29:47.834: INFO: stdout: "e2e-test-crd-publish-openapi-8220-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 27 05:29:47.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-2369 explain e2e-test-crd-publish-openapi-8220-crds'
May 27 05:29:48.117: INFO: stderr: ""
May 27 05:29:48.117: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8220-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:29:53.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2369" for this suite.

• [SLOW TEST:15.468 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":47,"skipped":857,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:29:53.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 05:30:10.039: INFO: DNS probes using dns-3672/dns-test-e2b6e059-3bf4-4b85-a8ed-2be503f97ca5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:10.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3672" for this suite.

• [SLOW TEST:16.257 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":48,"skipped":857,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:10.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:30:10.189: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508" in namespace "downward-api-5339" to be "Succeeded or Failed"
May 27 05:30:10.214: INFO: Pod "downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508": Phase="Pending", Reason="", readiness=false. Elapsed: 24.821034ms
May 27 05:30:12.231: INFO: Pod "downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041927888s
May 27 05:30:14.245: INFO: Pod "downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055989048s
STEP: Saw pod success
May 27 05:30:14.245: INFO: Pod "downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508" satisfied condition "Succeeded or Failed"
May 27 05:30:14.253: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508 container client-container: <nil>
STEP: delete the pod
May 27 05:30:14.315: INFO: Waiting for pod downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508 to disappear
May 27 05:30:14.320: INFO: Pod downwardapi-volume-2ab6e1fc-1bac-4fc1-b36d-925baace3508 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:14.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5339" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":859,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:14.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-6091b19b-ebe8-4f7b-acb3-0f96434255c0
STEP: Creating a pod to test consume secrets
May 27 05:30:14.443: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d" in namespace "projected-8185" to be "Succeeded or Failed"
May 27 05:30:14.453: INFO: Pod "pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.31754ms
May 27 05:30:16.463: INFO: Pod "pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020385017s
May 27 05:30:18.477: INFO: Pod "pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034142056s
STEP: Saw pod success
May 27 05:30:18.477: INFO: Pod "pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d" satisfied condition "Succeeded or Failed"
May 27 05:30:18.483: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 05:30:18.514: INFO: Waiting for pod pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d to disappear
May 27 05:30:18.519: INFO: Pod pod-projected-secrets-5f793edf-ee90-4f2a-8c17-ba9676e7026d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:18.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8185" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":859,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:18.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:18.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6292" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":51,"skipped":868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:18.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-eb90b8b8-01c2-4029-a749-f690af052f82
STEP: Creating a pod to test consume secrets
May 27 05:30:18.728: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495" in namespace "projected-8027" to be "Succeeded or Failed"
May 27 05:30:18.733: INFO: Pod "pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495": Phase="Pending", Reason="", readiness=false. Elapsed: 5.129933ms
May 27 05:30:20.744: INFO: Pod "pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016420133s
May 27 05:30:22.758: INFO: Pod "pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030374894s
STEP: Saw pod success
May 27 05:30:22.758: INFO: Pod "pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495" satisfied condition "Succeeded or Failed"
May 27 05:30:22.765: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 05:30:22.796: INFO: Waiting for pod pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495 to disappear
May 27 05:30:22.799: INFO: Pod pod-projected-secrets-dbe33376-6953-43c6-9d4f-f77c7e2ab495 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:22.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8027" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:22.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:26.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-6372" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":53,"skipped":954,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:26.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-416
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-416
STEP: Deleting pre-stop pod
May 27 05:30:36.209: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:36.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-416" for this suite.

• [SLOW TEST:9.330 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":54,"skipped":962,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:36.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
May 27 05:30:36.383: INFO: Waiting up to 5m0s for pod "pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253" in namespace "emptydir-9064" to be "Succeeded or Failed"
May 27 05:30:36.395: INFO: Pod "pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029639ms
May 27 05:30:38.418: INFO: Pod "pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034334164s
May 27 05:30:40.430: INFO: Pod "pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047111427s
STEP: Saw pod success
May 27 05:30:40.431: INFO: Pod "pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253" satisfied condition "Succeeded or Failed"
May 27 05:30:40.438: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253 container test-container: <nil>
STEP: delete the pod
May 27 05:30:40.472: INFO: Waiting for pod pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253 to disappear
May 27 05:30:40.479: INFO: Pod pod-b6d22cbb-c036-45ad-90d5-5ec5c1559253 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:40.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9064" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":55,"skipped":976,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:40.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-5a41c21b-8963-41d0-a380-a2a083c3c150
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:30:40.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2778" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":56,"skipped":978,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:30:40.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3153
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-3153
May 27 05:30:40.715: INFO: Found 0 stateful pods, waiting for 1
May 27 05:30:50.725: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
May 27 05:30:50.768: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
May 27 05:30:50.785: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
May 27 05:30:50.790: INFO: Observed &StatefulSet event: ADDED
May 27 05:30:50.790: INFO: Found Statefulset ss in namespace statefulset-3153 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 05:30:50.790: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
May 27 05:30:50.790: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 27 05:30:50.803: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
May 27 05:30:50.806: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 05:30:50.806: INFO: Deleting all statefulset in ns statefulset-3153
May 27 05:30:50.811: INFO: Scaling statefulset ss to 0
May 27 05:31:00.845: INFO: Waiting for statefulset status.replicas updated to 0
May 27 05:31:00.853: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:31:00.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3153" for this suite.

• [SLOW TEST:20.343 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":57,"skipped":1005,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:31:00.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
May 27 05:31:00.980: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1099 proxy --unix-socket=/tmp/kubectl-proxy-unix2574574559/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:31:01.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1099" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":58,"skipped":1016,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:31:01.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-db7219d8-7552-433a-843d-1adec9f8b1d3
STEP: Creating a pod to test consume secrets
May 27 05:31:01.250: INFO: Waiting up to 5m0s for pod "pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399" in namespace "secrets-1473" to be "Succeeded or Failed"
May 27 05:31:01.256: INFO: Pod "pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005674ms
May 27 05:31:03.276: INFO: Pod "pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399": Phase="Running", Reason="", readiness=true. Elapsed: 2.025302132s
May 27 05:31:05.285: INFO: Pod "pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399": Phase="Running", Reason="", readiness=false. Elapsed: 4.034986194s
May 27 05:31:07.296: INFO: Pod "pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046225504s
STEP: Saw pod success
May 27 05:31:07.297: INFO: Pod "pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399" satisfied condition "Succeeded or Failed"
May 27 05:31:07.305: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399 container secret-volume-test: <nil>
STEP: delete the pod
May 27 05:31:07.343: INFO: Waiting for pod pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399 to disappear
May 27 05:31:07.350: INFO: Pod pod-secrets-6f6aad5c-aeec-4016-bd5a-7bad37811399 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:31:07.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1473" for this suite.

• [SLOW TEST:6.252 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:31:07.375: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:31:07.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1349" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":60,"skipped":1077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:31:07.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-3112
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3112
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3112
May 27 05:31:07.631: INFO: Found 0 stateful pods, waiting for 1
May 27 05:31:17.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 27 05:31:17.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 05:31:17.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 05:31:17.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 05:31:17.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 05:31:17.923: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 27 05:31:27.978: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 05:31:27.978: INFO: Waiting for statefulset status.replicas updated to 0
May 27 05:31:28.011: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999917s
May 27 05:31:29.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994678317s
May 27 05:31:30.028: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984325618s
May 27 05:31:31.040: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.977079068s
May 27 05:31:32.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.9653727s
May 27 05:31:33.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.950677911s
May 27 05:31:34.077: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.941103984s
May 27 05:31:35.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.927188947s
May 27 05:31:36.100: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.918655631s
May 27 05:31:37.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.245086ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3112
May 27 05:31:38.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 05:31:38.397: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 05:31:38.397: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 05:31:38.397: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 05:31:38.404: INFO: Found 1 stateful pods, waiting for 3
May 27 05:31:48.421: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 05:31:48.421: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 05:31:48.421: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 27 05:31:48.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 05:31:48.693: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 05:31:48.693: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 05:31:48.693: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 05:31:48.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 05:31:48.986: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 05:31:48.986: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 05:31:48.986: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 05:31:48.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 05:31:49.292: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 05:31:49.292: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 05:31:49.292: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 05:31:49.292: INFO: Waiting for statefulset status.replicas updated to 0
May 27 05:31:49.300: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 27 05:31:59.335: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 05:31:59.335: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 27 05:31:59.336: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 27 05:31:59.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999583s
May 27 05:32:00.378: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992289667s
May 27 05:32:01.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982863262s
May 27 05:32:02.416: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960506302s
May 27 05:32:03.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944949489s
May 27 05:32:04.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.933900735s
May 27 05:32:05.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.92340662s
May 27 05:32:06.459: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913024203s
May 27 05:32:07.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.901721565s
May 27 05:32:08.483: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.366972ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3112
May 27 05:32:09.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 05:32:09.744: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 05:32:09.744: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 05:32:09.744: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 05:32:09.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 05:32:09.974: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 05:32:09.974: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 05:32:09.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 05:32:09.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-3112 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 05:32:10.193: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 05:32:10.193: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 05:32:10.193: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 05:32:10.193: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 05:32:20.252: INFO: Deleting all statefulset in ns statefulset-3112
May 27 05:32:20.258: INFO: Scaling statefulset ss to 0
May 27 05:32:20.280: INFO: Waiting for statefulset status.replicas updated to 0
May 27 05:32:20.285: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:32:20.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3112" for this suite.

• [SLOW TEST:72.786 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":61,"skipped":1101,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:32:20.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-43dd1d60-61a6-4574-aa0a-7ae06c5cd59a
STEP: Creating a pod to test consume secrets
May 27 05:32:20.407: INFO: Waiting up to 5m0s for pod "pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd" in namespace "secrets-105" to be "Succeeded or Failed"
May 27 05:32:20.416: INFO: Pod "pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.459547ms
May 27 05:32:22.430: INFO: Pod "pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022589891s
May 27 05:32:24.444: INFO: Pod "pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036534401s
STEP: Saw pod success
May 27 05:32:24.444: INFO: Pod "pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd" satisfied condition "Succeeded or Failed"
May 27 05:32:24.452: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd container secret-volume-test: <nil>
STEP: delete the pod
May 27 05:32:24.494: INFO: Waiting for pod pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd to disappear
May 27 05:32:24.502: INFO: Pod pod-secrets-ab51c051-bbb8-467b-8ffa-a64ee65bc3fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:32:24.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-105" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":1103,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:32:24.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-74989424-9b0a-4697-a336-288f9af38cc9
STEP: Creating a pod to test consume configMaps
May 27 05:32:24.619: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4" in namespace "configmap-7784" to be "Succeeded or Failed"
May 27 05:32:24.628: INFO: Pod "pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.256042ms
May 27 05:32:26.637: INFO: Pod "pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0181711s
May 27 05:32:28.656: INFO: Pod "pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037455995s
May 27 05:32:30.669: INFO: Pod "pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050039754s
STEP: Saw pod success
May 27 05:32:30.669: INFO: Pod "pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4" satisfied condition "Succeeded or Failed"
May 27 05:32:30.674: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4 container configmap-volume-test: <nil>
STEP: delete the pod
May 27 05:32:30.706: INFO: Waiting for pod pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4 to disappear
May 27 05:32:30.712: INFO: Pod pod-configmaps-6d313909-d3b5-4ae2-b374-2a28ca8ca7b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:32:30.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7784" for this suite.

• [SLOW TEST:6.204 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":63,"skipped":1107,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:32:30.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:32:30.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df" in namespace "projected-6887" to be "Succeeded or Failed"
May 27 05:32:30.814: INFO: Pod "downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df": Phase="Pending", Reason="", readiness=false. Elapsed: 16.557575ms
May 27 05:32:32.831: INFO: Pod "downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033462412s
May 27 05:32:34.846: INFO: Pod "downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048807723s
STEP: Saw pod success
May 27 05:32:34.846: INFO: Pod "downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df" satisfied condition "Succeeded or Failed"
May 27 05:32:34.852: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df container client-container: <nil>
STEP: delete the pod
May 27 05:32:34.878: INFO: Waiting for pod downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df to disappear
May 27 05:32:34.883: INFO: Pod downwardapi-volume-47083fb3-e5dc-4bd8-9a52-bc081dea99df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:32:34.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6887" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":64,"skipped":1110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:32:34.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
May 27 05:32:34.969: INFO: Waiting up to 5m0s for pod "var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f" in namespace "var-expansion-567" to be "Succeeded or Failed"
May 27 05:32:34.986: INFO: Pod "var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.884391ms
May 27 05:32:36.995: INFO: Pod "var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02603446s
May 27 05:32:39.011: INFO: Pod "var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041600351s
STEP: Saw pod success
May 27 05:32:39.011: INFO: Pod "var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f" satisfied condition "Succeeded or Failed"
May 27 05:32:39.016: INFO: Trying to get logs from node ha9zeyohpei4-3 pod var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f container dapi-container: <nil>
STEP: delete the pod
May 27 05:32:39.051: INFO: Waiting for pod var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f to disappear
May 27 05:32:39.057: INFO: Pod var-expansion-28dbbf0a-1695-4487-9dfc-30efcfec497f no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:32:39.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-567" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":65,"skipped":1179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:32:39.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-8f9cbd82-8008-4c23-b34d-2ce809c87077 in namespace container-probe-5935
May 27 05:32:41.168: INFO: Started pod busybox-8f9cbd82-8008-4c23-b34d-2ce809c87077 in namespace container-probe-5935
STEP: checking the pod's current state and verifying that restartCount is present
May 27 05:32:41.196: INFO: Initial restart count of pod busybox-8f9cbd82-8008-4c23-b34d-2ce809c87077 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:36:43.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5935" for this suite.

• [SLOW TEST:244.032 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":66,"skipped":1202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:36:43.111: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:36:43.200: INFO: The status of Pod busybox-readonly-fs08604149-f387-4c49-9a78-38f46cb868f0 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:36:45.211: INFO: The status of Pod busybox-readonly-fs08604149-f387-4c49-9a78-38f46cb868f0 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:36:45.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6659" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":67,"skipped":1232,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:36:45.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
May 27 05:36:45.362: INFO: Waiting up to 5m0s for pod "client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0" in namespace "containers-7179" to be "Succeeded or Failed"
May 27 05:36:45.378: INFO: Pod "client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.243175ms
May 27 05:36:47.396: INFO: Pod "client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033902193s
May 27 05:36:49.424: INFO: Pod "client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061843658s
STEP: Saw pod success
May 27 05:36:49.424: INFO: Pod "client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0" satisfied condition "Succeeded or Failed"
May 27 05:36:49.444: INFO: Trying to get logs from node ha9zeyohpei4-2 pod client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0 container agnhost-container: <nil>
STEP: delete the pod
May 27 05:36:49.530: INFO: Waiting for pod client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0 to disappear
May 27 05:36:49.538: INFO: Pod client-containers-b40a1496-50d5-4a7f-9f08-73dc1e29a3c0 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:36:49.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7179" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1243,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:36:49.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-5178/configmap-test-6c804112-2fbb-41f1-8d4e-3cb537b25cf8
STEP: Creating a pod to test consume configMaps
May 27 05:36:49.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70" in namespace "configmap-5178" to be "Succeeded or Failed"
May 27 05:36:49.711: INFO: Pod "pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 15.467022ms
May 27 05:36:51.728: INFO: Pod "pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031629876s
May 27 05:36:53.750: INFO: Pod "pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054495042s
May 27 05:36:55.765: INFO: Pod "pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069352245s
STEP: Saw pod success
May 27 05:36:55.765: INFO: Pod "pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70" satisfied condition "Succeeded or Failed"
May 27 05:36:55.773: INFO: Trying to get logs from node ha9zeyohpei4-1 pod pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70 container env-test: <nil>
STEP: delete the pod
May 27 05:36:55.840: INFO: Waiting for pod pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70 to disappear
May 27 05:36:55.847: INFO: Pod pod-configmaps-68b06621-2608-4fba-975b-41bf302e4c70 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:36:55.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5178" for this suite.

• [SLOW TEST:6.312 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1244,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:36:55.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
May 27 05:36:55.948: INFO: Waiting up to 5m0s for pod "var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8" in namespace "var-expansion-3827" to be "Succeeded or Failed"
May 27 05:36:55.966: INFO: Pod "var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.140665ms
May 27 05:36:57.980: INFO: Pod "var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031833224s
May 27 05:36:59.991: INFO: Pod "var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043512176s
May 27 05:37:02.005: INFO: Pod "var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057236584s
STEP: Saw pod success
May 27 05:37:02.005: INFO: Pod "var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8" satisfied condition "Succeeded or Failed"
May 27 05:37:02.013: INFO: Trying to get logs from node ha9zeyohpei4-1 pod var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8 container dapi-container: <nil>
STEP: delete the pod
May 27 05:37:02.058: INFO: Waiting for pod var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8 to disappear
May 27 05:37:02.067: INFO: Pod var-expansion-96620dd3-5931-45d7-ad34-429a3a6c09a8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:37:02.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3827" for this suite.

• [SLOW TEST:6.225 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:37:02.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 05:37:02.192: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 05:37:02.241: INFO: Waiting for terminating namespaces to be deleted...
May 27 05:37:02.252: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-1 before test
May 27 05:37:02.297: INFO: echo-other-node-59d779959c-4vckc from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container echo-other-node ready: true, restart count 0
May 27 05:37:02.297: INFO: cilium-node-init-ph2ps from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container node-init ready: true, restart count 0
May 27 05:37:02.297: INFO: cilium-wjfhd from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 05:37:02.297: INFO: kube-addon-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 05:37:02.297: INFO: kube-apiserver-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 05:37:02.297: INFO: kube-controller-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 05:37:02.297: INFO: kube-proxy-4t9kh from kube-system started at 2022-05-27 04:57:26 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 05:37:02.297: INFO: kube-scheduler-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 05:37:02.297: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-lcdqs from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:37:02.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:37:02.297: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 05:37:02.297: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-2 before test
May 27 05:37:02.324: INFO: cilium-dfq7b from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 05:37:02.324: INFO: cilium-node-init-942c6 from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container node-init ready: true, restart count 0
May 27 05:37:02.324: INFO: coredns-64897985d-5hvb2 from kube-system started at 2022-05-27 05:10:55 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container coredns ready: true, restart count 0
May 27 05:37:02.324: INFO: kube-addon-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 05:37:02.324: INFO: kube-apiserver-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 05:37:02.324: INFO: kube-controller-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:57:59 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 05:37:02.324: INFO: kube-proxy-lpjdp from kube-system started at 2022-05-27 04:58:12 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 05:37:02.324: INFO: kube-scheduler-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 05:37:02.324: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-sgkpw from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:37:02.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:37:02.324: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 05:37:02.324: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-3 before test
May 27 05:37:02.362: INFO: client-7568bc7f86-94lb8 from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container client ready: true, restart count 0
May 27 05:37:02.362: INFO: client2-686d5f784b-xpnfm from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container client2 ready: true, restart count 0
May 27 05:37:02.362: INFO: echo-same-node-5767b7b99d-bwpk2 from cilium-test started at 2022-05-27 05:12:20 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 05:37:02.362: INFO: cilium-7dvhf from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 05:37:02.362: INFO: cilium-node-init-zxkdp from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container node-init ready: true, restart count 0
May 27 05:37:02.362: INFO: cilium-operator-59d6f769d4-nlxfs from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container cilium-operator ready: true, restart count 0
May 27 05:37:02.362: INFO: coredns-64897985d-484hx from kube-system started at 2022-05-27 05:11:10 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container coredns ready: true, restart count 0
May 27 05:37:02.362: INFO: kube-proxy-hzzmk from kube-system started at 2022-05-27 04:58:38 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 05:37:02.362: INFO: busybox-readonly-fs08604149-f387-4c49-9a78-38f46cb868f0 from kubelet-test-6659 started at 2022-05-27 05:36:43 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container busybox-readonly-fs08604149-f387-4c49-9a78-38f46cb868f0 ready: true, restart count 0
May 27 05:37:02.362: INFO: sonobuoy from sonobuoy started at 2022-05-27 05:16:07 +0000 UTC (1 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 05:37:02.362: INFO: sonobuoy-e2e-job-7e88fd63f0e849eb from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container e2e ready: true, restart count 0
May 27 05:37:02.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:37:02.362: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-tz8zz from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 05:37:02.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 05:37:02.362: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node ha9zeyohpei4-1
STEP: verifying the node has the label node ha9zeyohpei4-2
STEP: verifying the node has the label node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod client-7568bc7f86-94lb8 requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod client2-686d5f784b-xpnfm requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod echo-other-node-59d779959c-4vckc requesting resource cpu=0m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod echo-same-node-5767b7b99d-bwpk2 requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod cilium-7dvhf requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod cilium-dfq7b requesting resource cpu=0m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod cilium-node-init-942c6 requesting resource cpu=0m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod cilium-node-init-ph2ps requesting resource cpu=0m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod cilium-node-init-zxkdp requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod cilium-operator-59d6f769d4-nlxfs requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod cilium-wjfhd requesting resource cpu=0m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod coredns-64897985d-484hx requesting resource cpu=100m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod coredns-64897985d-5hvb2 requesting resource cpu=100m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod kube-addon-manager-ha9zeyohpei4-1 requesting resource cpu=5m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod kube-addon-manager-ha9zeyohpei4-2 requesting resource cpu=5m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod kube-apiserver-ha9zeyohpei4-1 requesting resource cpu=250m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod kube-apiserver-ha9zeyohpei4-2 requesting resource cpu=250m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod kube-controller-manager-ha9zeyohpei4-1 requesting resource cpu=200m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod kube-controller-manager-ha9zeyohpei4-2 requesting resource cpu=200m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod kube-proxy-4t9kh requesting resource cpu=0m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod kube-proxy-hzzmk requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod kube-proxy-lpjdp requesting resource cpu=0m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod kube-scheduler-ha9zeyohpei4-1 requesting resource cpu=100m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod kube-scheduler-ha9zeyohpei4-2 requesting resource cpu=100m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod busybox-readonly-fs08604149-f387-4c49-9a78-38f46cb868f0 requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod sonobuoy requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod sonobuoy-e2e-job-7e88fd63f0e849eb requesting resource cpu=0m on Node ha9zeyohpei4-3
May 27 05:37:02.584: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-lcdqs requesting resource cpu=0m on Node ha9zeyohpei4-1
May 27 05:37:02.584: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-sgkpw requesting resource cpu=0m on Node ha9zeyohpei4-2
May 27 05:37:02.584: INFO: Pod sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-tz8zz requesting resource cpu=0m on Node ha9zeyohpei4-3
STEP: Starting Pods to consume most of the cluster CPU.
May 27 05:37:02.584: INFO: Creating a pod which consumes cpu=731m on Node ha9zeyohpei4-1
May 27 05:37:02.614: INFO: Creating a pod which consumes cpu=661m on Node ha9zeyohpei4-2
May 27 05:37:02.639: INFO: Creating a pod which consumes cpu=1050m on Node ha9zeyohpei4-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa.16f2df8ffad83f1a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3932/filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa to ha9zeyohpei4-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa.16f2df90395b0b8b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa.16f2df9046ea2449], Reason = [Created], Message = [Created container filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa.16f2df904a38c7ec], Reason = [Started], Message = [Started container filler-pod-10aced47-6754-4637-8a0d-20e773c5e4fa]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc.16f2df8ff9ebe407], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3932/filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc to ha9zeyohpei4-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc.16f2df90434427ff], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc.16f2df90596fdd67], Reason = [Created], Message = [Created container filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc.16f2df905fd86bde], Reason = [Started], Message = [Started container filler-pod-613ea6d0-172b-4445-a777-f612a112c1bc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9c214050-f266-46ee-b524-457baa15197d.16f2df8ff783ad95], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3932/filler-pod-9c214050-f266-46ee-b524-457baa15197d to ha9zeyohpei4-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9c214050-f266-46ee-b524-457baa15197d.16f2df904407fa2d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9c214050-f266-46ee-b524-457baa15197d.16f2df905829b9b9], Reason = [Created], Message = [Created container filler-pod-9c214050-f266-46ee-b524-457baa15197d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9c214050-f266-46ee-b524-457baa15197d.16f2df905b07350d], Reason = [Started], Message = [Started container filler-pod-9c214050-f266-46ee-b524-457baa15197d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f2df90eb860f75], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ha9zeyohpei4-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ha9zeyohpei4-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ha9zeyohpei4-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:37:07.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3932" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.795 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":71,"skipped":1275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:37:07.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:37:07.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 05:37:08.043: INFO: The status of Pod pod-exec-websocket-6e205958-48c1-454f-93a2-29413dbb8bf1 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:37:10.051: INFO: The status of Pod pod-exec-websocket-6e205958-48c1-454f-93a2-29413dbb8bf1 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:37:12.061: INFO: The status of Pod pod-exec-websocket-6e205958-48c1-454f-93a2-29413dbb8bf1 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:37:12.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9239" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:37:12.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:37:12.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 27 05:37:18.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 create -f -'
May 27 05:37:20.361: INFO: stderr: ""
May 27 05:37:20.361: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 27 05:37:20.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 delete e2e-test-crd-publish-openapi-5539-crds test-cr'
May 27 05:37:20.630: INFO: stderr: ""
May 27 05:37:20.630: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 27 05:37:20.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 apply -f -'
May 27 05:37:21.100: INFO: stderr: ""
May 27 05:37:21.101: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 27 05:37:21.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-6833 --namespace=crd-publish-openapi-6833 delete e2e-test-crd-publish-openapi-5539-crds test-cr'
May 27 05:37:21.253: INFO: stderr: ""
May 27 05:37:21.253: INFO: stdout: "e2e-test-crd-publish-openapi-5539-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 27 05:37:21.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-6833 explain e2e-test-crd-publish-openapi-5539-crds'
May 27 05:37:21.554: INFO: stderr: ""
May 27 05:37:21.554: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5539-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:37:25.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6833" for this suite.

• [SLOW TEST:13.382 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":73,"skipped":1438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:37:25.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-3057
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 05:37:25.678: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 05:37:25.791: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:37:27.805: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:29.818: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:31.810: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:33.808: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:35.808: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:37.813: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:39.808: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:41.806: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:43.806: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 05:37:45.803: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 05:37:45.816: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 27 05:37:47.830: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 05:37:47.842: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 05:37:51.896: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 05:37:51.896: INFO: Breadth first check of 10.233.65.234 on host 192.168.121.43...
May 27 05:37:51.900: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.105:9080/dial?request=hostname&protocol=udp&host=10.233.65.234&port=8081&tries=1'] Namespace:pod-network-test-3057 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:37:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:37:51.902: INFO: ExecWithOptions: Clientset creation
May 27 05:37:51.902: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3057/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.234%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 05:37:52.104: INFO: Waiting for responses: map[]
May 27 05:37:52.104: INFO: reached 10.233.65.234 after 0/1 tries
May 27 05:37:52.104: INFO: Breadth first check of 10.233.64.115 on host 192.168.121.209...
May 27 05:37:52.112: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.105:9080/dial?request=hostname&protocol=udp&host=10.233.64.115&port=8081&tries=1'] Namespace:pod-network-test-3057 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:37:52.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:37:52.114: INFO: ExecWithOptions: Clientset creation
May 27 05:37:52.114: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3057/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.115%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 05:37:52.226: INFO: Waiting for responses: map[]
May 27 05:37:52.226: INFO: reached 10.233.64.115 after 0/1 tries
May 27 05:37:52.226: INFO: Breadth first check of 10.233.66.94 on host 192.168.121.191...
May 27 05:37:52.236: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.105:9080/dial?request=hostname&protocol=udp&host=10.233.66.94&port=8081&tries=1'] Namespace:pod-network-test-3057 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:37:52.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:37:52.238: INFO: ExecWithOptions: Clientset creation
May 27 05:37:52.238: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3057/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.94%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 05:37:52.361: INFO: Waiting for responses: map[]
May 27 05:37:52.361: INFO: reached 10.233.66.94 after 0/1 tries
May 27 05:37:52.361: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:37:52.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3057" for this suite.

• [SLOW TEST:26.759 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1470,"failed":0}
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:37:52.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-db5b6997-ebcf-4b3a-b3bc-fa114685945a
STEP: Creating secret with name s-test-opt-upd-f1f83c33-d906-4e5a-9c19-f12f67753854
STEP: Creating the pod
May 27 05:37:52.499: INFO: The status of Pod pod-secrets-95f7f646-edb8-4deb-87d6-1eab39669f53 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:37:54.520: INFO: The status of Pod pod-secrets-95f7f646-edb8-4deb-87d6-1eab39669f53 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-db5b6997-ebcf-4b3a-b3bc-fa114685945a
STEP: Updating secret s-test-opt-upd-f1f83c33-d906-4e5a-9c19-f12f67753854
STEP: Creating secret with name s-test-opt-create-ae60c4d6-4f7e-42ac-a925-f97dea0ba5ee
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:37:56.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6389" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1470,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:37:56.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 05:37:56.768: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 05:38:56.852: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
May 27 05:38:56.922: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 27 05:38:56.933: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 27 05:38:56.979: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 27 05:38:57.000: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 27 05:38:57.050: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 27 05:38:57.073: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:39:15.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1943" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:78.776 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":76,"skipped":1479,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:39:15.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-7781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7781 to expose endpoints map[]
May 27 05:39:15.594: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 27 05:39:16.619: INFO: successfully validated that service endpoint-test2 in namespace services-7781 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7781
May 27 05:39:16.646: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:39:18.669: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:39:20.658: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7781 to expose endpoints map[pod1:[80]]
May 27 05:39:20.691: INFO: successfully validated that service endpoint-test2 in namespace services-7781 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
May 27 05:39:20.691: INFO: Creating new exec pod
May 27 05:39:23.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-7781 exec execpod2bx87 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 27 05:39:24.028: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 27 05:39:24.028: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:39:24.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-7781 exec execpod2bx87 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.199 80'
May 27 05:39:24.258: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.199 80\nConnection to 10.233.7.199 80 port [tcp/http] succeeded!\n"
May 27 05:39:24.258: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-7781
May 27 05:39:24.276: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:39:26.287: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7781 to expose endpoints map[pod1:[80] pod2:[80]]
May 27 05:39:26.318: INFO: successfully validated that service endpoint-test2 in namespace services-7781 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
May 27 05:39:27.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-7781 exec execpod2bx87 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 27 05:39:27.580: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 27 05:39:27.580: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:39:27.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-7781 exec execpod2bx87 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.199 80'
May 27 05:39:27.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.199 80\nConnection to 10.233.7.199 80 port [tcp/http] succeeded!\n"
May 27 05:39:27.802: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7781 to expose endpoints map[pod2:[80]]
May 27 05:39:27.901: INFO: successfully validated that service endpoint-test2 in namespace services-7781 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
May 27 05:39:28.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-7781 exec execpod2bx87 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 27 05:39:29.122: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 27 05:39:29.122: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:39:29.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-7781 exec execpod2bx87 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.199 80'
May 27 05:39:29.364: INFO: stderr: "+ nc -v -t -w 2 10.233.7.199 80\n+ echo hostName\nConnection to 10.233.7.199 80 port [tcp/http] succeeded!\n"
May 27 05:39:29.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-7781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7781 to expose endpoints map[]
May 27 05:39:30.484: INFO: successfully validated that service endpoint-test2 in namespace services-7781 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:39:30.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7781" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:15.070 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":77,"skipped":1486,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:39:30.549: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
May 27 05:41:31.195: INFO: Successfully updated pod "var-expansion-4f401f0f-b581-47a7-b67f-2540b939fcb3"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 27 05:41:33.215: INFO: Deleting pod "var-expansion-4f401f0f-b581-47a7-b67f-2540b939fcb3" in namespace "var-expansion-5296"
May 27 05:41:33.229: INFO: Wait up to 5m0s for pod "var-expansion-4f401f0f-b581-47a7-b67f-2540b939fcb3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:42:05.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5296" for this suite.

• [SLOW TEST:154.738 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":78,"skipped":1528,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:42:05.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-8416
May 27 05:42:05.391: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 05:42:07.409: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 27 05:42:07.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 27 05:42:07.752: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 27 05:42:07.753: INFO: stdout: "iptables"
May 27 05:42:07.753: INFO: proxyMode: iptables
May 27 05:42:07.786: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 27 05:42:07.793: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-8416
STEP: creating replication controller affinity-nodeport-timeout in namespace services-8416
I0527 05:42:07.879848      14 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-8416, replica count: 3
I0527 05:42:10.933012      14 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 05:42:10.962: INFO: Creating new exec pod
May 27 05:42:14.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May 27 05:42:14.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 27 05:42:14.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:42:14.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.150 80'
May 27 05:42:14.544: INFO: stderr: "+ + nc -v -t -w 2 10.233.23.150 80\necho hostName\nConnection to 10.233.23.150 80 port [tcp/http] succeeded!\n"
May 27 05:42:14.544: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:42:14.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 31571'
May 27 05:42:14.776: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 31571\nConnection to 192.168.121.43 31571 port [tcp/*] succeeded!\n"
May 27 05:42:14.776: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:42:14.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.191 31571'
May 27 05:42:15.006: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.191 31571\nConnection to 192.168.121.191 31571 port [tcp/*] succeeded!\n"
May 27 05:42:15.006: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:42:15.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.43:31571/ ; done'
May 27 05:42:15.420: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n"
May 27 05:42:15.420: INFO: stdout: "\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f\naffinity-nodeport-timeout-grc7f"
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Received response from host: affinity-nodeport-timeout-grc7f
May 27 05:42:15.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.43:31571/'
May 27 05:42:15.652: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n"
May 27 05:42:15.652: INFO: stdout: "affinity-nodeport-timeout-grc7f"
May 27 05:42:35.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8416 exec execpod-affinity9thbr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.43:31571/'
May 27 05:42:35.941: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.43:31571/\n"
May 27 05:42:35.941: INFO: stdout: "affinity-nodeport-timeout-tdxz2"
May 27 05:42:35.941: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-8416, will wait for the garbage collector to delete the pods
May 27 05:42:36.046: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.816929ms
May 27 05:42:36.147: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.070698ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:42:38.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8416" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:33.377 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":79,"skipped":1560,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:42:38.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
May 27 05:42:38.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8971 api-versions'
May 27 05:42:38.873: INFO: stderr: ""
May 27 05:42:38.873: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncilium.io/v2\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:42:38.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8971" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":80,"skipped":1572,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:42:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:42:56.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2321" for this suite.

• [SLOW TEST:17.183 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":81,"skipped":1572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:42:56.091: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:42:56.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351" in namespace "downward-api-46" to be "Succeeded or Failed"
May 27 05:42:56.174: INFO: Pod "downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351": Phase="Pending", Reason="", readiness=false. Elapsed: 8.74067ms
May 27 05:42:58.195: INFO: Pod "downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351": Phase="Running", Reason="", readiness=true. Elapsed: 2.029914829s
May 27 05:43:00.206: INFO: Pod "downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351": Phase="Running", Reason="", readiness=false. Elapsed: 4.040822209s
May 27 05:43:02.222: INFO: Pod "downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056780778s
STEP: Saw pod success
May 27 05:43:02.222: INFO: Pod "downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351" satisfied condition "Succeeded or Failed"
May 27 05:43:02.228: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351 container client-container: <nil>
STEP: delete the pod
May 27 05:43:02.294: INFO: Waiting for pod downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351 to disappear
May 27 05:43:02.300: INFO: Pod downwardapi-volume-5aff9d09-0a21-4a6c-af3f-46636369a351 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:43:02.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-46" for this suite.

• [SLOW TEST:6.238 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":82,"skipped":1619,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:43:02.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4013
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4013
STEP: creating replication controller externalsvc in namespace services-4013
I0527 05:43:02.444782      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4013, replica count: 2
I0527 05:43:05.496375      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 27 05:43:05.540: INFO: Creating new exec pod
May 27 05:43:07.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-4013 exec execpodfl7q7 -- /bin/sh -x -c nslookup clusterip-service.services-4013.svc.cluster.local'
May 27 05:43:07.970: INFO: stderr: "+ nslookup clusterip-service.services-4013.svc.cluster.local\n"
May 27 05:43:07.970: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-4013.svc.cluster.local\tcanonical name = externalsvc.services-4013.svc.cluster.local.\nName:\texternalsvc.services-4013.svc.cluster.local\nAddress: 10.233.58.159\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4013, will wait for the garbage collector to delete the pods
May 27 05:43:08.046: INFO: Deleting ReplicationController externalsvc took: 13.244681ms
May 27 05:43:08.147: INFO: Terminating ReplicationController externalsvc pods took: 101.063295ms
May 27 05:43:10.079: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:43:10.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4013" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.822 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":83,"skipped":1626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:43:10.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-518af4e8-f58e-4761-95b1-26962eb2b20c
STEP: Creating a pod to test consume configMaps
May 27 05:43:10.280: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58" in namespace "projected-5874" to be "Succeeded or Failed"
May 27 05:43:10.288: INFO: Pod "pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.989314ms
May 27 05:43:12.309: INFO: Pod "pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028994778s
May 27 05:43:14.320: INFO: Pod "pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039294162s
May 27 05:43:16.330: INFO: Pod "pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050033581s
STEP: Saw pod success
May 27 05:43:16.330: INFO: Pod "pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58" satisfied condition "Succeeded or Failed"
May 27 05:43:16.339: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58 container agnhost-container: <nil>
STEP: delete the pod
May 27 05:43:16.382: INFO: Waiting for pod pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58 to disappear
May 27 05:43:16.390: INFO: Pod pod-projected-configmaps-dcd81fcc-9122-4126-9919-032dea1d7c58 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:43:16.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5874" for this suite.

• [SLOW TEST:6.254 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:43:16.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:43:17.177: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 05:43:19.261: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 43, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 43, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 43, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 43, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:43:22.299: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:43:22.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9855-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:43:26.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8601" for this suite.
STEP: Destroying namespace "webhook-8601-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.720 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":85,"skipped":1702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:43:26.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
May 27 05:43:46.569: INFO: EndpointSlice for Service endpointslice-4305/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:43:56.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4305" for this suite.

• [SLOW TEST:30.488 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":86,"skipped":1729,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:43:56.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-013190a1-af89-4cc9-aefd-0b11584ad9e6
STEP: Creating the pod
May 27 05:43:56.772: INFO: The status of Pod pod-configmaps-9f1704d2-b677-452c-a178-9e1d5c2cc748 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:43:58.784: INFO: The status of Pod pod-configmaps-9f1704d2-b677-452c-a178-9e1d5c2cc748 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-013190a1-af89-4cc9-aefd-0b11584ad9e6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:44:00.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-477" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":87,"skipped":1736,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:44:00.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:44:00.931: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8c23bb08-4178-4880-9d87-c2a9376a6d57" in namespace "security-context-test-9988" to be "Succeeded or Failed"
May 27 05:44:00.936: INFO: Pod "busybox-readonly-false-8c23bb08-4178-4880-9d87-c2a9376a6d57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.846751ms
May 27 05:44:02.948: INFO: Pod "busybox-readonly-false-8c23bb08-4178-4880-9d87-c2a9376a6d57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017266795s
May 27 05:44:04.963: INFO: Pod "busybox-readonly-false-8c23bb08-4178-4880-9d87-c2a9376a6d57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032405734s
May 27 05:44:04.963: INFO: Pod "busybox-readonly-false-8c23bb08-4178-4880-9d87-c2a9376a6d57" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:44:04.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9988" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1751,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:44:04.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
May 27 05:44:05.049: INFO: Waiting up to 5m0s for pod "pod-27ed7048-b420-4b0f-b654-d232b1fab049" in namespace "emptydir-2228" to be "Succeeded or Failed"
May 27 05:44:05.060: INFO: Pod "pod-27ed7048-b420-4b0f-b654-d232b1fab049": Phase="Pending", Reason="", readiness=false. Elapsed: 10.80206ms
May 27 05:44:07.075: INFO: Pod "pod-27ed7048-b420-4b0f-b654-d232b1fab049": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026593204s
May 27 05:44:09.088: INFO: Pod "pod-27ed7048-b420-4b0f-b654-d232b1fab049": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03957398s
STEP: Saw pod success
May 27 05:44:09.089: INFO: Pod "pod-27ed7048-b420-4b0f-b654-d232b1fab049" satisfied condition "Succeeded or Failed"
May 27 05:44:09.094: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-27ed7048-b420-4b0f-b654-d232b1fab049 container test-container: <nil>
STEP: delete the pod
May 27 05:44:09.135: INFO: Waiting for pod pod-27ed7048-b420-4b0f-b654-d232b1fab049 to disappear
May 27 05:44:09.166: INFO: Pod pod-27ed7048-b420-4b0f-b654-d232b1fab049 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:44:09.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2228" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1755,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:44:09.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 05:44:13.350: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:44:13.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-956" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":90,"skipped":1763,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:44:13.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:44:43.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5433" for this suite.

• [SLOW TEST:29.777 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":91,"skipped":1773,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:44:43.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:44:43.269: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Pending, waiting for it to be Running (with Ready = true)
May 27 05:44:45.275: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:47.288: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:49.287: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:51.284: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:53.284: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:55.280: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:57.288: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:44:59.283: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:45:01.287: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:45:03.284: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = false)
May 27 05:45:05.277: INFO: The status of Pod test-webserver-77d35e3f-155d-46a1-b49d-c6ae45bf079b is Running (Ready = true)
May 27 05:45:05.284: INFO: Container started at 2022-05-27 05:44:44 +0000 UTC, pod became ready at 2022-05-27 05:45:03 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:05.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6194" for this suite.

• [SLOW TEST:22.128 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":92,"skipped":1782,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:05.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-9147fd79-ba62-4499-bdd8-447166c6e8bb
STEP: Creating a pod to test consume secrets
May 27 05:45:05.386: INFO: Waiting up to 5m0s for pod "pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589" in namespace "secrets-7231" to be "Succeeded or Failed"
May 27 05:45:05.394: INFO: Pod "pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589": Phase="Pending", Reason="", readiness=false. Elapsed: 7.911433ms
May 27 05:45:07.411: INFO: Pod "pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025030848s
May 27 05:45:09.426: INFO: Pod "pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040019012s
STEP: Saw pod success
May 27 05:45:09.426: INFO: Pod "pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589" satisfied condition "Succeeded or Failed"
May 27 05:45:09.432: INFO: Trying to get logs from node ha9zeyohpei4-1 pod pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589 container secret-volume-test: <nil>
STEP: delete the pod
May 27 05:45:09.485: INFO: Waiting for pod pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589 to disappear
May 27 05:45:09.493: INFO: Pod pod-secrets-f735d04c-e76e-4bd3-ab73-441d63377589 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:09.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7231" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":93,"skipped":1784,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:09.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:45:09.579: INFO: Creating pod...
May 27 05:45:09.612: INFO: Pod Quantity: 1 Status: Pending
May 27 05:45:10.641: INFO: Pod Quantity: 1 Status: Pending
May 27 05:45:11.636: INFO: Pod Quantity: 1 Status: Pending
May 27 05:45:12.625: INFO: Pod Status: Running
May 27 05:45:12.625: INFO: Creating service...
May 27 05:45:12.639: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/DELETE
May 27 05:45:12.672: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 27 05:45:12.672: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/GET
May 27 05:45:12.682: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 27 05:45:12.682: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/HEAD
May 27 05:45:12.691: INFO: http.Client request:HEAD | StatusCode:200
May 27 05:45:12.691: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/OPTIONS
May 27 05:45:12.700: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 27 05:45:12.700: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/PATCH
May 27 05:45:12.709: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 27 05:45:12.709: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/POST
May 27 05:45:12.730: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 27 05:45:12.730: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/pods/agnhost/proxy/some/path/with/PUT
May 27 05:45:12.740: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 27 05:45:12.740: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/DELETE
May 27 05:45:12.751: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 27 05:45:12.752: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/GET
May 27 05:45:12.763: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 27 05:45:12.763: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/HEAD
May 27 05:45:12.772: INFO: http.Client request:HEAD | StatusCode:200
May 27 05:45:12.772: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/OPTIONS
May 27 05:45:12.783: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 27 05:45:12.783: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/PATCH
May 27 05:45:12.792: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 27 05:45:12.792: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/POST
May 27 05:45:12.806: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 27 05:45:12.806: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7228/services/test-service/proxy/some/path/with/PUT
May 27 05:45:12.837: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:12.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7228" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":94,"skipped":1800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:12.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:45:14.353: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:45:17.415: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:45:17.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:20.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4248" for this suite.
STEP: Destroying namespace "webhook-4248-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.911 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":95,"skipped":1828,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:20.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:27.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7888" for this suite.

• [SLOW TEST:7.164 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":96,"skipped":1829,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:27.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 27 05:45:28.059: INFO: Waiting up to 5m0s for pod "pod-8f471762-6730-4917-9147-13cf392e9eb1" in namespace "emptydir-734" to be "Succeeded or Failed"
May 27 05:45:28.066: INFO: Pod "pod-8f471762-6730-4917-9147-13cf392e9eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.53326ms
May 27 05:45:30.077: INFO: Pod "pod-8f471762-6730-4917-9147-13cf392e9eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017261062s
May 27 05:45:32.095: INFO: Pod "pod-8f471762-6730-4917-9147-13cf392e9eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035908402s
May 27 05:45:34.114: INFO: Pod "pod-8f471762-6730-4917-9147-13cf392e9eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054517533s
STEP: Saw pod success
May 27 05:45:34.114: INFO: Pod "pod-8f471762-6730-4917-9147-13cf392e9eb1" satisfied condition "Succeeded or Failed"
May 27 05:45:34.122: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-8f471762-6730-4917-9147-13cf392e9eb1 container test-container: <nil>
STEP: delete the pod
May 27 05:45:34.180: INFO: Waiting for pod pod-8f471762-6730-4917-9147-13cf392e9eb1 to disappear
May 27 05:45:34.187: INFO: Pod pod-8f471762-6730-4917-9147-13cf392e9eb1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:34.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-734" for this suite.

• [SLOW TEST:6.254 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":97,"skipped":1844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:34.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
May 27 05:45:34.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9810 create -f -'
May 27 05:45:36.118: INFO: stderr: ""
May 27 05:45:36.118: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 27 05:45:37.155: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 05:45:37.155: INFO: Found 0 / 1
May 27 05:45:38.129: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 05:45:38.130: INFO: Found 1 / 1
May 27 05:45:38.130: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 27 05:45:38.136: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 05:45:38.136: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 27 05:45:38.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9810 patch pod agnhost-primary-7lxqc -p {"metadata":{"annotations":{"x":"y"}}}'
May 27 05:45:38.302: INFO: stderr: ""
May 27 05:45:38.302: INFO: stdout: "pod/agnhost-primary-7lxqc patched\n"
STEP: checking annotations
May 27 05:45:38.311: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 05:45:38.311: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:38.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9810" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":98,"skipped":1903,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:38.330: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
May 27 05:45:38.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 create -f -'
May 27 05:45:40.121: INFO: stderr: ""
May 27 05:45:40.121: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 05:45:40.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 05:45:40.269: INFO: stderr: ""
May 27 05:45:40.269: INFO: stdout: "update-demo-nautilus-7chvx update-demo-nautilus-l2c64 "
May 27 05:45:40.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods update-demo-nautilus-7chvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 05:45:40.385: INFO: stderr: ""
May 27 05:45:40.385: INFO: stdout: ""
May 27 05:45:40.385: INFO: update-demo-nautilus-7chvx is created but not running
May 27 05:45:45.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 05:45:45.522: INFO: stderr: ""
May 27 05:45:45.522: INFO: stdout: "update-demo-nautilus-7chvx update-demo-nautilus-l2c64 "
May 27 05:45:45.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods update-demo-nautilus-7chvx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 05:45:45.662: INFO: stderr: ""
May 27 05:45:45.662: INFO: stdout: "true"
May 27 05:45:45.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods update-demo-nautilus-7chvx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 05:45:45.791: INFO: stderr: ""
May 27 05:45:45.791: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 05:45:45.791: INFO: validating pod update-demo-nautilus-7chvx
May 27 05:45:45.801: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 05:45:45.802: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 05:45:45.802: INFO: update-demo-nautilus-7chvx is verified up and running
May 27 05:45:45.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods update-demo-nautilus-l2c64 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 05:45:45.915: INFO: stderr: ""
May 27 05:45:45.915: INFO: stdout: "true"
May 27 05:45:45.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods update-demo-nautilus-l2c64 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 05:45:46.038: INFO: stderr: ""
May 27 05:45:46.038: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 05:45:46.038: INFO: validating pod update-demo-nautilus-l2c64
May 27 05:45:46.054: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 05:45:46.054: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 05:45:46.054: INFO: update-demo-nautilus-l2c64 is verified up and running
STEP: using delete to clean up resources
May 27 05:45:46.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 delete --grace-period=0 --force -f -'
May 27 05:45:46.181: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 05:45:46.181: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 27 05:45:46.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get rc,svc -l name=update-demo --no-headers'
May 27 05:45:46.347: INFO: stderr: "No resources found in kubectl-1904 namespace.\n"
May 27 05:45:46.347: INFO: stdout: ""
May 27 05:45:46.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-1904 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 27 05:45:46.480: INFO: stderr: ""
May 27 05:45:46.480: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:45:46.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1904" for this suite.

• [SLOW TEST:8.176 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":99,"skipped":1914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:45:46.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:46:46.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-218" for this suite.

• [SLOW TEST:60.112 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":2018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:46:46.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:46:46.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461" in namespace "projected-249" to be "Succeeded or Failed"
May 27 05:46:46.710: INFO: Pod "downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461": Phase="Pending", Reason="", readiness=false. Elapsed: 9.017358ms
May 27 05:46:48.724: INFO: Pod "downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023072837s
May 27 05:46:50.740: INFO: Pod "downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039120199s
May 27 05:46:52.753: INFO: Pod "downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051683536s
STEP: Saw pod success
May 27 05:46:52.753: INFO: Pod "downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461" satisfied condition "Succeeded or Failed"
May 27 05:46:52.760: INFO: Trying to get logs from node ha9zeyohpei4-2 pod downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461 container client-container: <nil>
STEP: delete the pod
May 27 05:46:52.824: INFO: Waiting for pod downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461 to disappear
May 27 05:46:52.832: INFO: Pod downwardapi-volume-c7d78c91-6f79-4c7a-9605-066d08552461 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:46:52.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-249" for this suite.

• [SLOW TEST:6.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":101,"skipped":2054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:46:52.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
May 27 05:46:52.947: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 05:46:57.962: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
May 27 05:46:57.981: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
May 27 05:46:58.007: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
May 27 05:46:58.023: INFO: Observed &ReplicaSet event: ADDED
May 27 05:46:58.024: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.025: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.026: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.026: INFO: Found replicaset test-rs in namespace replicaset-6827 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 05:46:58.026: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
May 27 05:46:58.026: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 27 05:46:58.039: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
May 27 05:46:58.043: INFO: Observed &ReplicaSet event: ADDED
May 27 05:46:58.044: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.044: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.045: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.045: INFO: Observed replicaset test-rs in namespace replicaset-6827 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 05:46:58.045: INFO: Observed &ReplicaSet event: MODIFIED
May 27 05:46:58.046: INFO: Found replicaset test-rs in namespace replicaset-6827 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May 27 05:46:58.046: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:46:58.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6827" for this suite.

• [SLOW TEST:5.210 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":102,"skipped":2078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:46:58.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:47:26.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-108" for this suite.

• [SLOW TEST:28.209 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":103,"skipped":2126,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:47:26.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
May 27 05:47:26.385: INFO: Waiting up to 5m0s for pod "client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487" in namespace "containers-2670" to be "Succeeded or Failed"
May 27 05:47:26.394: INFO: Pod "client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487": Phase="Pending", Reason="", readiness=false. Elapsed: 8.306899ms
May 27 05:47:28.414: INFO: Pod "client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027693172s
May 27 05:47:30.427: INFO: Pod "client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041194395s
STEP: Saw pod success
May 27 05:47:30.428: INFO: Pod "client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487" satisfied condition "Succeeded or Failed"
May 27 05:47:30.434: INFO: Trying to get logs from node ha9zeyohpei4-3 pod client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487 container agnhost-container: <nil>
STEP: delete the pod
May 27 05:47:30.491: INFO: Waiting for pod client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487 to disappear
May 27 05:47:30.497: INFO: Pod client-containers-4d91b1ff-e3a4-4832-9f7d-29646d694487 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:47:30.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2670" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":2145,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:47:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-a16f0353-fc96-49ea-a086-54a6c4fdb0e0 in namespace container-probe-7781
May 27 05:47:32.619: INFO: Started pod busybox-a16f0353-fc96-49ea-a086-54a6c4fdb0e0 in namespace container-probe-7781
STEP: checking the pod's current state and verifying that restartCount is present
May 27 05:47:32.625: INFO: Initial restart count of pod busybox-a16f0353-fc96-49ea-a086-54a6c4fdb0e0 is 0
May 27 05:48:23.034: INFO: Restart count of pod container-probe-7781/busybox-a16f0353-fc96-49ea-a086-54a6c4fdb0e0 is now 1 (50.408994427s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:23.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7781" for this suite.

• [SLOW TEST:52.583 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":105,"skipped":2163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:23.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 05:48:23.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:48:23.311: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:48:24.348: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:48:24.348: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:48:25.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 05:48:25.335: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
May 27 05:48:25.413: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15237"},"items":null}

May 27 05:48:25.421: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15237"},"items":[{"metadata":{"name":"daemon-set-7x2k4","generateName":"daemon-set-","namespace":"daemonsets-968","uid":"0f121307-39a9-455b-86c0-8cd6ce14aa4f","resourceVersion":"15234","creationTimestamp":"2022-05-27T05:48:23Z","deletionTimestamp":"2022-05-27T05:48:55Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"789103c1-bb5d-4229-b8fe-c9fe81116709","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-27T05:48:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789103c1-bb5d-4229-b8fe-c9fe81116709\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-27T05:48:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-664ns","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-664ns","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ha9zeyohpei4-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ha9zeyohpei4-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:23Z"}],"hostIP":"192.168.121.43","podIP":"10.233.65.65","podIPs":[{"ip":"10.233.65.65"}],"startTime":"2022-05-27T05:48:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-27T05:48:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://fbe2c549ed82d501465d972321c4c6dccf98e0ab22bfc72c5bcb36cf1bd01cc1","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-b6zn8","generateName":"daemon-set-","namespace":"daemonsets-968","uid":"acfc7284-7075-4bc8-8176-539f6e04ad4d","resourceVersion":"15236","creationTimestamp":"2022-05-27T05:48:23Z","deletionTimestamp":"2022-05-27T05:48:55Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"789103c1-bb5d-4229-b8fe-c9fe81116709","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-27T05:48:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789103c1-bb5d-4229-b8fe-c9fe81116709\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-27T05:48:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cr4kb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cr4kb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ha9zeyohpei4-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ha9zeyohpei4-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:23Z"}],"hostIP":"192.168.121.191","podIP":"10.233.66.96","podIPs":[{"ip":"10.233.66.96"}],"startTime":"2022-05-27T05:48:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-27T05:48:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://35dea76ee4892804c06c4415ea7fddf743361bd91601ebf8b1ec3eb277c91d24","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hrht2","generateName":"daemon-set-","namespace":"daemonsets-968","uid":"de6120c0-89b7-43c3-8641-6d4d31c22206","resourceVersion":"15237","creationTimestamp":"2022-05-27T05:48:23Z","deletionTimestamp":"2022-05-27T05:48:55Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"789103c1-bb5d-4229-b8fe-c9fe81116709","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-27T05:48:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789103c1-bb5d-4229-b8fe-c9fe81116709\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-27T05:48:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c2rlb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c2rlb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ha9zeyohpei4-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ha9zeyohpei4-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-27T05:48:23Z"}],"hostIP":"192.168.121.209","podIP":"10.233.64.37","podIPs":[{"ip":"10.233.64.37"}],"startTime":"2022-05-27T05:48:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-27T05:48:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://f3604df80e1fca1e0374c9f66f6143f587b3f4544e4b243b6a8fd30ce7f17fa8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:25.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-968" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":106,"skipped":2229,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:25.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:48:25.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6" in namespace "projected-4251" to be "Succeeded or Failed"
May 27 05:48:25.559: INFO: Pod "downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.73584ms
May 27 05:48:27.580: INFO: Pod "downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02909167s
May 27 05:48:29.595: INFO: Pod "downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044131753s
STEP: Saw pod success
May 27 05:48:29.595: INFO: Pod "downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6" satisfied condition "Succeeded or Failed"
May 27 05:48:29.604: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6 container client-container: <nil>
STEP: delete the pod
May 27 05:48:29.670: INFO: Waiting for pod downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6 to disappear
May 27 05:48:29.679: INFO: Pod downwardapi-volume-b9c78785-bcef-433f-a160-596a19d718c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:29.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4251" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":2239,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:29.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:48:29.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:30.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9111" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":108,"skipped":2241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:30.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
May 27 05:48:30.921: INFO: Waiting up to 5m0s for pod "pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55" in namespace "emptydir-1276" to be "Succeeded or Failed"
May 27 05:48:30.931: INFO: Pod "pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55": Phase="Pending", Reason="", readiness=false. Elapsed: 10.586533ms
May 27 05:48:32.948: INFO: Pod "pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55": Phase="Running", Reason="", readiness=true. Elapsed: 2.026934016s
May 27 05:48:34.977: INFO: Pod "pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55": Phase="Running", Reason="", readiness=false. Elapsed: 4.056114853s
May 27 05:48:36.993: INFO: Pod "pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072401238s
STEP: Saw pod success
May 27 05:48:36.993: INFO: Pod "pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55" satisfied condition "Succeeded or Failed"
May 27 05:48:36.999: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55 container test-container: <nil>
STEP: delete the pod
May 27 05:48:37.050: INFO: Waiting for pod pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55 to disappear
May 27 05:48:37.054: INFO: Pod pod-9d615b6d-f571-4d97-96f2-0c5bc708ac55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:37.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1276" for this suite.

• [SLOW TEST:6.235 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":109,"skipped":2348,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:48:37.120: INFO: Creating deployment "webserver-deployment"
May 27 05:48:37.130: INFO: Waiting for observed generation 1
May 27 05:48:39.218: INFO: Waiting for all required pods to come up
May 27 05:48:39.308: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 27 05:48:43.458: INFO: Waiting for deployment "webserver-deployment" to complete
May 27 05:48:43.470: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 27 05:48:43.495: INFO: Updating deployment webserver-deployment
May 27 05:48:43.495: INFO: Waiting for observed generation 2
May 27 05:48:45.514: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 27 05:48:45.521: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 27 05:48:45.528: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 27 05:48:45.545: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 27 05:48:45.545: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 27 05:48:45.551: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 27 05:48:45.563: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 27 05:48:45.564: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 27 05:48:45.583: INFO: Updating deployment webserver-deployment
May 27 05:48:45.583: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 27 05:48:45.600: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 27 05:48:45.615: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 05:48:45.752: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3303  4a60d34b-747c-45c6-a9be-bbb332927d8e 15622 3 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f01988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-05-27 05:48:43 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-27 05:48:45 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 27 05:48:45.903: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-3303  f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 15610 3 2022-05-27 05:48:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4a60d34b-747c-45c6-a9be-bbb332927d8e 0xc008c8f097 0xc008c8f098}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a60d34b-747c-45c6-a9be-bbb332927d8e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c8f138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 05:48:45.903: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 27 05:48:45.904: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-3303  2230b97f-d9f0-4432-b796-e18166fab336 15607 3 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4a60d34b-747c-45c6-a9be-bbb332927d8e 0xc008c8f197 0xc008c8f198}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a60d34b-747c-45c6-a9be-bbb332927d8e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:48:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008c8f228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 27 05:48:45.983: INFO: Pod "webserver-deployment-566f96c878-6cwzf" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-6cwzf webserver-deployment-566f96c878- deployment-3303  082f5aee-e4cd-4074-be34-4ae84b377827 15584 0 2022-05-27 05:48:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc004f01d77 0xc004f01d78}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvfjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvfjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:,StartTime:2022-05-27 05:48:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.983: INFO: Pod "webserver-deployment-566f96c878-9ctgq" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-9ctgq webserver-deployment-566f96c878- deployment-3303  6f977efb-1bcb-488a-a912-c085cdcdc1c2 15641 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc004f01f77 0xc004f01f78}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.984: INFO: Pod "webserver-deployment-566f96c878-bwfzs" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-bwfzs webserver-deployment-566f96c878- deployment-3303  bcd52db1-db49-4f8f-bfd5-1a0d289ed4ef 15666 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc000f4e6d0 0xc000f4e6d1}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p82dd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p82dd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.984: INFO: Pod "webserver-deployment-566f96c878-c5zm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-c5zm6 webserver-deployment-566f96c878- deployment-3303  047a5596-1c0c-4e27-b371-5958acc35976 15658 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc000f4e990 0xc000f4e991}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7wvwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7wvwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.984: INFO: Pod "webserver-deployment-566f96c878-flpg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-flpg6 webserver-deployment-566f96c878- deployment-3303  9bd7ec2c-08b3-455e-824d-c0f3593b7f31 15667 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc000f4f9d0 0xc000f4f9d1}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2w5n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2w5n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.985: INFO: Pod "webserver-deployment-566f96c878-fx269" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-fx269 webserver-deployment-566f96c878- deployment-3303  c21e82ea-107e-4e26-98d7-4c24685bc74e 15656 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc000f4fb40 0xc000f4fb41}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cp4mc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cp4mc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-05-27 05:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.985: INFO: Pod "webserver-deployment-566f96c878-gzkv8" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-gzkv8 webserver-deployment-566f96c878- deployment-3303  7e696e1c-2226-4e43-9549-983aafefb302 15589 0 2022-05-27 05:48:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc000f4fd37 0xc000f4fd38}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ncvxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ncvxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2022-05-27 05:48:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.985: INFO: Pod "webserver-deployment-566f96c878-kmh4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-kmh4k webserver-deployment-566f96c878- deployment-3303  08e4fb4c-0f5d-47de-8607-b40ecdb164f7 15558 0 2022-05-27 05:48:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc000f4ff77 0xc000f4ff78}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tbx4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tbx4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:,StartTime:2022-05-27 05:48:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.986: INFO: Pod "webserver-deployment-566f96c878-kzjjt" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-kzjjt webserver-deployment-566f96c878- deployment-3303  bc45a530-825b-4012-b51b-1266c1cfd16b 15564 0 2022-05-27 05:48:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc004340177 0xc004340178}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv2cp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv2cp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-05-27 05:48:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.986: INFO: Pod "webserver-deployment-566f96c878-mkpch" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-mkpch webserver-deployment-566f96c878- deployment-3303  bc57b6ea-0f83-4067-baed-e4914365ca52 15665 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc004340387 0xc004340388}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8nllj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8nllj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.987: INFO: Pod "webserver-deployment-566f96c878-rnzm9" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-rnzm9 webserver-deployment-566f96c878- deployment-3303  622cbec1-d720-49f5-bfb9-030db542cdb6 15637 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc004340500 0xc004340501}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pnxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pnxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.987: INFO: Pod "webserver-deployment-566f96c878-wtdpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-wtdpp webserver-deployment-566f96c878- deployment-3303  6e380c47-069e-4427-ab49-944772648da6 15668 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc004340670 0xc004340671}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9dhk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dhk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.988: INFO: Pod "webserver-deployment-566f96c878-z9b6j" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-z9b6j webserver-deployment-566f96c878- deployment-3303  de0331b5-6d71-4163-b933-640c77d4efee 15561 0 2022-05-27 05:48:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 f5c94dc0-128e-4c11-b360-e1cfcb1ca66a 0xc0043407e0 0xc0043407e1}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5c94dc0-128e-4c11-b360-e1cfcb1ca66a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xns8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xns8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2022-05-27 05:48:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.988: INFO: Pod "webserver-deployment-5d9fdcc779-26g9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-26g9b webserver-deployment-5d9fdcc779- deployment-3303  0b515c29-66c7-4adb-9742-a63643d953de 15651 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004340ad7 0xc004340ad8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fcbcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fcbcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.989: INFO: Pod "webserver-deployment-5d9fdcc779-4gzh4" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4gzh4 webserver-deployment-5d9fdcc779- deployment-3303  79170e50-a943-45fb-8375-acb5612056c6 15492 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004340ee0 0xc004340ee1}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w87xh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w87xh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:10.233.66.248,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c9af3dc2e635b83aee35dd6d85752f11b7d82b2f2181562f20aee79d97b39c5a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.989: INFO: Pod "webserver-deployment-5d9fdcc779-4tdtm" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-4tdtm webserver-deployment-5d9fdcc779- deployment-3303  4b16b99f-3b05-46c2-9d96-d956ee66bd08 15644 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc0043410c7 0xc0043410c8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4d9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4d9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.994: INFO: Pod "webserver-deployment-5d9fdcc779-558rn" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-558rn webserver-deployment-5d9fdcc779- deployment-3303  42c49bc3-64c8-4b27-a3e2-7c21869e4c7d 15497 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341230 0xc004341231}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scjrb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scjrb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.73,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://4e3fc4b11014ddec4a0244845ef977a10976d7d7997e25a88b53f749157b9714,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.994: INFO: Pod "webserver-deployment-5d9fdcc779-9bk47" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9bk47 webserver-deployment-5d9fdcc779- deployment-3303  9651ee9a-917c-4794-ad88-db331019879a 15635 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341417 0xc004341418}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z49bb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z49bb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:45.995: INFO: Pod "webserver-deployment-5d9fdcc779-9nhx8" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9nhx8 webserver-deployment-5d9fdcc779- deployment-3303  d399654c-80a6-4f1e-bc9c-663c16c6cb70 15467 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341580 0xc004341581}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qm7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qm7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.164,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3ceaee37d2821dc6249b4213c6306fde7602455cc242a1607786b285f5acd7fe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.000: INFO: Pod "webserver-deployment-5d9fdcc779-9r5zc" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9r5zc webserver-deployment-5d9fdcc779- deployment-3303  6484d8e1-b1e6-41c7-ac86-40eafc9a7fe6 15516 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341767 0xc004341768}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdfnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdfnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.156,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://57b281b8424842b311bd038841cc992c20a79c8d4b42e6110b997dcab02823ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.000: INFO: Pod "webserver-deployment-5d9fdcc779-gn95b" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gn95b webserver-deployment-5d9fdcc779- deployment-3303  9c94052e-1524-4ff7-b7fd-e9effcc177ae 15633 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341957 0xc004341958}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqpd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqpd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-05-27 05:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.000: INFO: Pod "webserver-deployment-5d9fdcc779-gpf5n" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gpf5n webserver-deployment-5d9fdcc779- deployment-3303  7ed54301-b983-4097-aac4-cc75bf9b87a2 15655 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341b27 0xc004341b28}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csm79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csm79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:,StartTime:2022-05-27 05:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.001: INFO: Pod "webserver-deployment-5d9fdcc779-hsdbf" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hsdbf webserver-deployment-5d9fdcc779- deployment-3303  b1d2e449-e90f-485c-9571-f1282df929e2 15517 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341cf7 0xc004341cf8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkxkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkxkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.97,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d0e96f4bac3295aa581b56a1d3cf441c132df6f2d84b2f293e796fb88e99fafe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.001: INFO: Pod "webserver-deployment-5d9fdcc779-jt46h" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-jt46h webserver-deployment-5d9fdcc779- deployment-3303  a7774c78-2952-4e7a-8c19-bbf87aaf893d 15661 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc004341ef7 0xc004341ef8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mp8dt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mp8dt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.001: INFO: Pod "webserver-deployment-5d9fdcc779-k57z8" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-k57z8 webserver-deployment-5d9fdcc779- deployment-3303  fe3a4f1c-e4a5-48b8-a33b-12129bcb9b48 15634 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc003392060 0xc003392061}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bkfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bkfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.001: INFO: Pod "webserver-deployment-5d9fdcc779-kvp2k" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-kvp2k webserver-deployment-5d9fdcc779- deployment-3303  bb375c26-081a-45f0-a4c5-7d9c80636ba1 15487 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc0033921c0 0xc0033921c1}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gx4d7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gx4d7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:10.233.66.120,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8b42ccde3287a6a12ca840933dd80a73e0766ccc78b113e80df62f57cadfd419,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.002: INFO: Pod "webserver-deployment-5d9fdcc779-l8s9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-l8s9q webserver-deployment-5d9fdcc779- deployment-3303  c688c805-faf7-47a1-87d3-21159632e619 15654 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc0033923a7 0xc0033923a8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t99xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t99xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.002: INFO: Pod "webserver-deployment-5d9fdcc779-n4rfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-n4rfp webserver-deployment-5d9fdcc779- deployment-3303  d49f4830-e19e-462b-a952-75cf71093012 15629 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc003392510 0xc003392511}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbwn6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbwn6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:,StartTime:2022-05-27 05:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.003: INFO: Pod "webserver-deployment-5d9fdcc779-n6jps" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-n6jps webserver-deployment-5d9fdcc779- deployment-3303  23d5a4cf-8313-4747-9159-bca2fb5a7143 15464 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc0033926d7 0xc0033926d8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sw6bb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sw6bb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.106,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://09e9be8506d933604b79da3654d4c233f791b64d4f788e91b53d6bd72e690d18,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.003: INFO: Pod "webserver-deployment-5d9fdcc779-nqpcz" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-nqpcz webserver-deployment-5d9fdcc779- deployment-3303  0f8c35d3-11d5-47d8-ba7f-1601d1970b6a 15663 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc0033928e7 0xc0033928e8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnlfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnlfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.003: INFO: Pod "webserver-deployment-5d9fdcc779-q4mnp" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-q4mnp webserver-deployment-5d9fdcc779- deployment-3303  0bc79a43-2cab-4709-bec5-27273104a61e 15519 0 2022-05-27 05:48:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc003392a50 0xc003392a51}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqgvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqgvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.43,PodIP:10.233.65.203,StartTime:2022-05-27 05:48:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d80da911e1f7332b57215b28d22c4a2a470112a6e9672d89f4b3563eb6ca1876,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.003: INFO: Pod "webserver-deployment-5d9fdcc779-rgcrj" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rgcrj webserver-deployment-5d9fdcc779- deployment-3303  c3987902-af76-44f6-9bbf-6757c7ea058c 15670 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc003392c37 0xc003392c38}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfq9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfq9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:46.004: INFO: Pod "webserver-deployment-5d9fdcc779-rtwv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rtwv7 webserver-deployment-5d9fdcc779- deployment-3303  eff36515-8a79-40ba-8452-e544ce0c6ae1 15645 0 2022-05-27 05:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 2230b97f-d9f0-4432-b796-e18166fab336 0xc003392da0 0xc003392da1}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2230b97f-d9f0-4432-b796-e18166fab336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwq7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwq7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:,StartTime:2022-05-27 05:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:46.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3303" for this suite.

• [SLOW TEST:9.022 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":110,"skipped":2362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:46.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:48:46.384: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171" in namespace "security-context-test-9540" to be "Succeeded or Failed"
May 27 05:48:46.467: INFO: Pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171": Phase="Pending", Reason="", readiness=false. Elapsed: 82.325443ms
May 27 05:48:48.518: INFO: Pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134060014s
May 27 05:48:50.531: INFO: Pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14677347s
May 27 05:48:52.717: INFO: Pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171": Phase="Pending", Reason="", readiness=false. Elapsed: 6.332772536s
May 27 05:48:54.735: INFO: Pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.350270358s
May 27 05:48:54.735: INFO: Pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171" satisfied condition "Succeeded or Failed"
May 27 05:48:54.748: INFO: Got logs for pod "busybox-privileged-false-e84c3173-d55a-4103-9333-3c72fe152171": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:54.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9540" for this suite.

• [SLOW TEST:8.652 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:232
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2408,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:48:54.817: INFO: Creating simple deployment test-new-deployment
May 27 05:48:54.850: INFO: deployment "test-new-deployment" doesn't have the required revision set
May 27 05:48:56.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 5, 48, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 48, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 5, 48, 54, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 5, 48, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 05:48:58.972: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2737  a28d0753-ccbc-4e06-983c-480f81f6322d 15973 3 2022-05-27 05:48:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-05-27 05:48:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:48:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000907988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 05:48:57 +0000 UTC,LastTransitionTime:2022-05-27 05:48:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-05-27 05:48:57 +0000 UTC,LastTransitionTime:2022-05-27 05:48:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 05:48:59.024: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-2737  5d7ced46-7040-4653-83d3-af005b14d4d0 15979 2 2022-05-27 05:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a28d0753-ccbc-4e06-983c-480f81f6322d 0xc000b29827 0xc000b29828}] []  [{kube-controller-manager Update apps/v1 2022-05-27 05:48:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a28d0753-ccbc-4e06-983c-480f81f6322d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 05:48:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000b29e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 05:48:59.043: INFO: Pod "test-new-deployment-5d9fdcc779-5v2h9" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-5v2h9 test-new-deployment-5d9fdcc779- deployment-2737  47c22a08-e1a4-47cc-aa5a-e346801bfbdb 15965 0 2022-05-27 05:48:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 5d7ced46-7040-4653-83d3-af005b14d4d0 0xc0037382c7 0xc0037382c8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d7ced46-7040-4653-83d3-af005b14d4d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6kgr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6kgr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:10.233.66.79,StartTime:2022-05-27 05:48:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 05:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8be935bdd2e8c78f7c458742804c1591af02ff9eeb051d82a4d2781b5a5ead36,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 05:48:59.044: INFO: Pod "test-new-deployment-5d9fdcc779-flkdr" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-flkdr test-new-deployment-5d9fdcc779- deployment-2737  ec20a56d-a691-4ba8-b7cd-f1062d3d9b66 15981 0 2022-05-27 05:48:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 5d7ced46-7040-4653-83d3-af005b14d4d0 0xc0037384b7 0xc0037384b8}] []  [{kube-controller-manager Update v1 2022-05-27 05:48:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d7ced46-7040-4653-83d3-af005b14d4d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 05:48:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktzsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktzsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 05:48:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-05-27 05:48:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:48:59.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2737" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":112,"skipped":2429,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:48:59.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 05:48:59.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:48:59.389: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:49:00.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:49:00.440: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:49:01.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 05:49:01.412: INFO: Node ha9zeyohpei4-2 is running 0 daemon pod, expected 1
May 27 05:49:02.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 05:49:02.468: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 27 05:49:02.592: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 05:49:02.593: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:49:03.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 05:49:03.612: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:49:04.625: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 05:49:04.625: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:49:05.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 05:49:05.621: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:49:06.618: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 05:49:06.618: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3558, will wait for the garbage collector to delete the pods
May 27 05:49:06.699: INFO: Deleting DaemonSet.extensions daemon-set took: 15.690999ms
May 27 05:49:06.800: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.397811ms
May 27 05:49:09.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:49:09.413: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 05:49:09.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16138"},"items":null}

May 27 05:49:09.426: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16138"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:49:09.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3558" for this suite.

• [SLOW TEST:10.330 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":113,"skipped":2430,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:49:09.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 27 05:49:09.572: INFO: Waiting up to 5m0s for pod "pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f" in namespace "emptydir-4007" to be "Succeeded or Failed"
May 27 05:49:09.588: INFO: Pod "pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.933952ms
May 27 05:49:11.602: INFO: Pod "pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.030470871s
May 27 05:49:13.617: INFO: Pod "pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f": Phase="Running", Reason="", readiness=false. Elapsed: 4.045426587s
May 27 05:49:15.628: INFO: Pod "pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056205491s
STEP: Saw pod success
May 27 05:49:15.628: INFO: Pod "pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f" satisfied condition "Succeeded or Failed"
May 27 05:49:15.640: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f container test-container: <nil>
STEP: delete the pod
May 27 05:49:15.680: INFO: Waiting for pod pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f to disappear
May 27 05:49:15.686: INFO: Pod pod-2645de83-0ec4-4152-a0d4-c4f214b9db4f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:49:15.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4007" for this suite.

• [SLOW TEST:6.234 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2441,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:49:15.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 27 05:49:17.057: INFO: The status of Pod kube-controller-manager-ha9zeyohpei4-2 is Running (Ready = true)
May 27 05:49:17.171: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:49:17.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3442" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":115,"skipped":2446,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:49:17.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:49:17.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1377" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":116,"skipped":2451,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:49:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-87a2622e-4386-45ed-a287-c5240d354922
STEP: Creating configMap with name cm-test-opt-upd-5ce6dadb-9e42-47d1-8630-d83cf68eabd6
STEP: Creating the pod
May 27 05:49:17.473: INFO: The status of Pod pod-configmaps-a92ca682-c73f-44f3-8a82-b1590348dcfe is Pending, waiting for it to be Running (with Ready = true)
May 27 05:49:19.489: INFO: The status of Pod pod-configmaps-a92ca682-c73f-44f3-8a82-b1590348dcfe is Pending, waiting for it to be Running (with Ready = true)
May 27 05:49:21.485: INFO: The status of Pod pod-configmaps-a92ca682-c73f-44f3-8a82-b1590348dcfe is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-87a2622e-4386-45ed-a287-c5240d354922
STEP: Updating configmap cm-test-opt-upd-5ce6dadb-9e42-47d1-8630-d83cf68eabd6
STEP: Creating configMap with name cm-test-opt-create-24239f8a-4398-4129-a66d-e3bb53b51cd8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:50:50.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4162" for this suite.

• [SLOW TEST:93.027 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:50:50.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
May 27 05:50:50.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:19.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8146" for this suite.

• [SLOW TEST:28.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":118,"skipped":2490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
May 27 05:51:19.128: INFO: Waiting up to 5m0s for pod "var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b" in namespace "var-expansion-2547" to be "Succeeded or Failed"
May 27 05:51:19.133: INFO: Pod "var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78671ms
May 27 05:51:21.153: INFO: Pod "var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024398882s
May 27 05:51:23.168: INFO: Pod "var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039116121s
May 27 05:51:25.180: INFO: Pod "var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051589324s
STEP: Saw pod success
May 27 05:51:25.180: INFO: Pod "var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b" satisfied condition "Succeeded or Failed"
May 27 05:51:25.186: INFO: Trying to get logs from node ha9zeyohpei4-3 pod var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b container dapi-container: <nil>
STEP: delete the pod
May 27 05:51:25.232: INFO: Waiting for pod var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b to disappear
May 27 05:51:25.247: INFO: Pod var-expansion-b30299f0-ae4a-4592-8922-64bff0de5b3b no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:25.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2547" for this suite.

• [SLOW TEST:6.219 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":119,"skipped":2527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:25.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:25.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1460" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":120,"skipped":2566,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:25.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
May 27 05:51:25.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May 27 05:51:25.805: INFO: stderr: ""
May 27 05:51:25.805: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
May 27 05:51:25.805: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 27 05:51:25.805: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5231" to be "running and ready, or succeeded"
May 27 05:51:25.824: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.228291ms
May 27 05:51:27.833: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.028384204s
May 27 05:51:27.833: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 27 05:51:27.833: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 27 05:51:27.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 logs logs-generator logs-generator'
May 27 05:51:28.010: INFO: stderr: ""
May 27 05:51:28.010: INFO: stdout: "I0527 05:51:27.017426       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/sbss 239\nI0527 05:51:27.217433       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/k2w 341\nI0527 05:51:27.417941       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rbz 411\nI0527 05:51:27.617465       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/vrp 269\nI0527 05:51:27.817852       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/qwc 291\n"
STEP: limiting log lines
May 27 05:51:28.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 logs logs-generator logs-generator --tail=1'
May 27 05:51:28.191: INFO: stderr: ""
May 27 05:51:28.191: INFO: stdout: "I0527 05:51:28.017069       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wg8l 412\n"
May 27 05:51:28.191: INFO: got output "I0527 05:51:28.017069       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wg8l 412\n"
STEP: limiting log bytes
May 27 05:51:28.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 logs logs-generator logs-generator --limit-bytes=1'
May 27 05:51:28.313: INFO: stderr: ""
May 27 05:51:28.313: INFO: stdout: "I"
May 27 05:51:28.313: INFO: got output "I"
STEP: exposing timestamps
May 27 05:51:28.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 logs logs-generator logs-generator --tail=1 --timestamps'
May 27 05:51:28.463: INFO: stderr: ""
May 27 05:51:28.464: INFO: stdout: "2022-05-27T05:51:28.417553847Z I0527 05:51:28.417466       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/clc 582\n"
May 27 05:51:28.464: INFO: got output "2022-05-27T05:51:28.417553847Z I0527 05:51:28.417466       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/clc 582\n"
STEP: restricting to a time range
May 27 05:51:30.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 logs logs-generator logs-generator --since=1s'
May 27 05:51:31.121: INFO: stderr: ""
May 27 05:51:31.121: INFO: stdout: "I0527 05:51:30.217370       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/xkgp 414\nI0527 05:51:30.417864       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/7cjd 284\nI0527 05:51:30.617170       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/j6nn 236\nI0527 05:51:30.817498       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/8mw 519\nI0527 05:51:31.017187       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/xn4s 290\n"
May 27 05:51:31.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 logs logs-generator logs-generator --since=24h'
May 27 05:51:31.267: INFO: stderr: ""
May 27 05:51:31.267: INFO: stdout: "I0527 05:51:27.017426       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/sbss 239\nI0527 05:51:27.217433       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/k2w 341\nI0527 05:51:27.417941       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rbz 411\nI0527 05:51:27.617465       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/vrp 269\nI0527 05:51:27.817852       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/qwc 291\nI0527 05:51:28.017069       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wg8l 412\nI0527 05:51:28.220497       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/dcrf 271\nI0527 05:51:28.417466       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/clc 582\nI0527 05:51:28.617787       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/fg7 308\nI0527 05:51:28.817535       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/pfpq 555\nI0527 05:51:29.017997       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/rrwp 514\nI0527 05:51:29.217438       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/dfk 396\nI0527 05:51:29.417897       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/qlm 468\nI0527 05:51:29.617214       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/khlc 391\nI0527 05:51:29.817587       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/qpdh 467\nI0527 05:51:30.018041       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/4nf 382\nI0527 05:51:30.217370       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/xkgp 414\nI0527 05:51:30.417864       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/7cjd 284\nI0527 05:51:30.617170       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/j6nn 236\nI0527 05:51:30.817498       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/8mw 519\nI0527 05:51:31.017187       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/xn4s 290\nI0527 05:51:31.218992       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/c2n 212\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
May 27 05:51:31.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5231 delete pod logs-generator'
May 27 05:51:32.433: INFO: stderr: ""
May 27 05:51:32.433: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:32.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5231" for this suite.

• [SLOW TEST:7.039 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":121,"skipped":2567,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:32.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 05:51:32.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:32.621: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:33.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:33.648: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:34.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 05:51:34.644: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:35.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 05:51:35.651: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 27 05:51:35.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 05:51:35.719: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2561, will wait for the garbage collector to delete the pods
May 27 05:51:35.817: INFO: Deleting DaemonSet.extensions daemon-set took: 17.010497ms
May 27 05:51:35.917: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.524906ms
May 27 05:51:38.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:38.527: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 05:51:38.534: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16815"},"items":null}

May 27 05:51:38.540: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16815"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2561" for this suite.

• [SLOW TEST:6.150 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":122,"skipped":2586,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:38.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-cbd2d4ea-b31c-47c6-8fba-eaec8cde66ab
STEP: Creating a pod to test consume secrets
May 27 05:51:38.755: INFO: Waiting up to 5m0s for pod "pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320" in namespace "secrets-9139" to be "Succeeded or Failed"
May 27 05:51:38.765: INFO: Pod "pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465145ms
May 27 05:51:40.781: INFO: Pod "pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025993417s
May 27 05:51:42.796: INFO: Pod "pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040781494s
STEP: Saw pod success
May 27 05:51:42.796: INFO: Pod "pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320" satisfied condition "Succeeded or Failed"
May 27 05:51:42.811: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320 container secret-volume-test: <nil>
STEP: delete the pod
May 27 05:51:42.855: INFO: Waiting for pod pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320 to disappear
May 27 05:51:42.861: INFO: Pod pod-secrets-e9e35224-706b-4bc9-a322-7aacd402d320 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:42.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9139" for this suite.
STEP: Destroying namespace "secret-namespace-2545" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2597,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:42.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:51:42.987: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 27 05:51:43.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:43.001: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
May 27 05:51:43.050: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:43.050: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:44.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:44.063: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:45.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 05:51:45.065: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 27 05:51:45.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 05:51:45.119: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May 27 05:51:46.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:46.142: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 27 05:51:46.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:46.198: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:47.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:47.226: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:48.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:48.206: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:49.246: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:49.246: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 05:51:50.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 05:51:50.232: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5033, will wait for the garbage collector to delete the pods
May 27 05:51:50.321: INFO: Deleting DaemonSet.extensions daemon-set took: 13.369537ms
May 27 05:51:50.422: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.972069ms
May 27 05:51:52.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 05:51:52.937: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 05:51:52.944: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16983"},"items":null}

May 27 05:51:52.952: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16983"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:53.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5033" for this suite.

• [SLOW TEST:10.129 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":124,"skipped":2610,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:53.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 27 05:51:53.115: INFO: Waiting up to 5m0s for pod "security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba" in namespace "security-context-4801" to be "Succeeded or Failed"
May 27 05:51:53.121: INFO: Pod "security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.832833ms
May 27 05:51:55.135: INFO: Pod "security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019392197s
May 27 05:51:57.166: INFO: Pod "security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050499849s
STEP: Saw pod success
May 27 05:51:57.166: INFO: Pod "security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba" satisfied condition "Succeeded or Failed"
May 27 05:51:57.179: INFO: Trying to get logs from node ha9zeyohpei4-3 pod security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba container test-container: <nil>
STEP: delete the pod
May 27 05:51:57.228: INFO: Waiting for pod security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba to disappear
May 27 05:51:57.235: INFO: Pod security-context-a5419e23-eb0e-425e-b77a-33f3568e85ba no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:51:57.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4801" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":125,"skipped":2618,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:51:57.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:56:57.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3924" for this suite.

• [SLOW TEST:300.134 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":126,"skipped":2622,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:56:57.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 27 05:56:57.476: INFO: Waiting up to 5m0s for pod "pod-3d40866b-1ce2-47a0-859e-7e71801fc228" in namespace "emptydir-4905" to be "Succeeded or Failed"
May 27 05:56:57.486: INFO: Pod "pod-3d40866b-1ce2-47a0-859e-7e71801fc228": Phase="Pending", Reason="", readiness=false. Elapsed: 9.811302ms
May 27 05:56:59.504: INFO: Pod "pod-3d40866b-1ce2-47a0-859e-7e71801fc228": Phase="Running", Reason="", readiness=true. Elapsed: 2.028184327s
May 27 05:57:01.515: INFO: Pod "pod-3d40866b-1ce2-47a0-859e-7e71801fc228": Phase="Running", Reason="", readiness=false. Elapsed: 4.039551121s
May 27 05:57:03.533: INFO: Pod "pod-3d40866b-1ce2-47a0-859e-7e71801fc228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056898803s
STEP: Saw pod success
May 27 05:57:03.533: INFO: Pod "pod-3d40866b-1ce2-47a0-859e-7e71801fc228" satisfied condition "Succeeded or Failed"
May 27 05:57:03.541: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-3d40866b-1ce2-47a0-859e-7e71801fc228 container test-container: <nil>
STEP: delete the pod
May 27 05:57:03.609: INFO: Waiting for pod pod-3d40866b-1ce2-47a0-859e-7e71801fc228 to disappear
May 27 05:57:03.616: INFO: Pod pod-3d40866b-1ce2-47a0-859e-7e71801fc228 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:03.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4905" for this suite.

• [SLOW TEST:6.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2652,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:03.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:03.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8608" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":128,"skipped":2657,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:03.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:57:04.468: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:57:07.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 27 05:57:07.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:07.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8444" for this suite.
STEP: Destroying namespace "webhook-8444-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":129,"skipped":2677,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:07.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 05:57:12.001: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:12.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4890" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2683,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:12.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
May 27 05:57:12.146: INFO: The status of Pod pod-hostip-c2ce8285-c6dc-4a75-91b9-d26e8a4f5654 is Pending, waiting for it to be Running (with Ready = true)
May 27 05:57:14.161: INFO: The status of Pod pod-hostip-c2ce8285-c6dc-4a75-91b9-d26e8a4f5654 is Running (Ready = true)
May 27 05:57:14.180: INFO: Pod pod-hostip-c2ce8285-c6dc-4a75-91b9-d26e8a4f5654 has hostIP: 192.168.121.191
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:14.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7043" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":131,"skipped":2689,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:14.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:57:14.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 27 05:57:18.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 create -f -'
May 27 05:57:20.183: INFO: stderr: ""
May 27 05:57:20.183: INFO: stdout: "e2e-test-crd-publish-openapi-7704-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 27 05:57:20.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 delete e2e-test-crd-publish-openapi-7704-crds test-foo'
May 27 05:57:20.454: INFO: stderr: ""
May 27 05:57:20.454: INFO: stdout: "e2e-test-crd-publish-openapi-7704-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 27 05:57:20.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 apply -f -'
May 27 05:57:20.772: INFO: stderr: ""
May 27 05:57:20.772: INFO: stdout: "e2e-test-crd-publish-openapi-7704-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 27 05:57:20.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 delete e2e-test-crd-publish-openapi-7704-crds test-foo'
May 27 05:57:20.960: INFO: stderr: ""
May 27 05:57:20.961: INFO: stdout: "e2e-test-crd-publish-openapi-7704-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
May 27 05:57:20.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 create -f -'
May 27 05:57:21.928: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 27 05:57:21.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 create -f -'
May 27 05:57:22.203: INFO: rc: 1
May 27 05:57:22.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 apply -f -'
May 27 05:57:22.452: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 27 05:57:22.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 create -f -'
May 27 05:57:22.694: INFO: rc: 1
May 27 05:57:22.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 --namespace=crd-publish-openapi-1308 apply -f -'
May 27 05:57:22.980: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 27 05:57:22.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 explain e2e-test-crd-publish-openapi-7704-crds'
May 27 05:57:23.294: INFO: stderr: ""
May 27 05:57:23.294: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7704-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 27 05:57:23.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 explain e2e-test-crd-publish-openapi-7704-crds.metadata'
May 27 05:57:23.575: INFO: stderr: ""
May 27 05:57:23.575: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7704-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 27 05:57:23.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 explain e2e-test-crd-publish-openapi-7704-crds.spec'
May 27 05:57:23.871: INFO: stderr: ""
May 27 05:57:23.872: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7704-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 27 05:57:23.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 explain e2e-test-crd-publish-openapi-7704-crds.spec.bars'
May 27 05:57:24.156: INFO: stderr: ""
May 27 05:57:24.156: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7704-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 27 05:57:24.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-1308 explain e2e-test-crd-publish-openapi-7704-crds.spec.bars2'
May 27 05:57:24.429: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:30.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1308" for this suite.

• [SLOW TEST:16.066 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":132,"skipped":2707,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:30.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 05:57:30.368: INFO: Waiting up to 5m0s for pod "downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e" in namespace "downward-api-9421" to be "Succeeded or Failed"
May 27 05:57:30.373: INFO: Pod "downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.412432ms
May 27 05:57:32.383: INFO: Pod "downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015775503s
May 27 05:57:34.402: INFO: Pod "downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034125679s
STEP: Saw pod success
May 27 05:57:34.403: INFO: Pod "downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e" satisfied condition "Succeeded or Failed"
May 27 05:57:34.418: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e container dapi-container: <nil>
STEP: delete the pod
May 27 05:57:34.462: INFO: Waiting for pod downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e to disappear
May 27 05:57:34.470: INFO: Pod downward-api-08342f46-8ded-45e4-bc8a-f1a3145bca3e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:34.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9421" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2723,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:34.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:34.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8574" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":134,"skipped":2730,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:34.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 05:57:34.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-7301 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 27 05:57:34.773: INFO: stderr: ""
May 27 05:57:34.773: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 27 05:57:34.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-7301 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May 27 05:57:36.153: INFO: stderr: ""
May 27 05:57:36.153: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 05:57:36.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-7301 delete pods e2e-test-httpd-pod'
May 27 05:57:38.980: INFO: stderr: ""
May 27 05:57:38.980: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:38.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7301" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":135,"skipped":2745,"failed":0}
SSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:39.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:57:39.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: creating replication controller svc-latency-rc in namespace svc-latency-143
I0527 05:57:39.091212      14 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-143, replica count: 1
I0527 05:57:40.142282      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0527 05:57:41.143533      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 05:57:41.269: INFO: Created: latency-svc-cf8xr
May 27 05:57:41.283: INFO: Got endpoints: latency-svc-cf8xr [38.022157ms]
May 27 05:57:41.319: INFO: Created: latency-svc-x22f5
May 27 05:57:41.333: INFO: Created: latency-svc-xk44q
May 27 05:57:41.355: INFO: Created: latency-svc-srlzl
May 27 05:57:41.358: INFO: Got endpoints: latency-svc-x22f5 [72.698055ms]
May 27 05:57:41.359: INFO: Got endpoints: latency-svc-xk44q [72.421193ms]
May 27 05:57:41.366: INFO: Created: latency-svc-rscrf
May 27 05:57:41.381: INFO: Created: latency-svc-k4cd2
May 27 05:57:41.396: INFO: Got endpoints: latency-svc-srlzl [107.186078ms]
May 27 05:57:41.397: INFO: Got endpoints: latency-svc-rscrf [113.42501ms]
May 27 05:57:41.405: INFO: Got endpoints: latency-svc-k4cd2 [115.678155ms]
May 27 05:57:41.416: INFO: Created: latency-svc-ntzmw
May 27 05:57:41.422: INFO: Created: latency-svc-tz4z2
May 27 05:57:41.437: INFO: Got endpoints: latency-svc-ntzmw [145.437564ms]
May 27 05:57:41.445: INFO: Created: latency-svc-kkhrv
May 27 05:57:41.451: INFO: Got endpoints: latency-svc-tz4z2 [161.627629ms]
May 27 05:57:41.472: INFO: Got endpoints: latency-svc-kkhrv [181.534561ms]
May 27 05:57:41.478: INFO: Created: latency-svc-6d65r
May 27 05:57:41.483: INFO: Got endpoints: latency-svc-6d65r [192.594197ms]
May 27 05:57:41.487: INFO: Created: latency-svc-b7d5v
May 27 05:57:41.502: INFO: Created: latency-svc-xczcp
May 27 05:57:41.509: INFO: Got endpoints: latency-svc-b7d5v [218.720311ms]
May 27 05:57:41.523: INFO: Created: latency-svc-bxpbx
May 27 05:57:41.527: INFO: Got endpoints: latency-svc-xczcp [235.798775ms]
May 27 05:57:41.538: INFO: Created: latency-svc-znbb9
May 27 05:57:41.562: INFO: Got endpoints: latency-svc-znbb9 [271.039596ms]
May 27 05:57:41.564: INFO: Created: latency-svc-jcjqv
May 27 05:57:41.570: INFO: Created: latency-svc-57dch
May 27 05:57:41.575: INFO: Got endpoints: latency-svc-bxpbx [284.307922ms]
May 27 05:57:41.579: INFO: Created: latency-svc-svjth
May 27 05:57:41.592: INFO: Created: latency-svc-ffcrt
May 27 05:57:41.594: INFO: Got endpoints: latency-svc-57dch [235.465337ms]
May 27 05:57:41.603: INFO: Created: latency-svc-zz6qm
May 27 05:57:41.612: INFO: Got endpoints: latency-svc-svjth [253.621509ms]
May 27 05:57:41.612: INFO: Got endpoints: latency-svc-jcjqv [323.430429ms]
May 27 05:57:41.621: INFO: Created: latency-svc-42kfp
May 27 05:57:41.629: INFO: Got endpoints: latency-svc-zz6qm [231.694374ms]
May 27 05:57:41.639: INFO: Got endpoints: latency-svc-ffcrt [242.423093ms]
May 27 05:57:41.639: INFO: Created: latency-svc-cwdf9
May 27 05:57:41.646: INFO: Created: latency-svc-827vr
May 27 05:57:41.650: INFO: Created: latency-svc-glj56
May 27 05:57:41.670: INFO: Created: latency-svc-2sg9j
May 27 05:57:41.677: INFO: Got endpoints: latency-svc-cwdf9 [239.782771ms]
May 27 05:57:41.677: INFO: Got endpoints: latency-svc-42kfp [271.443108ms]
May 27 05:57:41.685: INFO: Created: latency-svc-phws9
May 27 05:57:41.698: INFO: Got endpoints: latency-svc-827vr [407.680847ms]
May 27 05:57:41.699: INFO: Got endpoints: latency-svc-glj56 [246.960987ms]
May 27 05:57:41.710: INFO: Got endpoints: latency-svc-2sg9j [238.777702ms]
May 27 05:57:41.714: INFO: Created: latency-svc-qrz7g
May 27 05:57:41.725: INFO: Got endpoints: latency-svc-qrz7g [215.849026ms]
May 27 05:57:41.743: INFO: Got endpoints: latency-svc-phws9 [260.241262ms]
May 27 05:57:41.759: INFO: Created: latency-svc-tq5gh
May 27 05:57:41.772: INFO: Created: latency-svc-mf6fj
May 27 05:57:41.792: INFO: Got endpoints: latency-svc-tq5gh [265.146207ms]
May 27 05:57:41.792: INFO: Got endpoints: latency-svc-mf6fj [229.676059ms]
May 27 05:57:41.794: INFO: Created: latency-svc-n7sfk
May 27 05:57:41.808: INFO: Got endpoints: latency-svc-n7sfk [232.473078ms]
May 27 05:57:41.811: INFO: Created: latency-svc-8tctw
May 27 05:57:41.826: INFO: Got endpoints: latency-svc-8tctw [231.516948ms]
May 27 05:57:41.834: INFO: Created: latency-svc-5r5hq
May 27 05:57:41.850: INFO: Got endpoints: latency-svc-5r5hq [237.231528ms]
May 27 05:57:41.865: INFO: Created: latency-svc-w4z52
May 27 05:57:41.882: INFO: Got endpoints: latency-svc-w4z52 [270.725068ms]
May 27 05:57:41.910: INFO: Created: latency-svc-gvz5f
May 27 05:57:41.922: INFO: Got endpoints: latency-svc-gvz5f [293.429987ms]
May 27 05:57:41.923: INFO: Created: latency-svc-24ms6
May 27 05:57:41.934: INFO: Got endpoints: latency-svc-24ms6 [295.179047ms]
May 27 05:57:41.945: INFO: Created: latency-svc-gmzlz
May 27 05:57:41.950: INFO: Got endpoints: latency-svc-gmzlz [273.235946ms]
May 27 05:57:41.969: INFO: Created: latency-svc-pwlvg
May 27 05:57:41.985: INFO: Got endpoints: latency-svc-pwlvg [307.762775ms]
May 27 05:57:42.000: INFO: Created: latency-svc-hv8hk
May 27 05:57:42.015: INFO: Got endpoints: latency-svc-hv8hk [316.82365ms]
May 27 05:57:42.184: INFO: Created: latency-svc-c8gd2
May 27 05:57:42.185: INFO: Created: latency-svc-9jqt5
May 27 05:57:42.194: INFO: Created: latency-svc-24hjt
May 27 05:57:42.194: INFO: Created: latency-svc-pfwrh
May 27 05:57:42.195: INFO: Created: latency-svc-jd45l
May 27 05:57:42.196: INFO: Created: latency-svc-xvkjq
May 27 05:57:42.196: INFO: Created: latency-svc-xkks2
May 27 05:57:42.208: INFO: Created: latency-svc-r54c9
May 27 05:57:42.208: INFO: Created: latency-svc-n8wxl
May 27 05:57:42.210: INFO: Created: latency-svc-4kbd7
May 27 05:57:42.222: INFO: Got endpoints: latency-svc-9jqt5 [395.295005ms]
May 27 05:57:42.223: INFO: Got endpoints: latency-svc-c8gd2 [238.005045ms]
May 27 05:57:42.225: INFO: Got endpoints: latency-svc-jd45l [499.606674ms]
May 27 05:57:42.225: INFO: Got endpoints: latency-svc-24hjt [415.722376ms]
May 27 05:57:42.237: INFO: Created: latency-svc-29w7z
May 27 05:57:42.250: INFO: Created: latency-svc-qm5h8
May 27 05:57:42.250: INFO: Created: latency-svc-m2tzn
May 27 05:57:42.254: INFO: Created: latency-svc-sng4t
May 27 05:57:42.262: INFO: Created: latency-svc-fctjf
May 27 05:57:42.266: INFO: Got endpoints: latency-svc-qm5h8 [521.612407ms]
May 27 05:57:42.266: INFO: Got endpoints: latency-svc-29w7z [555.204438ms]
May 27 05:57:42.266: INFO: Got endpoints: latency-svc-xvkjq [473.872516ms]
May 27 05:57:42.267: INFO: Got endpoints: latency-svc-sng4t [417.714515ms]
May 27 05:57:42.271: INFO: Got endpoints: latency-svc-m2tzn [348.457769ms]
May 27 05:57:42.271: INFO: Got endpoints: latency-svc-pfwrh [572.614588ms]
May 27 05:57:42.271: INFO: Got endpoints: latency-svc-xkks2 [336.999321ms]
May 27 05:57:42.283: INFO: Got endpoints: latency-svc-fctjf [268.501971ms]
May 27 05:57:42.302: INFO: Created: latency-svc-z4rnq
May 27 05:57:42.308: INFO: Got endpoints: latency-svc-4kbd7 [516.501969ms]
May 27 05:57:42.318: INFO: Created: latency-svc-xb4xz
May 27 05:57:42.331: INFO: Created: latency-svc-zcxn6
May 27 05:57:42.345: INFO: Created: latency-svc-hg6l9
May 27 05:57:42.355: INFO: Got endpoints: latency-svc-n8wxl [404.803662ms]
May 27 05:57:42.364: INFO: Created: latency-svc-22pw6
May 27 05:57:42.375: INFO: Created: latency-svc-7p7lh
May 27 05:57:42.390: INFO: Created: latency-svc-65dbq
May 27 05:57:42.404: INFO: Got endpoints: latency-svc-r54c9 [521.657048ms]
May 27 05:57:42.413: INFO: Created: latency-svc-xznnh
May 27 05:57:42.432: INFO: Created: latency-svc-77wqb
May 27 05:57:42.437: INFO: Created: latency-svc-sgfjv
May 27 05:57:42.448: INFO: Created: latency-svc-bjw5h
May 27 05:57:42.460: INFO: Got endpoints: latency-svc-z4rnq [237.850607ms]
May 27 05:57:42.469: INFO: Created: latency-svc-kx5gn
May 27 05:57:42.474: INFO: Created: latency-svc-ljmrw
May 27 05:57:42.487: INFO: Created: latency-svc-467vn
May 27 05:57:42.500: INFO: Created: latency-svc-tv9dc
May 27 05:57:42.500: INFO: Got endpoints: latency-svc-xb4xz [277.136932ms]
May 27 05:57:42.516: INFO: Created: latency-svc-sgnhv
May 27 05:57:42.535: INFO: Created: latency-svc-wxzvc
May 27 05:57:42.548: INFO: Got endpoints: latency-svc-zcxn6 [322.832234ms]
May 27 05:57:42.580: INFO: Created: latency-svc-pnh8r
May 27 05:57:42.596: INFO: Got endpoints: latency-svc-hg6l9 [371.222774ms]
May 27 05:57:42.623: INFO: Created: latency-svc-pnbjd
May 27 05:57:42.648: INFO: Got endpoints: latency-svc-22pw6 [382.103159ms]
May 27 05:57:42.667: INFO: Created: latency-svc-p5mzc
May 27 05:57:42.702: INFO: Got endpoints: latency-svc-7p7lh [430.894825ms]
May 27 05:57:42.722: INFO: Created: latency-svc-whzxn
May 27 05:57:42.746: INFO: Got endpoints: latency-svc-65dbq [480.052287ms]
May 27 05:57:42.765: INFO: Created: latency-svc-9wwdl
May 27 05:57:42.799: INFO: Got endpoints: latency-svc-xznnh [532.891543ms]
May 27 05:57:42.816: INFO: Created: latency-svc-bffd6
May 27 05:57:42.850: INFO: Got endpoints: latency-svc-77wqb [579.05755ms]
May 27 05:57:42.873: INFO: Created: latency-svc-6dwvz
May 27 05:57:42.898: INFO: Got endpoints: latency-svc-sgfjv [626.491339ms]
May 27 05:57:42.916: INFO: Created: latency-svc-qs4kn
May 27 05:57:42.961: INFO: Got endpoints: latency-svc-bjw5h [692.872531ms]
May 27 05:57:42.978: INFO: Created: latency-svc-sqbkg
May 27 05:57:42.998: INFO: Got endpoints: latency-svc-kx5gn [714.450352ms]
May 27 05:57:43.021: INFO: Created: latency-svc-bqv9f
May 27 05:57:43.050: INFO: Got endpoints: latency-svc-ljmrw [741.585183ms]
May 27 05:57:43.068: INFO: Created: latency-svc-6p6n7
May 27 05:57:43.102: INFO: Got endpoints: latency-svc-467vn [746.409394ms]
May 27 05:57:43.129: INFO: Created: latency-svc-rwwlz
May 27 05:57:43.150: INFO: Got endpoints: latency-svc-tv9dc [745.757775ms]
May 27 05:57:43.167: INFO: Created: latency-svc-5cg4f
May 27 05:57:43.200: INFO: Got endpoints: latency-svc-sgnhv [736.360969ms]
May 27 05:57:43.218: INFO: Created: latency-svc-jxh7z
May 27 05:57:43.253: INFO: Got endpoints: latency-svc-wxzvc [751.873019ms]
May 27 05:57:43.285: INFO: Created: latency-svc-mqpm4
May 27 05:57:43.299: INFO: Got endpoints: latency-svc-pnh8r [751.322976ms]
May 27 05:57:43.325: INFO: Created: latency-svc-rsq8q
May 27 05:57:43.351: INFO: Got endpoints: latency-svc-pnbjd [755.187622ms]
May 27 05:57:43.374: INFO: Created: latency-svc-zmn2w
May 27 05:57:43.397: INFO: Got endpoints: latency-svc-p5mzc [749.131215ms]
May 27 05:57:43.418: INFO: Created: latency-svc-dzxw6
May 27 05:57:43.455: INFO: Got endpoints: latency-svc-whzxn [752.435056ms]
May 27 05:57:43.475: INFO: Created: latency-svc-7dpfn
May 27 05:57:43.502: INFO: Got endpoints: latency-svc-9wwdl [755.711162ms]
May 27 05:57:43.520: INFO: Created: latency-svc-86glq
May 27 05:57:43.549: INFO: Got endpoints: latency-svc-bffd6 [749.82394ms]
May 27 05:57:43.576: INFO: Created: latency-svc-8qnxn
May 27 05:57:43.598: INFO: Got endpoints: latency-svc-6dwvz [747.840971ms]
May 27 05:57:43.621: INFO: Created: latency-svc-h2gfc
May 27 05:57:43.647: INFO: Got endpoints: latency-svc-qs4kn [749.185434ms]
May 27 05:57:43.665: INFO: Created: latency-svc-56hv7
May 27 05:57:43.698: INFO: Got endpoints: latency-svc-sqbkg [735.994743ms]
May 27 05:57:43.718: INFO: Created: latency-svc-nqsbb
May 27 05:57:43.747: INFO: Got endpoints: latency-svc-bqv9f [749.011742ms]
May 27 05:57:43.769: INFO: Created: latency-svc-955rj
May 27 05:57:43.798: INFO: Got endpoints: latency-svc-6p6n7 [747.491812ms]
May 27 05:57:43.819: INFO: Created: latency-svc-mgkd5
May 27 05:57:43.846: INFO: Got endpoints: latency-svc-rwwlz [743.617695ms]
May 27 05:57:43.871: INFO: Created: latency-svc-ldtpj
May 27 05:57:43.898: INFO: Got endpoints: latency-svc-5cg4f [747.387003ms]
May 27 05:57:43.928: INFO: Created: latency-svc-6wjjm
May 27 05:57:43.951: INFO: Got endpoints: latency-svc-jxh7z [751.151657ms]
May 27 05:57:43.970: INFO: Created: latency-svc-85gl4
May 27 05:57:43.999: INFO: Got endpoints: latency-svc-mqpm4 [746.644789ms]
May 27 05:57:44.037: INFO: Created: latency-svc-z254s
May 27 05:57:44.057: INFO: Got endpoints: latency-svc-rsq8q [757.551115ms]
May 27 05:57:44.077: INFO: Created: latency-svc-fh2h7
May 27 05:57:44.094: INFO: Got endpoints: latency-svc-zmn2w [742.9087ms]
May 27 05:57:44.114: INFO: Created: latency-svc-6tgwm
May 27 05:57:44.151: INFO: Got endpoints: latency-svc-dzxw6 [753.651997ms]
May 27 05:57:44.176: INFO: Created: latency-svc-zbbgh
May 27 05:57:44.207: INFO: Got endpoints: latency-svc-7dpfn [751.36132ms]
May 27 05:57:44.230: INFO: Created: latency-svc-rpqqp
May 27 05:57:44.252: INFO: Got endpoints: latency-svc-86glq [749.561552ms]
May 27 05:57:44.280: INFO: Created: latency-svc-nx25d
May 27 05:57:44.306: INFO: Got endpoints: latency-svc-8qnxn [756.980902ms]
May 27 05:57:44.357: INFO: Created: latency-svc-6qkbp
May 27 05:57:44.362: INFO: Got endpoints: latency-svc-h2gfc [764.00861ms]
May 27 05:57:44.453: INFO: Got endpoints: latency-svc-56hv7 [805.288995ms]
May 27 05:57:44.455: INFO: Got endpoints: latency-svc-nqsbb [757.297949ms]
May 27 05:57:44.480: INFO: Created: latency-svc-5sll4
May 27 05:57:44.497: INFO: Created: latency-svc-6wvvz
May 27 05:57:44.510: INFO: Got endpoints: latency-svc-955rj [762.703316ms]
May 27 05:57:44.527: INFO: Created: latency-svc-976l9
May 27 05:57:44.554: INFO: Created: latency-svc-69dl5
May 27 05:57:44.555: INFO: Got endpoints: latency-svc-mgkd5 [756.381295ms]
May 27 05:57:44.593: INFO: Created: latency-svc-chtb5
May 27 05:57:44.601: INFO: Got endpoints: latency-svc-ldtpj [755.181463ms]
May 27 05:57:44.627: INFO: Created: latency-svc-smbc9
May 27 05:57:44.649: INFO: Got endpoints: latency-svc-6wjjm [751.363657ms]
May 27 05:57:44.675: INFO: Created: latency-svc-sq859
May 27 05:57:44.702: INFO: Got endpoints: latency-svc-85gl4 [750.549039ms]
May 27 05:57:44.728: INFO: Created: latency-svc-8f6vd
May 27 05:57:44.745: INFO: Got endpoints: latency-svc-z254s [745.740168ms]
May 27 05:57:44.765: INFO: Created: latency-svc-7pr77
May 27 05:57:44.800: INFO: Got endpoints: latency-svc-fh2h7 [743.393011ms]
May 27 05:57:44.824: INFO: Created: latency-svc-4gmj2
May 27 05:57:44.848: INFO: Got endpoints: latency-svc-6tgwm [754.026735ms]
May 27 05:57:44.867: INFO: Created: latency-svc-pzfqh
May 27 05:57:44.911: INFO: Got endpoints: latency-svc-zbbgh [759.885303ms]
May 27 05:57:44.932: INFO: Created: latency-svc-hppcl
May 27 05:57:44.967: INFO: Got endpoints: latency-svc-rpqqp [759.559738ms]
May 27 05:57:44.991: INFO: Created: latency-svc-7zg2s
May 27 05:57:44.998: INFO: Got endpoints: latency-svc-nx25d [745.484129ms]
May 27 05:57:45.039: INFO: Created: latency-svc-m57hn
May 27 05:57:45.048: INFO: Got endpoints: latency-svc-6qkbp [741.062218ms]
May 27 05:57:45.070: INFO: Created: latency-svc-qfxhg
May 27 05:57:45.109: INFO: Got endpoints: latency-svc-5sll4 [747.008307ms]
May 27 05:57:45.167: INFO: Created: latency-svc-4d2zh
May 27 05:57:45.182: INFO: Got endpoints: latency-svc-6wvvz [726.272837ms]
May 27 05:57:45.208: INFO: Got endpoints: latency-svc-976l9 [755.600222ms]
May 27 05:57:45.221: INFO: Created: latency-svc-jrthj
May 27 05:57:45.233: INFO: Created: latency-svc-mccnz
May 27 05:57:45.249: INFO: Got endpoints: latency-svc-69dl5 [738.744657ms]
May 27 05:57:45.280: INFO: Created: latency-svc-qmqbw
May 27 05:57:45.298: INFO: Got endpoints: latency-svc-chtb5 [743.001398ms]
May 27 05:57:45.319: INFO: Created: latency-svc-srzvg
May 27 05:57:45.352: INFO: Got endpoints: latency-svc-smbc9 [750.474379ms]
May 27 05:57:45.377: INFO: Created: latency-svc-fvrnp
May 27 05:57:45.402: INFO: Got endpoints: latency-svc-sq859 [752.826223ms]
May 27 05:57:45.434: INFO: Created: latency-svc-9wgqk
May 27 05:57:45.452: INFO: Got endpoints: latency-svc-8f6vd [749.355478ms]
May 27 05:57:45.468: INFO: Created: latency-svc-hnlwz
May 27 05:57:45.499: INFO: Got endpoints: latency-svc-7pr77 [753.797628ms]
May 27 05:57:45.520: INFO: Created: latency-svc-h942l
May 27 05:57:45.551: INFO: Got endpoints: latency-svc-4gmj2 [749.809009ms]
May 27 05:57:45.574: INFO: Created: latency-svc-sntjf
May 27 05:57:45.598: INFO: Got endpoints: latency-svc-pzfqh [749.565975ms]
May 27 05:57:45.615: INFO: Created: latency-svc-9vnql
May 27 05:57:45.649: INFO: Got endpoints: latency-svc-hppcl [737.573321ms]
May 27 05:57:45.670: INFO: Created: latency-svc-9rdkq
May 27 05:57:45.700: INFO: Got endpoints: latency-svc-7zg2s [733.255799ms]
May 27 05:57:45.723: INFO: Created: latency-svc-sdfz9
May 27 05:57:45.749: INFO: Got endpoints: latency-svc-m57hn [751.368025ms]
May 27 05:57:45.772: INFO: Created: latency-svc-kx7c9
May 27 05:57:45.796: INFO: Got endpoints: latency-svc-qfxhg [748.317526ms]
May 27 05:57:45.818: INFO: Created: latency-svc-tqs4r
May 27 05:57:45.852: INFO: Got endpoints: latency-svc-4d2zh [743.340551ms]
May 27 05:57:45.882: INFO: Created: latency-svc-j2l9v
May 27 05:57:45.897: INFO: Got endpoints: latency-svc-jrthj [714.53851ms]
May 27 05:57:45.926: INFO: Created: latency-svc-k2jzp
May 27 05:57:45.950: INFO: Got endpoints: latency-svc-mccnz [741.999496ms]
May 27 05:57:45.970: INFO: Created: latency-svc-72f8g
May 27 05:57:45.996: INFO: Got endpoints: latency-svc-qmqbw [747.015732ms]
May 27 05:57:46.014: INFO: Created: latency-svc-lxnw6
May 27 05:57:46.051: INFO: Got endpoints: latency-svc-srzvg [752.824656ms]
May 27 05:57:46.075: INFO: Created: latency-svc-zr6j6
May 27 05:57:46.101: INFO: Got endpoints: latency-svc-fvrnp [749.100248ms]
May 27 05:57:46.122: INFO: Created: latency-svc-qxtdc
May 27 05:57:46.149: INFO: Got endpoints: latency-svc-9wgqk [746.445524ms]
May 27 05:57:46.173: INFO: Created: latency-svc-pzppp
May 27 05:57:46.209: INFO: Got endpoints: latency-svc-hnlwz [756.896318ms]
May 27 05:57:46.234: INFO: Created: latency-svc-hkc7q
May 27 05:57:46.250: INFO: Got endpoints: latency-svc-h942l [751.123753ms]
May 27 05:57:46.276: INFO: Created: latency-svc-lv7ml
May 27 05:57:46.299: INFO: Got endpoints: latency-svc-sntjf [748.034515ms]
May 27 05:57:46.333: INFO: Created: latency-svc-cv7jj
May 27 05:57:46.349: INFO: Got endpoints: latency-svc-9vnql [750.768327ms]
May 27 05:57:46.375: INFO: Created: latency-svc-8n4r4
May 27 05:57:46.401: INFO: Got endpoints: latency-svc-9rdkq [750.24463ms]
May 27 05:57:46.417: INFO: Created: latency-svc-jp7ds
May 27 05:57:46.457: INFO: Got endpoints: latency-svc-sdfz9 [756.309099ms]
May 27 05:57:46.488: INFO: Created: latency-svc-lzsbn
May 27 05:57:46.504: INFO: Got endpoints: latency-svc-kx7c9 [754.871302ms]
May 27 05:57:46.532: INFO: Created: latency-svc-cvc9n
May 27 05:57:46.551: INFO: Got endpoints: latency-svc-tqs4r [755.013301ms]
May 27 05:57:46.574: INFO: Created: latency-svc-2th7n
May 27 05:57:46.600: INFO: Got endpoints: latency-svc-j2l9v [746.928325ms]
May 27 05:57:46.634: INFO: Created: latency-svc-46hlh
May 27 05:57:46.654: INFO: Got endpoints: latency-svc-k2jzp [756.996566ms]
May 27 05:57:46.677: INFO: Created: latency-svc-hs6rr
May 27 05:57:46.702: INFO: Got endpoints: latency-svc-72f8g [751.391954ms]
May 27 05:57:46.719: INFO: Created: latency-svc-ftwjs
May 27 05:57:46.751: INFO: Got endpoints: latency-svc-lxnw6 [754.567732ms]
May 27 05:57:46.773: INFO: Created: latency-svc-k4rk2
May 27 05:57:46.808: INFO: Got endpoints: latency-svc-zr6j6 [757.457736ms]
May 27 05:57:46.832: INFO: Created: latency-svc-ks9zp
May 27 05:57:46.848: INFO: Got endpoints: latency-svc-qxtdc [746.803228ms]
May 27 05:57:46.873: INFO: Created: latency-svc-7rbzs
May 27 05:57:46.897: INFO: Got endpoints: latency-svc-pzppp [747.970079ms]
May 27 05:57:46.927: INFO: Created: latency-svc-2k2z9
May 27 05:57:46.948: INFO: Got endpoints: latency-svc-hkc7q [738.629903ms]
May 27 05:57:46.982: INFO: Created: latency-svc-9hghd
May 27 05:57:46.998: INFO: Got endpoints: latency-svc-lv7ml [747.484852ms]
May 27 05:57:47.020: INFO: Created: latency-svc-cq5p2
May 27 05:57:47.054: INFO: Got endpoints: latency-svc-cv7jj [754.887905ms]
May 27 05:57:47.088: INFO: Created: latency-svc-f5v56
May 27 05:57:47.098: INFO: Got endpoints: latency-svc-8n4r4 [749.374751ms]
May 27 05:57:47.122: INFO: Created: latency-svc-l767s
May 27 05:57:47.153: INFO: Got endpoints: latency-svc-jp7ds [752.04084ms]
May 27 05:57:47.188: INFO: Created: latency-svc-srvhp
May 27 05:57:47.199: INFO: Got endpoints: latency-svc-lzsbn [741.65131ms]
May 27 05:57:47.256: INFO: Created: latency-svc-7f5b8
May 27 05:57:47.257: INFO: Got endpoints: latency-svc-cvc9n [752.162581ms]
May 27 05:57:47.284: INFO: Created: latency-svc-mmxxp
May 27 05:57:47.299: INFO: Got endpoints: latency-svc-2th7n [747.550958ms]
May 27 05:57:47.326: INFO: Created: latency-svc-9fpmn
May 27 05:57:47.352: INFO: Got endpoints: latency-svc-46hlh [752.575765ms]
May 27 05:57:47.372: INFO: Created: latency-svc-bvcgt
May 27 05:57:47.403: INFO: Got endpoints: latency-svc-hs6rr [748.57805ms]
May 27 05:57:47.426: INFO: Created: latency-svc-22444
May 27 05:57:47.446: INFO: Got endpoints: latency-svc-ftwjs [743.994771ms]
May 27 05:57:47.485: INFO: Created: latency-svc-s8g7b
May 27 05:57:47.504: INFO: Got endpoints: latency-svc-k4rk2 [752.306665ms]
May 27 05:57:47.541: INFO: Created: latency-svc-v4pjd
May 27 05:57:47.558: INFO: Got endpoints: latency-svc-ks9zp [749.00761ms]
May 27 05:57:47.592: INFO: Created: latency-svc-ml87r
May 27 05:57:47.616: INFO: Got endpoints: latency-svc-7rbzs [767.44768ms]
May 27 05:57:47.645: INFO: Created: latency-svc-pc7z7
May 27 05:57:47.651: INFO: Got endpoints: latency-svc-2k2z9 [753.92304ms]
May 27 05:57:47.683: INFO: Created: latency-svc-dgtbs
May 27 05:57:47.705: INFO: Got endpoints: latency-svc-9hghd [756.591464ms]
May 27 05:57:47.727: INFO: Created: latency-svc-b8twz
May 27 05:57:47.778: INFO: Got endpoints: latency-svc-cq5p2 [779.030794ms]
May 27 05:57:47.797: INFO: Created: latency-svc-rzccm
May 27 05:57:47.806: INFO: Got endpoints: latency-svc-f5v56 [751.430459ms]
May 27 05:57:47.823: INFO: Created: latency-svc-nf4r2
May 27 05:57:47.858: INFO: Got endpoints: latency-svc-l767s [758.778615ms]
May 27 05:57:47.887: INFO: Created: latency-svc-plss5
May 27 05:57:47.895: INFO: Got endpoints: latency-svc-srvhp [741.7325ms]
May 27 05:57:47.916: INFO: Created: latency-svc-wm5nq
May 27 05:57:47.953: INFO: Got endpoints: latency-svc-7f5b8 [754.047781ms]
May 27 05:57:47.971: INFO: Created: latency-svc-zqfdd
May 27 05:57:48.001: INFO: Got endpoints: latency-svc-mmxxp [743.941782ms]
May 27 05:57:48.027: INFO: Created: latency-svc-rbwvc
May 27 05:57:48.050: INFO: Got endpoints: latency-svc-9fpmn [751.274766ms]
May 27 05:57:48.071: INFO: Created: latency-svc-zkmrk
May 27 05:57:48.106: INFO: Got endpoints: latency-svc-bvcgt [753.919816ms]
May 27 05:57:48.128: INFO: Created: latency-svc-m47fm
May 27 05:57:48.153: INFO: Got endpoints: latency-svc-22444 [750.366938ms]
May 27 05:57:48.175: INFO: Created: latency-svc-tvskl
May 27 05:57:48.196: INFO: Got endpoints: latency-svc-s8g7b [749.574639ms]
May 27 05:57:48.218: INFO: Created: latency-svc-vs4t4
May 27 05:57:48.251: INFO: Got endpoints: latency-svc-v4pjd [746.525594ms]
May 27 05:57:48.277: INFO: Created: latency-svc-mlj4g
May 27 05:57:48.301: INFO: Got endpoints: latency-svc-ml87r [743.31154ms]
May 27 05:57:48.328: INFO: Created: latency-svc-wsjh6
May 27 05:57:48.347: INFO: Got endpoints: latency-svc-pc7z7 [730.793854ms]
May 27 05:57:48.369: INFO: Created: latency-svc-wq6h2
May 27 05:57:48.396: INFO: Got endpoints: latency-svc-dgtbs [744.543314ms]
May 27 05:57:48.423: INFO: Created: latency-svc-4hv7h
May 27 05:57:48.451: INFO: Got endpoints: latency-svc-b8twz [745.542599ms]
May 27 05:57:48.470: INFO: Created: latency-svc-pqmkh
May 27 05:57:48.499: INFO: Got endpoints: latency-svc-rzccm [720.171081ms]
May 27 05:57:48.524: INFO: Created: latency-svc-vksjn
May 27 05:57:48.551: INFO: Got endpoints: latency-svc-nf4r2 [744.773191ms]
May 27 05:57:48.573: INFO: Created: latency-svc-k4l2w
May 27 05:57:48.600: INFO: Got endpoints: latency-svc-plss5 [742.462749ms]
May 27 05:57:48.631: INFO: Created: latency-svc-sh5c2
May 27 05:57:48.645: INFO: Got endpoints: latency-svc-wm5nq [750.220618ms]
May 27 05:57:48.667: INFO: Created: latency-svc-66lc2
May 27 05:57:48.696: INFO: Got endpoints: latency-svc-zqfdd [742.183965ms]
May 27 05:57:48.719: INFO: Created: latency-svc-87s2l
May 27 05:57:48.753: INFO: Got endpoints: latency-svc-rbwvc [751.488859ms]
May 27 05:57:48.771: INFO: Created: latency-svc-2jpvl
May 27 05:57:48.799: INFO: Got endpoints: latency-svc-zkmrk [748.439044ms]
May 27 05:57:48.822: INFO: Created: latency-svc-hcg8f
May 27 05:57:48.853: INFO: Got endpoints: latency-svc-m47fm [745.961419ms]
May 27 05:57:48.872: INFO: Created: latency-svc-cvmc4
May 27 05:57:48.900: INFO: Got endpoints: latency-svc-tvskl [746.950727ms]
May 27 05:57:48.937: INFO: Created: latency-svc-nc9v7
May 27 05:57:48.950: INFO: Got endpoints: latency-svc-vs4t4 [753.88449ms]
May 27 05:57:48.973: INFO: Created: latency-svc-rp4vt
May 27 05:57:48.999: INFO: Got endpoints: latency-svc-mlj4g [748.753462ms]
May 27 05:57:49.019: INFO: Created: latency-svc-vf255
May 27 05:57:49.049: INFO: Got endpoints: latency-svc-wsjh6 [747.591189ms]
May 27 05:57:49.095: INFO: Created: latency-svc-26hb5
May 27 05:57:49.104: INFO: Got endpoints: latency-svc-wq6h2 [757.087772ms]
May 27 05:57:49.124: INFO: Created: latency-svc-7t4fc
May 27 05:57:49.181: INFO: Got endpoints: latency-svc-4hv7h [784.841613ms]
May 27 05:57:49.198: INFO: Got endpoints: latency-svc-pqmkh [746.837662ms]
May 27 05:57:49.250: INFO: Got endpoints: latency-svc-vksjn [748.844995ms]
May 27 05:57:49.298: INFO: Got endpoints: latency-svc-k4l2w [746.5572ms]
May 27 05:57:49.347: INFO: Got endpoints: latency-svc-sh5c2 [746.620899ms]
May 27 05:57:49.401: INFO: Got endpoints: latency-svc-66lc2 [755.16842ms]
May 27 05:57:49.450: INFO: Got endpoints: latency-svc-87s2l [754.002541ms]
May 27 05:57:49.497: INFO: Got endpoints: latency-svc-2jpvl [743.35542ms]
May 27 05:57:49.551: INFO: Got endpoints: latency-svc-hcg8f [751.046205ms]
May 27 05:57:49.597: INFO: Got endpoints: latency-svc-cvmc4 [743.567687ms]
May 27 05:57:49.647: INFO: Got endpoints: latency-svc-nc9v7 [746.462716ms]
May 27 05:57:49.702: INFO: Got endpoints: latency-svc-rp4vt [751.407507ms]
May 27 05:57:49.747: INFO: Got endpoints: latency-svc-vf255 [747.701076ms]
May 27 05:57:49.802: INFO: Got endpoints: latency-svc-26hb5 [752.497088ms]
May 27 05:57:49.846: INFO: Got endpoints: latency-svc-7t4fc [741.210787ms]
May 27 05:57:49.846: INFO: Latencies: [72.421193ms 72.698055ms 107.186078ms 113.42501ms 115.678155ms 145.437564ms 161.627629ms 181.534561ms 192.594197ms 215.849026ms 218.720311ms 229.676059ms 231.516948ms 231.694374ms 232.473078ms 235.465337ms 235.798775ms 237.231528ms 237.850607ms 238.005045ms 238.777702ms 239.782771ms 242.423093ms 246.960987ms 253.621509ms 260.241262ms 265.146207ms 268.501971ms 270.725068ms 271.039596ms 271.443108ms 273.235946ms 277.136932ms 284.307922ms 293.429987ms 295.179047ms 307.762775ms 316.82365ms 322.832234ms 323.430429ms 336.999321ms 348.457769ms 371.222774ms 382.103159ms 395.295005ms 404.803662ms 407.680847ms 415.722376ms 417.714515ms 430.894825ms 473.872516ms 480.052287ms 499.606674ms 516.501969ms 521.612407ms 521.657048ms 532.891543ms 555.204438ms 572.614588ms 579.05755ms 626.491339ms 692.872531ms 714.450352ms 714.53851ms 720.171081ms 726.272837ms 730.793854ms 733.255799ms 735.994743ms 736.360969ms 737.573321ms 738.629903ms 738.744657ms 741.062218ms 741.210787ms 741.585183ms 741.65131ms 741.7325ms 741.999496ms 742.183965ms 742.462749ms 742.9087ms 743.001398ms 743.31154ms 743.340551ms 743.35542ms 743.393011ms 743.567687ms 743.617695ms 743.941782ms 743.994771ms 744.543314ms 744.773191ms 745.484129ms 745.542599ms 745.740168ms 745.757775ms 745.961419ms 746.409394ms 746.445524ms 746.462716ms 746.525594ms 746.5572ms 746.620899ms 746.644789ms 746.803228ms 746.837662ms 746.928325ms 746.950727ms 747.008307ms 747.015732ms 747.387003ms 747.484852ms 747.491812ms 747.550958ms 747.591189ms 747.701076ms 747.840971ms 747.970079ms 748.034515ms 748.317526ms 748.439044ms 748.57805ms 748.753462ms 748.844995ms 749.00761ms 749.011742ms 749.100248ms 749.131215ms 749.185434ms 749.355478ms 749.374751ms 749.561552ms 749.565975ms 749.574639ms 749.809009ms 749.82394ms 750.220618ms 750.24463ms 750.366938ms 750.474379ms 750.549039ms 750.768327ms 751.046205ms 751.123753ms 751.151657ms 751.274766ms 751.322976ms 751.36132ms 751.363657ms 751.368025ms 751.391954ms 751.407507ms 751.430459ms 751.488859ms 751.873019ms 752.04084ms 752.162581ms 752.306665ms 752.435056ms 752.497088ms 752.575765ms 752.824656ms 752.826223ms 753.651997ms 753.797628ms 753.88449ms 753.919816ms 753.92304ms 754.002541ms 754.026735ms 754.047781ms 754.567732ms 754.871302ms 754.887905ms 755.013301ms 755.16842ms 755.181463ms 755.187622ms 755.600222ms 755.711162ms 756.309099ms 756.381295ms 756.591464ms 756.896318ms 756.980902ms 756.996566ms 757.087772ms 757.297949ms 757.457736ms 757.551115ms 758.778615ms 759.559738ms 759.885303ms 762.703316ms 764.00861ms 767.44768ms 779.030794ms 784.841613ms 805.288995ms]
May 27 05:57:49.847: INFO: 50 %ile: 746.462716ms
May 27 05:57:49.847: INFO: 90 %ile: 755.711162ms
May 27 05:57:49.847: INFO: 99 %ile: 784.841613ms
May 27 05:57:49.847: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:57:49.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-143" for this suite.

• [SLOW TEST:10.871 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":136,"skipped":2751,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:57:49.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:58:06.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3220" for this suite.

• [SLOW TEST:16.414 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":137,"skipped":2753,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:58:06.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-607.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-607.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-607.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-607.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 05:58:10.504: INFO: DNS probes using dns-607/dns-test-3e2ba0c7-b514-4d77-9807-5924d72ad1dc succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:58:10.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-607" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":138,"skipped":2758,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:58:10.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2
May 27 05:58:10.674: INFO: Pod name my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2: Found 0 pods out of 1
May 27 05:58:15.686: INFO: Pod name my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2: Found 1 pods out of 1
May 27 05:58:15.686: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2" are running
May 27 05:58:15.692: INFO: Pod "my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2-4vwfs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 05:58:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 05:58:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 05:58:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 05:58:10 +0000 UTC Reason: Message:}])
May 27 05:58:15.692: INFO: Trying to dial the pod
May 27 05:58:20.723: INFO: Controller my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2: Got expected result from replica 1 [my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2-4vwfs]: "my-hostname-basic-2df88116-7089-4e6c-9838-9deac3dffda2-4vwfs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:58:20.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1469" for this suite.

• [SLOW TEST:10.158 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":139,"skipped":2776,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:58:20.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 27 05:58:20.821: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20050 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 05:58:20.822: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20050 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 27 05:58:20.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20052 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 05:58:20.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20052 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 27 05:58:20.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20053 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 05:58:20.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20053 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 27 05:58:20.867: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20054 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 05:58:20.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1909  ea5e2ae7-4d3d-44e5-b2d6-7f6c9df0e964 20054 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 27 05:58:20.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1909  c4629eba-0edb-4df8-b0de-6d822be24aa3 20055 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 05:58:20.877: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1909  c4629eba-0edb-4df8-b0de-6d822be24aa3 20055 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 27 05:58:30.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1909  c4629eba-0edb-4df8-b0de-6d822be24aa3 20093 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 05:58:30.905: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1909  c4629eba-0edb-4df8-b0de-6d822be24aa3 20093 0 2022-05-27 05:58:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-27 05:58:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:58:40.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1909" for this suite.

• [SLOW TEST:20.183 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":140,"skipped":2780,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:58:40.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-6242
STEP: creating service affinity-nodeport in namespace services-6242
STEP: creating replication controller affinity-nodeport in namespace services-6242
I0527 05:58:41.061992      14 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6242, replica count: 3
I0527 05:58:44.114400      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 05:58:44.146: INFO: Creating new exec pod
May 27 05:58:49.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6242 exec execpod-affinityr984c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May 27 05:58:49.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 27 05:58:49.573: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:58:49.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6242 exec execpod-affinityr984c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.32.78 80'
May 27 05:58:49.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.32.78 80\nConnection to 10.233.32.78 80 port [tcp/http] succeeded!\n"
May 27 05:58:49.821: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:58:49.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6242 exec execpod-affinityr984c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 32344'
May 27 05:58:50.055: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 32344\nConnection to 192.168.121.43 32344 port [tcp/*] succeeded!\n"
May 27 05:58:50.055: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:58:50.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6242 exec execpod-affinityr984c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.191 32344'
May 27 05:58:50.277: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.191 32344\nConnection to 192.168.121.191 32344 port [tcp/*] succeeded!\n"
May 27 05:58:50.277: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 05:58:50.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6242 exec execpod-affinityr984c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.43:32344/ ; done'
May 27 05:58:50.720: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:32344/\n"
May 27 05:58:50.720: INFO: stdout: "\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx\naffinity-nodeport-w8cpx"
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Received response from host: affinity-nodeport-w8cpx
May 27 05:58:50.721: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6242, will wait for the garbage collector to delete the pods
May 27 05:58:50.825: INFO: Deleting ReplicationController affinity-nodeport took: 11.01616ms
May 27 05:58:50.926: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.195514ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:58:52.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6242" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.985 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":141,"skipped":2781,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:58:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:09.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1448" for this suite.

• [SLOW TEST:16.299 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":142,"skipped":2799,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:09.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 05:59:09.337: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e7c538d3-9583-4a23-94f9-3c44b685a75f" in namespace "security-context-test-9670" to be "Succeeded or Failed"
May 27 05:59:09.345: INFO: Pod "busybox-user-65534-e7c538d3-9583-4a23-94f9-3c44b685a75f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.530067ms
May 27 05:59:11.366: INFO: Pod "busybox-user-65534-e7c538d3-9583-4a23-94f9-3c44b685a75f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028718852s
May 27 05:59:13.382: INFO: Pod "busybox-user-65534-e7c538d3-9583-4a23-94f9-3c44b685a75f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045193985s
May 27 05:59:15.397: INFO: Pod "busybox-user-65534-e7c538d3-9583-4a23-94f9-3c44b685a75f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060201486s
May 27 05:59:15.397: INFO: Pod "busybox-user-65534-e7c538d3-9583-4a23-94f9-3c44b685a75f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:15.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9670" for this suite.

• [SLOW TEST:6.202 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2829,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-231b803a-c0a9-4770-a5b1-dcf159469d90 in namespace container-probe-9434
May 27 05:59:17.523: INFO: Started pod liveness-231b803a-c0a9-4770-a5b1-dcf159469d90 in namespace container-probe-9434
STEP: checking the pod's current state and verifying that restartCount is present
May 27 05:59:17.528: INFO: Initial restart count of pod liveness-231b803a-c0a9-4770-a5b1-dcf159469d90 is 0
May 27 05:59:37.711: INFO: Restart count of pod container-probe-9434/liveness-231b803a-c0a9-4770-a5b1-dcf159469d90 is now 1 (20.182588894s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:37.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9434" for this suite.

• [SLOW TEST:22.331 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":144,"skipped":2832,"failed":0}
SS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 05:59:37.867: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 27 05:59:37.875: INFO: starting watch
STEP: patching
STEP: updating
May 27 05:59:37.904: INFO: waiting for watch events with expected annotations
May 27 05:59:37.904: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:37.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-408" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":145,"skipped":2834,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:38.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 05:59:38.810: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 05:59:41.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:41.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6322" for this suite.
STEP: Destroying namespace "webhook-6322-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":146,"skipped":2855,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:42.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 27 05:59:46.114: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-815 PodName:pod-sharedvolume-f0a12733-b472-43db-a03c-325cad3bf930 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 05:59:46.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 05:59:46.117: INFO: ExecWithOptions: Clientset creation
May 27 05:59:46.117: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-815/pods/pod-sharedvolume-f0a12733-b472-43db-a03c-325cad3bf930/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
May 27 05:59:46.233: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-815" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":147,"skipped":2859,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:46.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-8ad26202-8b0f-4b84-b5f9-a0cb3ab0f996
STEP: Creating a pod to test consume secrets
May 27 05:59:46.368: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4" in namespace "projected-4471" to be "Succeeded or Failed"
May 27 05:59:46.378: INFO: Pod "pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015807ms
May 27 05:59:48.399: INFO: Pod "pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031219176s
May 27 05:59:50.412: INFO: Pod "pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044683688s
May 27 05:59:52.425: INFO: Pod "pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057573065s
STEP: Saw pod success
May 27 05:59:52.425: INFO: Pod "pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4" satisfied condition "Succeeded or Failed"
May 27 05:59:52.435: INFO: Trying to get logs from node ha9zeyohpei4-1 pod pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4 container secret-volume-test: <nil>
STEP: delete the pod
May 27 05:59:52.500: INFO: Waiting for pod pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4 to disappear
May 27 05:59:52.505: INFO: Pod pod-projected-secrets-a3cffd88-16a4-44ae-9fd4-be04e16227c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:52.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4471" for this suite.

• [SLOW TEST:6.256 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2870,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:52.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 05:59:52.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a" in namespace "projected-5144" to be "Succeeded or Failed"
May 27 05:59:52.615: INFO: Pod "downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.02191ms
May 27 05:59:54.631: INFO: Pod "downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02313568s
May 27 05:59:56.647: INFO: Pod "downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03902309s
May 27 05:59:58.660: INFO: Pod "downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052627324s
STEP: Saw pod success
May 27 05:59:58.660: INFO: Pod "downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a" satisfied condition "Succeeded or Failed"
May 27 05:59:58.665: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a container client-container: <nil>
STEP: delete the pod
May 27 05:59:58.715: INFO: Waiting for pod downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a to disappear
May 27 05:59:58.721: INFO: Pod downwardapi-volume-1ef26480-2e8b-44d8-a00a-77e4b91e769a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 05:59:58.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5144" for this suite.

• [SLOW TEST:6.210 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":149,"skipped":2875,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 05:59:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-7697
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 05:59:58.796: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 05:59:58.864: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:00:00.881: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:02.876: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:04.891: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:06.881: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:08.879: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:10.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:12.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:14.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:16.896: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:00:18.886: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 06:00:18.904: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 27 06:00:20.917: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 06:00:20.926: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 06:00:22.975: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 06:00:22.975: INFO: Breadth first check of 10.233.65.81 on host 192.168.121.43...
May 27 06:00:22.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.52:9080/dial?request=hostname&protocol=http&host=10.233.65.81&port=8083&tries=1'] Namespace:pod-network-test-7697 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:00:22.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:00:22.983: INFO: ExecWithOptions: Clientset creation
May 27 06:00:22.983: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7697/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.52%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.81%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 06:00:23.123: INFO: Waiting for responses: map[]
May 27 06:00:23.123: INFO: reached 10.233.65.81 after 0/1 tries
May 27 06:00:23.124: INFO: Breadth first check of 10.233.64.49 on host 192.168.121.209...
May 27 06:00:23.136: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.52:9080/dial?request=hostname&protocol=http&host=10.233.64.49&port=8083&tries=1'] Namespace:pod-network-test-7697 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:00:23.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:00:23.137: INFO: ExecWithOptions: Clientset creation
May 27 06:00:23.138: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7697/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.52%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.49%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 06:00:23.263: INFO: Waiting for responses: map[]
May 27 06:00:23.263: INFO: reached 10.233.64.49 after 0/1 tries
May 27 06:00:23.263: INFO: Breadth first check of 10.233.66.87 on host 192.168.121.191...
May 27 06:00:23.271: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.52:9080/dial?request=hostname&protocol=http&host=10.233.66.87&port=8083&tries=1'] Namespace:pod-network-test-7697 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:00:23.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:00:23.272: INFO: ExecWithOptions: Clientset creation
May 27 06:00:23.272: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7697/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.52%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 27 06:00:23.371: INFO: Waiting for responses: map[]
May 27 06:00:23.371: INFO: reached 10.233.66.87 after 0/1 tries
May 27 06:00:23.371: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:00:23.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7697" for this suite.

• [SLOW TEST:24.644 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":2878,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:00:23.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:01.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-802" for this suite.

• [SLOW TEST:98.147 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":151,"skipped":2890,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:01.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 27 06:02:01.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:02:05.938: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9266" for this suite.

• [SLOW TEST:24.495 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":152,"skipped":2892,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:26.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:26.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2567" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":153,"skipped":2904,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:26.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:02:26.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 create -f -'
May 27 06:02:27.673: INFO: stderr: ""
May 27 06:02:27.673: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 27 06:02:27.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 create -f -'
May 27 06:02:28.000: INFO: stderr: ""
May 27 06:02:28.000: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 27 06:02:29.011: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:02:29.012: INFO: Found 0 / 1
May 27 06:02:30.012: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:02:30.012: INFO: Found 0 / 1
May 27 06:02:31.015: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:02:31.016: INFO: Found 1 / 1
May 27 06:02:31.016: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 27 06:02:31.021: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:02:31.021: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 27 06:02:31.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 describe pod agnhost-primary-6tn28'
May 27 06:02:31.165: INFO: stderr: ""
May 27 06:02:31.165: INFO: stdout: "Name:         agnhost-primary-6tn28\nNamespace:    kubectl-8201\nPriority:     0\nNode:         ha9zeyohpei4-1/192.168.121.43\nStart Time:   Fri, 27 May 2022 06:02:27 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.233.65.16\nIPs:\n  IP:           10.233.65.16\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://fe84ad8b0acf8eec8c77328c35127fddd9ec7c4ee31e96249715acd2115ee795\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 27 May 2022 06:02:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k8rg4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-k8rg4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-8201/agnhost-primary-6tn28 to ha9zeyohpei4-1\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
May 27 06:02:31.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 describe rc agnhost-primary'
May 27 06:02:31.399: INFO: stderr: ""
May 27 06:02:31.399: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8201\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-6tn28\n"
May 27 06:02:31.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 describe service agnhost-primary'
May 27 06:02:31.619: INFO: stderr: ""
May 27 06:02:31.619: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8201\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.27.52\nIPs:               10.233.27.52\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.65.16:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 27 06:02:31.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 describe node ha9zeyohpei4-1'
May 27 06:02:31.868: INFO: stderr: ""
May 27 06:02:31.868: INFO: stdout: "Name:               ha9zeyohpei4-1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ha9zeyohpei4-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        io.cilium.network.ipv4-cilium-host: 10.233.65.240\n                    io.cilium.network.ipv4-health-ip: 10.233.65.194\n                    io.cilium.network.ipv4-pod-cidr: 10.233.65.0/24\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 27 May 2022 04:57:09 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ha9zeyohpei4-1\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 27 May 2022 06:02:26 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 27 May 2022 05:11:10 +0000   Fri, 27 May 2022 05:11:10 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 27 May 2022 06:01:25 +0000   Fri, 27 May 2022 04:57:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 27 May 2022 06:01:25 +0000   Fri, 27 May 2022 04:57:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 27 May 2022 06:01:25 +0000   Fri, 27 May 2022 04:57:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 27 May 2022 06:01:25 +0000   Fri, 27 May 2022 05:08:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.43\n  Hostname:    ha9zeyohpei4-1\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      122749536Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8142496Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    1600m\n  ephemeral-storage:      119410748528\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3292832Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 7f19e7d403af427b99c816c5dec39d8e\n  System UUID:                7f19e7d4-03af-427b-99c8-16c5dec39d8e\n  Boot ID:                    f1cc9b19-6085-4d13-8aea-e401aa10e926\n  Kernel Version:             5.15.0-33-generic\n  OS Image:                   Ubuntu 22.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.23.2\n  Kubelet Version:            v1.23.7\n  Kube-Proxy Version:         v1.23.7\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  cilium-test                 echo-other-node-59d779959c-4vckc                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 cilium-node-init-ph2ps                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\n  kube-system                 cilium-wjfhd                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         52m\n  kube-system                 kube-addon-manager-ha9zeyohpei4-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         52m\n  kube-system                 kube-apiserver-ha9zeyohpei4-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 kube-controller-manager-ha9zeyohpei4-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 kube-proxy-4t9kh                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 kube-scheduler-ha9zeyohpei4-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         65m\n  kubectl-8201                agnhost-primary-6tn28                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-lcdqs    0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    655m (40%)  0 (0%)\n  memory                 150Mi (4%)  0 (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age   From     Message\n  ----    ------                   ----  ----     -------\n  Normal  Starting                 54m   kubelet  Starting kubelet.\n  Normal  NodeHasSufficientMemory  54m   kubelet  Node ha9zeyohpei4-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    54m   kubelet  Node ha9zeyohpei4-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     54m   kubelet  Node ha9zeyohpei4-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             54m   kubelet  Node ha9zeyohpei4-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  54m   kubelet  Updated Node Allocatable limit across pods\n  Normal  NodeReady                54m   kubelet  Node ha9zeyohpei4-1 status is now: NodeReady\n"
May 27 06:02:31.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-8201 describe namespace kubectl-8201'
May 27 06:02:32.038: INFO: stderr: ""
May 27 06:02:32.038: INFO: stdout: "Name:         kubectl-8201\nLabels:       e2e-framework=kubectl\n              e2e-run=da49959c-2be8-47a9-96c5-d742768aac68\n              kubernetes.io/metadata.name=kubectl-8201\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:32.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8201" for this suite.

• [SLOW TEST:5.897 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1109
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":154,"skipped":2906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:32.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 06:02:32.140: INFO: The status of Pod pod-update-48bb2e59-2d7d-496d-97d5-a5113c9ea460 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:02:34.148: INFO: The status of Pod pod-update-48bb2e59-2d7d-496d-97d5-a5113c9ea460 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:02:36.155: INFO: The status of Pod pod-update-48bb2e59-2d7d-496d-97d5-a5113c9ea460 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 27 06:02:36.704: INFO: Successfully updated pod "pod-update-48bb2e59-2d7d-496d-97d5-a5113c9ea460"
STEP: verifying the updated pod is in kubernetes
May 27 06:02:36.717: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:36.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8926" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2928,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:36.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:02:36.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9212 version'
May 27 06:02:36.909: INFO: stderr: ""
May 27 06:02:36.909: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.7\", GitCommit:\"42c05a547468804b2053ecf60a3bd15560362fc2\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:30:55Z\", GoVersion:\"go1.17.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.7\", GitCommit:\"42c05a547468804b2053ecf60a3bd15560362fc2\", GitTreeState:\"clean\", BuildDate:\"2022-05-24T12:24:41Z\", GoVersion:\"go1.17.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9212" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":156,"skipped":2933,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:36.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 06:02:37.009: INFO: The status of Pod annotationupdate60edc035-87ba-4cb8-a038-cfe3948123b8 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:02:39.023: INFO: The status of Pod annotationupdate60edc035-87ba-4cb8-a038-cfe3948123b8 is Running (Ready = true)
May 27 06:02:39.584: INFO: Successfully updated pod "annotationupdate60edc035-87ba-4cb8-a038-cfe3948123b8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:41.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-610" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":157,"skipped":2936,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:41.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
May 27 06:02:41.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 create -f -'
May 27 06:02:42.848: INFO: stderr: ""
May 27 06:02:42.849: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 06:02:42.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 06:02:43.019: INFO: stderr: ""
May 27 06:02:43.019: INFO: stdout: "update-demo-nautilus-6jqdq update-demo-nautilus-7bzcd "
May 27 06:02:43.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 06:02:43.168: INFO: stderr: ""
May 27 06:02:43.169: INFO: stdout: ""
May 27 06:02:43.169: INFO: update-demo-nautilus-6jqdq is created but not running
May 27 06:02:48.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 06:02:48.379: INFO: stderr: ""
May 27 06:02:48.380: INFO: stdout: "update-demo-nautilus-6jqdq update-demo-nautilus-7bzcd "
May 27 06:02:48.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 06:02:48.514: INFO: stderr: ""
May 27 06:02:48.514: INFO: stdout: "true"
May 27 06:02:48.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 06:02:48.634: INFO: stderr: ""
May 27 06:02:48.635: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 06:02:48.636: INFO: validating pod update-demo-nautilus-6jqdq
May 27 06:02:48.660: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 06:02:48.660: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 06:02:48.661: INFO: update-demo-nautilus-6jqdq is verified up and running
May 27 06:02:48.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-7bzcd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 06:02:48.787: INFO: stderr: ""
May 27 06:02:48.788: INFO: stdout: "true"
May 27 06:02:48.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-7bzcd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 06:02:48.903: INFO: stderr: ""
May 27 06:02:48.903: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 06:02:48.903: INFO: validating pod update-demo-nautilus-7bzcd
May 27 06:02:48.918: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 06:02:48.918: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 06:02:48.918: INFO: update-demo-nautilus-7bzcd is verified up and running
STEP: scaling down the replication controller
May 27 06:02:48.936: INFO: scanned /root for discovery docs: <nil>
May 27 06:02:48.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 27 06:02:50.164: INFO: stderr: ""
May 27 06:02:50.164: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 06:02:50.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 06:02:50.306: INFO: stderr: ""
May 27 06:02:50.306: INFO: stdout: "update-demo-nautilus-6jqdq "
May 27 06:02:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 06:02:50.440: INFO: stderr: ""
May 27 06:02:50.440: INFO: stdout: "true"
May 27 06:02:50.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 06:02:50.585: INFO: stderr: ""
May 27 06:02:50.585: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 06:02:50.585: INFO: validating pod update-demo-nautilus-6jqdq
May 27 06:02:50.596: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 06:02:50.596: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 06:02:50.596: INFO: update-demo-nautilus-6jqdq is verified up and running
STEP: scaling up the replication controller
May 27 06:02:50.613: INFO: scanned /root for discovery docs: <nil>
May 27 06:02:50.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 27 06:02:51.793: INFO: stderr: ""
May 27 06:02:51.793: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 27 06:02:51.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 27 06:02:51.932: INFO: stderr: ""
May 27 06:02:51.932: INFO: stdout: "update-demo-nautilus-6jqdq update-demo-nautilus-lnkgr "
May 27 06:02:51.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 06:02:52.054: INFO: stderr: ""
May 27 06:02:52.054: INFO: stdout: "true"
May 27 06:02:52.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-6jqdq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 06:02:52.228: INFO: stderr: ""
May 27 06:02:52.228: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 06:02:52.228: INFO: validating pod update-demo-nautilus-6jqdq
May 27 06:02:52.236: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 06:02:52.236: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 06:02:52.237: INFO: update-demo-nautilus-6jqdq is verified up and running
May 27 06:02:52.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-lnkgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 27 06:02:52.398: INFO: stderr: ""
May 27 06:02:52.398: INFO: stdout: "true"
May 27 06:02:52.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods update-demo-nautilus-lnkgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 27 06:02:52.530: INFO: stderr: ""
May 27 06:02:52.531: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 27 06:02:52.531: INFO: validating pod update-demo-nautilus-lnkgr
May 27 06:02:52.544: INFO: got data: {
  "image": "nautilus.jpg"
}

May 27 06:02:52.544: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 27 06:02:52.544: INFO: update-demo-nautilus-lnkgr is verified up and running
STEP: using delete to clean up resources
May 27 06:02:52.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 delete --grace-period=0 --force -f -'
May 27 06:02:52.664: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:02:52.664: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 27 06:02:52.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get rc,svc -l name=update-demo --no-headers'
May 27 06:02:52.819: INFO: stderr: "No resources found in kubectl-4010 namespace.\n"
May 27 06:02:52.819: INFO: stdout: ""
May 27 06:02:52.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-4010 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 27 06:02:52.949: INFO: stderr: ""
May 27 06:02:52.949: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:02:52.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4010" for this suite.

• [SLOW TEST:11.335 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":158,"skipped":2948,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:02:52.979: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 27 06:02:53.044: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 27 06:02:53.065: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 27 06:02:53.066: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 27 06:02:53.089: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 27 06:02:53.090: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 27 06:02:53.118: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 27 06:02:53.119: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 27 06:03:00.219: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:00.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3988" for this suite.

• [SLOW TEST:7.294 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":159,"skipped":2958,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:00.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-875f96c2-e16f-48b6-9632-612d6a165b8c
STEP: Creating a pod to test consume configMaps
May 27 06:03:00.366: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb" in namespace "projected-6810" to be "Succeeded or Failed"
May 27 06:03:00.374: INFO: Pod "pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.702524ms
May 27 06:03:02.389: INFO: Pod "pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023097474s
May 27 06:03:04.406: INFO: Pod "pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040597716s
STEP: Saw pod success
May 27 06:03:04.407: INFO: Pod "pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb" satisfied condition "Succeeded or Failed"
May 27 06:03:04.412: INFO: Trying to get logs from node ha9zeyohpei4-1 pod pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 27 06:03:04.460: INFO: Waiting for pod pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb to disappear
May 27 06:03:04.466: INFO: Pod pod-projected-configmaps-36aa7364-c923-4cf5-b0a9-473a4dfabefb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:04.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6810" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":2979,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:04.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:03:04.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4" in namespace "downward-api-4818" to be "Succeeded or Failed"
May 27 06:03:04.550: INFO: Pod "downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607417ms
May 27 06:03:06.561: INFO: Pod "downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017058588s
May 27 06:03:08.580: INFO: Pod "downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035505632s
STEP: Saw pod success
May 27 06:03:08.580: INFO: Pod "downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4" satisfied condition "Succeeded or Failed"
May 27 06:03:08.586: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4 container client-container: <nil>
STEP: delete the pod
May 27 06:03:08.639: INFO: Waiting for pod downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4 to disappear
May 27 06:03:08.644: INFO: Pod downwardapi-volume-1b720183-f4d8-4c2c-b908-6bc00bcbe5d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:08.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4818" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":2993,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:08.665: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-deba3c28-67e8-4036-aa4a-5af835759f80
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:10.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6074" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":162,"skipped":2997,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:10.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:10.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5103" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":163,"skipped":3018,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:10.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 27 06:03:10.986: INFO: Waiting up to 5m0s for pod "security-context-325391b9-1082-4d97-838d-8667cc5a6a43" in namespace "security-context-7496" to be "Succeeded or Failed"
May 27 06:03:10.991: INFO: Pod "security-context-325391b9-1082-4d97-838d-8667cc5a6a43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27554ms
May 27 06:03:13.003: INFO: Pod "security-context-325391b9-1082-4d97-838d-8667cc5a6a43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01649054s
May 27 06:03:15.014: INFO: Pod "security-context-325391b9-1082-4d97-838d-8667cc5a6a43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027872855s
STEP: Saw pod success
May 27 06:03:15.015: INFO: Pod "security-context-325391b9-1082-4d97-838d-8667cc5a6a43" satisfied condition "Succeeded or Failed"
May 27 06:03:15.021: INFO: Trying to get logs from node ha9zeyohpei4-2 pod security-context-325391b9-1082-4d97-838d-8667cc5a6a43 container test-container: <nil>
STEP: delete the pod
May 27 06:03:15.062: INFO: Waiting for pod security-context-325391b9-1082-4d97-838d-8667cc5a6a43 to disappear
May 27 06:03:15.070: INFO: Pod security-context-325391b9-1082-4d97-838d-8667cc5a6a43 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:15.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7496" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":164,"skipped":3033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:15.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 27 06:03:15.190: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6597  f9f78a2f-cce3-43c0-8bfc-2e55f74b0fa2 21714 0 2022-05-27 06:03:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 06:03:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:03:15.191: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6597  f9f78a2f-cce3-43c0-8bfc-2e55f74b0fa2 21716 0 2022-05-27 06:03:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 06:03:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:03:15.191: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6597  f9f78a2f-cce3-43c0-8bfc-2e55f74b0fa2 21717 0 2022-05-27 06:03:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 06:03:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 27 06:03:25.265: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6597  f9f78a2f-cce3-43c0-8bfc-2e55f74b0fa2 21771 0 2022-05-27 06:03:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 06:03:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:03:25.266: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6597  f9f78a2f-cce3-43c0-8bfc-2e55f74b0fa2 21772 0 2022-05-27 06:03:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 06:03:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:03:25.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6597  f9f78a2f-cce3-43c0-8bfc-2e55f74b0fa2 21773 0 2022-05-27 06:03:15 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-27 06:03:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:25.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6597" for this suite.

• [SLOW TEST:10.194 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":165,"skipped":3076,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:25.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 06:03:25.376: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 06:03:27.391: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 06:03:27.416: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 06:03:29.434: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 27 06:03:29.475: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 27 06:03:29.481: INFO: Pod pod-with-poststart-exec-hook still exists
May 27 06:03:31.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 27 06:03:31.495: INFO: Pod pod-with-poststart-exec-hook still exists
May 27 06:03:33.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 27 06:03:33.504: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:03:33.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1088" for this suite.

• [SLOW TEST:8.241 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":166,"skipped":3077,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:03:33.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 06:03:33.581: INFO: PodSpec: initContainers in spec.initContainers
May 27 06:04:17.836: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a95ec4ab-2235-4c34-a760-a384eebba226", GenerateName:"", Namespace:"init-container-1355", SelfLink:"", UID:"2ecdef24-ee29-464f-a428-d1e5c7a473dd", ResourceVersion:"21967", Generation:0, CreationTimestamp:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"581654684"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00384f518), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 27, 6, 3, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00384f548), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-47kn7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc007eb9460), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47kn7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47kn7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47kn7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005530c58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ha9zeyohpei4-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0042f0460), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005530d10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005530d30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005530d38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005530d3c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004ce7360), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.191", PodIP:"10.233.66.71", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.71"}}, StartTime:time.Date(2022, time.May, 27, 6, 3, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0042f0540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0042f05b0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://1b968417eb6cd18376bec967f4947f2bd1c6395372aff7b9ec9c84ecc452ad4a", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007eb94e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007eb94c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc005530e2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:17.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1355" for this suite.

• [SLOW TEST:44.350 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":167,"skipped":3081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:17.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
May 27 06:04:17.956: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 27 06:04:19.964: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:20.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1115" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":168,"skipped":3127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:21.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:04:21.128: INFO: The status of Pod pod-secrets-e939284b-4077-4c5f-97d7-e0281fc912b7 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:04:23.141: INFO: The status of Pod pod-secrets-e939284b-4077-4c5f-97d7-e0281fc912b7 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:23.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7068" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":169,"skipped":3173,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:23.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:23.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2551" for this suite.
STEP: Destroying namespace "nspatchtest-e5f45c66-b3bc-466d-b202-b92a6f2fbb5c-6518" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":170,"skipped":3191,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:23.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 06:04:23.533: INFO: The status of Pod labelsupdatea98d055d-a869-4758-a376-3683e775a6f9 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:04:25.551: INFO: The status of Pod labelsupdatea98d055d-a869-4758-a376-3683e775a6f9 is Running (Ready = true)
May 27 06:04:26.102: INFO: Successfully updated pod "labelsupdatea98d055d-a869-4758-a376-3683e775a6f9"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:28.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1034" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3198,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:28.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:34.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9404" for this suite.
STEP: Destroying namespace "nsdeletetest-2117" for this suite.
May 27 06:04:34.449: INFO: Namespace nsdeletetest-2117 was already deleted
STEP: Destroying namespace "nsdeletetest-9030" for this suite.

• [SLOW TEST:6.288 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":172,"skipped":3199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:34.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-ba988650-67b9-4de3-8097-6393722b2603
STEP: Creating a pod to test consume secrets
May 27 06:04:34.587: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17" in namespace "projected-9682" to be "Succeeded or Failed"
May 27 06:04:34.599: INFO: Pod "pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17": Phase="Pending", Reason="", readiness=false. Elapsed: 12.620856ms
May 27 06:04:36.614: INFO: Pod "pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027261339s
May 27 06:04:38.627: INFO: Pod "pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04031597s
May 27 06:04:40.639: INFO: Pod "pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052528076s
STEP: Saw pod success
May 27 06:04:40.639: INFO: Pod "pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17" satisfied condition "Succeeded or Failed"
May 27 06:04:40.646: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 06:04:40.685: INFO: Waiting for pod pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17 to disappear
May 27 06:04:40.698: INFO: Pod pod-projected-secrets-652d868f-c35d-4d2e-8228-0d81044d2c17 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:40.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9682" for this suite.

• [SLOW TEST:6.269 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":173,"skipped":3222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:40.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 06:04:40.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9636 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 27 06:04:40.954: INFO: stderr: ""
May 27 06:04:40.954: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 27 06:04:46.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9636 get pod e2e-test-httpd-pod -o json'
May 27 06:04:46.168: INFO: stderr: ""
May 27 06:04:46.168: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-05-27T06:04:40Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9636\",\n        \"resourceVersion\": \"22247\",\n        \"uid\": \"8c8f9643-f4d4-4b8b-b72b-3c460f613b83\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-8pdq2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ha9zeyohpei4-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-8pdq2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T06:04:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T06:04:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T06:04:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-27T06:04:40Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://5caa031de2cff88c9955fee7dea3deda6ecd572d377d279600b9980f397f7b1b\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-05-27T06:04:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.191\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.237\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.237\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-05-27T06:04:40Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 27 06:04:46.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9636 replace -f -'
May 27 06:04:46.600: INFO: stderr: ""
May 27 06:04:46.600: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
May 27 06:04:46.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9636 delete pods e2e-test-httpd-pod'
May 27 06:04:49.024: INFO: stderr: ""
May 27 06:04:49.024: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:49.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9636" for this suite.

• [SLOW TEST:8.309 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":174,"skipped":3249,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:49.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:04:49.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 06:04:51.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 6, 4, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 4, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 6, 4, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 4, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:04:54.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 27 06:04:59.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=webhook-7569 attach --namespace=webhook-7569 to-be-attached-pod -i -c=container1'
May 27 06:04:59.221: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:59.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7569" for this suite.
STEP: Destroying namespace "webhook-7569-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.372 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":175,"skipped":3260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:59.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
May 27 06:04:59.510: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5944 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:04:59.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5944" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":176,"skipped":3300,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:04:59.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 06:04:59.796: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 06:05:59.867: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:05:59.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 27 06:06:01.984: INFO: found a healthy node: ha9zeyohpei4-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:06:14.170: INFO: pods created so far: [1 1 1]
May 27 06:06:14.171: INFO: length of pods created so far: 3
May 27 06:06:16.206: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:06:23.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7895" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:06:23.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3593" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:83.777 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":177,"skipped":3304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:06:23.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 27 06:06:23.620: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:06:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-236" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":178,"skipped":3350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:06:23.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 06:06:23.785: INFO: The status of Pod labelsupdatee5dbcdb4-2c12-47f0-8490-45c3c5adf60b is Pending, waiting for it to be Running (with Ready = true)
May 27 06:06:25.799: INFO: The status of Pod labelsupdatee5dbcdb4-2c12-47f0-8490-45c3c5adf60b is Running (Ready = true)
May 27 06:06:26.375: INFO: Successfully updated pod "labelsupdatee5dbcdb4-2c12-47f0-8490-45c3c5adf60b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:06:28.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2820" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:06:28.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May 27 06:06:38.811: INFO: The status of Pod kube-controller-manager-ha9zeyohpei4-2 is Running (Ready = true)
May 27 06:06:38.934: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:06:38.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7882" for this suite.

• [SLOW TEST:10.461 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":180,"skipped":3430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:06:38.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8337
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-8337
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8337
May 27 06:06:39.041: INFO: Found 0 stateful pods, waiting for 1
May 27 06:06:49.057: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 27 06:06:49.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 06:06:49.371: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 06:06:49.371: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 06:06:49.371: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 06:06:49.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 27 06:06:59.418: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 06:06:59.423: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:06:59.478: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 27 06:06:59.479: INFO: ss-0  ha9zeyohpei4-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:39 +0000 UTC  }]
May 27 06:06:59.480: INFO: 
May 27 06:06:59.481: INFO: StatefulSet ss has not reached scale 3, at 1
May 27 06:07:00.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988112755s
May 27 06:07:01.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973308316s
May 27 06:07:02.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96254802s
May 27 06:07:03.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.948758877s
May 27 06:07:04.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.93643076s
May 27 06:07:05.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.923501733s
May 27 06:07:06.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907094087s
May 27 06:07:07.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.8948342s
May 27 06:07:08.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 881.011845ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8337
May 27 06:07:09.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 06:07:09.880: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 06:07:09.880: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 06:07:09.880: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 06:07:09.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 06:07:10.139: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 27 06:07:10.140: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 06:07:10.140: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 06:07:10.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 06:07:10.366: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 27 06:07:10.366: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 06:07:10.366: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 27 06:07:10.375: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:07:10.375: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:07:10.375: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 27 06:07:10.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 06:07:10.622: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 06:07:10.622: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 06:07:10.622: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 06:07:10.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 06:07:10.841: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 06:07:10.841: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 06:07:10.841: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 06:07:10.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-8337 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 06:07:11.091: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 06:07:11.091: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 06:07:11.091: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 06:07:11.091: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:07:11.100: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 27 06:07:21.120: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 27 06:07:21.120: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 27 06:07:21.120: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 27 06:07:21.176: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May 27 06:07:21.176: INFO: ss-0  ha9zeyohpei4-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:39 +0000 UTC  }]
May 27 06:07:21.176: INFO: ss-1  ha9zeyohpei4-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:59 +0000 UTC  }]
May 27 06:07:21.176: INFO: ss-2  ha9zeyohpei4-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:07:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:06:59 +0000 UTC  }]
May 27 06:07:21.176: INFO: 
May 27 06:07:21.176: INFO: StatefulSet ss has not reached scale 0, at 3
May 27 06:07:22.188: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.987522756s
May 27 06:07:23.199: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.97555606s
May 27 06:07:24.210: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.964150299s
May 27 06:07:25.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.952025243s
May 27 06:07:26.230: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.943577944s
May 27 06:07:27.240: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.933550988s
May 27 06:07:28.252: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.923342461s
May 27 06:07:29.262: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.910284389s
May 27 06:07:30.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 900.387477ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8337
May 27 06:07:31.280: INFO: Scaling statefulset ss to 0
May 27 06:07:31.301: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 06:07:31.307: INFO: Deleting all statefulset in ns statefulset-8337
May 27 06:07:31.312: INFO: Scaling statefulset ss to 0
May 27 06:07:31.329: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:07:31.334: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:07:31.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8337" for this suite.

• [SLOW TEST:52.418 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":181,"skipped":3463,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:07:31.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc in namespace container-probe-1009
May 27 06:07:33.469: INFO: Started pod liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc in namespace container-probe-1009
STEP: checking the pod's current state and verifying that restartCount is present
May 27 06:07:33.477: INFO: Initial restart count of pod liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc is 0
May 27 06:07:53.613: INFO: Restart count of pod container-probe-1009/liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc is now 1 (20.135387662s elapsed)
May 27 06:08:13.743: INFO: Restart count of pod container-probe-1009/liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc is now 2 (40.265905317s elapsed)
May 27 06:08:33.873: INFO: Restart count of pod container-probe-1009/liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc is now 3 (1m0.395437608s elapsed)
May 27 06:08:54.003: INFO: Restart count of pod container-probe-1009/liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc is now 4 (1m20.525989443s elapsed)
May 27 06:09:54.422: INFO: Restart count of pod container-probe-1009/liveness-3831bf41-684d-43eb-971b-a4a0f06cabfc is now 5 (2m20.945054861s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:09:54.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1009" for this suite.

• [SLOW TEST:143.108 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":182,"skipped":3478,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:09:54.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
May 27 06:09:54.556: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 06:09:59.573: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:09:59.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3559" for this suite.

• [SLOW TEST:5.187 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":183,"skipped":3485,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:09:59.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
May 27 06:09:59.800: INFO: Pod name sample-pod: Found 0 pods out of 3
May 27 06:10:04.822: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
May 27 06:10:04.831: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:04.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1952" for this suite.

• [SLOW TEST:5.261 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":184,"skipped":3497,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:04.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:10:07.098: INFO: Deleting pod "var-expansion-fd8d3d49-d9d0-406f-b937-4b31af6cbb0c" in namespace "var-expansion-2009"
May 27 06:10:07.115: INFO: Wait up to 5m0s for pod "var-expansion-fd8d3d49-d9d0-406f-b937-4b31af6cbb0c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:09.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2009" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":185,"skipped":3503,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:09.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-2367
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2367 to expose endpoints map[]
May 27 06:10:09.300: INFO: successfully validated that service multi-endpoint-test in namespace services-2367 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2367
May 27 06:10:09.322: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:10:11.336: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:10:13.335: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2367 to expose endpoints map[pod1:[100]]
May 27 06:10:13.360: INFO: successfully validated that service multi-endpoint-test in namespace services-2367 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2367
May 27 06:10:13.377: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:10:15.391: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2367 to expose endpoints map[pod1:[100] pod2:[101]]
May 27 06:10:15.425: INFO: successfully validated that service multi-endpoint-test in namespace services-2367 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
May 27 06:10:15.425: INFO: Creating new exec pod
May 27 06:10:18.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-2367 exec execpodq75vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 27 06:10:18.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May 27 06:10:18.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:10:18.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-2367 exec execpodq75vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.51 80'
May 27 06:10:19.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.51 80\nConnection to 10.233.3.51 80 port [tcp/http] succeeded!\n"
May 27 06:10:19.002: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:10:19.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-2367 exec execpodq75vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May 27 06:10:19.278: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May 27 06:10:19.278: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:10:19.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-2367 exec execpodq75vr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.3.51 81'
May 27 06:10:19.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.3.51 81\nConnection to 10.233.3.51 81 port [tcp/*] succeeded!\n"
May 27 06:10:19.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2367
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2367 to expose endpoints map[pod2:[101]]
May 27 06:10:20.584: INFO: successfully validated that service multi-endpoint-test in namespace services-2367 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2367
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2367 to expose endpoints map[]
May 27 06:10:21.712: INFO: successfully validated that service multi-endpoint-test in namespace services-2367 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:21.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2367" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.607 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":186,"skipped":3516,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:21.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:34.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1877" for this suite.
STEP: Destroying namespace "nsdeletetest-7894" for this suite.
May 27 06:10:35.012: INFO: Namespace nsdeletetest-7894 was already deleted
STEP: Destroying namespace "nsdeletetest-5396" for this suite.

• [SLOW TEST:13.224 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":187,"skipped":3525,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:35.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:10:36.245: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:10:39.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:10:39.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5671-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:42.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6434" for this suite.
STEP: Destroying namespace "webhook-6434-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.670 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":188,"skipped":3533,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:42.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 27 06:10:42.785: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.785: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.812: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.812: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.849: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.849: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.952: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:42.952: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 27 06:10:44.254: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 27 06:10:44.254: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 27 06:10:44.568: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 27 06:10:44.595: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 27 06:10:44.597: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.598: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.598: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.598: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.598: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.598: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.599: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.599: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 0
May 27 06:10:44.599: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:44.599: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:44.599: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.599: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.600: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.600: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.631: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.631: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.665: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.665: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.726: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.726: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:44.750: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:44.750: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:46.838: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:46.838: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:46.903: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
STEP: listing Deployments
May 27 06:10:46.920: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 27 06:10:46.947: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 27 06:10:46.962: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:46.988: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:47.041: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:47.153: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:47.197: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:47.214: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:48.617: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:49.312: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:49.415: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:49.448: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 27 06:10:50.615: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 27 06:10:50.752: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:50.752: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:50.752: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:50.753: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:50.753: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:50.753: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 1
May 27 06:10:50.753: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:50.754: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 3
May 27 06:10:50.754: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 3
May 27 06:10:50.754: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 2
May 27 06:10:50.754: INFO: observed Deployment test-deployment in namespace deployment-2418 with ReadyReplicas 3
STEP: deleting the Deployment
May 27 06:10:50.771: INFO: observed event type MODIFIED
May 27 06:10:50.771: INFO: observed event type MODIFIED
May 27 06:10:50.771: INFO: observed event type MODIFIED
May 27 06:10:50.772: INFO: observed event type MODIFIED
May 27 06:10:50.772: INFO: observed event type MODIFIED
May 27 06:10:50.772: INFO: observed event type MODIFIED
May 27 06:10:50.772: INFO: observed event type MODIFIED
May 27 06:10:50.772: INFO: observed event type MODIFIED
May 27 06:10:50.773: INFO: observed event type MODIFIED
May 27 06:10:50.773: INFO: observed event type MODIFIED
May 27 06:10:50.773: INFO: observed event type MODIFIED
May 27 06:10:50.773: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 06:10:50.781: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:50.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2418" for this suite.

• [SLOW TEST:8.166 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":189,"skipped":3550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:50.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:10:51.638: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 06:10:53.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 6, 10, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 10, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 6, 10, 51, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 10, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:10:56.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:56.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-294" for this suite.
STEP: Destroying namespace "webhook-294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.077 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":190,"skipped":3606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:56.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
May 27 06:10:57.063: INFO: Creating simple deployment test-deployment-2jfbj
May 27 06:10:57.086: INFO: new replicaset for deployment "test-deployment-2jfbj" is yet to be created
STEP: Getting /status
May 27 06:10:59.136: INFO: Deployment test-deployment-2jfbj has Conditions: [{Available True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2jfbj-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
May 27 06:10:59.152: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 6, 10, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 10, 58, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 6, 10, 58, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 10, 57, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2jfbj-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
May 27 06:10:59.156: INFO: Observed &Deployment event: ADDED
May 27 06:10:59.156: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2jfbj-764bc7c4b7"}
May 27 06:10:59.157: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.158: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2jfbj-764bc7c4b7"}
May 27 06:10:59.158: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 06:10:59.159: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.159: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 06:10:59.159: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2jfbj-764bc7c4b7" is progressing.}
May 27 06:10:59.160: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.160: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 06:10:59.160: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2jfbj-764bc7c4b7" has successfully progressed.}
May 27 06:10:59.161: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.161: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 06:10:59.161: INFO: Observed Deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2jfbj-764bc7c4b7" has successfully progressed.}
May 27 06:10:59.161: INFO: Found Deployment test-deployment-2jfbj in namespace deployment-1434 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 06:10:59.162: INFO: Deployment test-deployment-2jfbj has an updated status
STEP: patching the Statefulset Status
May 27 06:10:59.162: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 27 06:10:59.173: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
May 27 06:10:59.178: INFO: Observed &Deployment event: ADDED
May 27 06:10:59.178: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2jfbj-764bc7c4b7"}
May 27 06:10:59.179: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.179: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2jfbj-764bc7c4b7"}
May 27 06:10:59.179: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 06:10:59.180: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.180: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 27 06:10:59.180: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:57 +0000 UTC 2022-05-27 06:10:57 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2jfbj-764bc7c4b7" is progressing.}
May 27 06:10:59.181: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.181: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 06:10:59.181: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2jfbj-764bc7c4b7" has successfully progressed.}
May 27 06:10:59.182: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.182: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:58 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 27 06:10:59.182: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-27 06:10:58 +0000 UTC 2022-05-27 06:10:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2jfbj-764bc7c4b7" has successfully progressed.}
May 27 06:10:59.183: INFO: Observed deployment test-deployment-2jfbj in namespace deployment-1434 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 27 06:10:59.183: INFO: Observed &Deployment event: MODIFIED
May 27 06:10:59.183: INFO: Found deployment test-deployment-2jfbj in namespace deployment-1434 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May 27 06:10:59.183: INFO: Deployment test-deployment-2jfbj has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 06:10:59.196: INFO: Deployment "test-deployment-2jfbj":
&Deployment{ObjectMeta:{test-deployment-2jfbj  deployment-1434  e0ade3fd-1c1a-4a3e-9c61-d058ffa188fe 24474 1 2022-05-27 06:10:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-05-27 06:10:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-05-27 06:10:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004108348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 06:10:59.209: INFO: New ReplicaSet "test-deployment-2jfbj-764bc7c4b7" of Deployment "test-deployment-2jfbj":
&ReplicaSet{ObjectMeta:{test-deployment-2jfbj-764bc7c4b7  deployment-1434  f138947a-5141-41c5-9d18-bf1e5d066c94 24471 1 2022-05-27 06:10:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2jfbj e0ade3fd-1c1a-4a3e-9c61-d058ffa188fe 0xc0041086e0 0xc0041086e1}] []  [{kube-controller-manager Update apps/v1 2022-05-27 06:10:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0ade3fd-1c1a-4a3e-9c61-d058ffa188fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:10:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004108788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 06:10:59.216: INFO: Pod "test-deployment-2jfbj-764bc7c4b7-2vvhd" is available:
&Pod{ObjectMeta:{test-deployment-2jfbj-764bc7c4b7-2vvhd test-deployment-2jfbj-764bc7c4b7- deployment-1434  adca534a-0fbf-4ec1-91b0-3cfd6dd19c4b 24470 0 2022-05-27 06:10:57 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [{apps/v1 ReplicaSet test-deployment-2jfbj-764bc7c4b7 f138947a-5141-41c5-9d18-bf1e5d066c94 0xc004108b40 0xc004108b41}] []  [{kube-controller-manager Update v1 2022-05-27 06:10:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f138947a-5141-41c5-9d18-bf1e5d066c94\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 06:10:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djvl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djvl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:10:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:10:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:10:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:10.233.66.159,StartTime:2022-05-27 06:10:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 06:10:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2e168feffb68b9f24d1dd8a99e595b2f952eaaf37ae3a3bbbe4a415642ab1aab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:10:59.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1434" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":191,"skipped":3635,"failed":0}
S
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:10:59.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
May 27 06:10:59.323: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:11:01.332: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.191 on the node which pod1 resides and expect scheduled
May 27 06:11:01.346: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:11:03.363: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.191 but use UDP protocol on the node which pod2 resides
May 27 06:11:03.384: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:11:05.400: INFO: The status of Pod pod3 is Running (Ready = true)
May 27 06:11:05.432: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 27 06:11:07.449: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
May 27 06:11:07.455: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.191 http://127.0.0.1:54323/hostname] Namespace:hostport-3335 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:11:07.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:11:07.457: INFO: ExecWithOptions: Clientset creation
May 27 06:11:07.457: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3335/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.191+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.191, port: 54323
May 27 06:11:07.577: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.191:54323/hostname] Namespace:hostport-3335 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:11:07.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:11:07.579: INFO: ExecWithOptions: Clientset creation
May 27 06:11:07.579: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3335/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.191%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.191, port: 54323 UDP
May 27 06:11:07.671: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.121.191 54323] Namespace:hostport-3335 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:11:07.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:11:07.672: INFO: ExecWithOptions: Clientset creation
May 27 06:11:07.673: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-3335/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+192.168.121.191+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:11:12.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3335" for this suite.

• [SLOW TEST:13.561 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":192,"skipped":3636,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:11:12.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:11:12.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 27 06:11:16.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-7291 --namespace=crd-publish-openapi-7291 create -f -'
May 27 06:11:18.105: INFO: stderr: ""
May 27 06:11:18.106: INFO: stdout: "e2e-test-crd-publish-openapi-3161-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 27 06:11:18.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-7291 --namespace=crd-publish-openapi-7291 delete e2e-test-crd-publish-openapi-3161-crds test-cr'
May 27 06:11:18.481: INFO: stderr: ""
May 27 06:11:18.481: INFO: stdout: "e2e-test-crd-publish-openapi-3161-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 27 06:11:18.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-7291 --namespace=crd-publish-openapi-7291 apply -f -'
May 27 06:11:19.065: INFO: stderr: ""
May 27 06:11:19.065: INFO: stdout: "e2e-test-crd-publish-openapi-3161-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 27 06:11:19.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-7291 --namespace=crd-publish-openapi-7291 delete e2e-test-crd-publish-openapi-3161-crds test-cr'
May 27 06:11:19.211: INFO: stderr: ""
May 27 06:11:19.211: INFO: stdout: "e2e-test-crd-publish-openapi-3161-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 27 06:11:19.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=crd-publish-openapi-7291 explain e2e-test-crd-publish-openapi-3161-crds'
May 27 06:11:19.519: INFO: stderr: ""
May 27 06:11:19.519: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3161-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:11:23.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7291" for this suite.

• [SLOW TEST:10.296 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":193,"skipped":3637,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:11:23.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:13:01.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5343" for this suite.

• [SLOW TEST:98.249 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":194,"skipped":3648,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:13:01.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 27 06:13:03.604: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8281 PodName:var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:13:03.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:13:03.607: INFO: ExecWithOptions: Clientset creation
May 27 06:13:03.607: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8281/pods/var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
May 27 06:13:03.751: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8281 PodName:var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:13:03.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:13:03.752: INFO: ExecWithOptions: Clientset creation
May 27 06:13:03.752: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8281/pods/var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
May 27 06:13:04.407: INFO: Successfully updated pod "var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 27 06:13:04.416: INFO: Deleting pod "var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85" in namespace "var-expansion-8281"
May 27 06:13:04.447: INFO: Wait up to 5m0s for pod "var-expansion-0bd84933-b5ae-4ce7-b8ff-71e066162c85" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:13:38.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8281" for this suite.

• [SLOW TEST:37.143 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":195,"skipped":3652,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:13:38.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-1c0265b0-1c9f-4fe1-8c82-eb785a386155
STEP: Creating configMap with name cm-test-opt-upd-7f6df6a1-37e2-4203-b3ee-27945cee9bf7
STEP: Creating the pod
May 27 06:13:38.610: INFO: The status of Pod pod-projected-configmaps-627ca449-acbf-45db-823c-15c142b5735e is Pending, waiting for it to be Running (with Ready = true)
May 27 06:13:40.621: INFO: The status of Pod pod-projected-configmaps-627ca449-acbf-45db-823c-15c142b5735e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-1c0265b0-1c9f-4fe1-8c82-eb785a386155
STEP: Updating configmap cm-test-opt-upd-7f6df6a1-37e2-4203-b3ee-27945cee9bf7
STEP: Creating configMap with name cm-test-opt-create-875c531d-5ad6-4ba4-87ee-c150ead7c829
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:13:42.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7330" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":196,"skipped":3656,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:13:42.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 27 06:13:42.893: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 27 06:13:42.933: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:13:42.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4444" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":197,"skipped":3667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:13:43.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1932.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1932.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:13:47.145: INFO: DNS probes using dns-test-5e1ed85d-04d1-49cd-9467-59b125a74cf0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1932.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1932.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:13:51.282: INFO: File wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:13:51.291: INFO: File jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:13:51.291: INFO: Lookups using dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 failed for: [wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local]

May 27 06:13:56.301: INFO: File wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:13:56.308: INFO: File jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:13:56.308: INFO: Lookups using dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 failed for: [wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local]

May 27 06:14:01.299: INFO: File wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:14:01.306: INFO: File jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:14:01.306: INFO: Lookups using dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 failed for: [wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local]

May 27 06:14:06.310: INFO: File wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains '' instead of 'bar.example.com.'
May 27 06:14:06.318: INFO: File jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:14:06.318: INFO: Lookups using dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 failed for: [wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local]

May 27 06:14:11.303: INFO: File wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:14:11.311: INFO: File jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local from pod  dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 contains 'foo.example.com.
' instead of 'bar.example.com.'
May 27 06:14:11.311: INFO: Lookups using dns-1932/dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 failed for: [wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local]

May 27 06:14:16.308: INFO: DNS probes using dns-test-1b2b04da-9a31-494c-aac8-951cb9d335c3 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1932.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1932.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1932.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1932.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:14:20.573: INFO: DNS probes using dns-test-51aef277-abc1-41e4-9c32-c46673a40876 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:14:20.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1932" for this suite.

• [SLOW TEST:37.662 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":198,"skipped":3702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:14:20.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:14:20.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415" in namespace "downward-api-2287" to be "Succeeded or Failed"
May 27 06:14:20.829: INFO: Pod "downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415": Phase="Pending", Reason="", readiness=false. Elapsed: 14.561495ms
May 27 06:14:22.841: INFO: Pod "downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026668377s
May 27 06:14:24.849: INFO: Pod "downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035444404s
STEP: Saw pod success
May 27 06:14:24.850: INFO: Pod "downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415" satisfied condition "Succeeded or Failed"
May 27 06:14:24.855: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415 container client-container: <nil>
STEP: delete the pod
May 27 06:14:24.886: INFO: Waiting for pod downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415 to disappear
May 27 06:14:24.891: INFO: Pod downwardapi-volume-f1a0d802-0917-4d09-acc5-9ff541937415 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:14:24.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2287" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:14:24.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
May 27 06:14:24.970: INFO: Waiting up to 5m0s for pod "pod-aa1391d3-fa75-4278-9e0e-69c448aa4921" in namespace "emptydir-4376" to be "Succeeded or Failed"
May 27 06:14:24.979: INFO: Pod "pod-aa1391d3-fa75-4278-9e0e-69c448aa4921": Phase="Pending", Reason="", readiness=false. Elapsed: 9.140061ms
May 27 06:14:27.000: INFO: Pod "pod-aa1391d3-fa75-4278-9e0e-69c448aa4921": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030137706s
May 27 06:14:29.016: INFO: Pod "pod-aa1391d3-fa75-4278-9e0e-69c448aa4921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046043452s
STEP: Saw pod success
May 27 06:14:29.016: INFO: Pod "pod-aa1391d3-fa75-4278-9e0e-69c448aa4921" satisfied condition "Succeeded or Failed"
May 27 06:14:29.021: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-aa1391d3-fa75-4278-9e0e-69c448aa4921 container test-container: <nil>
STEP: delete the pod
May 27 06:14:29.056: INFO: Waiting for pod pod-aa1391d3-fa75-4278-9e0e-69c448aa4921 to disappear
May 27 06:14:29.079: INFO: Pod pod-aa1391d3-fa75-4278-9e0e-69c448aa4921 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:14:29.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4376" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":200,"skipped":3760,"failed":0}
SSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:14:29.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 06:14:29.300: INFO: The status of Pod pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e is Pending, waiting for it to be Running (with Ready = true)
May 27 06:14:31.318: INFO: The status of Pod pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 27 06:14:31.855: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e"
May 27 06:14:31.856: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e" in namespace "pods-6460" to be "terminated due to deadline exceeded"
May 27 06:14:31.860: INFO: Pod "pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e": Phase="Running", Reason="", readiness=true. Elapsed: 4.129775ms
May 27 06:14:33.878: INFO: Pod "pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e": Phase="Running", Reason="", readiness=true. Elapsed: 2.022492127s
May 27 06:14:35.887: INFO: Pod "pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.031003795s
May 27 06:14:35.887: INFO: Pod "pod-update-activedeadlineseconds-c050109e-b1e1-4f7f-ad54-8aea11f9458e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:14:35.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6460" for this suite.

• [SLOW TEST:6.801 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":201,"skipped":3765,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:14:35.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2, will wait for the garbage collector to delete the pods
May 27 06:14:38.085: INFO: Deleting Job.batch foo took: 20.851796ms
May 27 06:14:38.186: INFO: Terminating Job.batch foo pods took: 101.045659ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:15:11.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2" for this suite.

• [SLOW TEST:35.118 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":202,"skipped":3767,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:15:11.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-bcpv
STEP: Creating a pod to test atomic-volume-subpath
May 27 06:15:11.127: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bcpv" in namespace "subpath-107" to be "Succeeded or Failed"
May 27 06:15:11.135: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.945089ms
May 27 06:15:13.167: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.039880518s
May 27 06:15:15.196: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 4.069530867s
May 27 06:15:17.209: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 6.082233837s
May 27 06:15:19.222: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 8.094858955s
May 27 06:15:21.233: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 10.106199588s
May 27 06:15:23.256: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 12.12885576s
May 27 06:15:25.267: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 14.139924113s
May 27 06:15:27.289: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 16.16244734s
May 27 06:15:29.309: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 18.182347817s
May 27 06:15:31.328: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=true. Elapsed: 20.201072623s
May 27 06:15:33.347: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Running", Reason="", readiness=false. Elapsed: 22.219988783s
May 27 06:15:35.358: INFO: Pod "pod-subpath-test-configmap-bcpv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.230924326s
STEP: Saw pod success
May 27 06:15:35.358: INFO: Pod "pod-subpath-test-configmap-bcpv" satisfied condition "Succeeded or Failed"
May 27 06:15:35.365: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-subpath-test-configmap-bcpv container test-container-subpath-configmap-bcpv: <nil>
STEP: delete the pod
May 27 06:15:35.401: INFO: Waiting for pod pod-subpath-test-configmap-bcpv to disappear
May 27 06:15:35.406: INFO: Pod pod-subpath-test-configmap-bcpv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bcpv
May 27 06:15:35.408: INFO: Deleting pod "pod-subpath-test-configmap-bcpv" in namespace "subpath-107"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:15:35.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-107" for this suite.

• [SLOW TEST:24.423 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":203,"skipped":3770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:15:35.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:15:35.503: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd657094-5830-4b44-8712-967697b72992" in namespace "downward-api-9350" to be "Succeeded or Failed"
May 27 06:15:35.509: INFO: Pod "downwardapi-volume-fd657094-5830-4b44-8712-967697b72992": Phase="Pending", Reason="", readiness=false. Elapsed: 5.391316ms
May 27 06:15:37.529: INFO: Pod "downwardapi-volume-fd657094-5830-4b44-8712-967697b72992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026013647s
May 27 06:15:39.547: INFO: Pod "downwardapi-volume-fd657094-5830-4b44-8712-967697b72992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043214503s
STEP: Saw pod success
May 27 06:15:39.547: INFO: Pod "downwardapi-volume-fd657094-5830-4b44-8712-967697b72992" satisfied condition "Succeeded or Failed"
May 27 06:15:39.552: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-fd657094-5830-4b44-8712-967697b72992 container client-container: <nil>
STEP: delete the pod
May 27 06:15:39.588: INFO: Waiting for pod downwardapi-volume-fd657094-5830-4b44-8712-967697b72992 to disappear
May 27 06:15:39.593: INFO: Pod downwardapi-volume-fd657094-5830-4b44-8712-967697b72992 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:15:39.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9350" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3793,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:15:39.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
May 27 06:15:39.702: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
May 27 06:15:41.768: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:15:43.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-7856" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":205,"skipped":3806,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:15:43.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 27 06:15:43.865: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:15:48.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8198" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":206,"skipped":3814,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:15:48.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:15:48.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5350" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":207,"skipped":3820,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:15:48.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
May 27 06:16:29.507: INFO: The status of Pod kube-controller-manager-ha9zeyohpei4-2 is Running (Ready = true)
May 27 06:16:29.624: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 27 06:16:29.625: INFO: Deleting pod "simpletest.rc-2fmbz" in namespace "gc-2676"
May 27 06:16:29.648: INFO: Deleting pod "simpletest.rc-2hcxg" in namespace "gc-2676"
May 27 06:16:29.689: INFO: Deleting pod "simpletest.rc-2v9g8" in namespace "gc-2676"
May 27 06:16:29.751: INFO: Deleting pod "simpletest.rc-2xgk5" in namespace "gc-2676"
May 27 06:16:29.776: INFO: Deleting pod "simpletest.rc-2z487" in namespace "gc-2676"
May 27 06:16:29.806: INFO: Deleting pod "simpletest.rc-45l6m" in namespace "gc-2676"
May 27 06:16:29.836: INFO: Deleting pod "simpletest.rc-475lm" in namespace "gc-2676"
May 27 06:16:29.872: INFO: Deleting pod "simpletest.rc-4nsq5" in namespace "gc-2676"
May 27 06:16:29.929: INFO: Deleting pod "simpletest.rc-4rsn2" in namespace "gc-2676"
May 27 06:16:29.970: INFO: Deleting pod "simpletest.rc-4w9k4" in namespace "gc-2676"
May 27 06:16:30.034: INFO: Deleting pod "simpletest.rc-4zcqx" in namespace "gc-2676"
May 27 06:16:30.111: INFO: Deleting pod "simpletest.rc-5dhdh" in namespace "gc-2676"
May 27 06:16:30.263: INFO: Deleting pod "simpletest.rc-5mfnd" in namespace "gc-2676"
May 27 06:16:30.320: INFO: Deleting pod "simpletest.rc-6j5x4" in namespace "gc-2676"
May 27 06:16:30.386: INFO: Deleting pod "simpletest.rc-7kc82" in namespace "gc-2676"
May 27 06:16:30.567: INFO: Deleting pod "simpletest.rc-7q42v" in namespace "gc-2676"
May 27 06:16:30.733: INFO: Deleting pod "simpletest.rc-826s7" in namespace "gc-2676"
May 27 06:16:30.790: INFO: Deleting pod "simpletest.rc-8n4mn" in namespace "gc-2676"
May 27 06:16:30.892: INFO: Deleting pod "simpletest.rc-8qg68" in namespace "gc-2676"
May 27 06:16:31.013: INFO: Deleting pod "simpletest.rc-8tqjj" in namespace "gc-2676"
May 27 06:16:31.388: INFO: Deleting pod "simpletest.rc-9bqj8" in namespace "gc-2676"
May 27 06:16:31.527: INFO: Deleting pod "simpletest.rc-9bvz8" in namespace "gc-2676"
May 27 06:16:31.587: INFO: Deleting pod "simpletest.rc-9hp59" in namespace "gc-2676"
May 27 06:16:31.759: INFO: Deleting pod "simpletest.rc-9sfwz" in namespace "gc-2676"
May 27 06:16:31.846: INFO: Deleting pod "simpletest.rc-9w8qp" in namespace "gc-2676"
May 27 06:16:32.014: INFO: Deleting pod "simpletest.rc-b9j6n" in namespace "gc-2676"
May 27 06:16:32.200: INFO: Deleting pod "simpletest.rc-bcg65" in namespace "gc-2676"
May 27 06:16:32.376: INFO: Deleting pod "simpletest.rc-bfghl" in namespace "gc-2676"
May 27 06:16:32.448: INFO: Deleting pod "simpletest.rc-bfvks" in namespace "gc-2676"
May 27 06:16:32.559: INFO: Deleting pod "simpletest.rc-bnzxj" in namespace "gc-2676"
May 27 06:16:32.651: INFO: Deleting pod "simpletest.rc-cpd97" in namespace "gc-2676"
May 27 06:16:32.978: INFO: Deleting pod "simpletest.rc-cw7hh" in namespace "gc-2676"
May 27 06:16:33.227: INFO: Deleting pod "simpletest.rc-d4q5j" in namespace "gc-2676"
May 27 06:16:33.428: INFO: Deleting pod "simpletest.rc-d7k4w" in namespace "gc-2676"
May 27 06:16:33.550: INFO: Deleting pod "simpletest.rc-dv75l" in namespace "gc-2676"
May 27 06:16:33.641: INFO: Deleting pod "simpletest.rc-f9l7x" in namespace "gc-2676"
May 27 06:16:33.735: INFO: Deleting pod "simpletest.rc-fjhrr" in namespace "gc-2676"
May 27 06:16:33.897: INFO: Deleting pod "simpletest.rc-ft4n9" in namespace "gc-2676"
May 27 06:16:34.003: INFO: Deleting pod "simpletest.rc-fvdwk" in namespace "gc-2676"
May 27 06:16:34.086: INFO: Deleting pod "simpletest.rc-g86dv" in namespace "gc-2676"
May 27 06:16:34.226: INFO: Deleting pod "simpletest.rc-gxkg9" in namespace "gc-2676"
May 27 06:16:34.524: INFO: Deleting pod "simpletest.rc-gz87w" in namespace "gc-2676"
May 27 06:16:34.592: INFO: Deleting pod "simpletest.rc-h5qp8" in namespace "gc-2676"
May 27 06:16:34.712: INFO: Deleting pod "simpletest.rc-h7hff" in namespace "gc-2676"
May 27 06:16:34.838: INFO: Deleting pod "simpletest.rc-hdhdn" in namespace "gc-2676"
May 27 06:16:34.976: INFO: Deleting pod "simpletest.rc-hg2jx" in namespace "gc-2676"
May 27 06:16:35.140: INFO: Deleting pod "simpletest.rc-hqkb6" in namespace "gc-2676"
May 27 06:16:35.648: INFO: Deleting pod "simpletest.rc-jthqc" in namespace "gc-2676"
May 27 06:16:35.761: INFO: Deleting pod "simpletest.rc-jvkjp" in namespace "gc-2676"
May 27 06:16:35.871: INFO: Deleting pod "simpletest.rc-k7kp4" in namespace "gc-2676"
May 27 06:16:35.979: INFO: Deleting pod "simpletest.rc-krq4q" in namespace "gc-2676"
May 27 06:16:36.123: INFO: Deleting pod "simpletest.rc-ksp5x" in namespace "gc-2676"
May 27 06:16:36.242: INFO: Deleting pod "simpletest.rc-ktt8l" in namespace "gc-2676"
May 27 06:16:36.336: INFO: Deleting pod "simpletest.rc-kwkvs" in namespace "gc-2676"
May 27 06:16:36.413: INFO: Deleting pod "simpletest.rc-l54pg" in namespace "gc-2676"
May 27 06:16:36.490: INFO: Deleting pod "simpletest.rc-ljxsz" in namespace "gc-2676"
May 27 06:16:36.595: INFO: Deleting pod "simpletest.rc-lm6tk" in namespace "gc-2676"
May 27 06:16:36.759: INFO: Deleting pod "simpletest.rc-m9bwm" in namespace "gc-2676"
May 27 06:16:36.868: INFO: Deleting pod "simpletest.rc-mbg8l" in namespace "gc-2676"
May 27 06:16:37.012: INFO: Deleting pod "simpletest.rc-mvhd7" in namespace "gc-2676"
May 27 06:16:37.488: INFO: Deleting pod "simpletest.rc-mzkzq" in namespace "gc-2676"
May 27 06:16:37.775: INFO: Deleting pod "simpletest.rc-n944g" in namespace "gc-2676"
May 27 06:16:37.896: INFO: Deleting pod "simpletest.rc-njn9n" in namespace "gc-2676"
May 27 06:16:38.027: INFO: Deleting pod "simpletest.rc-nqxw9" in namespace "gc-2676"
May 27 06:16:38.278: INFO: Deleting pod "simpletest.rc-p5v4f" in namespace "gc-2676"
May 27 06:16:38.430: INFO: Deleting pod "simpletest.rc-pdv5z" in namespace "gc-2676"
May 27 06:16:38.604: INFO: Deleting pod "simpletest.rc-pnppx" in namespace "gc-2676"
May 27 06:16:38.755: INFO: Deleting pod "simpletest.rc-pw2nl" in namespace "gc-2676"
May 27 06:16:38.907: INFO: Deleting pod "simpletest.rc-qgp8n" in namespace "gc-2676"
May 27 06:16:39.025: INFO: Deleting pod "simpletest.rc-qn2ds" in namespace "gc-2676"
May 27 06:16:39.388: INFO: Deleting pod "simpletest.rc-qpd75" in namespace "gc-2676"
May 27 06:16:39.494: INFO: Deleting pod "simpletest.rc-qpkdh" in namespace "gc-2676"
May 27 06:16:39.638: INFO: Deleting pod "simpletest.rc-qtsvm" in namespace "gc-2676"
May 27 06:16:39.727: INFO: Deleting pod "simpletest.rc-r7bg8" in namespace "gc-2676"
May 27 06:16:39.806: INFO: Deleting pod "simpletest.rc-r94p8" in namespace "gc-2676"
May 27 06:16:39.838: INFO: Deleting pod "simpletest.rc-r9992" in namespace "gc-2676"
May 27 06:16:39.934: INFO: Deleting pod "simpletest.rc-rzhmn" in namespace "gc-2676"
May 27 06:16:39.999: INFO: Deleting pod "simpletest.rc-s5jwv" in namespace "gc-2676"
May 27 06:16:40.074: INFO: Deleting pod "simpletest.rc-s9lqt" in namespace "gc-2676"
May 27 06:16:40.177: INFO: Deleting pod "simpletest.rc-ss824" in namespace "gc-2676"
May 27 06:16:40.239: INFO: Deleting pod "simpletest.rc-ssx7t" in namespace "gc-2676"
May 27 06:16:40.288: INFO: Deleting pod "simpletest.rc-t2zlm" in namespace "gc-2676"
May 27 06:16:40.358: INFO: Deleting pod "simpletest.rc-tckc8" in namespace "gc-2676"
May 27 06:16:40.482: INFO: Deleting pod "simpletest.rc-tmdqz" in namespace "gc-2676"
May 27 06:16:40.520: INFO: Deleting pod "simpletest.rc-vb9rf" in namespace "gc-2676"
May 27 06:16:40.584: INFO: Deleting pod "simpletest.rc-vh787" in namespace "gc-2676"
May 27 06:16:40.658: INFO: Deleting pod "simpletest.rc-vhwq4" in namespace "gc-2676"
May 27 06:16:40.802: INFO: Deleting pod "simpletest.rc-vvv8s" in namespace "gc-2676"
May 27 06:16:40.900: INFO: Deleting pod "simpletest.rc-w5vkb" in namespace "gc-2676"
May 27 06:16:41.007: INFO: Deleting pod "simpletest.rc-w75mb" in namespace "gc-2676"
May 27 06:16:41.287: INFO: Deleting pod "simpletest.rc-w7brq" in namespace "gc-2676"
May 27 06:16:41.591: INFO: Deleting pod "simpletest.rc-wt7q2" in namespace "gc-2676"
May 27 06:16:41.706: INFO: Deleting pod "simpletest.rc-x4gkk" in namespace "gc-2676"
May 27 06:16:41.789: INFO: Deleting pod "simpletest.rc-x6m6w" in namespace "gc-2676"
May 27 06:16:41.890: INFO: Deleting pod "simpletest.rc-xnf7j" in namespace "gc-2676"
May 27 06:16:41.960: INFO: Deleting pod "simpletest.rc-xw4r6" in namespace "gc-2676"
May 27 06:16:42.055: INFO: Deleting pod "simpletest.rc-z7xx6" in namespace "gc-2676"
May 27 06:16:42.253: INFO: Deleting pod "simpletest.rc-zfr74" in namespace "gc-2676"
May 27 06:16:42.358: INFO: Deleting pod "simpletest.rc-zq6t7" in namespace "gc-2676"
May 27 06:16:42.464: INFO: Deleting pod "simpletest.rc-zvfth" in namespace "gc-2676"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:16:42.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2676" for this suite.

• [SLOW TEST:54.298 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":208,"skipped":3829,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:16:42.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
May 27 06:16:42.722: INFO: created test-event-1
May 27 06:16:42.745: INFO: created test-event-2
May 27 06:16:42.776: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 27 06:16:42.796: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 27 06:16:42.965: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:16:42.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1818" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":209,"skipped":3832,"failed":0}
SSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:16:43.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
May 27 06:16:43.221: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 27 06:16:45.229: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 27 06:16:47.232: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
May 27 06:16:47.260: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 27 06:16:49.273: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 27 06:16:51.277: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 27 06:16:51.283: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:51.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:51.285: INFO: ExecWithOptions: Clientset creation
May 27 06:16:51.285: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:51.436: INFO: Exec stderr: ""
May 27 06:16:51.436: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:51.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:51.438: INFO: ExecWithOptions: Clientset creation
May 27 06:16:51.438: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:51.556: INFO: Exec stderr: ""
May 27 06:16:51.556: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:51.560: INFO: ExecWithOptions: Clientset creation
May 27 06:16:51.560: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:51.684: INFO: Exec stderr: ""
May 27 06:16:51.684: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:51.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:51.685: INFO: ExecWithOptions: Clientset creation
May 27 06:16:51.685: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:51.801: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 27 06:16:51.802: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:51.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:51.804: INFO: ExecWithOptions: Clientset creation
May 27 06:16:51.804: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:51.900: INFO: Exec stderr: ""
May 27 06:16:51.901: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:51.904: INFO: ExecWithOptions: Clientset creation
May 27 06:16:51.904: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:52.007: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 27 06:16:52.007: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:52.009: INFO: ExecWithOptions: Clientset creation
May 27 06:16:52.009: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:52.176: INFO: Exec stderr: ""
May 27 06:16:52.176: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:52.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:52.178: INFO: ExecWithOptions: Clientset creation
May 27 06:16:52.178: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:52.280: INFO: Exec stderr: ""
May 27 06:16:52.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:52.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:52.282: INFO: ExecWithOptions: Clientset creation
May 27 06:16:52.282: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:52.401: INFO: Exec stderr: ""
May 27 06:16:52.401: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5261 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:16:52.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:16:52.403: INFO: ExecWithOptions: Clientset creation
May 27 06:16:52.403: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5261/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 27 06:16:52.481: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:16:52.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5261" for this suite.

• [SLOW TEST:9.486 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":210,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:16:52.517: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-4026
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-4026
May 27 06:16:52.603: INFO: Found 0 stateful pods, waiting for 1
May 27 06:17:02.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 06:17:02.699: INFO: Deleting all statefulset in ns statefulset-4026
May 27 06:17:02.719: INFO: Scaling statefulset ss to 0
May 27 06:17:12.771: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:17:12.776: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:12.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4026" for this suite.

• [SLOW TEST:20.337 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":211,"skipped":3853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 27 06:17:14.016: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:17:17.066: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:17:17.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:20.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7486" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.960 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":212,"skipped":3880,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:20.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 27 06:17:20.954: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5403  224d65d6-12e6-4ba1-8a27-d66478178049 28169 0 2022-05-27 06:17:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 06:17:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:17:20.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5403  224d65d6-12e6-4ba1-8a27-d66478178049 28171 0 2022-05-27 06:17:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 06:17:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 27 06:17:20.984: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5403  224d65d6-12e6-4ba1-8a27-d66478178049 28172 0 2022-05-27 06:17:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 06:17:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:17:20.991: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5403  224d65d6-12e6-4ba1-8a27-d66478178049 28173 0 2022-05-27 06:17:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-27 06:17:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:20.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5403" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":213,"skipped":3892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:21.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:31.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5919" for this suite.

• [SLOW TEST:10.111 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":214,"skipped":3920,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:31.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-dd3ccd9a-5dc0-48cc-8360-b5ea00e61bf6
STEP: Creating a pod to test consume configMaps
May 27 06:17:31.225: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb" in namespace "projected-189" to be "Succeeded or Failed"
May 27 06:17:31.244: INFO: Pod "pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.208158ms
May 27 06:17:33.268: INFO: Pod "pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043299222s
May 27 06:17:35.283: INFO: Pod "pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058043853s
STEP: Saw pod success
May 27 06:17:35.283: INFO: Pod "pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb" satisfied condition "Succeeded or Failed"
May 27 06:17:35.289: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb container agnhost-container: <nil>
STEP: delete the pod
May 27 06:17:35.341: INFO: Waiting for pod pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb to disappear
May 27 06:17:35.347: INFO: Pod pod-projected-configmaps-8b46c09b-dac9-4c69-9f69-8f0d806587bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:35.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-189" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":215,"skipped":3934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:35.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
May 27 06:17:36.595: INFO: The status of Pod kube-controller-manager-ha9zeyohpei4-2 is Running (Ready = true)
May 27 06:17:36.683: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-274" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":216,"skipped":3985,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:36.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 27 06:17:36.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5135 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
May 27 06:17:37.005: INFO: stderr: ""
May 27 06:17:37.005: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
May 27 06:17:37.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5135 delete pods e2e-test-httpd-pod'
May 27 06:17:39.900: INFO: stderr: ""
May 27 06:17:39.900: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:39.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5135" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":217,"skipped":3999,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:39.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:40.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1552" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":218,"skipped":4006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:40.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:44.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9251" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":4092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:44.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:17:45.688: INFO: Checking APIGroup: apiregistration.k8s.io
May 27 06:17:45.690: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 27 06:17:45.690: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May 27 06:17:45.690: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 27 06:17:45.690: INFO: Checking APIGroup: apps
May 27 06:17:45.691: INFO: PreferredVersion.GroupVersion: apps/v1
May 27 06:17:45.691: INFO: Versions found [{apps/v1 v1}]
May 27 06:17:45.691: INFO: apps/v1 matches apps/v1
May 27 06:17:45.691: INFO: Checking APIGroup: events.k8s.io
May 27 06:17:45.693: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 27 06:17:45.693: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 27 06:17:45.693: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 27 06:17:45.693: INFO: Checking APIGroup: authentication.k8s.io
May 27 06:17:45.694: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 27 06:17:45.694: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May 27 06:17:45.694: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 27 06:17:45.694: INFO: Checking APIGroup: authorization.k8s.io
May 27 06:17:45.696: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 27 06:17:45.696: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May 27 06:17:45.696: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 27 06:17:45.696: INFO: Checking APIGroup: autoscaling
May 27 06:17:45.698: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May 27 06:17:45.698: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 27 06:17:45.698: INFO: autoscaling/v2 matches autoscaling/v2
May 27 06:17:45.698: INFO: Checking APIGroup: batch
May 27 06:17:45.699: INFO: PreferredVersion.GroupVersion: batch/v1
May 27 06:17:45.699: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 27 06:17:45.699: INFO: batch/v1 matches batch/v1
May 27 06:17:45.699: INFO: Checking APIGroup: certificates.k8s.io
May 27 06:17:45.701: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 27 06:17:45.701: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May 27 06:17:45.701: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 27 06:17:45.702: INFO: Checking APIGroup: networking.k8s.io
May 27 06:17:45.703: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 27 06:17:45.703: INFO: Versions found [{networking.k8s.io/v1 v1}]
May 27 06:17:45.704: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 27 06:17:45.704: INFO: Checking APIGroup: policy
May 27 06:17:45.705: INFO: PreferredVersion.GroupVersion: policy/v1
May 27 06:17:45.705: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
May 27 06:17:45.705: INFO: policy/v1 matches policy/v1
May 27 06:17:45.706: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 27 06:17:45.707: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 27 06:17:45.707: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May 27 06:17:45.708: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 27 06:17:45.708: INFO: Checking APIGroup: storage.k8s.io
May 27 06:17:45.711: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 27 06:17:45.711: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 27 06:17:45.712: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 27 06:17:45.712: INFO: Checking APIGroup: admissionregistration.k8s.io
May 27 06:17:45.714: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 27 06:17:45.714: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May 27 06:17:45.714: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 27 06:17:45.714: INFO: Checking APIGroup: apiextensions.k8s.io
May 27 06:17:45.716: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 27 06:17:45.716: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May 27 06:17:45.716: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 27 06:17:45.716: INFO: Checking APIGroup: scheduling.k8s.io
May 27 06:17:45.718: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 27 06:17:45.718: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May 27 06:17:45.718: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 27 06:17:45.718: INFO: Checking APIGroup: coordination.k8s.io
May 27 06:17:45.721: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 27 06:17:45.721: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May 27 06:17:45.721: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 27 06:17:45.721: INFO: Checking APIGroup: node.k8s.io
May 27 06:17:45.725: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 27 06:17:45.725: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 27 06:17:45.725: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 27 06:17:45.725: INFO: Checking APIGroup: discovery.k8s.io
May 27 06:17:45.731: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 27 06:17:45.731: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
May 27 06:17:45.731: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 27 06:17:45.731: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 27 06:17:45.735: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May 27 06:17:45.735: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 27 06:17:45.735: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May 27 06:17:45.735: INFO: Checking APIGroup: cilium.io
May 27 06:17:45.737: INFO: PreferredVersion.GroupVersion: cilium.io/v2
May 27 06:17:45.737: INFO: Versions found [{cilium.io/v2 v2}]
May 27 06:17:45.737: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:45.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7173" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":220,"skipped":4143,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:17:45.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:49.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5638" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":221,"skipped":4156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:49.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:17:49.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710" in namespace "projected-7573" to be "Succeeded or Failed"
May 27 06:17:49.518: INFO: Pod "downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710": Phase="Pending", Reason="", readiness=false. Elapsed: 17.521961ms
May 27 06:17:51.534: INFO: Pod "downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710": Phase="Running", Reason="", readiness=true. Elapsed: 2.03297842s
May 27 06:17:53.556: INFO: Pod "downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710": Phase="Running", Reason="", readiness=false. Elapsed: 4.054937691s
May 27 06:17:55.565: INFO: Pod "downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06428286s
STEP: Saw pod success
May 27 06:17:55.565: INFO: Pod "downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710" satisfied condition "Succeeded or Failed"
May 27 06:17:55.572: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710 container client-container: <nil>
STEP: delete the pod
May 27 06:17:55.613: INFO: Waiting for pod downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710 to disappear
May 27 06:17:55.618: INFO: Pod downwardapi-volume-de9ccbd4-bac1-45a7-8ff1-9a6340e23710 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:55.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7573" for this suite.

• [SLOW TEST:6.287 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":4247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:55.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-1233/configmap-test-16f1d7e6-416e-4c53-be9a-a3e9aca79c3e
STEP: Creating a pod to test consume configMaps
May 27 06:17:55.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5" in namespace "configmap-1233" to be "Succeeded or Failed"
May 27 06:17:55.732: INFO: Pod "pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.18887ms
May 27 06:17:57.752: INFO: Pod "pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027524873s
May 27 06:17:59.766: INFO: Pod "pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040947086s
STEP: Saw pod success
May 27 06:17:59.766: INFO: Pod "pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5" satisfied condition "Succeeded or Failed"
May 27 06:17:59.773: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5 container env-test: <nil>
STEP: delete the pod
May 27 06:17:59.807: INFO: Waiting for pod pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5 to disappear
May 27 06:17:59.813: INFO: Pod pod-configmaps-07fc07fe-dce7-4bcd-8550-49db3a8e6cb5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:59.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1233" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:59.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 27 06:17:59.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4836  6415e49d-386b-457e-aa1c-d8c9290176d9 28639 0 2022-05-27 06:17:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-27 06:17:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 27 06:17:59.918: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4836  6415e49d-386b-457e-aa1c-d8c9290176d9 28640 0 2022-05-27 06:17:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-27 06:17:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:17:59.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4836" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":224,"skipped":4324,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:17:59.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 06:18:00.009: INFO: Waiting up to 5m0s for pod "downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a" in namespace "downward-api-1611" to be "Succeeded or Failed"
May 27 06:18:00.020: INFO: Pod "downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.404965ms
May 27 06:18:02.035: INFO: Pod "downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02591653s
May 27 06:18:04.049: INFO: Pod "downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040123965s
STEP: Saw pod success
May 27 06:18:04.050: INFO: Pod "downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a" satisfied condition "Succeeded or Failed"
May 27 06:18:04.055: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a container dapi-container: <nil>
STEP: delete the pod
May 27 06:18:04.089: INFO: Waiting for pod downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a to disappear
May 27 06:18:04.098: INFO: Pod downward-api-f5c68b9e-aff8-48cd-9138-dd24e5fcdc5a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:18:04.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1611" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4340,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:18:04.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
May 27 06:18:04.164: INFO: namespace kubectl-5454
May 27 06:18:04.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5454 create -f -'
May 27 06:18:05.968: INFO: stderr: ""
May 27 06:18:05.968: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 27 06:18:06.990: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:18:06.990: INFO: Found 0 / 1
May 27 06:18:07.983: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:18:07.983: INFO: Found 0 / 1
May 27 06:18:08.980: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:18:08.980: INFO: Found 1 / 1
May 27 06:18:08.980: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 27 06:18:08.986: INFO: Selector matched 1 pods for map[app:agnhost]
May 27 06:18:08.986: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 27 06:18:08.986: INFO: wait on agnhost-primary startup in kubectl-5454 
May 27 06:18:08.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5454 logs agnhost-primary-qfsrw agnhost-primary'
May 27 06:18:09.205: INFO: stderr: ""
May 27 06:18:09.205: INFO: stdout: "Paused\n"
STEP: exposing RC
May 27 06:18:09.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5454 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 27 06:18:09.386: INFO: stderr: ""
May 27 06:18:09.386: INFO: stdout: "service/rm2 exposed\n"
May 27 06:18:09.394: INFO: Service rm2 in namespace kubectl-5454 found.
STEP: exposing service
May 27 06:18:11.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-5454 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 27 06:18:11.563: INFO: stderr: ""
May 27 06:18:11.563: INFO: stdout: "service/rm3 exposed\n"
May 27 06:18:11.577: INFO: Service rm3 in namespace kubectl-5454 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:18:13.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5454" for this suite.

• [SLOW TEST:9.506 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":226,"skipped":4358,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:18:13.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 27 06:18:13.722: INFO: observed Pod pod-test in namespace pods-7086 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 27 06:18:13.729: INFO: observed Pod pod-test in namespace pods-7086 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC  }]
May 27 06:18:13.757: INFO: observed Pod pod-test in namespace pods-7086 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC  }]
May 27 06:18:15.790: INFO: Found Pod pod-test in namespace pods-7086 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-27 06:18:13 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 27 06:18:15.810: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 27 06:18:15.868: INFO: observed event type ADDED
May 27 06:18:15.868: INFO: observed event type MODIFIED
May 27 06:18:15.869: INFO: observed event type MODIFIED
May 27 06:18:15.869: INFO: observed event type MODIFIED
May 27 06:18:15.869: INFO: observed event type MODIFIED
May 27 06:18:15.869: INFO: observed event type MODIFIED
May 27 06:18:15.869: INFO: observed event type MODIFIED
May 27 06:18:17.810: INFO: observed event type MODIFIED
May 27 06:18:18.808: INFO: observed event type MODIFIED
May 27 06:18:18.837: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:18:18.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7086" for this suite.

• [SLOW TEST:5.304 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":227,"skipped":4365,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:18:18.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
May 27 06:18:19.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:18:47.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5607" for this suite.

• [SLOW TEST:28.985 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":228,"skipped":4368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:18:47.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-f7d4d177-94b9-4ac3-9919-824b79da32c5
STEP: Creating a pod to test consume configMaps
May 27 06:18:47.989: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f" in namespace "configmap-9058" to be "Succeeded or Failed"
May 27 06:18:47.998: INFO: Pod "pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.586939ms
May 27 06:18:50.016: INFO: Pod "pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027256613s
May 27 06:18:52.037: INFO: Pod "pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047966791s
STEP: Saw pod success
May 27 06:18:52.038: INFO: Pod "pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f" satisfied condition "Succeeded or Failed"
May 27 06:18:52.043: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f container agnhost-container: <nil>
STEP: delete the pod
May 27 06:18:52.096: INFO: Waiting for pod pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f to disappear
May 27 06:18:52.101: INFO: Pod pod-configmaps-6c9bac69-5249-45f0-87d4-0eac57549b4f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:18:52.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9058" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":229,"skipped":4405,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:18:52.124: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 06:18:52.174: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:18:57.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8774" for this suite.

• [SLOW TEST:5.151 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":230,"skipped":4425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:18:57.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 27 06:18:57.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 27 06:19:15.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:19:19.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:19:41.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1482" for this suite.

• [SLOW TEST:44.429 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":231,"skipped":4468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:19:41.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-8872e87f-d48d-4199-bee2-9e28e1befe7c
STEP: Creating a pod to test consume secrets
May 27 06:19:41.787: INFO: Waiting up to 5m0s for pod "pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449" in namespace "secrets-961" to be "Succeeded or Failed"
May 27 06:19:41.796: INFO: Pod "pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449": Phase="Pending", Reason="", readiness=false. Elapsed: 8.913874ms
May 27 06:19:43.808: INFO: Pod "pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021358109s
May 27 06:19:45.819: INFO: Pod "pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032121698s
STEP: Saw pod success
May 27 06:19:45.820: INFO: Pod "pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449" satisfied condition "Succeeded or Failed"
May 27 06:19:45.828: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449 container secret-volume-test: <nil>
STEP: delete the pod
May 27 06:19:45.865: INFO: Waiting for pod pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449 to disappear
May 27 06:19:45.871: INFO: Pod pod-secrets-a78269fa-6b3f-4bb6-9269-24dfca8a5449 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:19:45.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-961" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:19:45.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
May 27 06:19:48.005: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:19:50.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7573" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":233,"skipped":4527,"failed":0}
SSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:19:50.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May 27 06:19:50.369: INFO: running pods: 0 < 3
May 27 06:19:52.393: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:19:54.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6107" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":234,"skipped":4531,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:19:54.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-vll7
STEP: Creating a pod to test atomic-volume-subpath
May 27 06:19:54.496: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vll7" in namespace "subpath-3401" to be "Succeeded or Failed"
May 27 06:19:54.515: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.914814ms
May 27 06:19:56.526: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 2.030445385s
May 27 06:19:58.545: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 4.048761786s
May 27 06:20:00.558: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 6.062061263s
May 27 06:20:02.576: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 8.079692203s
May 27 06:20:04.590: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 10.094040605s
May 27 06:20:06.604: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 12.108010805s
May 27 06:20:08.617: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 14.12058265s
May 27 06:20:10.625: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 16.129182382s
May 27 06:20:12.647: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 18.150783266s
May 27 06:20:14.661: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=true. Elapsed: 20.164845477s
May 27 06:20:16.674: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Running", Reason="", readiness=false. Elapsed: 22.178287589s
May 27 06:20:18.687: INFO: Pod "pod-subpath-test-downwardapi-vll7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.191501665s
STEP: Saw pod success
May 27 06:20:18.687: INFO: Pod "pod-subpath-test-downwardapi-vll7" satisfied condition "Succeeded or Failed"
May 27 06:20:18.693: INFO: Trying to get logs from node ha9zeyohpei4-1 pod pod-subpath-test-downwardapi-vll7 container test-container-subpath-downwardapi-vll7: <nil>
STEP: delete the pod
May 27 06:20:18.746: INFO: Waiting for pod pod-subpath-test-downwardapi-vll7 to disappear
May 27 06:20:18.752: INFO: Pod pod-subpath-test-downwardapi-vll7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vll7
May 27 06:20:18.752: INFO: Deleting pod "pod-subpath-test-downwardapi-vll7" in namespace "subpath-3401"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:18.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3401" for this suite.

• [SLOW TEST:24.365 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":235,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:18.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-fa94a731-c857-41cd-aa8c-8f1626f8b5da
STEP: Creating a pod to test consume configMaps
May 27 06:20:18.845: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04" in namespace "configmap-9452" to be "Succeeded or Failed"
May 27 06:20:18.856: INFO: Pod "pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04": Phase="Pending", Reason="", readiness=false. Elapsed: 10.220859ms
May 27 06:20:20.868: INFO: Pod "pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022856925s
May 27 06:20:22.887: INFO: Pod "pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041378913s
STEP: Saw pod success
May 27 06:20:22.887: INFO: Pod "pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04" satisfied condition "Succeeded or Failed"
May 27 06:20:22.895: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04 container agnhost-container: <nil>
STEP: delete the pod
May 27 06:20:22.935: INFO: Waiting for pod pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04 to disappear
May 27 06:20:22.940: INFO: Pod pod-configmaps-1d7805f5-4775-440b-99c0-61b301a2de04 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:22.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9452" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4568,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:22.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:20:23.020: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 27 06:20:25.103: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:26.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8172" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":237,"skipped":4589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:26.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5602
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5602
I0527 06:20:26.332226      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5602, replica count: 2
I0527 06:20:29.384195      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:20:29.384: INFO: Creating new exec pod
May 27 06:20:32.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-5602 exec execpod2dtrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 27 06:20:32.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 27 06:20:32.755: INFO: stdout: "externalname-service-l8kl2"
May 27 06:20:32.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-5602 exec execpod2dtrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.3 80'
May 27 06:20:32.988: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.3 80\nConnection to 10.233.48.3 80 port [tcp/http] succeeded!\n"
May 27 06:20:32.988: INFO: stdout: "externalname-service-mw44h"
May 27 06:20:32.988: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:33.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5602" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:6.899 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":238,"skipped":4644,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:33.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 06:20:34.719: INFO: starting watch
STEP: patching
STEP: updating
May 27 06:20:34.739: INFO: waiting for watch events with expected annotations
May 27 06:20:34.739: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:34.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1802" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":239,"skipped":4660,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:34.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 06:20:34.920: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 06:20:34.936: INFO: Waiting for terminating namespaces to be deleted...
May 27 06:20:34.941: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-1 before test
May 27 06:20:34.955: INFO: echo-other-node-59d779959c-4vckc from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.955: INFO: 	Container echo-other-node ready: true, restart count 0
May 27 06:20:34.955: INFO: cilium-node-init-ph2ps from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.955: INFO: 	Container node-init ready: true, restart count 0
May 27 06:20:34.955: INFO: cilium-wjfhd from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 06:20:34.956: INFO: kube-addon-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 06:20:34.956: INFO: kube-apiserver-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 06:20:34.956: INFO: kube-controller-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 06:20:34.956: INFO: kube-proxy-4t9kh from kube-system started at 2022-05-27 04:57:26 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 06:20:34.956: INFO: kube-scheduler-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 06:20:34.956: INFO: execpod2dtrd from services-5602 started at 2022-05-27 06:20:29 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container agnhost-container ready: true, restart count 0
May 27 06:20:34.956: INFO: externalname-service-l8kl2 from services-5602 started at 2022-05-27 06:20:26 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.956: INFO: 	Container externalname-service ready: true, restart count 0
May 27 06:20:34.957: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-lcdqs from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:20:34.957: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:20:34.957: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 06:20:34.957: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-2 before test
May 27 06:20:34.974: INFO: cilium-dfq7b from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.974: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 06:20:34.974: INFO: cilium-node-init-942c6 from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.974: INFO: 	Container node-init ready: true, restart count 0
May 27 06:20:34.974: INFO: coredns-64897985d-5hvb2 from kube-system started at 2022-05-27 05:10:55 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.974: INFO: 	Container coredns ready: true, restart count 0
May 27 06:20:34.974: INFO: kube-addon-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.974: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 06:20:34.974: INFO: kube-apiserver-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.974: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 06:20:34.975: INFO: kube-controller-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:57:59 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.975: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 06:20:34.975: INFO: kube-proxy-lpjdp from kube-system started at 2022-05-27 04:58:12 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.975: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 06:20:34.975: INFO: kube-scheduler-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.975: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 06:20:34.975: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-sgkpw from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:20:34.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:20:34.975: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 06:20:34.975: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-3 before test
May 27 06:20:34.989: INFO: client-7568bc7f86-94lb8 from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.989: INFO: 	Container client ready: true, restart count 0
May 27 06:20:34.989: INFO: client2-686d5f784b-xpnfm from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.989: INFO: 	Container client2 ready: true, restart count 0
May 27 06:20:34.989: INFO: echo-same-node-5767b7b99d-bwpk2 from cilium-test started at 2022-05-27 05:12:20 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 06:20:34.990: INFO: cilium-7dvhf from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 06:20:34.990: INFO: cilium-node-init-zxkdp from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container node-init ready: true, restart count 0
May 27 06:20:34.990: INFO: cilium-operator-59d6f769d4-nlxfs from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container cilium-operator ready: true, restart count 0
May 27 06:20:34.990: INFO: coredns-64897985d-484hx from kube-system started at 2022-05-27 05:11:10 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container coredns ready: true, restart count 0
May 27 06:20:34.990: INFO: kube-proxy-hzzmk from kube-system started at 2022-05-27 04:58:38 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 06:20:34.990: INFO: externalname-service-mw44h from services-5602 started at 2022-05-27 06:20:26 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container externalname-service ready: true, restart count 0
May 27 06:20:34.990: INFO: sonobuoy from sonobuoy started at 2022-05-27 05:16:07 +0000 UTC (1 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 06:20:34.990: INFO: sonobuoy-e2e-job-7e88fd63f0e849eb from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:20:34.990: INFO: 	Container e2e ready: true, restart count 0
May 27 06:20:34.990: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:20:34.990: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-tz8zz from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:20:34.991: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:20:34.991: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f2e1f0377caeb3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:36.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1880" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":240,"skipped":4669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:36.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-850ea2ee-b9aa-4a40-a159-bc61692f24af
STEP: Creating a pod to test consume configMaps
May 27 06:20:36.164: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a" in namespace "projected-9682" to be "Succeeded or Failed"
May 27 06:20:36.174: INFO: Pod "pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.430803ms
May 27 06:20:38.282: INFO: Pod "pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a": Phase="Running", Reason="", readiness=true. Elapsed: 2.118419694s
May 27 06:20:40.291: INFO: Pod "pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a": Phase="Running", Reason="", readiness=false. Elapsed: 4.127695s
May 27 06:20:42.307: INFO: Pod "pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.143112292s
STEP: Saw pod success
May 27 06:20:42.307: INFO: Pod "pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a" satisfied condition "Succeeded or Failed"
May 27 06:20:42.312: INFO: Trying to get logs from node ha9zeyohpei4-2 pod pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a container agnhost-container: <nil>
STEP: delete the pod
May 27 06:20:42.364: INFO: Waiting for pod pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a to disappear
May 27 06:20:42.374: INFO: Pod pod-projected-configmaps-0fd25285-ea49-4c60-a80c-0570da39e87a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:20:42.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9682" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4724,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:20:42.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 06:20:42.475: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 06:21:42.536: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:21:42.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:21:42.630: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 27 06:21:42.653: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:21:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8052" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:21:42.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3645" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.431 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":242,"skipped":4735,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:21:42.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 06:21:42.896: INFO: Waiting up to 5m0s for pod "downward-api-7da029a4-f6e2-4a70-b190-56551598d864" in namespace "downward-api-924" to be "Succeeded or Failed"
May 27 06:21:42.912: INFO: Pod "downward-api-7da029a4-f6e2-4a70-b190-56551598d864": Phase="Pending", Reason="", readiness=false. Elapsed: 15.795409ms
May 27 06:21:44.928: INFO: Pod "downward-api-7da029a4-f6e2-4a70-b190-56551598d864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031255865s
May 27 06:21:46.948: INFO: Pod "downward-api-7da029a4-f6e2-4a70-b190-56551598d864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052054614s
STEP: Saw pod success
May 27 06:21:46.949: INFO: Pod "downward-api-7da029a4-f6e2-4a70-b190-56551598d864" satisfied condition "Succeeded or Failed"
May 27 06:21:46.956: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downward-api-7da029a4-f6e2-4a70-b190-56551598d864 container dapi-container: <nil>
STEP: delete the pod
May 27 06:21:47.002: INFO: Waiting for pod downward-api-7da029a4-f6e2-4a70-b190-56551598d864 to disappear
May 27 06:21:47.008: INFO: Pod downward-api-7da029a4-f6e2-4a70-b190-56551598d864 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:21:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-924" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4746,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:21:47.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
May 27 06:21:47.616: INFO: created pod pod-service-account-defaultsa
May 27 06:21:47.616: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 27 06:21:47.630: INFO: created pod pod-service-account-mountsa
May 27 06:21:47.630: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 27 06:21:47.645: INFO: created pod pod-service-account-nomountsa
May 27 06:21:47.645: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 27 06:21:47.679: INFO: created pod pod-service-account-defaultsa-mountspec
May 27 06:21:47.679: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 27 06:21:47.706: INFO: created pod pod-service-account-mountsa-mountspec
May 27 06:21:47.706: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 27 06:21:47.730: INFO: created pod pod-service-account-nomountsa-mountspec
May 27 06:21:47.730: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 27 06:21:47.752: INFO: created pod pod-service-account-defaultsa-nomountspec
May 27 06:21:47.752: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 27 06:21:47.771: INFO: created pod pod-service-account-mountsa-nomountspec
May 27 06:21:47.771: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 27 06:21:47.815: INFO: created pod pod-service-account-nomountsa-nomountspec
May 27 06:21:47.815: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:21:47.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6144" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":244,"skipped":4760,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:21:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-80f51229-2100-4e33-9c5a-6f34bb46ecce
STEP: Creating a pod to test consume configMaps
May 27 06:21:48.244: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123" in namespace "projected-8899" to be "Succeeded or Failed"
May 27 06:21:48.260: INFO: Pod "pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123": Phase="Pending", Reason="", readiness=false. Elapsed: 16.451292ms
May 27 06:21:50.278: INFO: Pod "pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033711787s
May 27 06:21:52.298: INFO: Pod "pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054521935s
STEP: Saw pod success
May 27 06:21:52.299: INFO: Pod "pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123" satisfied condition "Succeeded or Failed"
May 27 06:21:52.304: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123 container agnhost-container: <nil>
STEP: delete the pod
May 27 06:21:52.336: INFO: Waiting for pod pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123 to disappear
May 27 06:21:52.341: INFO: Pod pod-projected-configmaps-f1a7c1a6-04fe-4546-a420-0873ae77e123 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:21:52.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8899" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4773,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:21:52.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-3407
STEP: creating service affinity-clusterip in namespace services-3407
STEP: creating replication controller affinity-clusterip in namespace services-3407
I0527 06:21:52.460666      14 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3407, replica count: 3
I0527 06:21:55.515164      14 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:21:55.533: INFO: Creating new exec pod
May 27 06:22:00.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3407 exec execpod-affinityw9qjx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May 27 06:22:00.953: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 27 06:22:00.953: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:22:00.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3407 exec execpod-affinityw9qjx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.22.15 80'
May 27 06:22:01.203: INFO: stderr: "+ nc -v -t -w 2 10.233.22.15 80\n+ echo hostName\nConnection to 10.233.22.15 80 port [tcp/http] succeeded!\n"
May 27 06:22:01.203: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:22:01.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3407 exec execpod-affinityw9qjx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.22.15:80/ ; done'
May 27 06:22:01.605: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.15:80/\n"
May 27 06:22:01.605: INFO: stdout: "\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt\naffinity-clusterip-2qtrt"
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Received response from host: affinity-clusterip-2qtrt
May 27 06:22:01.605: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3407, will wait for the garbage collector to delete the pods
May 27 06:22:01.714: INFO: Deleting ReplicationController affinity-clusterip took: 11.123075ms
May 27 06:22:01.817: INFO: Terminating ReplicationController affinity-clusterip pods took: 102.095217ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:22:04.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3407" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.876 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":246,"skipped":4776,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:22:04.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 27 06:22:04.320: INFO: Waiting up to 5m0s for pod "pod-eda67c28-f0a5-4431-9736-1abe925936a6" in namespace "emptydir-5874" to be "Succeeded or Failed"
May 27 06:22:04.331: INFO: Pod "pod-eda67c28-f0a5-4431-9736-1abe925936a6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.043441ms
May 27 06:22:06.344: INFO: Pod "pod-eda67c28-f0a5-4431-9736-1abe925936a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023973901s
May 27 06:22:08.361: INFO: Pod "pod-eda67c28-f0a5-4431-9736-1abe925936a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040722116s
STEP: Saw pod success
May 27 06:22:08.361: INFO: Pod "pod-eda67c28-f0a5-4431-9736-1abe925936a6" satisfied condition "Succeeded or Failed"
May 27 06:22:08.370: INFO: Trying to get logs from node ha9zeyohpei4-2 pod pod-eda67c28-f0a5-4431-9736-1abe925936a6 container test-container: <nil>
STEP: delete the pod
May 27 06:22:08.410: INFO: Waiting for pod pod-eda67c28-f0a5-4431-9736-1abe925936a6 to disappear
May 27 06:22:08.418: INFO: Pod pod-eda67c28-f0a5-4431-9736-1abe925936a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:22:08.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5874" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":247,"skipped":4782,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:22:08.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 254.28.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.28.254_udp@PTR;check="$$(dig +tcp +noall +answer +search 254.28.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.28.254_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-829.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-829.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-829.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-829.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 254.28.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.28.254_udp@PTR;check="$$(dig +tcp +noall +answer +search 254.28.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.28.254_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:22:12.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.644: INFO: Unable to read wheezy_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.656: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.665: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.710: INFO: Unable to read jessie_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.715: INFO: Unable to read jessie_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.723: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.732: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:12.768: INFO: Lookups using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e failed for: [wheezy_udp@dns-test-service.dns-829.svc.cluster.local wheezy_tcp@dns-test-service.dns-829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_udp@dns-test-service.dns-829.svc.cluster.local jessie_tcp@dns-test-service.dns-829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local]

May 27 06:22:17.780: INFO: Unable to read wheezy_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.798: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.803: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.835: INFO: Unable to read jessie_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.844: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.852: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:17.889: INFO: Lookups using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e failed for: [wheezy_udp@dns-test-service.dns-829.svc.cluster.local wheezy_tcp@dns-test-service.dns-829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_udp@dns-test-service.dns-829.svc.cluster.local jessie_tcp@dns-test-service.dns-829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local]

May 27 06:22:22.782: INFO: Unable to read wheezy_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.800: INFO: Unable to read wheezy_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.811: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.822: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.882: INFO: Unable to read jessie_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.902: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.911: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:22.949: INFO: Lookups using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e failed for: [wheezy_udp@dns-test-service.dns-829.svc.cluster.local wheezy_tcp@dns-test-service.dns-829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_udp@dns-test-service.dns-829.svc.cluster.local jessie_tcp@dns-test-service.dns-829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local]

May 27 06:22:27.781: INFO: Unable to read wheezy_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.787: INFO: Unable to read wheezy_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.794: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.801: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.830: INFO: Unable to read jessie_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.847: INFO: Unable to read jessie_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.856: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:27.886: INFO: Lookups using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e failed for: [wheezy_udp@dns-test-service.dns-829.svc.cluster.local wheezy_tcp@dns-test-service.dns-829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_udp@dns-test-service.dns-829.svc.cluster.local jessie_tcp@dns-test-service.dns-829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local]

May 27 06:22:32.782: INFO: Unable to read wheezy_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.796: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.802: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.834: INFO: Unable to read jessie_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.845: INFO: Unable to read jessie_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.853: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.860: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:32.885: INFO: Lookups using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e failed for: [wheezy_udp@dns-test-service.dns-829.svc.cluster.local wheezy_tcp@dns-test-service.dns-829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_udp@dns-test-service.dns-829.svc.cluster.local jessie_tcp@dns-test-service.dns-829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local]

May 27 06:22:37.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.788: INFO: Unable to read wheezy_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.797: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.806: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.839: INFO: Unable to read jessie_udp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.845: INFO: Unable to read jessie_tcp@dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.850: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.859: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local from pod dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e: the server could not find the requested resource (get pods dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e)
May 27 06:22:37.883: INFO: Lookups using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e failed for: [wheezy_udp@dns-test-service.dns-829.svc.cluster.local wheezy_tcp@dns-test-service.dns-829.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_udp@dns-test-service.dns-829.svc.cluster.local jessie_tcp@dns-test-service.dns-829.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-829.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-829.svc.cluster.local]

May 27 06:22:42.876: INFO: DNS probes using dns-829/dns-test-7f928216-71d1-475d-b70e-39b0100a2b4e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:22:43.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-829" for this suite.

• [SLOW TEST:34.604 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":248,"skipped":4792,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:22:43.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 27 06:22:43.625: INFO: Pod name wrapped-volume-race-cc5c568a-56b7-4050-8a1e-8be0b25ca747: Found 0 pods out of 5
May 27 06:22:48.667: INFO: Pod name wrapped-volume-race-cc5c568a-56b7-4050-8a1e-8be0b25ca747: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cc5c568a-56b7-4050-8a1e-8be0b25ca747 in namespace emptydir-wrapper-5165, will wait for the garbage collector to delete the pods
May 27 06:23:00.787: INFO: Deleting ReplicationController wrapped-volume-race-cc5c568a-56b7-4050-8a1e-8be0b25ca747 took: 14.814938ms
May 27 06:23:00.888: INFO: Terminating ReplicationController wrapped-volume-race-cc5c568a-56b7-4050-8a1e-8be0b25ca747 pods took: 101.15034ms
STEP: Creating RC which spawns configmap-volume pods
May 27 06:23:03.148: INFO: Pod name wrapped-volume-race-b6d7f44a-77e5-44ec-aaa3-bb5b1dd206e5: Found 0 pods out of 5
May 27 06:23:08.173: INFO: Pod name wrapped-volume-race-b6d7f44a-77e5-44ec-aaa3-bb5b1dd206e5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b6d7f44a-77e5-44ec-aaa3-bb5b1dd206e5 in namespace emptydir-wrapper-5165, will wait for the garbage collector to delete the pods
May 27 06:23:20.297: INFO: Deleting ReplicationController wrapped-volume-race-b6d7f44a-77e5-44ec-aaa3-bb5b1dd206e5 took: 15.030098ms
May 27 06:23:20.398: INFO: Terminating ReplicationController wrapped-volume-race-b6d7f44a-77e5-44ec-aaa3-bb5b1dd206e5 pods took: 100.972764ms
STEP: Creating RC which spawns configmap-volume pods
May 27 06:23:22.944: INFO: Pod name wrapped-volume-race-a1ef0883-b9e6-453a-99e6-4e48f7b29f41: Found 0 pods out of 5
May 27 06:23:27.968: INFO: Pod name wrapped-volume-race-a1ef0883-b9e6-453a-99e6-4e48f7b29f41: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a1ef0883-b9e6-453a-99e6-4e48f7b29f41 in namespace emptydir-wrapper-5165, will wait for the garbage collector to delete the pods
May 27 06:23:40.094: INFO: Deleting ReplicationController wrapped-volume-race-a1ef0883-b9e6-453a-99e6-4e48f7b29f41 took: 17.422718ms
May 27 06:23:40.195: INFO: Terminating ReplicationController wrapped-volume-race-a1ef0883-b9e6-453a-99e6-4e48f7b29f41 pods took: 101.28167ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:23:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5165" for this suite.

• [SLOW TEST:60.788 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":249,"skipped":4813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:23:43.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:23:55.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2231" for this suite.

• [SLOW TEST:11.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":250,"skipped":4835,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:23:55.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8990
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
May 27 06:23:55.172: INFO: Found 0 stateful pods, waiting for 3
May 27 06:24:05.183: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:24:05.183: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:24:05.183: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May 27 06:24:05.237: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 27 06:24:15.298: INFO: Updating stateful set ss2
May 27 06:24:15.315: INFO: Waiting for Pod statefulset-8990/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
May 27 06:24:25.471: INFO: Found 1 stateful pods, waiting for 3
May 27 06:24:35.491: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:24:35.492: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:24:35.492: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 27 06:24:35.538: INFO: Updating stateful set ss2
May 27 06:24:35.573: INFO: Waiting for Pod statefulset-8990/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
May 27 06:24:45.621: INFO: Updating stateful set ss2
May 27 06:24:45.641: INFO: Waiting for StatefulSet statefulset-8990/ss2 to complete update
May 27 06:24:45.641: INFO: Waiting for Pod statefulset-8990/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
May 27 06:24:55.660: INFO: Waiting for StatefulSet statefulset-8990/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 06:25:05.659: INFO: Deleting all statefulset in ns statefulset-8990
May 27 06:25:05.665: INFO: Scaling statefulset ss2 to 0
May 27 06:25:15.715: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:25:15.719: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:25:15.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8990" for this suite.

• [SLOW TEST:80.699 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":251,"skipped":4843,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:25:15.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1308 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1308;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1308 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1308;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1308.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1308.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1308.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1308.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1308.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1308.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1308.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1308.svc;check="$$(dig +notcp +noall +answer +search 201.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.201_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1308 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1308;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1308 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1308;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1308.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1308.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1308.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1308.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1308.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1308.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1308.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1308.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1308.svc;check="$$(dig +notcp +noall +answer +search 201.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.201_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:25:19.943: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.951: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.956: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.964: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.970: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.977: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.983: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.988: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.992: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:19.998: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.003: INFO: Unable to read 10.233.33.201_udp@PTR from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.008: INFO: Unable to read 10.233.33.201_tcp@PTR from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.014: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.020: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.025: INFO: Unable to read jessie_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.033: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.038: INFO: Unable to read jessie_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.054: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.059: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.066: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.070: INFO: Unable to read 10.233.33.201_udp@PTR from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.075: INFO: Unable to read 10.233.33.201_tcp@PTR from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:20.075: INFO: Lookups using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1308 wheezy_tcp@dns-test-service.dns-1308 wheezy_udp@dns-test-service.dns-1308.svc wheezy_tcp@dns-test-service.dns-1308.svc wheezy_udp@_http._tcp.dns-test-service.dns-1308.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1308.svc wheezy_udp@_http._tcp.test-service-2.dns-1308.svc wheezy_tcp@_http._tcp.test-service-2.dns-1308.svc 10.233.33.201_udp@PTR 10.233.33.201_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1308 jessie_tcp@dns-test-service.dns-1308 jessie_udp@dns-test-service.dns-1308.svc jessie_tcp@dns-test-service.dns-1308.svc jessie_udp@_http._tcp.dns-test-service.dns-1308.svc jessie_tcp@_http._tcp.dns-test-service.dns-1308.svc jessie_udp@_http._tcp.test-service-2.dns-1308.svc jessie_tcp@_http._tcp.test-service-2.dns-1308.svc 10.233.33.201_udp@PTR 10.233.33.201_tcp@PTR]

May 27 06:25:25.083: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.089: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.096: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.106: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.113: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.121: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.191: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.197: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.203: INFO: Unable to read jessie_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.207: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.213: INFO: Unable to read jessie_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.217: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:25.247: INFO: Lookups using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1308 wheezy_tcp@dns-test-service.dns-1308 wheezy_udp@dns-test-service.dns-1308.svc wheezy_tcp@dns-test-service.dns-1308.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1308 jessie_tcp@dns-test-service.dns-1308 jessie_udp@dns-test-service.dns-1308.svc jessie_tcp@dns-test-service.dns-1308.svc]

May 27 06:25:30.087: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.095: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.102: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.110: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.117: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.122: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.177: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.183: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.192: INFO: Unable to read jessie_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.224: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.231: INFO: Unable to read jessie_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:30.272: INFO: Lookups using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1308 wheezy_tcp@dns-test-service.dns-1308 wheezy_udp@dns-test-service.dns-1308.svc wheezy_tcp@dns-test-service.dns-1308.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1308 jessie_tcp@dns-test-service.dns-1308 jessie_udp@dns-test-service.dns-1308.svc jessie_tcp@dns-test-service.dns-1308.svc]

May 27 06:25:35.103: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.112: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.118: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.127: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.133: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.139: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.209: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.219: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.227: INFO: Unable to read jessie_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.259: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.267: INFO: Unable to read jessie_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:35.314: INFO: Lookups using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1308 wheezy_tcp@dns-test-service.dns-1308 wheezy_udp@dns-test-service.dns-1308.svc wheezy_tcp@dns-test-service.dns-1308.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1308 jessie_tcp@dns-test-service.dns-1308 jessie_udp@dns-test-service.dns-1308.svc jessie_tcp@dns-test-service.dns-1308.svc]

May 27 06:25:40.086: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.098: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.104: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.110: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.116: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.121: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.181: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.190: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.202: INFO: Unable to read jessie_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.215: INFO: Unable to read jessie_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.221: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:40.258: INFO: Lookups using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1308 wheezy_tcp@dns-test-service.dns-1308 wheezy_udp@dns-test-service.dns-1308.svc wheezy_tcp@dns-test-service.dns-1308.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1308 jessie_tcp@dns-test-service.dns-1308 jessie_udp@dns-test-service.dns-1308.svc jessie_tcp@dns-test-service.dns-1308.svc]

May 27 06:25:45.084: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.092: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.103: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.110: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.116: INFO: Unable to read wheezy_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.121: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.200: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.209: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.215: INFO: Unable to read jessie_udp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.222: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308 from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.228: INFO: Unable to read jessie_udp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.233: INFO: Unable to read jessie_tcp@dns-test-service.dns-1308.svc from pod dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686: the server could not find the requested resource (get pods dns-test-65474935-734b-4674-a743-172483a1d686)
May 27 06:25:45.278: INFO: Lookups using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1308 wheezy_tcp@dns-test-service.dns-1308 wheezy_udp@dns-test-service.dns-1308.svc wheezy_tcp@dns-test-service.dns-1308.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1308 jessie_tcp@dns-test-service.dns-1308 jessie_udp@dns-test-service.dns-1308.svc jessie_tcp@dns-test-service.dns-1308.svc]

May 27 06:25:50.280: INFO: DNS probes using dns-1308/dns-test-65474935-734b-4674-a743-172483a1d686 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:25:50.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1308" for this suite.

• [SLOW TEST:34.711 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":252,"skipped":4855,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:25:50.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
May 27 06:25:50.599: INFO: Found Service test-service-j7h6x in namespace services-4378 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 27 06:25:50.599: INFO: Service test-service-j7h6x created
STEP: Getting /status
May 27 06:25:50.608: INFO: Service test-service-j7h6x has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
May 27 06:25:50.628: INFO: observed Service test-service-j7h6x in namespace services-4378 with annotations: map[] & LoadBalancer: {[]}
May 27 06:25:50.628: INFO: Found Service test-service-j7h6x in namespace services-4378 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 27 06:25:50.628: INFO: Service test-service-j7h6x has service status patched
STEP: updating the ServiceStatus
May 27 06:25:50.657: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
May 27 06:25:50.659: INFO: Observed Service test-service-j7h6x in namespace services-4378 with annotations: map[] & Conditions: {[]}
May 27 06:25:50.659: INFO: Observed event: &Service{ObjectMeta:{test-service-j7h6x  services-4378  cb220016-512b-490e-a490-426d712270c5 32040 0 2022-05-27 06:25:50 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-05-27 06:25:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-05-27 06:25:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.6.8,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.6.8],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 27 06:25:50.660: INFO: Found Service test-service-j7h6x in namespace services-4378 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 06:25:50.660: INFO: Service test-service-j7h6x has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
May 27 06:25:50.694: INFO: observed Service test-service-j7h6x in namespace services-4378 with labels: map[test-service-static:true]
May 27 06:25:50.694: INFO: observed Service test-service-j7h6x in namespace services-4378 with labels: map[test-service-static:true]
May 27 06:25:50.695: INFO: observed Service test-service-j7h6x in namespace services-4378 with labels: map[test-service-static:true]
May 27 06:25:50.695: INFO: Found Service test-service-j7h6x in namespace services-4378 with labels: map[test-service:patched test-service-static:true]
May 27 06:25:50.695: INFO: Service test-service-j7h6x patched
STEP: deleting the service
STEP: watching for the Service to be deleted
May 27 06:25:50.757: INFO: Observed event: ADDED
May 27 06:25:50.757: INFO: Observed event: MODIFIED
May 27 06:25:50.757: INFO: Observed event: MODIFIED
May 27 06:25:50.757: INFO: Observed event: MODIFIED
May 27 06:25:50.757: INFO: Found Service test-service-j7h6x in namespace services-4378 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 27 06:25:50.757: INFO: Service test-service-j7h6x deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:25:50.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4378" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":253,"skipped":4863,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:25:50.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 27 06:25:55.961: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:25:55.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2299" for this suite.

• [SLOW TEST:5.223 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":254,"skipped":4867,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:25:56.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:25:56.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2858" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":255,"skipped":4881,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:25:56.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:25:56.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6556" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":256,"skipped":4885,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:25:56.428: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-c755fc82-4306-440b-9fc8-9a9be9c0b09c
STEP: Creating a pod to test consume configMaps
May 27 06:25:56.522: INFO: Waiting up to 5m0s for pod "pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b" in namespace "configmap-9479" to be "Succeeded or Failed"
May 27 06:25:56.562: INFO: Pod "pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.400626ms
May 27 06:25:58.578: INFO: Pod "pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056221565s
May 27 06:26:00.591: INFO: Pod "pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069534385s
STEP: Saw pod success
May 27 06:26:00.592: INFO: Pod "pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b" satisfied condition "Succeeded or Failed"
May 27 06:26:00.598: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b container agnhost-container: <nil>
STEP: delete the pod
May 27 06:26:00.652: INFO: Waiting for pod pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b to disappear
May 27 06:26:00.658: INFO: Pod pod-configmaps-902966ea-bcbd-49c7-9417-54bd536ea15b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:26:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9479" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:26:00.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-4e60fca5-e34b-4832-ab81-024e798d1a4b in namespace container-probe-1671
May 27 06:26:02.775: INFO: Started pod test-webserver-4e60fca5-e34b-4832-ab81-024e798d1a4b in namespace container-probe-1671
STEP: checking the pod's current state and verifying that restartCount is present
May 27 06:26:02.781: INFO: Initial restart count of pod test-webserver-4e60fca5-e34b-4832-ab81-024e798d1a4b is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:04.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1671" for this suite.

• [SLOW TEST:244.016 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:04.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
May 27 06:30:04.775: INFO: Waiting up to 5m0s for pod "client-containers-c973b71f-bc4a-4a92-809e-82448910061f" in namespace "containers-1897" to be "Succeeded or Failed"
May 27 06:30:04.784: INFO: Pod "client-containers-c973b71f-bc4a-4a92-809e-82448910061f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.953491ms
May 27 06:30:06.796: INFO: Pod "client-containers-c973b71f-bc4a-4a92-809e-82448910061f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021263685s
May 27 06:30:08.815: INFO: Pod "client-containers-c973b71f-bc4a-4a92-809e-82448910061f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040393127s
STEP: Saw pod success
May 27 06:30:08.815: INFO: Pod "client-containers-c973b71f-bc4a-4a92-809e-82448910061f" satisfied condition "Succeeded or Failed"
May 27 06:30:08.822: INFO: Trying to get logs from node ha9zeyohpei4-3 pod client-containers-c973b71f-bc4a-4a92-809e-82448910061f container agnhost-container: <nil>
STEP: delete the pod
May 27 06:30:08.876: INFO: Waiting for pod client-containers-c973b71f-bc4a-4a92-809e-82448910061f to disappear
May 27 06:30:08.880: INFO: Pod client-containers-c973b71f-bc4a-4a92-809e-82448910061f no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:08.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1897" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":4959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:08.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 27 06:30:09.765: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:30:12.816: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:30:12.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:16.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-299" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.482 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":260,"skipped":4988,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:16.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:18.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3978" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":261,"skipped":5000,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:18.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:30:18.724: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14" in namespace "downward-api-6960" to be "Succeeded or Failed"
May 27 06:30:18.730: INFO: Pod "downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.567059ms
May 27 06:30:20.741: INFO: Pod "downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017457836s
May 27 06:30:22.776: INFO: Pod "downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052454073s
STEP: Saw pod success
May 27 06:30:22.776: INFO: Pod "downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14" satisfied condition "Succeeded or Failed"
May 27 06:30:22.781: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14 container client-container: <nil>
STEP: delete the pod
May 27 06:30:22.811: INFO: Waiting for pod downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14 to disappear
May 27 06:30:22.818: INFO: Pod downwardapi-volume-79f92074-2554-4856-b7e2-6ed0a7edaa14 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:22.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6960" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":5001,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:22.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 27 06:30:22.912: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:26.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7543" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":263,"skipped":5012,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:26.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-7074
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 27 06:30:26.328: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 27 06:30:26.431: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:30:28.445: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:30.441: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:32.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:34.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:36.451: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:38.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:40.444: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:42.450: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:44.448: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 27 06:30:46.441: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 27 06:30:46.454: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 27 06:30:48.471: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 27 06:30:48.482: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 27 06:30:50.573: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 27 06:30:50.573: INFO: Going to poll 10.233.65.128 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 27 06:30:50.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.128:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7074 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:30:50.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:30:50.593: INFO: ExecWithOptions: Clientset creation
May 27 06:30:50.593: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7074/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.128%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 06:30:50.759: INFO: Found all 1 expected endpoints: [netserver-0]
May 27 06:30:50.759: INFO: Going to poll 10.233.64.158 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 27 06:30:50.766: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.158:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7074 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:30:50.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:30:50.768: INFO: ExecWithOptions: Clientset creation
May 27 06:30:50.768: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7074/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.158%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 06:30:50.886: INFO: Found all 1 expected endpoints: [netserver-1]
May 27 06:30:50.886: INFO: Going to poll 10.233.66.170 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 27 06:30:50.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.170:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7074 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:30:50.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:30:50.896: INFO: ExecWithOptions: Clientset creation
May 27 06:30:50.896: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7074/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.170%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 06:30:51.002: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:30:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7074" for this suite.

• [SLOW TEST:24.758 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":5018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:30:51.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6858
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
May 27 06:30:51.121: INFO: Found 0 stateful pods, waiting for 3
May 27 06:31:01.153: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:31:01.153: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:31:01.153: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 27 06:31:01.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-6858 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 06:31:01.464: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 06:31:01.464: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 06:31:01.464: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May 27 06:31:11.534: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 27 06:31:21.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-6858 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 06:31:21.826: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 06:31:21.826: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 06:31:21.826: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
May 27 06:31:31.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-6858 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 27 06:31:32.140: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 27 06:31:32.140: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 27 06:31:32.140: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 27 06:31:42.236: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 27 06:31:52.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=statefulset-6858 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 27 06:31:52.578: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 27 06:31:52.578: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 27 06:31:52.578: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 06:32:02.636: INFO: Deleting all statefulset in ns statefulset-6858
May 27 06:32:02.641: INFO: Scaling statefulset ss2 to 0
May 27 06:32:12.702: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:32:12.708: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:32:12.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6858" for this suite.

• [SLOW TEST:81.732 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":265,"skipped":5060,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:32:12.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 27 06:32:12.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:32:18.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:32:38.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5237" for this suite.

• [SLOW TEST:25.658 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":266,"skipped":5068,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:32:38.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6965
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6965
STEP: creating replication controller externalsvc in namespace services-6965
I0527 06:32:38.590254      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6965, replica count: 2
I0527 06:32:41.641714      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 27 06:32:41.714: INFO: Creating new exec pod
May 27 06:32:43.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6965 exec execpodbrd24 -- /bin/sh -x -c nslookup nodeport-service.services-6965.svc.cluster.local'
May 27 06:32:44.296: INFO: stderr: "+ nslookup nodeport-service.services-6965.svc.cluster.local\n"
May 27 06:32:44.296: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-6965.svc.cluster.local\tcanonical name = externalsvc.services-6965.svc.cluster.local.\nName:\texternalsvc.services-6965.svc.cluster.local\nAddress: 10.233.48.87\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6965, will wait for the garbage collector to delete the pods
May 27 06:32:44.375: INFO: Deleting ReplicationController externalsvc took: 19.366471ms
May 27 06:32:44.476: INFO: Terminating ReplicationController externalsvc pods took: 101.125632ms
May 27 06:32:46.924: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:32:46.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6965" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.551 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":267,"skipped":5072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:32:47.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-5dacb530-e3be-4fb9-8ec0-bdcb0d633290
STEP: Creating a pod to test consume configMaps
May 27 06:32:47.075: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec" in namespace "projected-3485" to be "Succeeded or Failed"
May 27 06:32:47.080: INFO: Pod "pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671657ms
May 27 06:32:49.093: INFO: Pod "pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018087768s
May 27 06:32:51.108: INFO: Pod "pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03239024s
STEP: Saw pod success
May 27 06:32:51.108: INFO: Pod "pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec" satisfied condition "Succeeded or Failed"
May 27 06:32:51.114: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec container agnhost-container: <nil>
STEP: delete the pod
May 27 06:32:51.175: INFO: Waiting for pod pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec to disappear
May 27 06:32:51.180: INFO: Pod pod-projected-configmaps-a5bb0665-b55b-426b-8579-0a37db1395ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:32:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3485" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":5119,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:32:51.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:32:51.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Creating first CR 
May 27 06:32:53.985: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T06:32:53Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T06:32:53Z]] name:name1 resourceVersion:34001 uid:d272c40b-88f5-4565-85c4-aa6f6a8d324a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 27 06:33:04.010: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T06:33:04Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T06:33:04Z]] name:name2 resourceVersion:34033 uid:5e96a4e5-931e-44e4-bdda-92e4c6b1870d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 27 06:33:14.039: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T06:32:53Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T06:33:14Z]] name:name1 resourceVersion:34054 uid:d272c40b-88f5-4565-85c4-aa6f6a8d324a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 27 06:33:24.070: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T06:33:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T06:33:24Z]] name:name2 resourceVersion:34075 uid:5e96a4e5-931e-44e4-bdda-92e4c6b1870d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 27 06:33:34.106: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T06:32:53Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T06:33:14Z]] name:name1 resourceVersion:34095 uid:d272c40b-88f5-4565-85c4-aa6f6a8d324a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 27 06:33:44.154: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-27T06:33:04Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-27T06:33:24Z]] name:name2 resourceVersion:34116 uid:5e96a4e5-931e-44e4-bdda-92e4c6b1870d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:33:54.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1887" for this suite.

• [SLOW TEST:63.519 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":269,"skipped":5120,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:33:54.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:33:54.813: INFO: The status of Pod busybox-scheduling-fa66d0bf-7b6a-4e34-8dea-d7836d580bbf is Pending, waiting for it to be Running (with Ready = true)
May 27 06:33:56.824: INFO: The status of Pod busybox-scheduling-fa66d0bf-7b6a-4e34-8dea-d7836d580bbf is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:33:56.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8766" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":5130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:33:56.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-0180a64c-43bf-4338-afd2-e0771a8acad4
STEP: Creating secret with name s-test-opt-upd-e7e71ed6-ca89-4d8f-95a0-ed9ea8496910
STEP: Creating the pod
May 27 06:33:56.985: INFO: The status of Pod pod-projected-secrets-16912a98-702c-4e08-b9d7-33fa6f793cc6 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:33:59.016: INFO: The status of Pod pod-projected-secrets-16912a98-702c-4e08-b9d7-33fa6f793cc6 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:34:00.999: INFO: The status of Pod pod-projected-secrets-16912a98-702c-4e08-b9d7-33fa6f793cc6 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-0180a64c-43bf-4338-afd2-e0771a8acad4
STEP: Updating secret s-test-opt-upd-e7e71ed6-ca89-4d8f-95a0-ed9ea8496910
STEP: Creating secret with name s-test-opt-create-c4fde83d-db5a-4fbc-bbb7-896a3401b421
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:07.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8431" for this suite.

• [SLOW TEST:70.893 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":5155,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:07.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3455" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":272,"skipped":5158,"failed":0}
SS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:07.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
May 27 06:35:07.901: INFO: Major version: 1
STEP: Confirm minor version
May 27 06:35:07.901: INFO: cleanMinorVersion: 23
May 27 06:35:07.901: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:07.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1300" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":273,"skipped":5160,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:07.929: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:07.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6063" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":274,"skipped":5163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:08.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:35:08.155: INFO: Create a RollingUpdate DaemonSet
May 27 06:35:08.168: INFO: Check that daemon pods launch on every node of the cluster
May 27 06:35:08.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:35:08.197: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:35:09.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:35:09.216: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:35:10.220: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 06:35:10.220: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:35:11.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 06:35:11.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May 27 06:35:11.239: INFO: Update the DaemonSet to trigger a rollout
May 27 06:35:11.257: INFO: Updating DaemonSet daemon-set
May 27 06:35:14.389: INFO: Roll back the DaemonSet before rollout is complete
May 27 06:35:14.464: INFO: Updating DaemonSet daemon-set
May 27 06:35:14.464: INFO: Make sure DaemonSet rollback is complete
May 27 06:35:14.484: INFO: Wrong image for pod: daemon-set-jw242. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May 27 06:35:14.484: INFO: Pod daemon-set-jw242 is not available
May 27 06:35:19.512: INFO: Pod daemon-set-x7c9d is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2456, will wait for the garbage collector to delete the pods
May 27 06:35:19.618: INFO: Deleting DaemonSet.extensions daemon-set took: 26.909314ms
May 27 06:35:19.719: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.19208ms
May 27 06:35:22.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:35:22.535: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 06:35:22.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34546"},"items":null}

May 27 06:35:22.547: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34546"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:22.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2456" for this suite.

• [SLOW TEST:14.585 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":275,"skipped":5192,"failed":0}
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:22.608: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:24.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6069" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":276,"skipped":5192,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:24.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:36.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5045" for this suite.

• [SLOW TEST:11.342 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":277,"skipped":5207,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:36.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
May 27 06:35:36.142: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 27 06:35:36.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 create -f -'
May 27 06:35:36.646: INFO: stderr: ""
May 27 06:35:36.646: INFO: stdout: "service/agnhost-replica created\n"
May 27 06:35:36.646: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 27 06:35:36.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 create -f -'
May 27 06:35:38.398: INFO: stderr: ""
May 27 06:35:38.398: INFO: stdout: "service/agnhost-primary created\n"
May 27 06:35:38.399: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 27 06:35:38.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 create -f -'
May 27 06:35:38.742: INFO: stderr: ""
May 27 06:35:38.742: INFO: stdout: "service/frontend created\n"
May 27 06:35:38.742: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 27 06:35:38.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 create -f -'
May 27 06:35:39.109: INFO: stderr: ""
May 27 06:35:39.109: INFO: stdout: "deployment.apps/frontend created\n"
May 27 06:35:39.110: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 27 06:35:39.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 create -f -'
May 27 06:35:39.433: INFO: stderr: ""
May 27 06:35:39.433: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 27 06:35:39.433: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 27 06:35:39.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 create -f -'
May 27 06:35:40.371: INFO: stderr: ""
May 27 06:35:40.371: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 27 06:35:40.371: INFO: Waiting for all frontend pods to be Running.
May 27 06:35:45.430: INFO: Waiting for frontend to serve content.
May 27 06:35:45.451: INFO: Trying to add a new entry to the guestbook.
May 27 06:35:45.478: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 27 06:35:45.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 delete --grace-period=0 --force -f -'
May 27 06:35:45.672: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:35:45.672: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 27 06:35:45.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 delete --grace-period=0 --force -f -'
May 27 06:35:45.863: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:35:45.863: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 27 06:35:45.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 delete --grace-period=0 --force -f -'
May 27 06:35:46.038: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:35:46.038: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 27 06:35:46.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 delete --grace-period=0 --force -f -'
May 27 06:35:46.182: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:35:46.182: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 27 06:35:46.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 delete --grace-period=0 --force -f -'
May 27 06:35:46.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:35:46.379: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 27 06:35:46.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-6175 delete --grace-period=0 --force -f -'
May 27 06:35:46.571: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:35:46.571: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:35:46.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6175" for this suite.

• [SLOW TEST:10.526 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":278,"skipped":5208,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:35:46.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-bb3fbc90-6334-474a-8d83-5fe258a6d11f in namespace container-probe-9050
May 27 06:35:48.761: INFO: Started pod liveness-bb3fbc90-6334-474a-8d83-5fe258a6d11f in namespace container-probe-9050
STEP: checking the pod's current state and verifying that restartCount is present
May 27 06:35:48.770: INFO: Initial restart count of pod liveness-bb3fbc90-6334-474a-8d83-5fe258a6d11f is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:39:50.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9050" for this suite.

• [SLOW TEST:244.002 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":279,"skipped":5229,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:39:50.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:03.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9197" for this suite.

• [SLOW TEST:13.265 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":280,"skipped":5249,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:03.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:40:04.117: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"07531018-4b8e-46a1-95e1-5a93b2b2c458", Controller:(*bool)(0xc00440f226), BlockOwnerDeletion:(*bool)(0xc00440f227)}}
May 27 06:40:04.144: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"281c50a9-21f4-4039-bb4f-19fc31a01b04", Controller:(*bool)(0xc004c75426), BlockOwnerDeletion:(*bool)(0xc004c75427)}}
May 27 06:40:04.161: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dcc4fca0-aadd-4c5c-9df6-7349f03c6cd0", Controller:(*bool)(0xc00440f536), BlockOwnerDeletion:(*bool)(0xc00440f537)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:09.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5410" for this suite.

• [SLOW TEST:5.332 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":281,"skipped":5255,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:09.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:40:09.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a" in namespace "projected-2447" to be "Succeeded or Failed"
May 27 06:40:09.316: INFO: Pod "downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.635577ms
May 27 06:40:11.331: INFO: Pod "downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026371563s
May 27 06:40:13.347: INFO: Pod "downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042371666s
STEP: Saw pod success
May 27 06:40:13.347: INFO: Pod "downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a" satisfied condition "Succeeded or Failed"
May 27 06:40:13.354: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a container client-container: <nil>
STEP: delete the pod
May 27 06:40:13.408: INFO: Waiting for pod downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a to disappear
May 27 06:40:13.414: INFO: Pod downwardapi-volume-7ea4883c-90f6-45f8-ae95-5501f15fe00a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:13.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2447" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":282,"skipped":5255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:13.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
May 27 06:40:13.486: INFO: Waiting up to 5m0s for pod "test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9" in namespace "svcaccounts-9987" to be "Succeeded or Failed"
May 27 06:40:13.491: INFO: Pod "test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.605116ms
May 27 06:40:15.500: INFO: Pod "test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014076741s
May 27 06:40:17.513: INFO: Pod "test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027490515s
May 27 06:40:19.527: INFO: Pod "test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040873273s
STEP: Saw pod success
May 27 06:40:19.527: INFO: Pod "test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9" satisfied condition "Succeeded or Failed"
May 27 06:40:19.532: INFO: Trying to get logs from node ha9zeyohpei4-3 pod test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9 container agnhost-container: <nil>
STEP: delete the pod
May 27 06:40:19.568: INFO: Waiting for pod test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9 to disappear
May 27 06:40:19.573: INFO: Pod test-pod-a88976a0-fdcf-4e9c-81ed-600638474ea9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:19.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9987" for this suite.

• [SLOW TEST:6.162 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":283,"skipped":5321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:19.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 27 06:40:19.663: INFO: The status of Pod annotationupdate68b3b98e-b72e-44f6-b9f5-eb792e165bee is Pending, waiting for it to be Running (with Ready = true)
May 27 06:40:21.672: INFO: The status of Pod annotationupdate68b3b98e-b72e-44f6-b9f5-eb792e165bee is Running (Ready = true)
May 27 06:40:22.219: INFO: Successfully updated pod "annotationupdate68b3b98e-b72e-44f6-b9f5-eb792e165bee"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:24.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-468" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":284,"skipped":5354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:24.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:40:24.995: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:40:28.051: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
May 27 06:40:28.095: INFO: Waiting for webhook configuration to be ready...
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:40.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2527" for this suite.
STEP: Destroying namespace "webhook-2527-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":285,"skipped":5399,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:40:41.046: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:40:44.096: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:44.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5893" for this suite.
STEP: Destroying namespace "webhook-5893-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":286,"skipped":5413,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:40:44.451: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280" in namespace "downward-api-2508" to be "Succeeded or Failed"
May 27 06:40:44.455: INFO: Pod "downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280": Phase="Pending", Reason="", readiness=false. Elapsed: 4.46364ms
May 27 06:40:46.469: INFO: Pod "downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280": Phase="Running", Reason="", readiness=true. Elapsed: 2.017768724s
May 27 06:40:48.484: INFO: Pod "downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280": Phase="Running", Reason="", readiness=false. Elapsed: 4.033407906s
May 27 06:40:50.496: INFO: Pod "downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04550722s
STEP: Saw pod success
May 27 06:40:50.497: INFO: Pod "downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280" satisfied condition "Succeeded or Failed"
May 27 06:40:50.500: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280 container client-container: <nil>
STEP: delete the pod
May 27 06:40:50.532: INFO: Waiting for pod downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280 to disappear
May 27 06:40:50.537: INFO: Pod downwardapi-volume-02db4508-ccbe-4b10-b789-14cfebbe6280 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:50.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2508" for this suite.

• [SLOW TEST:6.194 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":287,"skipped":5430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 06:40:50.649: INFO: starting watch
STEP: patching
STEP: updating
May 27 06:40:50.674: INFO: waiting for watch events with expected annotations
May 27 06:40:50.674: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:40:50.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1929" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":288,"skipped":5466,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:40:50.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6903
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6903
STEP: Waiting until pod test-pod will start running in namespace statefulset-6903
STEP: Creating statefulset with conflicting port in namespace statefulset-6903
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6903
May 27 06:40:54.933: INFO: Observed stateful pod in namespace: statefulset-6903, name: ss-0, uid: e8edf349-4d4e-4c0e-8747-1b6cb8033e97, status phase: Pending. Waiting for statefulset controller to delete.
May 27 06:40:54.966: INFO: Observed stateful pod in namespace: statefulset-6903, name: ss-0, uid: e8edf349-4d4e-4c0e-8747-1b6cb8033e97, status phase: Failed. Waiting for statefulset controller to delete.
May 27 06:40:54.982: INFO: Observed stateful pod in namespace: statefulset-6903, name: ss-0, uid: e8edf349-4d4e-4c0e-8747-1b6cb8033e97, status phase: Failed. Waiting for statefulset controller to delete.
May 27 06:40:54.989: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6903
STEP: Removing pod with conflicting port in namespace statefulset-6903
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6903 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 27 06:40:57.101: INFO: Deleting all statefulset in ns statefulset-6903
May 27 06:40:57.106: INFO: Scaling statefulset ss to 0
May 27 06:41:07.147: INFO: Waiting for statefulset status.replicas updated to 0
May 27 06:41:07.155: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:41:07.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6903" for this suite.

• [SLOW TEST:16.512 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":289,"skipped":5480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:41:07.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 27 06:41:07.301: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 06:42:07.368: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:42:07.376: INFO: Starting informer...
STEP: Starting pod...
May 27 06:42:07.613: INFO: Pod is running on ha9zeyohpei4-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 27 06:42:07.673: INFO: Pod wasn't evicted. Proceeding
May 27 06:42:07.673: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 27 06:43:22.737: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:22.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4992" for this suite.

• [SLOW TEST:135.518 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":290,"skipped":5506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:22.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:43:22.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 27 06:43:22.868: INFO: The status of Pod pod-logs-websocket-b88d4011-4ed2-4f64-ab4d-0b55779fe830 is Pending, waiting for it to be Running (with Ready = true)
May 27 06:43:24.885: INFO: The status of Pod pod-logs-websocket-b88d4011-4ed2-4f64-ab4d-0b55779fe830 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:24.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8012" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5553,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:24.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5094.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5094.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5094.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5094.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:43:29.098: INFO: DNS probes using dns-5094/dns-test-cd45dd6e-e241-4a6e-878e-8626d797571e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:29.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5094" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":292,"skipped":5556,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:29.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:40.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-62" for this suite.

• [SLOW TEST:11.217 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":293,"skipped":5575,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:40.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 27 06:43:40.531: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 27 06:43:42.544: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 27 06:43:42.563: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 27 06:43:44.573: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 27 06:43:44.595: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 27 06:43:44.603: INFO: Pod pod-with-prestop-exec-hook still exists
May 27 06:43:46.604: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 27 06:43:46.617: INFO: Pod pod-with-prestop-exec-hook still exists
May 27 06:43:48.605: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 27 06:43:48.619: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:48.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6066" for this suite.

• [SLOW TEST:8.234 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:48.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 27 06:43:48.757: INFO: Waiting up to 5m0s for pod "pod-17986e61-9d1e-4eed-9e1e-f89c91245e42" in namespace "emptydir-4114" to be "Succeeded or Failed"
May 27 06:43:48.763: INFO: Pod "pod-17986e61-9d1e-4eed-9e1e-f89c91245e42": Phase="Pending", Reason="", readiness=false. Elapsed: 6.467592ms
May 27 06:43:50.774: INFO: Pod "pod-17986e61-9d1e-4eed-9e1e-f89c91245e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017030979s
May 27 06:43:52.790: INFO: Pod "pod-17986e61-9d1e-4eed-9e1e-f89c91245e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033776398s
STEP: Saw pod success
May 27 06:43:52.790: INFO: Pod "pod-17986e61-9d1e-4eed-9e1e-f89c91245e42" satisfied condition "Succeeded or Failed"
May 27 06:43:52.798: INFO: Trying to get logs from node ha9zeyohpei4-2 pod pod-17986e61-9d1e-4eed-9e1e-f89c91245e42 container test-container: <nil>
STEP: delete the pod
May 27 06:43:52.851: INFO: Waiting for pod pod-17986e61-9d1e-4eed-9e1e-f89c91245e42 to disappear
May 27 06:43:52.858: INFO: Pod pod-17986e61-9d1e-4eed-9e1e-f89c91245e42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:52.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4114" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:52.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 06:43:52.956: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 27 06:43:52.966: INFO: starting watch
STEP: patching
STEP: updating
May 27 06:43:53.009: INFO: waiting for watch events with expected annotations
May 27 06:43:53.009: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:53.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8428" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":296,"skipped":5701,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:53.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 27 06:43:53.236: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 27 06:43:53.244: INFO: starting watch
STEP: patching
STEP: updating
May 27 06:43:53.271: INFO: waiting for watch events with expected annotations
May 27 06:43:53.271: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:53.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6949" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":297,"skipped":5702,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 27 06:43:53.391: INFO: Pod name pod-release: Found 0 pods out of 1
May 27 06:43:58.411: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:43:59.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2737" for this suite.

• [SLOW TEST:6.166 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":298,"skipped":5720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:43:59.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:43:59.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac" in namespace "projected-6925" to be "Succeeded or Failed"
May 27 06:43:59.592: INFO: Pod "downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323003ms
May 27 06:44:01.608: INFO: Pod "downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021781712s
May 27 06:44:03.623: INFO: Pod "downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036815949s
STEP: Saw pod success
May 27 06:44:03.623: INFO: Pod "downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac" satisfied condition "Succeeded or Failed"
May 27 06:44:03.628: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac container client-container: <nil>
STEP: delete the pod
May 27 06:44:03.663: INFO: Waiting for pod downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac to disappear
May 27 06:44:03.670: INFO: Pod downwardapi-volume-87ec243b-8ca9-422d-bcd2-c262e7ebd7ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:03.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6925" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5766,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:03.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
May 27 06:44:03.753: INFO: Waiting up to 5m0s for pod "pod-33e4e85d-9732-48fd-b817-d5d5184339c0" in namespace "emptydir-986" to be "Succeeded or Failed"
May 27 06:44:03.760: INFO: Pod "pod-33e4e85d-9732-48fd-b817-d5d5184339c0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.16094ms
May 27 06:44:05.769: INFO: Pod "pod-33e4e85d-9732-48fd-b817-d5d5184339c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016661286s
May 27 06:44:07.785: INFO: Pod "pod-33e4e85d-9732-48fd-b817-d5d5184339c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0323122s
STEP: Saw pod success
May 27 06:44:07.785: INFO: Pod "pod-33e4e85d-9732-48fd-b817-d5d5184339c0" satisfied condition "Succeeded or Failed"
May 27 06:44:07.791: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-33e4e85d-9732-48fd-b817-d5d5184339c0 container test-container: <nil>
STEP: delete the pod
May 27 06:44:07.831: INFO: Waiting for pod pod-33e4e85d-9732-48fd-b817-d5d5184339c0 to disappear
May 27 06:44:07.837: INFO: Pod pod-33e4e85d-9732-48fd-b817-d5d5184339c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:07.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-986" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5784,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:07.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ae641c4d-aa41-4576-8212-3ef7d7ce0a76
STEP: Creating the pod
May 27 06:44:07.968: INFO: The status of Pod pod-projected-configmaps-96396264-5a9d-44de-b290-3d99d2fc8c9d is Pending, waiting for it to be Running (with Ready = true)
May 27 06:44:09.984: INFO: The status of Pod pod-projected-configmaps-96396264-5a9d-44de-b290-3d99d2fc8c9d is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-ae641c4d-aa41-4576-8212-3ef7d7ce0a76
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:12.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5557" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":301,"skipped":5792,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:12.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:44:12.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11" in namespace "downward-api-2712" to be "Succeeded or Failed"
May 27 06:44:12.172: INFO: Pod "downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11": Phase="Pending", Reason="", readiness=false. Elapsed: 9.262806ms
May 27 06:44:14.182: INFO: Pod "downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019123008s
May 27 06:44:16.194: INFO: Pod "downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031354877s
STEP: Saw pod success
May 27 06:44:16.195: INFO: Pod "downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11" satisfied condition "Succeeded or Failed"
May 27 06:44:16.200: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11 container client-container: <nil>
STEP: delete the pod
May 27 06:44:16.234: INFO: Waiting for pod downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11 to disappear
May 27 06:44:16.240: INFO: Pod downwardapi-volume-722d2610-643a-4612-aa7c-06b02be48a11 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:16.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2712" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5792,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:16.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
May 27 06:44:16.321: INFO: Waiting up to 5m0s for pod "pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39" in namespace "emptydir-7251" to be "Succeeded or Failed"
May 27 06:44:16.327: INFO: Pod "pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39": Phase="Pending", Reason="", readiness=false. Elapsed: 5.981572ms
May 27 06:44:18.342: INFO: Pod "pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021197409s
May 27 06:44:20.352: INFO: Pod "pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031744696s
STEP: Saw pod success
May 27 06:44:20.353: INFO: Pod "pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39" satisfied condition "Succeeded or Failed"
May 27 06:44:20.359: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39 container test-container: <nil>
STEP: delete the pod
May 27 06:44:20.391: INFO: Waiting for pod pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39 to disappear
May 27 06:44:20.398: INFO: Pod pod-d47fcee9-5f6d-44d5-b1aa-070dcce9da39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:20.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7251" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5798,"failed":0}
S
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:20.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:44:20.487: INFO: The status of Pod server-envvars-99db4a79-9136-4989-865b-59dbc4dbd89f is Pending, waiting for it to be Running (with Ready = true)
May 27 06:44:22.502: INFO: The status of Pod server-envvars-99db4a79-9136-4989-865b-59dbc4dbd89f is Running (Ready = true)
May 27 06:44:22.543: INFO: Waiting up to 5m0s for pod "client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f" in namespace "pods-2627" to be "Succeeded or Failed"
May 27 06:44:22.558: INFO: Pod "client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.62112ms
May 27 06:44:24.568: INFO: Pod "client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025260469s
May 27 06:44:26.581: INFO: Pod "client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038215409s
STEP: Saw pod success
May 27 06:44:26.581: INFO: Pod "client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f" satisfied condition "Succeeded or Failed"
May 27 06:44:26.587: INFO: Trying to get logs from node ha9zeyohpei4-3 pod client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f container env3cont: <nil>
STEP: delete the pod
May 27 06:44:26.624: INFO: Waiting for pod client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f to disappear
May 27 06:44:26.630: INFO: Pod client-envvars-a7e0a4a2-d148-4f05-a009-23cb99d3e65f no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:26.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2627" for this suite.

• [SLOW TEST:6.232 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5799,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:26.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3074
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3074
I0527 06:44:26.770932      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3074, replica count: 2
I0527 06:44:29.823374      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:44:29.823: INFO: Creating new exec pod
May 27 06:44:32.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3074 exec execpodzvw7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 27 06:44:33.441: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 27 06:44:33.441: INFO: stdout: "externalname-service-mplck"
May 27 06:44:33.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3074 exec execpodzvw7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.49.58 80'
May 27 06:44:33.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.49.58 80\nConnection to 10.233.49.58 80 port [tcp/http] succeeded!\n"
May 27 06:44:33.723: INFO: stdout: "externalname-service-rvtxr"
May 27 06:44:33.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3074 exec execpodzvw7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.191 32681'
May 27 06:44:33.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.191 32681\nConnection to 192.168.121.191 32681 port [tcp/*] succeeded!\n"
May 27 06:44:33.930: INFO: stdout: ""
May 27 06:44:34.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3074 exec execpodzvw7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.191 32681'
May 27 06:44:35.204: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.191 32681\nConnection to 192.168.121.191 32681 port [tcp/*] succeeded!\n"
May 27 06:44:35.204: INFO: stdout: "externalname-service-mplck"
May 27 06:44:35.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-3074 exec execpodzvw7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 32681'
May 27 06:44:35.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 32681\nConnection to 192.168.121.43 32681 port [tcp/*] succeeded!\n"
May 27 06:44:35.476: INFO: stdout: "externalname-service-mplck"
May 27 06:44:35.476: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:35.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3074" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.901 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":305,"skipped":5814,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:35.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:44:35.602: INFO: Creating ReplicaSet my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894
May 27 06:44:35.636: INFO: Pod name my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894: Found 0 pods out of 1
May 27 06:44:40.650: INFO: Pod name my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894: Found 1 pods out of 1
May 27 06:44:40.650: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894" is running
May 27 06:44:40.656: INFO: Pod "my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894-6vxzn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 06:44:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 06:44:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 06:44:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-27 06:44:35 +0000 UTC Reason: Message:}])
May 27 06:44:40.657: INFO: Trying to dial the pod
May 27 06:44:45.684: INFO: Controller my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894: Got expected result from replica 1 [my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894-6vxzn]: "my-hostname-basic-518a350e-6d3c-4aca-b529-aacdf36bc894-6vxzn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:45.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9943" for this suite.

• [SLOW TEST:10.146 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":306,"skipped":5816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:45.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 27 06:44:45.790: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5763  c688ac8f-454e-42c5-a2f1-0b39adf16c5f 37353 0 2022-05-27 06:44:45 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-05-27 06:44:45 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nsp47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nsp47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 27 06:44:45.795: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 27 06:44:47.808: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 27 06:44:49.807: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 27 06:44:49.807: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5763 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:44:49.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:44:49.809: INFO: ExecWithOptions: Clientset creation
May 27 06:44:49.809: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5763/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
May 27 06:44:49.930: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5763 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 27 06:44:49.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
May 27 06:44:49.932: INFO: ExecWithOptions: Clientset creation
May 27 06:44:49.932: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5763/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 27 06:44:50.049: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:50.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5763" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":307,"skipped":5848,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:50.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:44:50.198: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 06:44:55.233: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
May 27 06:44:55.258: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
May 27 06:44:55.283: INFO: observed ReplicaSet test-rs in namespace replicaset-4977 with ReadyReplicas 1, AvailableReplicas 1
May 27 06:44:55.298: INFO: observed ReplicaSet test-rs in namespace replicaset-4977 with ReadyReplicas 1, AvailableReplicas 1
May 27 06:44:55.390: INFO: observed ReplicaSet test-rs in namespace replicaset-4977 with ReadyReplicas 1, AvailableReplicas 1
May 27 06:44:55.402: INFO: observed ReplicaSet test-rs in namespace replicaset-4977 with ReadyReplicas 1, AvailableReplicas 1
May 27 06:44:56.764: INFO: observed ReplicaSet test-rs in namespace replicaset-4977 with ReadyReplicas 2, AvailableReplicas 2
May 27 06:44:57.350: INFO: observed Replicaset test-rs in namespace replicaset-4977 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:44:57.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4977" for this suite.

• [SLOW TEST:7.266 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":308,"skipped":5857,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:44:57.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:44:57.443: INFO: created pod
May 27 06:44:57.443: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8635" to be "Succeeded or Failed"
May 27 06:44:57.457: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.708799ms
May 27 06:44:59.468: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025431875s
May 27 06:45:01.482: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039316492s
STEP: Saw pod success
May 27 06:45:01.482: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 27 06:45:31.483: INFO: polling logs
May 27 06:45:31.522: INFO: Pod logs: 
2022/05/27 06:44:58 OK: Got token
2022/05/27 06:44:58 validating with in-cluster discovery
2022/05/27 06:44:58 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/05/27 06:44:58 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1653634497, NotBefore:1653633897, IssuedAt:1653633897, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f9c087c9-4859-4fb5-b687-b52f0b3b2a06"}}}
2022/05/27 06:44:58 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/05/27 06:44:58 OK: Validated signature on JWT
2022/05/27 06:44:58 OK: Got valid claims from token!
2022/05/27 06:44:58 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8635:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1653634497, NotBefore:1653633897, IssuedAt:1653633897, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8635", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f9c087c9-4859-4fb5-b687-b52f0b3b2a06"}}}

May 27 06:45:31.522: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:45:31.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8635" for this suite.

• [SLOW TEST:34.184 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":309,"skipped":5858,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:45:31.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:45:31.643: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3" in namespace "projected-9651" to be "Succeeded or Failed"
May 27 06:45:31.649: INFO: Pod "downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10805ms
May 27 06:45:33.664: INFO: Pod "downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020327149s
May 27 06:45:35.682: INFO: Pod "downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038970431s
STEP: Saw pod success
May 27 06:45:35.682: INFO: Pod "downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3" satisfied condition "Succeeded or Failed"
May 27 06:45:35.687: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3 container client-container: <nil>
STEP: delete the pod
May 27 06:45:35.731: INFO: Waiting for pod downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3 to disappear
May 27 06:45:35.737: INFO: Pod downwardapi-volume-be49eef3-f893-482d-ab23-c345d32655d3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:45:35.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9651" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":5867,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:45:35.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 27 06:45:35.825: INFO: Waiting up to 5m0s for pod "downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23" in namespace "downward-api-4638" to be "Succeeded or Failed"
May 27 06:45:35.832: INFO: Pod "downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23": Phase="Pending", Reason="", readiness=false. Elapsed: 7.82281ms
May 27 06:45:37.853: INFO: Pod "downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028225146s
May 27 06:45:39.867: INFO: Pod "downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042876386s
STEP: Saw pod success
May 27 06:45:39.868: INFO: Pod "downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23" satisfied condition "Succeeded or Failed"
May 27 06:45:39.875: INFO: Trying to get logs from node ha9zeyohpei4-3 pod downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23 container dapi-container: <nil>
STEP: delete the pod
May 27 06:45:39.920: INFO: Waiting for pod downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23 to disappear
May 27 06:45:39.944: INFO: Pod downward-api-34dc8add-0e5a-45f6-958c-ac93031b8b23 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:45:39.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4638" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5873,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:45:39.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:45:40.083: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 27 06:45:40.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:45:40.118: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:45:41.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:45:41.232: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:45:42.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 06:45:42.153: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:45:43.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 06:45:43.141: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 27 06:45:43.202: INFO: Wrong image for pod: daemon-set-7hvj9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:43.202: INFO: Wrong image for pod: daemon-set-brllw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:43.202: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:44.222: INFO: Wrong image for pod: daemon-set-brllw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:44.222: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:45.219: INFO: Wrong image for pod: daemon-set-brllw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:45.219: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:46.230: INFO: Wrong image for pod: daemon-set-brllw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:46.230: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:46.231: INFO: Pod daemon-set-xdzqp is not available
May 27 06:45:47.221: INFO: Wrong image for pod: daemon-set-brllw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:47.221: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:47.221: INFO: Pod daemon-set-xdzqp is not available
May 27 06:45:48.223: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:49.224: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:49.224: INFO: Pod daemon-set-xxksf is not available
May 27 06:45:50.216: INFO: Wrong image for pod: daemon-set-h5nhx. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 27 06:45:50.216: INFO: Pod daemon-set-xxksf is not available
May 27 06:45:53.222: INFO: Pod daemon-set-q522t is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 27 06:45:53.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 27 06:45:53.257: INFO: Node ha9zeyohpei4-3 is running 0 daemon pod, expected 1
May 27 06:45:54.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 06:45:54.279: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9386, will wait for the garbage collector to delete the pods
May 27 06:45:54.386: INFO: Deleting DaemonSet.extensions daemon-set took: 14.473139ms
May 27 06:45:54.488: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.18134ms
May 27 06:45:57.305: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:45:57.305: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 06:45:57.309: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37843"},"items":null}

May 27 06:45:57.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37843"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:45:57.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9386" for this suite.

• [SLOW TEST:17.389 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":312,"skipped":5881,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:45:57.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-c802c22f-c0d5-4149-bf3c-09f48c7fd96f
STEP: Creating a pod to test consume secrets
May 27 06:45:57.438: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5" in namespace "projected-7180" to be "Succeeded or Failed"
May 27 06:45:57.452: INFO: Pod "pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.064207ms
May 27 06:45:59.469: INFO: Pod "pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030247429s
May 27 06:46:01.491: INFO: Pod "pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052169835s
STEP: Saw pod success
May 27 06:46:01.491: INFO: Pod "pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5" satisfied condition "Succeeded or Failed"
May 27 06:46:01.498: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 27 06:46:01.530: INFO: Waiting for pod pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5 to disappear
May 27 06:46:01.540: INFO: Pod pod-projected-secrets-473469d6-7f63-4f80-a980-c7b3dfd4d7e5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:01.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7180" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":5886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:01.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:01.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3627
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:03.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8066" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:03.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3627" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":314,"skipped":6005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:03.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:46:03.971: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 27 06:46:08.997: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 27 06:46:08.997: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 06:46:11.084: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7324  8d598467-6274-436d-b9b6-8dcb21c0b07f 38042 1 2022-05-27 06:46:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-05-27 06:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:46:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c745f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 06:46:09 +0000 UTC,LastTransitionTime:2022-05-27 06:46:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-56cd759769" has successfully progressed.,LastUpdateTime:2022-05-27 06:46:10 +0000 UTC,LastTransitionTime:2022-05-27 06:46:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 06:46:11.092: INFO: New ReplicaSet "test-cleanup-deployment-56cd759769" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-56cd759769  deployment-7324  036dfa3f-6085-4b06-83da-6a41d059b410 38031 1 2022-05-27 06:46:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8d598467-6274-436d-b9b6-8dcb21c0b07f 0xc004c749d7 0xc004c749d8}] []  [{kube-controller-manager Update apps/v1 2022-05-27 06:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d598467-6274-436d-b9b6-8dcb21c0b07f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:46:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 56cd759769,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c74a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 06:46:11.101: INFO: Pod "test-cleanup-deployment-56cd759769-8pkwk" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-56cd759769-8pkwk test-cleanup-deployment-56cd759769- deployment-7324  b878bf8a-b467-4f14-a4e8-d01f8b9530eb 38030 0 2022-05-27 06:46:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-56cd759769 036dfa3f-6085-4b06-83da-6a41d059b410 0xc004c25377 0xc004c25378}] []  [{kube-controller-manager Update v1 2022-05-27 06:46:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"036dfa3f-6085-4b06-83da-6a41d059b410\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 06:46:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5zb6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5zb6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:46:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:46:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:46:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:46:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:10.233.66.114,StartTime:2022-05-27 06:46:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 06:46:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:cri-o://07b733e0af913148ab294c07323ae630bdca2041fd2595846ae3f59970255f46,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7324" for this suite.

• [SLOW TEST:7.219 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":315,"skipped":6037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:11.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
May 27 06:46:11.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9170 create -f -'
May 27 06:46:11.822: INFO: stderr: ""
May 27 06:46:11.822: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 27 06:46:11.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9170 diff -f -'
May 27 06:46:12.184: INFO: rc: 1
May 27 06:46:12.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-9170 delete -f -'
May 27 06:46:12.402: INFO: stderr: ""
May 27 06:46:12.402: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:12.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9170" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":316,"skipped":6061,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:12.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:46:12.533: INFO: Endpoints addresses: [192.168.121.209 192.168.121.43] , ports: [6443]
May 27 06:46:12.533: INFO: EndpointSlices addresses: [192.168.121.209 192.168.121.43] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:12.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8088" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":317,"skipped":6066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 27 06:46:12.677: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7" in namespace "downward-api-5886" to be "Succeeded or Failed"
May 27 06:46:12.690: INFO: Pod "downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.187843ms
May 27 06:46:14.710: INFO: Pod "downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.033597535s
May 27 06:46:16.737: INFO: Pod "downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7": Phase="Running", Reason="", readiness=false. Elapsed: 4.059888469s
May 27 06:46:18.752: INFO: Pod "downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074664209s
STEP: Saw pod success
May 27 06:46:18.752: INFO: Pod "downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7" satisfied condition "Succeeded or Failed"
May 27 06:46:18.758: INFO: Trying to get logs from node ha9zeyohpei4-2 pod downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7 container client-container: <nil>
STEP: delete the pod
May 27 06:46:18.813: INFO: Waiting for pod downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7 to disappear
May 27 06:46:18.819: INFO: Pod downwardapi-volume-bc1fda65-18a8-4b56-ae6c-e5ac724533a7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:18.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5886" for this suite.

• [SLOW TEST:6.265 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":6095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:18.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-3a08e04c-b42a-4c60-b73a-7178bb2dc00c
STEP: Creating a pod to test consume secrets
May 27 06:46:18.948: INFO: Waiting up to 5m0s for pod "pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa" in namespace "secrets-2423" to be "Succeeded or Failed"
May 27 06:46:18.962: INFO: Pod "pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa": Phase="Pending", Reason="", readiness=false. Elapsed: 13.62086ms
May 27 06:46:20.974: INFO: Pod "pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025810446s
May 27 06:46:22.987: INFO: Pod "pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039307636s
STEP: Saw pod success
May 27 06:46:22.987: INFO: Pod "pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa" satisfied condition "Succeeded or Failed"
May 27 06:46:22.993: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa container secret-volume-test: <nil>
STEP: delete the pod
May 27 06:46:23.032: INFO: Waiting for pod pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa to disappear
May 27 06:46:23.040: INFO: Pod pod-secrets-7a75d1e3-e14a-46f6-be29-055ac5d8dcaa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:46:23.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2423" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":319,"skipped":6164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:46:23.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 27 06:46:23.144: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 06:47:23.213: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:47:23.219: INFO: Starting informer...
STEP: Starting pods...
May 27 06:47:23.465: INFO: Pod1 is running on ha9zeyohpei4-3. Tainting Node
May 27 06:47:25.711: INFO: Pod2 is running on ha9zeyohpei4-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 27 06:47:31.744: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 27 06:47:51.767: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:47:51.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7494" for this suite.

• [SLOW TEST:88.750 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":320,"skipped":6246,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:47:51.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 27 06:47:51.870: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 27 06:47:51.887: INFO: Waiting for terminating namespaces to be deleted...
May 27 06:47:51.894: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-1 before test
May 27 06:47:51.910: INFO: echo-other-node-59d779959c-4vckc from cilium-test started at 2022-05-27 05:12:19 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.910: INFO: 	Container echo-other-node ready: true, restart count 0
May 27 06:47:51.910: INFO: cilium-node-init-ph2ps from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.910: INFO: 	Container node-init ready: true, restart count 0
May 27 06:47:51.911: INFO: cilium-wjfhd from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.911: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 06:47:51.911: INFO: coredns-64897985d-p8h24 from kube-system started at 2022-05-27 06:42:07 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.911: INFO: 	Container coredns ready: true, restart count 0
May 27 06:47:51.911: INFO: kube-addon-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.911: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 06:47:51.911: INFO: kube-apiserver-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.911: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 06:47:51.911: INFO: kube-controller-manager-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.912: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 06:47:51.912: INFO: kube-proxy-4t9kh from kube-system started at 2022-05-27 04:57:26 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.912: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 06:47:51.912: INFO: kube-scheduler-ha9zeyohpei4-1 from kube-system started at 2022-05-27 04:57:23 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.912: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 06:47:51.912: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-lcdqs from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:47:51.912: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:47:51.913: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 06:47:51.913: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-2 before test
May 27 06:47:51.929: INFO: client-7568bc7f86-k8d7m from cilium-test started at 2022-05-27 06:47:25 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container client ready: true, restart count 0
May 27 06:47:51.929: INFO: client2-686d5f784b-v7g9l from cilium-test started at 2022-05-27 06:47:27 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container client2 ready: true, restart count 0
May 27 06:47:51.929: INFO: echo-same-node-5767b7b99d-zmmb8 from cilium-test started at 2022-05-27 06:47:27 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container echo-same-node ready: false, restart count 0
May 27 06:47:51.929: INFO: cilium-dfq7b from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 06:47:51.929: INFO: cilium-node-init-942c6 from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container node-init ready: true, restart count 0
May 27 06:47:51.929: INFO: coredns-64897985d-5hvb2 from kube-system started at 2022-05-27 05:10:55 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container coredns ready: true, restart count 0
May 27 06:47:51.929: INFO: kube-addon-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 05:09:50 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container kube-addon-manager ready: true, restart count 0
May 27 06:47:51.929: INFO: kube-apiserver-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container kube-apiserver ready: true, restart count 0
May 27 06:47:51.929: INFO: kube-controller-manager-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:57:59 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container kube-controller-manager ready: true, restart count 0
May 27 06:47:51.929: INFO: kube-proxy-lpjdp from kube-system started at 2022-05-27 04:58:12 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 06:47:51.929: INFO: kube-scheduler-ha9zeyohpei4-2 from kube-system started at 2022-05-27 04:58:32 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container kube-scheduler ready: true, restart count 0
May 27 06:47:51.929: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-sgkpw from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:47:51.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:47:51.929: INFO: 	Container systemd-logs ready: true, restart count 0
May 27 06:47:51.929: INFO: 
Logging pods the apiserver thinks is on node ha9zeyohpei4-3 before test
May 27 06:47:51.942: INFO: client-7568bc7f86-fkhw6 from cilium-test started at 2022-05-27 06:42:07 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.942: INFO: 	Container client ready: true, restart count 0
May 27 06:47:51.943: INFO: client2-686d5f784b-drrz2 from cilium-test started at 2022-05-27 06:42:07 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.943: INFO: 	Container client2 ready: true, restart count 0
May 27 06:47:51.943: INFO: echo-same-node-5767b7b99d-lbms7 from cilium-test started at 2022-05-27 06:42:07 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.943: INFO: 	Container echo-same-node ready: true, restart count 0
May 27 06:47:51.943: INFO: cilium-7dvhf from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.943: INFO: 	Container cilium-agent ready: true, restart count 0
May 27 06:47:51.943: INFO: cilium-node-init-zxkdp from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.943: INFO: 	Container node-init ready: true, restart count 0
May 27 06:47:51.943: INFO: cilium-operator-59d6f769d4-nlxfs from kube-system started at 2022-05-27 05:10:02 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.943: INFO: 	Container cilium-operator ready: true, restart count 0
May 27 06:47:51.943: INFO: kube-proxy-hzzmk from kube-system started at 2022-05-27 04:58:38 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.943: INFO: 	Container kube-proxy ready: true, restart count 0
May 27 06:47:51.943: INFO: sonobuoy from sonobuoy started at 2022-05-27 05:16:07 +0000 UTC (1 container statuses recorded)
May 27 06:47:51.944: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 27 06:47:51.944: INFO: sonobuoy-e2e-job-7e88fd63f0e849eb from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:47:51.944: INFO: 	Container e2e ready: true, restart count 0
May 27 06:47:51.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:47:51.944: INFO: sonobuoy-systemd-logs-daemon-set-ac2afe454c324363-tz8zz from sonobuoy started at 2022-05-27 05:16:21 +0000 UTC (2 container statuses recorded)
May 27 06:47:51.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 27 06:47:51.944: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-de56ece0-50e4-4338-bf35-707ad67a55e5 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-de56ece0-50e4-4338-bf35-707ad67a55e5 off the node ha9zeyohpei4-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-de56ece0-50e4-4338-bf35-707ad67a55e5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:47:56.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9532" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":321,"skipped":6267,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:47:56.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:47:56.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:48:02.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2457" for this suite.

• [SLOW TEST:6.504 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":322,"skipped":6270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:48:02.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:54:00.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4363" for this suite.

• [SLOW TEST:358.197 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":323,"skipped":6328,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:54:00.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:54:01.020: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 27 06:54:01.055: INFO: Pod name sample-pod: Found 0 pods out of 1
May 27 06:54:06.097: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 27 06:54:06.098: INFO: Creating deployment "test-rolling-update-deployment"
May 27 06:54:06.109: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 27 06:54:06.144: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 27 06:54:08.167: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 27 06:54:08.173: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 27 06:54:08.189: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7770  3ba4eebd-40bb-44c4-8155-98ccf08e5e62 39498 1 2022-05-27 06:54:06 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-05-27 06:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:54:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e33db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-27 06:54:06 +0000 UTC,LastTransitionTime:2022-05-27 06:54:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-05-27 06:54:08 +0000 UTC,LastTransitionTime:2022-05-27 06:54:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 27 06:54:08.195: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-7770  a0224be1-e438-4041-a210-b46523df6fb0 39487 1 2022-05-27 06:54:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3ba4eebd-40bb-44c4-8155-98ccf08e5e62 0xc003ecf677 0xc003ecf678}] []  [{kube-controller-manager Update apps/v1 2022-05-27 06:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ba4eebd-40bb-44c4-8155-98ccf08e5e62\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:54:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ecf728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 27 06:54:08.195: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 27 06:54:08.195: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7770  ee3d89e1-c24c-4e44-8ba2-5960c017a38b 39497 2 2022-05-27 06:54:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3ba4eebd-40bb-44c4-8155-98ccf08e5e62 0xc003ecf547 0xc003ecf548}] []  [{e2e.test Update apps/v1 2022-05-27 06:54:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:54:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ba4eebd-40bb-44c4-8155-98ccf08e5e62\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-27 06:54:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ecf608 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 27 06:54:08.200: INFO: Pod "test-rolling-update-deployment-796dbc4547-v4qx2" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-v4qx2 test-rolling-update-deployment-796dbc4547- deployment-7770  0f558f18-8f6c-41dc-878b-2a5480739ea5 39486 0 2022-05-27 06:54:06 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 a0224be1-e438-4041-a210-b46523df6fb0 0xc004eb01a7 0xc004eb01a8}] []  [{kube-controller-manager Update v1 2022-05-27 06:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0224be1-e438-4041-a210-b46523df6fb0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-27 06:54:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqnq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqnq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ha9zeyohpei4-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-27 06:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.191,PodIP:10.233.66.7,StartTime:2022-05-27 06:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-27 06:54:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:cri-o://d615954981152525641c4f073552ff7e85b0442c01f2605a2cc0a0f023c7a1f2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:54:08.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7770" for this suite.

• [SLOW TEST:7.269 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":324,"skipped":6328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:54:08.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 27 06:54:08.350: INFO: Waiting up to 1m0s for all nodes to be ready
May 27 06:55:08.427: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
May 27 06:55:08.486: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 27 06:55:08.497: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 27 06:55:08.537: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 27 06:55:08.550: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 27 06:55:08.600: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 27 06:55:08.618: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:55:26.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3575" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:78.646 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":325,"skipped":6374,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:55:26.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
May 27 06:55:26.972: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 27 06:55:28.993: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 27 06:55:30.042: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:55:31.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8305" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":326,"skipped":6392,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:55:31.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-9b3dbd1b-d204-441f-aad1-9c319fb58e51
STEP: Creating a pod to test consume configMaps
May 27 06:55:31.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037" in namespace "configmap-5528" to be "Succeeded or Failed"
May 27 06:55:31.260: INFO: Pod "pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037": Phase="Pending", Reason="", readiness=false. Elapsed: 44.895735ms
May 27 06:55:33.283: INFO: Pod "pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068734035s
May 27 06:55:35.293: INFO: Pod "pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078674066s
May 27 06:55:37.307: INFO: Pod "pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092346922s
STEP: Saw pod success
May 27 06:55:37.307: INFO: Pod "pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037" satisfied condition "Succeeded or Failed"
May 27 06:55:37.314: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037 container agnhost-container: <nil>
STEP: delete the pod
May 27 06:55:37.379: INFO: Waiting for pod pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037 to disappear
May 27 06:55:37.384: INFO: Pod pod-configmaps-fa21d666-eacb-497d-b53e-2225ebcee037 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:55:37.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5528" for this suite.

• [SLOW TEST:6.277 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":327,"skipped":6394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:55:37.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:55:37.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:55:38.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5629" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":328,"skipped":6428,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:55:38.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-677da36e-222d-4d1a-b3d5-eeadd39ab9b3
STEP: Creating a pod to test consume secrets
May 27 06:55:38.372: INFO: Waiting up to 5m0s for pod "pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9" in namespace "secrets-5621" to be "Succeeded or Failed"
May 27 06:55:38.386: INFO: Pod "pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.419522ms
May 27 06:55:40.403: INFO: Pod "pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9": Phase="Running", Reason="", readiness=true. Elapsed: 2.030655099s
May 27 06:55:42.414: INFO: Pod "pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9": Phase="Running", Reason="", readiness=false. Elapsed: 4.041779109s
May 27 06:55:44.430: INFO: Pod "pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057703173s
STEP: Saw pod success
May 27 06:55:44.430: INFO: Pod "pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9" satisfied condition "Succeeded or Failed"
May 27 06:55:44.437: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9 container secret-volume-test: <nil>
STEP: delete the pod
May 27 06:55:44.472: INFO: Waiting for pod pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9 to disappear
May 27 06:55:44.479: INFO: Pod pod-secrets-f4fca47b-40b5-4cee-9457-2c938d45cba9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:55:44.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5621" for this suite.

• [SLOW TEST:6.205 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6443,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:55:44.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3317.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 27 06:55:46.624: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.631: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.637: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.643: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.649: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.655: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.660: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.666: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:46.666: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:55:51.675: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.684: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.693: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.709: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.718: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.727: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.744: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.756: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:51.756: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:55:56.675: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.682: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.689: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.694: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.700: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.707: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.714: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.730: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:55:56.730: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:56:01.673: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.681: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.689: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.695: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.708: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.716: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.721: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.731: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:01.731: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:56:06.677: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.688: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.694: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.701: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.707: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.713: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.728: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.734: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:06.734: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:56:11.677: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.683: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.690: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.696: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.704: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.717: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.731: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.739: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:11.739: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:56:16.673: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.680: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.685: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.691: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.697: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.703: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.709: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.718: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local from pod dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220: the server could not find the requested resource (get pods dns-test-648ca75e-4a52-4820-b565-730145f31220)
May 27 06:56:16.718: INFO: Lookups using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3317.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3317.svc.cluster.local jessie_udp@dns-test-service-2.dns-3317.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3317.svc.cluster.local]

May 27 06:56:21.729: INFO: DNS probes using dns-3317/dns-test-648ca75e-4a52-4820-b565-730145f31220 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:56:21.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3317" for this suite.

• [SLOW TEST:37.339 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":330,"skipped":6443,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:56:21.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:56:23.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 06:56:25.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 6, 56, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 56, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 6, 56, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 56, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:56:28.754: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:56:29.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6414" for this suite.
STEP: Destroying namespace "webhook-6414-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.511 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":331,"skipped":6453,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:56:29.354: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-8017
STEP: creating service affinity-nodeport-transition in namespace services-8017
STEP: creating replication controller affinity-nodeport-transition in namespace services-8017
I0527 06:56:29.488185      14 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8017, replica count: 3
I0527 06:56:32.540449      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:56:32.572: INFO: Creating new exec pod
May 27 06:56:35.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8017 exec execpod-affinityrknmh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May 27 06:56:36.059: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 27 06:56:36.059: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:56:36.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8017 exec execpod-affinityrknmh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.42.116 80'
May 27 06:56:36.295: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.42.116 80\nConnection to 10.233.42.116 80 port [tcp/http] succeeded!\n"
May 27 06:56:36.295: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:56:36.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8017 exec execpod-affinityrknmh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.209 31829'
May 27 06:56:36.541: INFO: stderr: "+ + ncecho -v -t hostName -w 2\n 192.168.121.209 31829\nConnection to 192.168.121.209 31829 port [tcp/*] succeeded!\n"
May 27 06:56:36.541: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:56:36.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8017 exec execpod-affinityrknmh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.191 31829'
May 27 06:56:36.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.191 31829\nConnection to 192.168.121.191 31829 port [tcp/*] succeeded!\n"
May 27 06:56:36.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:56:36.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8017 exec execpod-affinityrknmh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.43:31829/ ; done'
May 27 06:56:37.305: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n"
May 27 06:56:37.306: INFO: stdout: "\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-htsnh\naffinity-nodeport-transition-zp7gz\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-htsnh\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-htsnh\naffinity-nodeport-transition-zp7gz\naffinity-nodeport-transition-zp7gz\naffinity-nodeport-transition-htsnh\naffinity-nodeport-transition-htsnh\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-zp7gz\naffinity-nodeport-transition-qnnzt"
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-htsnh
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-zp7gz
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-htsnh
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-htsnh
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-zp7gz
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-zp7gz
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-htsnh
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-htsnh
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-zp7gz
May 27 06:56:37.306: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-8017 exec execpod-affinityrknmh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.43:31829/ ; done'
May 27 06:56:37.767: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.43:31829/\n"
May 27 06:56:37.767: INFO: stdout: "\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt\naffinity-nodeport-transition-qnnzt"
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Received response from host: affinity-nodeport-transition-qnnzt
May 27 06:56:37.767: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8017, will wait for the garbage collector to delete the pods
May 27 06:56:37.861: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.623195ms
May 27 06:56:37.963: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.983342ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:56:40.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8017" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.725 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":332,"skipped":6466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:56:40.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:56:41.132: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 27 06:56:43.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 27, 6, 56, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 56, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 27, 6, 56, 41, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 27, 6, 56, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:56:46.193: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 27 06:56:46.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8621-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:56:49.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8366" for this suite.
STEP: Destroying namespace "webhook-8366-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.731 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":333,"skipped":6507,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:56:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:56:50.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2213" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":334,"skipped":6514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:56:50.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-p256
STEP: Creating a pod to test atomic-volume-subpath
May 27 06:56:50.220: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p256" in namespace "subpath-778" to be "Succeeded or Failed"
May 27 06:56:50.271: INFO: Pod "pod-subpath-test-projected-p256": Phase="Pending", Reason="", readiness=false. Elapsed: 51.769166ms
May 27 06:56:52.281: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 2.061453771s
May 27 06:56:54.301: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 4.081731479s
May 27 06:56:56.317: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 6.097002927s
May 27 06:56:58.331: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 8.111889846s
May 27 06:57:00.344: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 10.124084067s
May 27 06:57:02.355: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 12.135791142s
May 27 06:57:04.368: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 14.1484964s
May 27 06:57:06.381: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 16.161483587s
May 27 06:57:08.392: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 18.172688979s
May 27 06:57:10.403: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=true. Elapsed: 20.183722966s
May 27 06:57:12.414: INFO: Pod "pod-subpath-test-projected-p256": Phase="Running", Reason="", readiness=false. Elapsed: 22.194497526s
May 27 06:57:14.430: INFO: Pod "pod-subpath-test-projected-p256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.210084021s
STEP: Saw pod success
May 27 06:57:14.430: INFO: Pod "pod-subpath-test-projected-p256" satisfied condition "Succeeded or Failed"
May 27 06:57:14.436: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-subpath-test-projected-p256 container test-container-subpath-projected-p256: <nil>
STEP: delete the pod
May 27 06:57:14.465: INFO: Waiting for pod pod-subpath-test-projected-p256 to disappear
May 27 06:57:14.471: INFO: Pod pod-subpath-test-projected-p256 no longer exists
STEP: Deleting pod pod-subpath-test-projected-p256
May 27 06:57:14.471: INFO: Deleting pod "pod-subpath-test-projected-p256" in namespace "subpath-778"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:57:14.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-778" for this suite.

• [SLOW TEST:24.427 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":335,"skipped":6569,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:57:14.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
May 27 06:57:14.555: INFO: Waiting up to 5m0s for pod "var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100" in namespace "var-expansion-3412" to be "Succeeded or Failed"
May 27 06:57:14.560: INFO: Pod "var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756443ms
May 27 06:57:16.585: INFO: Pod "var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029643495s
May 27 06:57:18.599: INFO: Pod "var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043441014s
STEP: Saw pod success
May 27 06:57:18.599: INFO: Pod "var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100" satisfied condition "Succeeded or Failed"
May 27 06:57:18.605: INFO: Trying to get logs from node ha9zeyohpei4-3 pod var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100 container dapi-container: <nil>
STEP: delete the pod
May 27 06:57:18.643: INFO: Waiting for pod var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100 to disappear
May 27 06:57:18.648: INFO: Pod var-expansion-ae150f72-29dd-434f-98f9-1c8578acc100 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:57:18.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3412" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6580,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:57:18.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-6176
May 27 06:57:18.749: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 27 06:57:20.757: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 27 06:57:20.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6176 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 27 06:57:21.033: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 27 06:57:21.033: INFO: stdout: "iptables"
May 27 06:57:21.033: INFO: proxyMode: iptables
May 27 06:57:21.065: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 27 06:57:21.071: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6176
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6176
I0527 06:57:21.131070      14 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6176, replica count: 3
I0527 06:57:24.183280      14 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:57:24.199: INFO: Creating new exec pod
May 27 06:57:27.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6176 exec execpod-affinity2lc48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May 27 06:57:27.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 27 06:57:27.443: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:57:27.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6176 exec execpod-affinity2lc48 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.86 80'
May 27 06:57:27.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.86 80\nConnection to 10.233.55.86 80 port [tcp/http] succeeded!\n"
May 27 06:57:27.642: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 27 06:57:27.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6176 exec execpod-affinity2lc48 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.55.86:80/ ; done'
May 27 06:57:28.008: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n"
May 27 06:57:28.009: INFO: stdout: "\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5\naffinity-clusterip-timeout-sxzm5"
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Received response from host: affinity-clusterip-timeout-sxzm5
May 27 06:57:28.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6176 exec execpod-affinity2lc48 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.55.86:80/'
May 27 06:57:28.229: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n"
May 27 06:57:28.229: INFO: stdout: "affinity-clusterip-timeout-sxzm5"
May 27 06:57:48.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6176 exec execpod-affinity2lc48 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.55.86:80/'
May 27 06:57:48.444: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.55.86:80/\n"
May 27 06:57:48.444: INFO: stdout: "affinity-clusterip-timeout-pqnmh"
May 27 06:57:48.444: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6176, will wait for the garbage collector to delete the pods
May 27 06:57:48.552: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 19.288056ms
May 27 06:57:48.652: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.418945ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:57:50.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6176" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:32.079 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":337,"skipped":6586,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:57:50.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-2590/secret-test-6404c4da-730f-4fe5-9f51-99eb276d7252
STEP: Creating a pod to test consume secrets
May 27 06:57:50.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5" in namespace "secrets-2590" to be "Succeeded or Failed"
May 27 06:57:50.978: INFO: Pod "pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.254437ms
May 27 06:57:52.990: INFO: Pod "pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5": Phase="Running", Reason="", readiness=true. Elapsed: 2.027988954s
May 27 06:57:55.005: INFO: Pod "pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5": Phase="Running", Reason="", readiness=false. Elapsed: 4.043424883s
May 27 06:57:57.018: INFO: Pod "pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056706118s
STEP: Saw pod success
May 27 06:57:57.019: INFO: Pod "pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5" satisfied condition "Succeeded or Failed"
May 27 06:57:57.023: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5 container env-test: <nil>
STEP: delete the pod
May 27 06:57:57.063: INFO: Waiting for pod pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5 to disappear
May 27 06:57:57.076: INFO: Pod pod-configmaps-01f84b40-81c9-44a2-887f-00d04eb0bde5 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:57:57.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2590" for this suite.

• [SLOW TEST:6.359 seconds]
[sig-node] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":6591,"failed":0}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:57:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
May 27 06:57:57.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 create -f -'
May 27 06:57:58.785: INFO: stderr: ""
May 27 06:57:58.785: INFO: stdout: "pod/pause created\n"
May 27 06:57:58.785: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 27 06:57:58.785: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3423" to be "running and ready"
May 27 06:57:58.800: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 15.245017ms
May 27 06:58:00.810: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.024569717s
May 27 06:58:00.810: INFO: Pod "pause" satisfied condition "running and ready"
May 27 06:58:00.810: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
May 27 06:58:00.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 label pods pause testing-label=testing-label-value'
May 27 06:58:00.940: INFO: stderr: ""
May 27 06:58:00.940: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 27 06:58:00.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 get pod pause -L testing-label'
May 27 06:58:01.080: INFO: stderr: ""
May 27 06:58:01.080: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 27 06:58:01.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 label pods pause testing-label-'
May 27 06:58:01.210: INFO: stderr: ""
May 27 06:58:01.210: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 27 06:58:01.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 get pod pause -L testing-label'
May 27 06:58:01.334: INFO: stderr: ""
May 27 06:58:01.334: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
May 27 06:58:01.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 delete --grace-period=0 --force -f -'
May 27 06:58:01.453: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 27 06:58:01.454: INFO: stdout: "pod \"pause\" force deleted\n"
May 27 06:58:01.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 get rc,svc -l name=pause --no-headers'
May 27 06:58:01.592: INFO: stderr: "No resources found in kubectl-3423 namespace.\n"
May 27 06:58:01.592: INFO: stdout: ""
May 27 06:58:01.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=kubectl-3423 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 27 06:58:01.706: INFO: stderr: ""
May 27 06:58:01.706: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:01.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3423" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":339,"skipped":6591,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:01.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 27 06:58:02.541: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 27 06:58:05.605: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:05.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3081" for this suite.
STEP: Destroying namespace "webhook-3081-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":340,"skipped":6594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:05.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 27 06:58:06.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:58:06.076: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:58:07.257: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:58:07.257: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:58:08.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 27 06:58:08.100: INFO: Node ha9zeyohpei4-1 is running 0 daemon pod, expected 1
May 27 06:58:09.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 27 06:58:09.103: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
May 27 06:58:09.115: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
May 27 06:58:09.166: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
May 27 06:58:09.170: INFO: Observed &DaemonSet event: ADDED
May 27 06:58:09.172: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.172: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.174: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.175: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.175: INFO: Found daemon set daemon-set in namespace daemonsets-8542 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 06:58:09.175: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
May 27 06:58:09.191: INFO: Observed &DaemonSet event: ADDED
May 27 06:58:09.192: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.192: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.193: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.194: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.194: INFO: Observed daemon set daemon-set in namespace daemonsets-8542 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 27 06:58:09.194: INFO: Observed &DaemonSet event: MODIFIED
May 27 06:58:09.194: INFO: Found daemon set daemon-set in namespace daemonsets-8542 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May 27 06:58:09.194: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8542, will wait for the garbage collector to delete the pods
May 27 06:58:09.274: INFO: Deleting DaemonSet.extensions daemon-set took: 12.571469ms
May 27 06:58:09.375: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.143141ms
May 27 06:58:11.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 27 06:58:11.392: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 27 06:58:11.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41207"},"items":null}

May 27 06:58:11.410: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41208"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:11.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8542" for this suite.

• [SLOW TEST:5.578 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":341,"skipped":6616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:11.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:11.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2250" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":342,"skipped":6648,"failed":0}

------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:11.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-6973
STEP: creating replication controller nodeport-test in namespace services-6973
I0527 06:58:11.668719      14 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6973, replica count: 2
I0527 06:58:14.721343      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:58:14.721: INFO: Creating new exec pod
May 27 06:58:17.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 27 06:58:18.054: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 27 06:58:18.054: INFO: stdout: "nodeport-test-xmhsk"
May 27 06:58:18.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.44.125 80'
May 27 06:58:18.300: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.44.125 80\nConnection to 10.233.44.125 80 port [tcp/http] succeeded!\n"
May 27 06:58:18.300: INFO: stdout: ""
May 27 06:58:19.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.44.125 80'
May 27 06:58:19.540: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.44.125 80\nConnection to 10.233.44.125 80 port [tcp/http] succeeded!\n"
May 27 06:58:19.540: INFO: stdout: "nodeport-test-wtf7h"
May 27 06:58:19.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.209 30731'
May 27 06:58:19.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.209 30731\nConnection to 192.168.121.209 30731 port [tcp/*] succeeded!\n"
May 27 06:58:19.755: INFO: stdout: ""
May 27 06:58:20.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.209 30731'
May 27 06:58:20.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.209 30731\nConnection to 192.168.121.209 30731 port [tcp/*] succeeded!\n"
May 27 06:58:20.983: INFO: stdout: "nodeport-test-xmhsk"
May 27 06:58:20.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 30731'
May 27 06:58:21.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 30731\nConnection to 192.168.121.43 30731 port [tcp/*] succeeded!\n"
May 27 06:58:21.223: INFO: stdout: ""
May 27 06:58:22.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1830107335 --namespace=services-6973 exec execpodpkcqp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.43 30731'
May 27 06:58:22.452: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.43 30731\nConnection to 192.168.121.43 30731 port [tcp/*] succeeded!\n"
May 27 06:58:22.452: INFO: stdout: "nodeport-test-xmhsk"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:22.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6973" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.926 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":343,"skipped":6648,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:22.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-76273e2e-2894-48d9-aeaf-8a0fbebf7c23
STEP: Creating a pod to test consume configMaps
May 27 06:58:22.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361" in namespace "configmap-7125" to be "Succeeded or Failed"
May 27 06:58:22.551: INFO: Pod "pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638218ms
May 27 06:58:24.565: INFO: Pod "pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022475673s
May 27 06:58:26.579: INFO: Pod "pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036717523s
STEP: Saw pod success
May 27 06:58:26.579: INFO: Pod "pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361" satisfied condition "Succeeded or Failed"
May 27 06:58:26.584: INFO: Trying to get logs from node ha9zeyohpei4-3 pod pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361 container agnhost-container: <nil>
STEP: delete the pod
May 27 06:58:26.610: INFO: Waiting for pod pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361 to disappear
May 27 06:58:26.615: INFO: Pod pod-configmaps-3176ce64-820c-449b-8f31-8c312880e361 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:26.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7125" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6669,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:26.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-s4rlv in namespace proxy-499
I0527 06:58:26.710393      14 runners.go:193] Created replication controller with name: proxy-service-s4rlv, namespace: proxy-499, replica count: 1
I0527 06:58:27.761186      14 runners.go:193] proxy-service-s4rlv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0527 06:58:28.761843      14 runners.go:193] proxy-service-s4rlv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 27 06:58:28.775: INFO: setup took 2.105606064s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 27 06:58:28.792: INFO: (0) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 16.216286ms)
May 27 06:58:28.816: INFO: (0) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 38.752472ms)
May 27 06:58:28.817: INFO: (0) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 40.399942ms)
May 27 06:58:28.819: INFO: (0) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 42.463922ms)
May 27 06:58:28.820: INFO: (0) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 43.991793ms)
May 27 06:58:28.820: INFO: (0) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 43.168206ms)
May 27 06:58:28.823: INFO: (0) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 45.84876ms)
May 27 06:58:28.830: INFO: (0) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 52.00331ms)
May 27 06:58:28.834: INFO: (0) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 56.441055ms)
May 27 06:58:28.834: INFO: (0) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 56.458423ms)
May 27 06:58:28.839: INFO: (0) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 60.255565ms)
May 27 06:58:28.840: INFO: (0) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 61.579945ms)
May 27 06:58:28.840: INFO: (0) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 61.809497ms)
May 27 06:58:28.843: INFO: (0) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 65.103572ms)
May 27 06:58:28.844: INFO: (0) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 65.543962ms)
May 27 06:58:28.844: INFO: (0) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 67.697913ms)
May 27 06:58:28.874: INFO: (1) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 29.683546ms)
May 27 06:58:28.875: INFO: (1) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 29.055596ms)
May 27 06:58:28.879: INFO: (1) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 32.301312ms)
May 27 06:58:28.879: INFO: (1) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 33.402615ms)
May 27 06:58:28.879: INFO: (1) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 32.522491ms)
May 27 06:58:28.879: INFO: (1) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 33.278102ms)
May 27 06:58:28.880: INFO: (1) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 33.32551ms)
May 27 06:58:28.881: INFO: (1) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 33.679212ms)
May 27 06:58:28.899: INFO: (1) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 54.429031ms)
May 27 06:58:28.900: INFO: (1) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 52.682332ms)
May 27 06:58:28.900: INFO: (1) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 53.381031ms)
May 27 06:58:28.900: INFO: (1) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 55.017241ms)
May 27 06:58:28.900: INFO: (1) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 54.802429ms)
May 27 06:58:28.901: INFO: (1) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 54.03922ms)
May 27 06:58:28.901: INFO: (1) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 54.776364ms)
May 27 06:58:28.908: INFO: (1) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 61.317659ms)
May 27 06:58:28.919: INFO: (2) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 9.628835ms)
May 27 06:58:28.953: INFO: (2) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 43.918234ms)
May 27 06:58:28.953: INFO: (2) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 43.966936ms)
May 27 06:58:28.953: INFO: (2) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 43.920522ms)
May 27 06:58:28.953: INFO: (2) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 44.109581ms)
May 27 06:58:28.953: INFO: (2) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 44.238872ms)
May 27 06:58:28.953: INFO: (2) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 44.608706ms)
May 27 06:58:28.954: INFO: (2) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 44.566099ms)
May 27 06:58:28.957: INFO: (2) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 48.575657ms)
May 27 06:58:28.958: INFO: (2) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 48.723736ms)
May 27 06:58:28.960: INFO: (2) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 51.440915ms)
May 27 06:58:28.968: INFO: (2) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 59.49195ms)
May 27 06:58:28.971: INFO: (2) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 62.174645ms)
May 27 06:58:28.971: INFO: (2) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 61.775882ms)
May 27 06:58:28.971: INFO: (2) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 62.098045ms)
May 27 06:58:28.971: INFO: (2) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 62.361335ms)
May 27 06:58:28.984: INFO: (3) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 12.90097ms)
May 27 06:58:28.993: INFO: (3) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 21.088791ms)
May 27 06:58:28.993: INFO: (3) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 21.463217ms)
May 27 06:58:28.996: INFO: (3) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 23.243466ms)
May 27 06:58:28.996: INFO: (3) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 24.704006ms)
May 27 06:58:28.996: INFO: (3) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 24.352145ms)
May 27 06:58:29.000: INFO: (3) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 27.280128ms)
May 27 06:58:29.000: INFO: (3) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 28.797773ms)
May 27 06:58:29.000: INFO: (3) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 28.108777ms)
May 27 06:58:29.002: INFO: (3) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 29.385376ms)
May 27 06:58:29.002: INFO: (3) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 29.290912ms)
May 27 06:58:29.002: INFO: (3) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 30.685053ms)
May 27 06:58:29.002: INFO: (3) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 30.43711ms)
May 27 06:58:29.005: INFO: (3) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 32.794737ms)
May 27 06:58:29.006: INFO: (3) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 33.415463ms)
May 27 06:58:29.020: INFO: (3) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 47.17761ms)
May 27 06:58:29.063: INFO: (4) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 42.512272ms)
May 27 06:58:29.064: INFO: (4) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 43.220629ms)
May 27 06:58:29.064: INFO: (4) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 42.998252ms)
May 27 06:58:29.067: INFO: (4) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 45.620675ms)
May 27 06:58:29.067: INFO: (4) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 46.884032ms)
May 27 06:58:29.067: INFO: (4) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 45.960705ms)
May 27 06:58:29.072: INFO: (4) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 51.254204ms)
May 27 06:58:29.073: INFO: (4) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 52.172545ms)
May 27 06:58:29.073: INFO: (4) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 51.77548ms)
May 27 06:58:29.073: INFO: (4) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 52.115628ms)
May 27 06:58:29.073: INFO: (4) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 52.214367ms)
May 27 06:58:29.074: INFO: (4) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 52.459346ms)
May 27 06:58:29.075: INFO: (4) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 53.935467ms)
May 27 06:58:29.076: INFO: (4) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 55.544483ms)
May 27 06:58:29.083: INFO: (4) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 61.400097ms)
May 27 06:58:29.089: INFO: (4) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 67.939698ms)
May 27 06:58:29.104: INFO: (5) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 14.178865ms)
May 27 06:58:29.104: INFO: (5) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 13.693943ms)
May 27 06:58:29.105: INFO: (5) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 14.559133ms)
May 27 06:58:29.105: INFO: (5) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 16.288477ms)
May 27 06:58:29.110: INFO: (5) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 19.348638ms)
May 27 06:58:29.112: INFO: (5) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 19.814219ms)
May 27 06:58:29.113: INFO: (5) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 21.135165ms)
May 27 06:58:29.114: INFO: (5) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 22.853175ms)
May 27 06:58:29.120: INFO: (5) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 29.601925ms)
May 27 06:58:29.120: INFO: (5) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 28.444673ms)
May 27 06:58:29.129: INFO: (5) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 37.260556ms)
May 27 06:58:29.130: INFO: (5) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 39.310218ms)
May 27 06:58:29.130: INFO: (5) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 39.250748ms)
May 27 06:58:29.130: INFO: (5) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 38.6404ms)
May 27 06:58:29.131: INFO: (5) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 39.51977ms)
May 27 06:58:29.140: INFO: (5) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 48.8677ms)
May 27 06:58:29.175: INFO: (6) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 33.568338ms)
May 27 06:58:29.175: INFO: (6) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 33.844234ms)
May 27 06:58:29.175: INFO: (6) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 34.560496ms)
May 27 06:58:29.175: INFO: (6) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 33.647188ms)
May 27 06:58:29.213: INFO: (6) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 71.730136ms)
May 27 06:58:29.214: INFO: (6) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 72.842678ms)
May 27 06:58:29.215: INFO: (6) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 74.383655ms)
May 27 06:58:29.216: INFO: (6) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 74.466659ms)
May 27 06:58:29.219: INFO: (6) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 78.475968ms)
May 27 06:58:29.219: INFO: (6) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 77.835399ms)
May 27 06:58:29.220: INFO: (6) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 78.944227ms)
May 27 06:58:29.220: INFO: (6) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 78.452919ms)
May 27 06:58:29.228: INFO: (6) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 86.065798ms)
May 27 06:58:29.228: INFO: (6) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 86.481968ms)
May 27 06:58:29.230: INFO: (6) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 87.84563ms)
May 27 06:58:29.232: INFO: (6) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 90.467514ms)
May 27 06:58:29.247: INFO: (7) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 13.369464ms)
May 27 06:58:29.248: INFO: (7) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 14.693164ms)
May 27 06:58:29.263: INFO: (7) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 29.174779ms)
May 27 06:58:29.264: INFO: (7) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 29.424522ms)
May 27 06:58:29.264: INFO: (7) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 29.632768ms)
May 27 06:58:29.264: INFO: (7) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 29.418397ms)
May 27 06:58:29.264: INFO: (7) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 29.134166ms)
May 27 06:58:29.265: INFO: (7) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 29.665368ms)
May 27 06:58:29.273: INFO: (7) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 38.39043ms)
May 27 06:58:29.273: INFO: (7) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 38.355592ms)
May 27 06:58:29.274: INFO: (7) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 40.821802ms)
May 27 06:58:29.274: INFO: (7) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 40.733073ms)
May 27 06:58:29.292: INFO: (7) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 58.875164ms)
May 27 06:58:29.294: INFO: (7) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 58.747045ms)
May 27 06:58:29.305: INFO: (7) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 69.931836ms)
May 27 06:58:29.305: INFO: (7) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 69.984236ms)
May 27 06:58:29.347: INFO: (8) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 41.577335ms)
May 27 06:58:29.347: INFO: (8) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 41.73718ms)
May 27 06:58:29.349: INFO: (8) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 42.716886ms)
May 27 06:58:29.354: INFO: (8) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 47.824914ms)
May 27 06:58:29.355: INFO: (8) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 48.759992ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 64.450262ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 64.690535ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 64.614536ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 64.643382ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 64.90849ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 64.909675ms)
May 27 06:58:29.371: INFO: (8) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 65.099796ms)
May 27 06:58:29.378: INFO: (8) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 71.824751ms)
May 27 06:58:29.381: INFO: (8) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 74.794129ms)
May 27 06:58:29.381: INFO: (8) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 74.767504ms)
May 27 06:58:29.384: INFO: (8) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 77.724276ms)
May 27 06:58:29.412: INFO: (9) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 27.087419ms)
May 27 06:58:29.412: INFO: (9) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 26.841787ms)
May 27 06:58:29.412: INFO: (9) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 26.625545ms)
May 27 06:58:29.416: INFO: (9) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 30.512988ms)
May 27 06:58:29.416: INFO: (9) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 30.078228ms)
May 27 06:58:29.416: INFO: (9) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 30.074053ms)
May 27 06:58:29.416: INFO: (9) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 31.348465ms)
May 27 06:58:29.424: INFO: (9) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 39.434797ms)
May 27 06:58:29.426: INFO: (9) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 40.491919ms)
May 27 06:58:29.427: INFO: (9) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 41.663474ms)
May 27 06:58:29.433: INFO: (9) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 48.500529ms)
May 27 06:58:29.441: INFO: (9) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 54.847095ms)
May 27 06:58:29.442: INFO: (9) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 55.88417ms)
May 27 06:58:29.442: INFO: (9) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 56.813705ms)
May 27 06:58:29.447: INFO: (9) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 61.699675ms)
May 27 06:58:29.448: INFO: (9) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 62.381139ms)
May 27 06:58:29.463: INFO: (10) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 15.150681ms)
May 27 06:58:29.464: INFO: (10) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 13.644457ms)
May 27 06:58:29.464: INFO: (10) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 14.003872ms)
May 27 06:58:29.469: INFO: (10) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 18.18573ms)
May 27 06:58:29.470: INFO: (10) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 20.046588ms)
May 27 06:58:29.472: INFO: (10) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 21.9496ms)
May 27 06:58:29.472: INFO: (10) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 21.589546ms)
May 27 06:58:29.472: INFO: (10) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 21.855117ms)
May 27 06:58:29.473: INFO: (10) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 22.318951ms)
May 27 06:58:29.473: INFO: (10) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 21.993538ms)
May 27 06:58:29.481: INFO: (10) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 30.179863ms)
May 27 06:58:29.482: INFO: (10) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 33.154841ms)
May 27 06:58:29.482: INFO: (10) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 33.767946ms)
May 27 06:58:29.483: INFO: (10) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 33.608948ms)
May 27 06:58:29.483: INFO: (10) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 33.260707ms)
May 27 06:58:29.483: INFO: (10) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 32.549497ms)
May 27 06:58:29.500: INFO: (11) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 15.881907ms)
May 27 06:58:29.501: INFO: (11) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 16.419675ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 24.887311ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 24.437333ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 25.033875ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 24.770903ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 25.495161ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 24.83766ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 24.96306ms)
May 27 06:58:29.509: INFO: (11) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 24.743759ms)
May 27 06:58:29.510: INFO: (11) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 25.369672ms)
May 27 06:58:29.522: INFO: (11) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 37.480033ms)
May 27 06:58:29.525: INFO: (11) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 40.55632ms)
May 27 06:58:29.525: INFO: (11) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 41.068441ms)
May 27 06:58:29.528: INFO: (11) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 43.645706ms)
May 27 06:58:29.529: INFO: (11) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 44.459307ms)
May 27 06:58:29.547: INFO: (12) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 17.128901ms)
May 27 06:58:29.548: INFO: (12) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 18.093558ms)
May 27 06:58:29.548: INFO: (12) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 18.199964ms)
May 27 06:58:29.548: INFO: (12) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 18.262318ms)
May 27 06:58:29.548: INFO: (12) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 18.901135ms)
May 27 06:58:29.548: INFO: (12) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 17.826476ms)
May 27 06:58:29.548: INFO: (12) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 18.504519ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 34.679042ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 34.661742ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 35.295212ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 35.053946ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 34.96033ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 35.706358ms)
May 27 06:58:29.565: INFO: (12) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 35.456066ms)
May 27 06:58:29.566: INFO: (12) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 36.03931ms)
May 27 06:58:29.567: INFO: (12) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 37.192832ms)
May 27 06:58:29.590: INFO: (13) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 22.076004ms)
May 27 06:58:29.591: INFO: (13) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 22.905009ms)
May 27 06:58:29.591: INFO: (13) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 23.210562ms)
May 27 06:58:29.604: INFO: (13) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 35.621313ms)
May 27 06:58:29.604: INFO: (13) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 35.396607ms)
May 27 06:58:29.604: INFO: (13) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 36.294789ms)
May 27 06:58:29.604: INFO: (13) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 36.075639ms)
May 27 06:58:29.606: INFO: (13) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 37.267753ms)
May 27 06:58:29.606: INFO: (13) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 38.278332ms)
May 27 06:58:29.607: INFO: (13) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 38.876319ms)
May 27 06:58:29.607: INFO: (13) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 38.434295ms)
May 27 06:58:29.620: INFO: (13) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 51.449577ms)
May 27 06:58:29.620: INFO: (13) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 51.65451ms)
May 27 06:58:29.626: INFO: (13) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 57.549156ms)
May 27 06:58:29.631: INFO: (13) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 62.82751ms)
May 27 06:58:29.634: INFO: (13) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 66.17384ms)
May 27 06:58:29.650: INFO: (14) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 15.623885ms)
May 27 06:58:29.652: INFO: (14) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 17.467785ms)
May 27 06:58:29.659: INFO: (14) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 24.263265ms)
May 27 06:58:29.661: INFO: (14) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 26.569482ms)
May 27 06:58:29.666: INFO: (14) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 31.001761ms)
May 27 06:58:29.667: INFO: (14) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 31.969238ms)
May 27 06:58:29.668: INFO: (14) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 32.941102ms)
May 27 06:58:29.668: INFO: (14) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 32.676982ms)
May 27 06:58:29.676: INFO: (14) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 40.959504ms)
May 27 06:58:29.684: INFO: (14) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 48.384273ms)
May 27 06:58:29.684: INFO: (14) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 48.745687ms)
May 27 06:58:29.684: INFO: (14) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 48.940918ms)
May 27 06:58:29.684: INFO: (14) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 48.917512ms)
May 27 06:58:29.688: INFO: (14) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 53.194797ms)
May 27 06:58:29.688: INFO: (14) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 52.957901ms)
May 27 06:58:29.688: INFO: (14) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 53.264424ms)
May 27 06:58:29.708: INFO: (15) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 18.693474ms)
May 27 06:58:29.708: INFO: (15) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 18.961272ms)
May 27 06:58:29.708: INFO: (15) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 19.597228ms)
May 27 06:58:29.709: INFO: (15) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 19.271232ms)
May 27 06:58:29.709: INFO: (15) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 20.139322ms)
May 27 06:58:29.710: INFO: (15) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 20.503331ms)
May 27 06:58:29.716: INFO: (15) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 26.915406ms)
May 27 06:58:29.716: INFO: (15) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 26.542065ms)
May 27 06:58:29.717: INFO: (15) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 27.17438ms)
May 27 06:58:29.722: INFO: (15) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 31.98864ms)
May 27 06:58:29.736: INFO: (15) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 46.275254ms)
May 27 06:58:29.736: INFO: (15) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 46.289457ms)
May 27 06:58:29.771: INFO: (15) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 81.181322ms)
May 27 06:58:29.774: INFO: (15) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 84.452116ms)
May 27 06:58:29.777: INFO: (15) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 87.402947ms)
May 27 06:58:29.778: INFO: (15) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 88.107558ms)
May 27 06:58:29.799: INFO: (16) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 20.963369ms)
May 27 06:58:29.800: INFO: (16) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 21.146011ms)
May 27 06:58:29.806: INFO: (16) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 27.332202ms)
May 27 06:58:29.807: INFO: (16) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 28.728641ms)
May 27 06:58:29.810: INFO: (16) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 31.7086ms)
May 27 06:58:29.811: INFO: (16) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 32.543861ms)
May 27 06:58:29.812: INFO: (16) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 33.675065ms)
May 27 06:58:29.812: INFO: (16) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 34.106525ms)
May 27 06:58:29.812: INFO: (16) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 33.130549ms)
May 27 06:58:29.812: INFO: (16) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 34.235733ms)
May 27 06:58:29.813: INFO: (16) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 33.840354ms)
May 27 06:58:29.820: INFO: (16) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 41.062594ms)
May 27 06:58:29.820: INFO: (16) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 42.197786ms)
May 27 06:58:29.821: INFO: (16) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 41.953003ms)
May 27 06:58:29.827: INFO: (16) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 47.544692ms)
May 27 06:58:29.827: INFO: (16) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 49.352911ms)
May 27 06:58:29.855: INFO: (17) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 27.715502ms)
May 27 06:58:29.855: INFO: (17) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 27.750445ms)
May 27 06:58:29.859: INFO: (17) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 30.918171ms)
May 27 06:58:29.860: INFO: (17) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 31.522034ms)
May 27 06:58:29.860: INFO: (17) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 31.162539ms)
May 27 06:58:29.860: INFO: (17) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 31.353479ms)
May 27 06:58:29.860: INFO: (17) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 31.832309ms)
May 27 06:58:29.860: INFO: (17) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 32.075783ms)
May 27 06:58:29.863: INFO: (17) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 34.851117ms)
May 27 06:58:29.863: INFO: (17) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 35.046316ms)
May 27 06:58:29.864: INFO: (17) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 35.362593ms)
May 27 06:58:29.864: INFO: (17) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 36.021656ms)
May 27 06:58:29.866: INFO: (17) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 37.16642ms)
May 27 06:58:29.868: INFO: (17) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 39.417163ms)
May 27 06:58:29.882: INFO: (17) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 53.69489ms)
May 27 06:58:29.886: INFO: (17) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 58.186569ms)
May 27 06:58:29.903: INFO: (18) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 16.48102ms)
May 27 06:58:29.903: INFO: (18) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 16.808068ms)
May 27 06:58:29.908: INFO: (18) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 20.790684ms)
May 27 06:58:29.908: INFO: (18) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 21.726127ms)
May 27 06:58:29.914: INFO: (18) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 26.748644ms)
May 27 06:58:29.914: INFO: (18) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 27.760094ms)
May 27 06:58:29.916: INFO: (18) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 28.660702ms)
May 27 06:58:29.916: INFO: (18) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 29.143023ms)
May 27 06:58:29.917: INFO: (18) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 29.333469ms)
May 27 06:58:29.917: INFO: (18) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 29.323018ms)
May 27 06:58:29.922: INFO: (18) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 34.464713ms)
May 27 06:58:29.922: INFO: (18) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 35.243386ms)
May 27 06:58:29.922: INFO: (18) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 35.458139ms)
May 27 06:58:29.924: INFO: (18) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 37.097413ms)
May 27 06:58:29.924: INFO: (18) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 37.689379ms)
May 27 06:58:29.927: INFO: (18) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 40.074468ms)
May 27 06:58:29.936: INFO: (19) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:443/proxy/tlsrewriteme... (200; 8.540406ms)
May 27 06:58:29.940: INFO: (19) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 13.173762ms)
May 27 06:58:29.950: INFO: (19) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 23.022072ms)
May 27 06:58:29.952: INFO: (19) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname1/proxy/: foo (200; 25.028737ms)
May 27 06:58:29.953: INFO: (19) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:460/proxy/: tls baz (200; 25.469394ms)
May 27 06:58:29.954: INFO: (19) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:162/proxy/: bar (200; 26.61687ms)
May 27 06:58:29.958: INFO: (19) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">test</... (200; 30.454387ms)
May 27 06:58:29.959: INFO: (19) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:1080/proxy/rewriteme">t... (200; 31.940999ms)
May 27 06:58:29.959: INFO: (19) /api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/: <a href="/api/v1/namespaces/proxy-499/pods/proxy-service-s4rlv-rvj74/proxy/rewriteme">test</a> (200; 31.935314ms)
May 27 06:58:29.960: INFO: (19) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname2/proxy/: tls qux (200; 33.114139ms)
May 27 06:58:29.960: INFO: (19) /api/v1/namespaces/proxy-499/pods/https:proxy-service-s4rlv-rvj74:462/proxy/: tls qux (200; 32.553261ms)
May 27 06:58:29.960: INFO: (19) /api/v1/namespaces/proxy-499/pods/http:proxy-service-s4rlv-rvj74:160/proxy/: foo (200; 32.638099ms)
May 27 06:58:29.965: INFO: (19) /api/v1/namespaces/proxy-499/services/https:proxy-service-s4rlv:tlsportname1/proxy/: tls baz (200; 38.11055ms)
May 27 06:58:29.968: INFO: (19) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname2/proxy/: bar (200; 40.470066ms)
May 27 06:58:29.979: INFO: (19) /api/v1/namespaces/proxy-499/services/proxy-service-s4rlv:portname2/proxy/: bar (200; 52.166262ms)
May 27 06:58:29.979: INFO: (19) /api/v1/namespaces/proxy-499/services/http:proxy-service-s4rlv:portname1/proxy/: foo (200; 51.71401ms)
STEP: deleting ReplicationController proxy-service-s4rlv in namespace proxy-499, will wait for the garbage collector to delete the pods
May 27 06:58:30.053: INFO: Deleting ReplicationController proxy-service-s4rlv took: 11.299755ms
May 27 06:58:30.154: INFO: Terminating ReplicationController proxy-service-s4rlv pods took: 101.264536ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:32.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-499" for this suite.

• [SLOW TEST:5.763 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":345,"skipped":6679,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 27 06:58:32.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1830107335
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
May 27 06:58:32.484: INFO: Creating e2e-svc-a-lxhnd
May 27 06:58:32.513: INFO: Creating e2e-svc-b-w72tv
May 27 06:58:32.534: INFO: Creating e2e-svc-c-xv5sd
STEP: deleting service collection
May 27 06:58:32.652: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 27 06:58:32.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-181" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":346,"skipped":6691,"failed":0}
SSSSSSSMay 27 06:58:32.679: INFO: Running AfterSuite actions on all nodes
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May 27 06:58:32.679: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
May 27 06:58:32.680: INFO: Running AfterSuite actions on node 1
May 27 06:58:32.680: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6698,"failed":0}

Ran 346 of 7044 Specs in 6093.864 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6698 Skipped
PASS

Ginkgo ran 1 suite in 1h41m38.047669498s
Test Suite Passed

I0220 21:51:56.275996      20 e2e.go:116] Starting e2e run "24e5f230-9e47-40a6-adcc-763f231ccfc4" on Ginkgo node 1
Feb 20 21:51:56.287: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1676929916 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Feb 20 21:51:56.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 21:51:56.414: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0220 21:51:56.415368      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0220 21:51:56.415368      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Feb 20 21:51:56.465: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 20 21:51:56.536: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 20 21:51:56.536: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 20 21:51:56.536: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 20 21:51:56.546: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Feb 20 21:51:56.546: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Feb 20 21:51:56.546: INFO: e2e test version: v1.25.4
Feb 20 21:51:56.551: INFO: kube-apiserver version: v1.25.4+a34b9e9
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Feb 20 21:51:56.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 21:51:56.567: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.155 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 20 21:51:56.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 21:51:56.414: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0220 21:51:56.415368      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Feb 20 21:51:56.465: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 20 21:51:56.536: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb 20 21:51:56.536: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 20 21:51:56.536: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 20 21:51:56.546: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Feb 20 21:51:56.546: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Feb 20 21:51:56.546: INFO: e2e test version: v1.25.4
    Feb 20 21:51:56.551: INFO: kube-apiserver version: v1.25.4+a34b9e9
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 20 21:51:56.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 21:51:56.567: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:51:56.614
Feb 20 21:51:56.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 21:51:56.615
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:51:56.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:51:56.683
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 21:51:56.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2039" for this suite. 02/20/23 21:51:56.763
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":1,"skipped":2,"failed":0}
------------------------------
• [0.181 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:51:56.614
    Feb 20 21:51:56.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 21:51:56.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:51:56.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:51:56.683
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 21:51:56.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2039" for this suite. 02/20/23 21:51:56.763
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSE0220 21:51:56.801646      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
E0220 21:51:56.801646      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:51:56.802
Feb 20 21:51:56.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 21:51:56.804
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:51:56.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:51:56.921
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 02/20/23 21:51:56.933
Feb 20 21:51:56.934: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6185 proxy --unix-socket=/tmp/kubectl-proxy-unix1255410814/test'
STEP: retrieving proxy /api/ output 02/20/23 21:51:57.014
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 21:51:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6185" for this suite. 02/20/23 21:51:57.062
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":2,"skipped":10,"failed":0}
------------------------------
• [0.284 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:51:56.802
    Feb 20 21:51:56.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 21:51:56.804
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:51:56.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:51:56.921
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 02/20/23 21:51:56.933
    Feb 20 21:51:56.934: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6185 proxy --unix-socket=/tmp/kubectl-proxy-unix1255410814/test'
    STEP: retrieving proxy /api/ output 02/20/23 21:51:57.014
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 21:51:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6185" for this suite. 02/20/23 21:51:57.062
  << End Captured GinkgoWriter Output
------------------------------
E0220 21:51:57.098584      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0220 21:51:57.098584      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:51:57.099
Feb 20 21:51:57.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename containers 02/20/23 21:51:57.1
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:51:57.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:51:57.199
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 02/20/23 21:51:57.216
Feb 20 21:51:57.265: INFO: Waiting up to 5m0s for pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf" in namespace "containers-9258" to be "Succeeded or Failed"
Feb 20 21:51:57.276: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.112151ms
Feb 20 21:51:59.326: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06110494s
Feb 20 21:52:01.289: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024743665s
Feb 20 21:52:03.290: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025209922s
Feb 20 21:52:05.291: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026504767s
Feb 20 21:52:07.289: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.02453681s
STEP: Saw pod success 02/20/23 21:52:07.289
Feb 20 21:52:07.290: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf" satisfied condition "Succeeded or Failed"
Feb 20 21:52:07.302: INFO: Trying to get logs from node 10.8.38.66 pod client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf container agnhost-container: <nil>
STEP: delete the pod 02/20/23 21:52:07.343
Feb 20 21:52:07.381: INFO: Waiting for pod client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf to disappear
Feb 20 21:52:07.391: INFO: Pod client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 20 21:52:07.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9258" for this suite. 02/20/23 21:52:07.41
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":3,"skipped":11,"failed":0}
------------------------------
• [SLOW TEST] [10.334 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:51:57.099
    Feb 20 21:51:57.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename containers 02/20/23 21:51:57.1
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:51:57.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:51:57.199
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 02/20/23 21:51:57.216
    Feb 20 21:51:57.265: INFO: Waiting up to 5m0s for pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf" in namespace "containers-9258" to be "Succeeded or Failed"
    Feb 20 21:51:57.276: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.112151ms
    Feb 20 21:51:59.326: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06110494s
    Feb 20 21:52:01.289: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024743665s
    Feb 20 21:52:03.290: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025209922s
    Feb 20 21:52:05.291: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026504767s
    Feb 20 21:52:07.289: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.02453681s
    STEP: Saw pod success 02/20/23 21:52:07.289
    Feb 20 21:52:07.290: INFO: Pod "client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf" satisfied condition "Succeeded or Failed"
    Feb 20 21:52:07.302: INFO: Trying to get logs from node 10.8.38.66 pod client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 21:52:07.343
    Feb 20 21:52:07.381: INFO: Waiting for pod client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf to disappear
    Feb 20 21:52:07.391: INFO: Pod client-containers-2121b499-ca5f-43be-a3ea-0297a36f5daf no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 20 21:52:07.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9258" for this suite. 02/20/23 21:52:07.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:52:07.444
Feb 20 21:52:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename namespaces 02/20/23 21:52:07.445
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:07.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:07.502
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 02/20/23 21:52:07.515
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:07.567
STEP: Creating a service in the namespace 02/20/23 21:52:07.585
STEP: Deleting the namespace 02/20/23 21:52:07.631
STEP: Waiting for the namespace to be removed. 02/20/23 21:52:07.665
STEP: Recreating the namespace 02/20/23 21:52:14.678
STEP: Verifying there is no service in the namespace 02/20/23 21:52:14.721
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 20 21:52:14.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6212" for this suite. 02/20/23 21:52:14.754
STEP: Destroying namespace "nsdeletetest-1239" for this suite. 02/20/23 21:52:14.778
Feb 20 21:52:14.793: INFO: Namespace nsdeletetest-1239 was already deleted
STEP: Destroying namespace "nsdeletetest-5815" for this suite. 02/20/23 21:52:14.793
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":4,"skipped":70,"failed":0}
------------------------------
• [SLOW TEST] [7.372 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:52:07.444
    Feb 20 21:52:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename namespaces 02/20/23 21:52:07.445
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:07.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:07.502
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 02/20/23 21:52:07.515
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:07.567
    STEP: Creating a service in the namespace 02/20/23 21:52:07.585
    STEP: Deleting the namespace 02/20/23 21:52:07.631
    STEP: Waiting for the namespace to be removed. 02/20/23 21:52:07.665
    STEP: Recreating the namespace 02/20/23 21:52:14.678
    STEP: Verifying there is no service in the namespace 02/20/23 21:52:14.721
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 21:52:14.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6212" for this suite. 02/20/23 21:52:14.754
    STEP: Destroying namespace "nsdeletetest-1239" for this suite. 02/20/23 21:52:14.778
    Feb 20 21:52:14.793: INFO: Namespace nsdeletetest-1239 was already deleted
    STEP: Destroying namespace "nsdeletetest-5815" for this suite. 02/20/23 21:52:14.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:52:14.819
Feb 20 21:52:14.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 21:52:14.822
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:14.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:14.889
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7205 02/20/23 21:52:14.899
STEP: changing the ExternalName service to type=ClusterIP 02/20/23 21:52:14.923
STEP: creating replication controller externalname-service in namespace services-7205 02/20/23 21:52:14.998
I0220 21:52:15.012367      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7205, replica count: 2
I0220 21:52:18.063813      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 21:52:21.067883      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 21:52:24.069186      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 21:52:24.069: INFO: Creating new exec pod
Feb 20 21:52:24.109: INFO: Waiting up to 5m0s for pod "execpod497x7" in namespace "services-7205" to be "running"
Feb 20 21:52:24.127: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.557937ms
Feb 20 21:52:26.139: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030111426s
Feb 20 21:52:28.141: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032145289s
Feb 20 21:52:30.145: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035486966s
Feb 20 21:52:32.140: INFO: Pod "execpod497x7": Phase="Running", Reason="", readiness=true. Elapsed: 8.030366199s
Feb 20 21:52:32.140: INFO: Pod "execpod497x7" satisfied condition "running"
Feb 20 21:52:33.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 21:52:33.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:33.593: INFO: stdout: ""
Feb 20 21:52:34.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 21:52:34.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:34.939: INFO: stdout: ""
Feb 20 21:52:35.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 21:52:35.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:35.906: INFO: stdout: ""
Feb 20 21:52:36.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 21:52:36.897: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:36.897: INFO: stdout: "externalname-service-45wc8"
Feb 20 21:52:36.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
Feb 20 21:52:37.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:37.221: INFO: stdout: ""
Feb 20 21:52:38.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
Feb 20 21:52:38.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:38.607: INFO: stdout: ""
Feb 20 21:52:39.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
Feb 20 21:52:39.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:39.531: INFO: stdout: ""
Feb 20 21:52:40.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
Feb 20 21:52:40.810: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
Feb 20 21:52:40.810: INFO: stdout: "externalname-service-45wc8"
Feb 20 21:52:40.810: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 21:52:40.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7205" for this suite. 02/20/23 21:52:40.877
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":5,"skipped":75,"failed":0}
------------------------------
• [SLOW TEST] [26.082 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:52:14.819
    Feb 20 21:52:14.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 21:52:14.822
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:14.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:14.889
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7205 02/20/23 21:52:14.899
    STEP: changing the ExternalName service to type=ClusterIP 02/20/23 21:52:14.923
    STEP: creating replication controller externalname-service in namespace services-7205 02/20/23 21:52:14.998
    I0220 21:52:15.012367      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7205, replica count: 2
    I0220 21:52:18.063813      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0220 21:52:21.067883      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0220 21:52:24.069186      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 21:52:24.069: INFO: Creating new exec pod
    Feb 20 21:52:24.109: INFO: Waiting up to 5m0s for pod "execpod497x7" in namespace "services-7205" to be "running"
    Feb 20 21:52:24.127: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.557937ms
    Feb 20 21:52:26.139: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030111426s
    Feb 20 21:52:28.141: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032145289s
    Feb 20 21:52:30.145: INFO: Pod "execpod497x7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035486966s
    Feb 20 21:52:32.140: INFO: Pod "execpod497x7": Phase="Running", Reason="", readiness=true. Elapsed: 8.030366199s
    Feb 20 21:52:32.140: INFO: Pod "execpod497x7" satisfied condition "running"
    Feb 20 21:52:33.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 21:52:33.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:33.593: INFO: stdout: ""
    Feb 20 21:52:34.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 21:52:34.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:34.939: INFO: stdout: ""
    Feb 20 21:52:35.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 21:52:35.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:35.906: INFO: stdout: ""
    Feb 20 21:52:36.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 21:52:36.897: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:36.897: INFO: stdout: "externalname-service-45wc8"
    Feb 20 21:52:36.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
    Feb 20 21:52:37.221: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:37.221: INFO: stdout: ""
    Feb 20 21:52:38.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
    Feb 20 21:52:38.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:38.607: INFO: stdout: ""
    Feb 20 21:52:39.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
    Feb 20 21:52:39.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:39.531: INFO: stdout: ""
    Feb 20 21:52:40.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7205 exec execpod497x7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.76.219 80'
    Feb 20 21:52:40.810: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.76.219 80\nConnection to 172.21.76.219 80 port [tcp/http] succeeded!\n"
    Feb 20 21:52:40.810: INFO: stdout: "externalname-service-45wc8"
    Feb 20 21:52:40.810: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 21:52:40.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7205" for this suite. 02/20/23 21:52:40.877
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:52:40.903
Feb 20 21:52:40.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 21:52:40.905
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:40.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:40.958
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 21:52:40.969
Feb 20 21:52:40.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6708 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Feb 20 21:52:41.134: INFO: stderr: ""
Feb 20 21:52:41.134: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/20/23 21:52:41.134
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Feb 20 21:52:41.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6708 delete pods e2e-test-httpd-pod'
Feb 20 21:52:50.275: INFO: stderr: ""
Feb 20 21:52:50.275: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 21:52:50.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6708" for this suite. 02/20/23 21:52:50.293
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":6,"skipped":93,"failed":0}
------------------------------
• [SLOW TEST] [9.412 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:52:40.903
    Feb 20 21:52:40.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 21:52:40.905
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:40.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:40.958
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 21:52:40.969
    Feb 20 21:52:40.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6708 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Feb 20 21:52:41.134: INFO: stderr: ""
    Feb 20 21:52:41.134: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/20/23 21:52:41.134
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Feb 20 21:52:41.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6708 delete pods e2e-test-httpd-pod'
    Feb 20 21:52:50.275: INFO: stderr: ""
    Feb 20 21:52:50.275: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 21:52:50.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6708" for this suite. 02/20/23 21:52:50.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:52:50.32
Feb 20 21:52:50.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 21:52:50.322
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:50.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:50.388
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 02/20/23 21:52:50.399
Feb 20 21:52:50.518: INFO: Waiting up to 5m0s for pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0" in namespace "emptydir-8437" to be "Succeeded or Failed"
Feb 20 21:52:50.529: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.060901ms
Feb 20 21:52:52.543: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024745683s
Feb 20 21:52:54.542: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024052125s
STEP: Saw pod success 02/20/23 21:52:54.542
Feb 20 21:52:54.542: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0" satisfied condition "Succeeded or Failed"
Feb 20 21:52:54.553: INFO: Trying to get logs from node 10.8.38.66 pod pod-defb0a74-25d4-4286-b5da-359612f1bdf0 container test-container: <nil>
STEP: delete the pod 02/20/23 21:52:54.574
Feb 20 21:52:54.606: INFO: Waiting for pod pod-defb0a74-25d4-4286-b5da-359612f1bdf0 to disappear
Feb 20 21:52:54.617: INFO: Pod pod-defb0a74-25d4-4286-b5da-359612f1bdf0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 21:52:54.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8437" for this suite. 02/20/23 21:52:54.633
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":7,"skipped":117,"failed":0}
------------------------------
• [4.347 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:52:50.32
    Feb 20 21:52:50.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 21:52:50.322
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:50.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:50.388
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/20/23 21:52:50.399
    Feb 20 21:52:50.518: INFO: Waiting up to 5m0s for pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0" in namespace "emptydir-8437" to be "Succeeded or Failed"
    Feb 20 21:52:50.529: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.060901ms
    Feb 20 21:52:52.543: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024745683s
    Feb 20 21:52:54.542: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024052125s
    STEP: Saw pod success 02/20/23 21:52:54.542
    Feb 20 21:52:54.542: INFO: Pod "pod-defb0a74-25d4-4286-b5da-359612f1bdf0" satisfied condition "Succeeded or Failed"
    Feb 20 21:52:54.553: INFO: Trying to get logs from node 10.8.38.66 pod pod-defb0a74-25d4-4286-b5da-359612f1bdf0 container test-container: <nil>
    STEP: delete the pod 02/20/23 21:52:54.574
    Feb 20 21:52:54.606: INFO: Waiting for pod pod-defb0a74-25d4-4286-b5da-359612f1bdf0 to disappear
    Feb 20 21:52:54.617: INFO: Pod pod-defb0a74-25d4-4286-b5da-359612f1bdf0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 21:52:54.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8437" for this suite. 02/20/23 21:52:54.633
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:52:54.668
Feb 20 21:52:54.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 21:52:54.67
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:54.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:54.738
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-1bc6c8a8-f442-4eb7-8ec3-f78c55c9c777 02/20/23 21:52:54.748
STEP: Creating a pod to test consume configMaps 02/20/23 21:52:54.762
Feb 20 21:52:54.848: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73" in namespace "projected-8052" to be "Succeeded or Failed"
Feb 20 21:52:54.860: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.561067ms
Feb 20 21:52:56.873: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024652992s
Feb 20 21:52:58.873: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025281258s
Feb 20 21:53:00.877: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028762476s
STEP: Saw pod success 02/20/23 21:53:00.877
Feb 20 21:53:00.877: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73" satisfied condition "Succeeded or Failed"
Feb 20 21:53:00.902: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 21:53:00.928
Feb 20 21:53:00.964: INFO: Waiting for pod pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73 to disappear
Feb 20 21:53:00.975: INFO: Pod pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 21:53:00.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8052" for this suite. 02/20/23 21:53:00.999
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":8,"skipped":120,"failed":0}
------------------------------
• [SLOW TEST] [6.355 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:52:54.668
    Feb 20 21:52:54.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 21:52:54.67
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:52:54.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:52:54.738
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-1bc6c8a8-f442-4eb7-8ec3-f78c55c9c777 02/20/23 21:52:54.748
    STEP: Creating a pod to test consume configMaps 02/20/23 21:52:54.762
    Feb 20 21:52:54.848: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73" in namespace "projected-8052" to be "Succeeded or Failed"
    Feb 20 21:52:54.860: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Pending", Reason="", readiness=false. Elapsed: 11.561067ms
    Feb 20 21:52:56.873: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024652992s
    Feb 20 21:52:58.873: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025281258s
    Feb 20 21:53:00.877: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028762476s
    STEP: Saw pod success 02/20/23 21:53:00.877
    Feb 20 21:53:00.877: INFO: Pod "pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73" satisfied condition "Succeeded or Failed"
    Feb 20 21:53:00.902: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 21:53:00.928
    Feb 20 21:53:00.964: INFO: Waiting for pod pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73 to disappear
    Feb 20 21:53:00.975: INFO: Pod pod-projected-configmaps-75db5b70-347e-47e8-a29b-53b9266d8f73 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 21:53:00.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8052" for this suite. 02/20/23 21:53:00.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:01.029
Feb 20 21:53:01.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 21:53:01.03
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:01.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:01.094
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 02/20/23 21:53:01.104
Feb 20 21:53:01.185: INFO: Waiting up to 5m0s for pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991" in namespace "projected-2790" to be "running and ready"
Feb 20 21:53:01.196: INFO: Pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991": Phase="Pending", Reason="", readiness=false. Elapsed: 10.913991ms
Feb 20 21:53:01.196: INFO: The phase of Pod annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:53:03.209: INFO: Pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991": Phase="Running", Reason="", readiness=true. Elapsed: 2.02395799s
Feb 20 21:53:03.209: INFO: The phase of Pod annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991 is Running (Ready = true)
Feb 20 21:53:03.209: INFO: Pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991" satisfied condition "running and ready"
Feb 20 21:53:03.795: INFO: Successfully updated pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 21:53:05.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2790" for this suite. 02/20/23 21:53:05.861
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":9,"skipped":220,"failed":0}
------------------------------
• [4.852 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:01.029
    Feb 20 21:53:01.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 21:53:01.03
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:01.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:01.094
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 02/20/23 21:53:01.104
    Feb 20 21:53:01.185: INFO: Waiting up to 5m0s for pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991" in namespace "projected-2790" to be "running and ready"
    Feb 20 21:53:01.196: INFO: Pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991": Phase="Pending", Reason="", readiness=false. Elapsed: 10.913991ms
    Feb 20 21:53:01.196: INFO: The phase of Pod annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:53:03.209: INFO: Pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991": Phase="Running", Reason="", readiness=true. Elapsed: 2.02395799s
    Feb 20 21:53:03.209: INFO: The phase of Pod annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991 is Running (Ready = true)
    Feb 20 21:53:03.209: INFO: Pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991" satisfied condition "running and ready"
    Feb 20 21:53:03.795: INFO: Successfully updated pod "annotationupdate9db05e57-22db-44e2-9976-13c2d55a2991"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 21:53:05.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2790" for this suite. 02/20/23 21:53:05.861
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:05.884
Feb 20 21:53:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 21:53:05.888
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:05.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:05.945
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 02/20/23 21:53:05.957
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 21:53:06.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6105" for this suite. 02/20/23 21:53:06.064
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":10,"skipped":222,"failed":0}
------------------------------
• [0.224 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:05.884
    Feb 20 21:53:05.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 21:53:05.888
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:05.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:05.945
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 02/20/23 21:53:05.957
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 21:53:06.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6105" for this suite. 02/20/23 21:53:06.064
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:06.109
Feb 20 21:53:06.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 21:53:06.112
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:06.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:06.205
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-00897527-c88b-438d-a816-f98686faa913 02/20/23 21:53:06.219
STEP: Creating a pod to test consume configMaps 02/20/23 21:53:06.247
Feb 20 21:53:06.315: INFO: Waiting up to 5m0s for pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8" in namespace "configmap-9316" to be "Succeeded or Failed"
Feb 20 21:53:06.327: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.732046ms
Feb 20 21:53:08.339: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024042013s
Feb 20 21:53:10.343: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027813245s
Feb 20 21:53:12.355: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040342387s
STEP: Saw pod success 02/20/23 21:53:12.355
Feb 20 21:53:12.355: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8" satisfied condition "Succeeded or Failed"
Feb 20 21:53:12.366: INFO: Trying to get logs from node 10.8.38.70 pod pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8 container configmap-volume-test: <nil>
STEP: delete the pod 02/20/23 21:53:12.41
Feb 20 21:53:12.443: INFO: Waiting for pod pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8 to disappear
Feb 20 21:53:12.456: INFO: Pod pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 21:53:12.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9316" for this suite. 02/20/23 21:53:12.473
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":11,"skipped":227,"failed":0}
------------------------------
• [SLOW TEST] [6.387 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:06.109
    Feb 20 21:53:06.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 21:53:06.112
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:06.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:06.205
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-00897527-c88b-438d-a816-f98686faa913 02/20/23 21:53:06.219
    STEP: Creating a pod to test consume configMaps 02/20/23 21:53:06.247
    Feb 20 21:53:06.315: INFO: Waiting up to 5m0s for pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8" in namespace "configmap-9316" to be "Succeeded or Failed"
    Feb 20 21:53:06.327: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.732046ms
    Feb 20 21:53:08.339: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024042013s
    Feb 20 21:53:10.343: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027813245s
    Feb 20 21:53:12.355: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040342387s
    STEP: Saw pod success 02/20/23 21:53:12.355
    Feb 20 21:53:12.355: INFO: Pod "pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8" satisfied condition "Succeeded or Failed"
    Feb 20 21:53:12.366: INFO: Trying to get logs from node 10.8.38.70 pod pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8 container configmap-volume-test: <nil>
    STEP: delete the pod 02/20/23 21:53:12.41
    Feb 20 21:53:12.443: INFO: Waiting for pod pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8 to disappear
    Feb 20 21:53:12.456: INFO: Pod pod-configmaps-147affb1-67dc-460d-935e-06b4692883b8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 21:53:12.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9316" for this suite. 02/20/23 21:53:12.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:12.497
Feb 20 21:53:12.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubelet-test 02/20/23 21:53:12.498
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:12.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:12.551
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 20 21:53:12.689: INFO: Waiting up to 5m0s for pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561" in namespace "kubelet-test-7775" to be "running and ready"
Feb 20 21:53:12.761: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Pending", Reason="", readiness=false. Elapsed: 71.145269ms
Feb 20 21:53:12.761: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:53:14.774: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084209622s
Feb 20 21:53:14.774: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:53:16.773: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083404186s
Feb 20 21:53:16.773: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:53:18.774: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Running", Reason="", readiness=true. Elapsed: 6.084687443s
Feb 20 21:53:18.775: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Running (Ready = true)
Feb 20 21:53:18.775: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 20 21:53:18.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7775" for this suite. 02/20/23 21:53:18.845
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":12,"skipped":233,"failed":0}
------------------------------
• [SLOW TEST] [6.389 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:12.497
    Feb 20 21:53:12.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubelet-test 02/20/23 21:53:12.498
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:12.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:12.551
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 20 21:53:12.689: INFO: Waiting up to 5m0s for pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561" in namespace "kubelet-test-7775" to be "running and ready"
    Feb 20 21:53:12.761: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Pending", Reason="", readiness=false. Elapsed: 71.145269ms
    Feb 20 21:53:12.761: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:53:14.774: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084209622s
    Feb 20 21:53:14.774: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:53:16.773: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083404186s
    Feb 20 21:53:16.773: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:53:18.774: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561": Phase="Running", Reason="", readiness=true. Elapsed: 6.084687443s
    Feb 20 21:53:18.775: INFO: The phase of Pod busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561 is Running (Ready = true)
    Feb 20 21:53:18.775: INFO: Pod "busybox-scheduling-cd6f40ab-abc0-4b20-b878-7c25d25e2561" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 20 21:53:18.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7775" for this suite. 02/20/23 21:53:18.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:18.888
Feb 20 21:53:18.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 21:53:18.889
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:18.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:18.982
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 20 21:53:19.006: INFO: Creating simple deployment test-new-deployment
W0220 21:53:19.019492      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 21:53:19.053: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Feb 20 21:53:21.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 02/20/23 21:53:23.107
STEP: updating a scale subresource 02/20/23 21:53:23.115
STEP: verifying the deployment Spec.Replicas was modified 02/20/23 21:53:23.133
STEP: Patch a scale subresource 02/20/23 21:53:23.144
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 21:53:23.190: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7453  be90e4df-3d0f-41a8-88e8-3f02ad108d4e 65945 3 2023-02-20 21:53:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-20 21:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 21:53:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c6d848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-20 21:53:21 +0000 UTC,LastTransitionTime:2023-02-20 21:53:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-20 21:53:21 +0000 UTC,LastTransitionTime:2023-02-20 21:53:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 21:53:23.218: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7453  5922a44e-b0ba-40af-8cd4-b63ae1147194 65944 2 2023-02-20 21:53:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment be90e4df-3d0f-41a8-88e8-3f02ad108d4e 0xc002ef7e57 0xc002ef7e58}] [] [{kube-controller-manager Update apps/v1 2023-02-20 21:53:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 21:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"be90e4df-3d0f-41a8-88e8-3f02ad108d4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef7ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 20 21:53:23.231: INFO: Pod "test-new-deployment-845c8977d9-cp2k9" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-cp2k9 test-new-deployment-845c8977d9- deployment-7453  99e66890-29c6-4656-96ae-93ec10e7a4c6 65930 0 2023-02-20 21:53:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5de2779fa478bcb9a0dce6aa7f75a37e64aa15777c860a6e49922bca120a2fea cni.projectcalico.org/podIP:172.30.181.218/32 cni.projectcalico.org/podIPs:172.30.181.218/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.218"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.218"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5922a44e-b0ba-40af-8cd4-b63ae1147194 0xc003c6dca7 0xc003c6dca8}] [] [{kube-controller-manager Update v1 2023-02-20 21:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5922a44e-b0ba-40af-8cd4-b63ae1147194\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 21:53:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 21:53:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 21:53:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dtm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dtm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c31,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zrqvz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.218,StartTime:2023-02-20 21:53:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 21:53:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3270a8bcaeb4e8413db8ca4ef74d752f1a77fd296dc65208d8770fe5687e809f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 21:53:23.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7453" for this suite. 02/20/23 21:53:23.248
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":13,"skipped":259,"failed":0}
------------------------------
• [4.404 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:18.888
    Feb 20 21:53:18.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 21:53:18.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:18.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:18.982
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 20 21:53:19.006: INFO: Creating simple deployment test-new-deployment
    W0220 21:53:19.019492      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 21:53:19.053: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    Feb 20 21:53:21.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 53, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 02/20/23 21:53:23.107
    STEP: updating a scale subresource 02/20/23 21:53:23.115
    STEP: verifying the deployment Spec.Replicas was modified 02/20/23 21:53:23.133
    STEP: Patch a scale subresource 02/20/23 21:53:23.144
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 21:53:23.190: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-7453  be90e4df-3d0f-41a8-88e8-3f02ad108d4e 65945 3 2023-02-20 21:53:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-20 21:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 21:53:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c6d848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-20 21:53:21 +0000 UTC,LastTransitionTime:2023-02-20 21:53:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-20 21:53:21 +0000 UTC,LastTransitionTime:2023-02-20 21:53:19 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 20 21:53:23.218: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7453  5922a44e-b0ba-40af-8cd4-b63ae1147194 65944 2 2023-02-20 21:53:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment be90e4df-3d0f-41a8-88e8-3f02ad108d4e 0xc002ef7e57 0xc002ef7e58}] [] [{kube-controller-manager Update apps/v1 2023-02-20 21:53:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 21:53:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"be90e4df-3d0f-41a8-88e8-3f02ad108d4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ef7ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 21:53:23.231: INFO: Pod "test-new-deployment-845c8977d9-cp2k9" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-cp2k9 test-new-deployment-845c8977d9- deployment-7453  99e66890-29c6-4656-96ae-93ec10e7a4c6 65930 0 2023-02-20 21:53:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5de2779fa478bcb9a0dce6aa7f75a37e64aa15777c860a6e49922bca120a2fea cni.projectcalico.org/podIP:172.30.181.218/32 cni.projectcalico.org/podIPs:172.30.181.218/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.218"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.218"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 5922a44e-b0ba-40af-8cd4-b63ae1147194 0xc003c6dca7 0xc003c6dca8}] [] [{kube-controller-manager Update v1 2023-02-20 21:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5922a44e-b0ba-40af-8cd4-b63ae1147194\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 21:53:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 21:53:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 21:53:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dtm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dtm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c31,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zrqvz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 21:53:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.218,StartTime:2023-02-20 21:53:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 21:53:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3270a8bcaeb4e8413db8ca4ef74d752f1a77fd296dc65208d8770fe5687e809f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 21:53:23.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7453" for this suite. 02/20/23 21:53:23.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:23.301
Feb 20 21:53:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename events 02/20/23 21:53:23.315
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:23.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:23.371
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/20/23 21:53:23.384
STEP: listing all events in all namespaces 02/20/23 21:53:23.405
STEP: patching the test event 02/20/23 21:53:23.488
STEP: fetching the test event 02/20/23 21:53:23.508
STEP: updating the test event 02/20/23 21:53:23.52
STEP: getting the test event 02/20/23 21:53:23.553
STEP: deleting the test event 02/20/23 21:53:23.565
STEP: listing all events in all namespaces 02/20/23 21:53:23.589
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 20 21:53:23.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8668" for this suite. 02/20/23 21:53:23.676
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":14,"skipped":274,"failed":0}
------------------------------
• [0.404 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:23.301
    Feb 20 21:53:23.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename events 02/20/23 21:53:23.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:23.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:23.371
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/20/23 21:53:23.384
    STEP: listing all events in all namespaces 02/20/23 21:53:23.405
    STEP: patching the test event 02/20/23 21:53:23.488
    STEP: fetching the test event 02/20/23 21:53:23.508
    STEP: updating the test event 02/20/23 21:53:23.52
    STEP: getting the test event 02/20/23 21:53:23.553
    STEP: deleting the test event 02/20/23 21:53:23.565
    STEP: listing all events in all namespaces 02/20/23 21:53:23.589
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 20 21:53:23.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8668" for this suite. 02/20/23 21:53:23.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:23.707
Feb 20 21:53:23.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 21:53:23.709
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:23.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:23.765
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 02/20/23 21:53:23.777
Feb 20 21:53:23.872: INFO: Waiting up to 5m0s for pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89" in namespace "var-expansion-953" to be "Succeeded or Failed"
Feb 20 21:53:23.884: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779972ms
Feb 20 21:53:25.929: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057337247s
Feb 20 21:53:27.897: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025030527s
Feb 20 21:53:29.897: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025129695s
Feb 20 21:53:31.896: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023896846s
STEP: Saw pod success 02/20/23 21:53:31.896
Feb 20 21:53:31.897: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89" satisfied condition "Succeeded or Failed"
Feb 20 21:53:31.913: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89 container dapi-container: <nil>
STEP: delete the pod 02/20/23 21:53:31.937
Feb 20 21:53:31.965: INFO: Waiting for pod var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89 to disappear
Feb 20 21:53:31.980: INFO: Pod var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 21:53:31.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-953" for this suite. 02/20/23 21:53:31.998
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":15,"skipped":284,"failed":0}
------------------------------
• [SLOW TEST] [8.314 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:23.707
    Feb 20 21:53:23.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 21:53:23.709
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:23.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:23.765
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 02/20/23 21:53:23.777
    Feb 20 21:53:23.872: INFO: Waiting up to 5m0s for pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89" in namespace "var-expansion-953" to be "Succeeded or Failed"
    Feb 20 21:53:23.884: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 11.779972ms
    Feb 20 21:53:25.929: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057337247s
    Feb 20 21:53:27.897: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025030527s
    Feb 20 21:53:29.897: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025129695s
    Feb 20 21:53:31.896: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023896846s
    STEP: Saw pod success 02/20/23 21:53:31.896
    Feb 20 21:53:31.897: INFO: Pod "var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89" satisfied condition "Succeeded or Failed"
    Feb 20 21:53:31.913: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89 container dapi-container: <nil>
    STEP: delete the pod 02/20/23 21:53:31.937
    Feb 20 21:53:31.965: INFO: Waiting for pod var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89 to disappear
    Feb 20 21:53:31.980: INFO: Pod var-expansion-6930cb1f-acba-4eaf-9719-ca06aafa8f89 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 21:53:31.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-953" for this suite. 02/20/23 21:53:31.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:32.035
Feb 20 21:53:32.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename init-container 02/20/23 21:53:32.037
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:32.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:32.11
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 02/20/23 21:53:32.123
Feb 20 21:53:32.124: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 21:53:38.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3566" for this suite. 02/20/23 21:53:38.318
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":16,"skipped":340,"failed":0}
------------------------------
• [SLOW TEST] [6.307 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:32.035
    Feb 20 21:53:32.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename init-container 02/20/23 21:53:32.037
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:32.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:32.11
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 02/20/23 21:53:32.123
    Feb 20 21:53:32.124: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 21:53:38.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3566" for this suite. 02/20/23 21:53:38.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:38.348
Feb 20 21:53:38.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename security-context 02/20/23 21:53:38.35
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:38.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:38.406
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/20/23 21:53:38.424
Feb 20 21:53:38.475: INFO: Waiting up to 5m0s for pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad" in namespace "security-context-705" to be "Succeeded or Failed"
Feb 20 21:53:38.497: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 22.029361ms
Feb 20 21:53:40.509: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034382772s
Feb 20 21:53:42.508: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033272307s
Feb 20 21:53:44.511: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036502137s
Feb 20 21:53:46.510: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034987878s
STEP: Saw pod success 02/20/23 21:53:46.51
Feb 20 21:53:46.510: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad" satisfied condition "Succeeded or Failed"
Feb 20 21:53:46.522: INFO: Trying to get logs from node 10.8.38.69 pod security-context-6483a352-4ece-45be-80f3-75f38e3d77ad container test-container: <nil>
STEP: delete the pod 02/20/23 21:53:46.595
Feb 20 21:53:46.621: INFO: Waiting for pod security-context-6483a352-4ece-45be-80f3-75f38e3d77ad to disappear
Feb 20 21:53:46.633: INFO: Pod security-context-6483a352-4ece-45be-80f3-75f38e3d77ad no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 20 21:53:46.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-705" for this suite. 02/20/23 21:53:46.649
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":17,"skipped":376,"failed":0}
------------------------------
• [SLOW TEST] [8.328 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:38.348
    Feb 20 21:53:38.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename security-context 02/20/23 21:53:38.35
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:38.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:38.406
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/20/23 21:53:38.424
    Feb 20 21:53:38.475: INFO: Waiting up to 5m0s for pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad" in namespace "security-context-705" to be "Succeeded or Failed"
    Feb 20 21:53:38.497: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 22.029361ms
    Feb 20 21:53:40.509: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034382772s
    Feb 20 21:53:42.508: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033272307s
    Feb 20 21:53:44.511: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036502137s
    Feb 20 21:53:46.510: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034987878s
    STEP: Saw pod success 02/20/23 21:53:46.51
    Feb 20 21:53:46.510: INFO: Pod "security-context-6483a352-4ece-45be-80f3-75f38e3d77ad" satisfied condition "Succeeded or Failed"
    Feb 20 21:53:46.522: INFO: Trying to get logs from node 10.8.38.69 pod security-context-6483a352-4ece-45be-80f3-75f38e3d77ad container test-container: <nil>
    STEP: delete the pod 02/20/23 21:53:46.595
    Feb 20 21:53:46.621: INFO: Waiting for pod security-context-6483a352-4ece-45be-80f3-75f38e3d77ad to disappear
    Feb 20 21:53:46.633: INFO: Pod security-context-6483a352-4ece-45be-80f3-75f38e3d77ad no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 20 21:53:46.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-705" for this suite. 02/20/23 21:53:46.649
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:46.678
Feb 20 21:53:46.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 21:53:46.681
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:46.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:46.74
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 02/20/23 21:53:46.752
Feb 20 21:53:46.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c" in namespace "projected-1233" to be "Succeeded or Failed"
Feb 20 21:53:46.829: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.524729ms
Feb 20 21:53:48.844: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025999246s
Feb 20 21:53:50.843: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024950641s
STEP: Saw pod success 02/20/23 21:53:50.843
Feb 20 21:53:50.843: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c" satisfied condition "Succeeded or Failed"
Feb 20 21:53:50.855: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c container client-container: <nil>
STEP: delete the pod 02/20/23 21:53:50.875
Feb 20 21:53:50.908: INFO: Waiting for pod downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c to disappear
Feb 20 21:53:50.921: INFO: Pod downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 21:53:50.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1233" for this suite. 02/20/23 21:53:50.938
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":18,"skipped":377,"failed":0}
------------------------------
• [4.284 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:46.678
    Feb 20 21:53:46.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 21:53:46.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:46.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:46.74
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 02/20/23 21:53:46.752
    Feb 20 21:53:46.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c" in namespace "projected-1233" to be "Succeeded or Failed"
    Feb 20 21:53:46.829: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.524729ms
    Feb 20 21:53:48.844: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025999246s
    Feb 20 21:53:50.843: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024950641s
    STEP: Saw pod success 02/20/23 21:53:50.843
    Feb 20 21:53:50.843: INFO: Pod "downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c" satisfied condition "Succeeded or Failed"
    Feb 20 21:53:50.855: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c container client-container: <nil>
    STEP: delete the pod 02/20/23 21:53:50.875
    Feb 20 21:53:50.908: INFO: Waiting for pod downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c to disappear
    Feb 20 21:53:50.921: INFO: Pod downwardapi-volume-0d6dc872-0d12-4118-8305-b554f187fb0c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 21:53:50.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1233" for this suite. 02/20/23 21:53:50.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:50.974
Feb 20 21:53:50.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 21:53:50.975
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:51.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:51.035
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-78/secret-test-3147ff63-95ae-42f1-b45a-8eca72cdfa58 02/20/23 21:53:51.051
STEP: Creating a pod to test consume secrets 02/20/23 21:53:51.071
Feb 20 21:53:51.121: INFO: Waiting up to 5m0s for pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d" in namespace "secrets-78" to be "Succeeded or Failed"
Feb 20 21:53:51.137: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.464445ms
Feb 20 21:53:53.151: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030035943s
Feb 20 21:53:55.150: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028858285s
Feb 20 21:53:57.150: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028506402s
STEP: Saw pod success 02/20/23 21:53:57.15
Feb 20 21:53:57.150: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d" satisfied condition "Succeeded or Failed"
Feb 20 21:53:57.172: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d container env-test: <nil>
STEP: delete the pod 02/20/23 21:53:57.202
Feb 20 21:53:57.237: INFO: Waiting for pod pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d to disappear
Feb 20 21:53:57.248: INFO: Pod pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 20 21:53:57.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-78" for this suite. 02/20/23 21:53:57.266
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":19,"skipped":422,"failed":0}
------------------------------
• [SLOW TEST] [6.319 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:50.974
    Feb 20 21:53:50.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 21:53:50.975
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:51.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:51.035
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-78/secret-test-3147ff63-95ae-42f1-b45a-8eca72cdfa58 02/20/23 21:53:51.051
    STEP: Creating a pod to test consume secrets 02/20/23 21:53:51.071
    Feb 20 21:53:51.121: INFO: Waiting up to 5m0s for pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d" in namespace "secrets-78" to be "Succeeded or Failed"
    Feb 20 21:53:51.137: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.464445ms
    Feb 20 21:53:53.151: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030035943s
    Feb 20 21:53:55.150: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028858285s
    Feb 20 21:53:57.150: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028506402s
    STEP: Saw pod success 02/20/23 21:53:57.15
    Feb 20 21:53:57.150: INFO: Pod "pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d" satisfied condition "Succeeded or Failed"
    Feb 20 21:53:57.172: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d container env-test: <nil>
    STEP: delete the pod 02/20/23 21:53:57.202
    Feb 20 21:53:57.237: INFO: Waiting for pod pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d to disappear
    Feb 20 21:53:57.248: INFO: Pod pod-configmaps-650137b3-a3b6-46a9-b0fc-f19524def25d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 21:53:57.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-78" for this suite. 02/20/23 21:53:57.266
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:53:57.294
Feb 20 21:53:57.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 21:53:57.299
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:57.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:57.373
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-9874 02/20/23 21:53:57.387
Feb 20 21:53:57.454: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9874" to be "running and ready"
Feb 20 21:53:57.468: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 13.654994ms
Feb 20 21:53:57.468: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:53:59.481: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.026891248s
Feb 20 21:53:59.481: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 20 21:53:59.481: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 20 21:53:59.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 20 21:53:59.834: INFO: rc: 7
Feb 20 21:53:59.874: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 20 21:53:59.885: INFO: Pod kube-proxy-mode-detector no longer exists
Feb 20 21:53:59.885: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-9874 02/20/23 21:53:59.885
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9874 02/20/23 21:53:59.928
I0220 21:53:59.939142      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9874, replica count: 3
I0220 21:54:02.990501      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 21:54:03.018: INFO: Creating new exec pod
Feb 20 21:54:03.055: INFO: Waiting up to 5m0s for pod "execpod-affinity9w9rc" in namespace "services-9874" to be "running"
Feb 20 21:54:03.074: INFO: Pod "execpod-affinity9w9rc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.611087ms
Feb 20 21:54:05.085: INFO: Pod "execpod-affinity9w9rc": Phase="Running", Reason="", readiness=true. Elapsed: 2.029664123s
Feb 20 21:54:05.085: INFO: Pod "execpod-affinity9w9rc" satisfied condition "running"
Feb 20 21:54:06.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 20 21:54:06.348: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 20 21:54:06.348: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 21:54:06.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.201.33 80'
Feb 20 21:54:06.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.201.33 80\nConnection to 172.21.201.33 80 port [tcp/http] succeeded!\n"
Feb 20 21:54:06.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 21:54:06.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.201.33:80/ ; done'
Feb 20 21:54:07.009: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
Feb 20 21:54:07.009: INFO: stdout: "\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5"
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
Feb 20 21:54:07.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.201.33:80/'
Feb 20 21:54:07.287: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
Feb 20 21:54:07.288: INFO: stdout: "affinity-clusterip-timeout-jwsf5"
Feb 20 21:54:27.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.201.33:80/'
Feb 20 21:54:27.555: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
Feb 20 21:54:27.555: INFO: stdout: "affinity-clusterip-timeout-jwsf5"
Feb 20 21:54:47.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.201.33:80/'
Feb 20 21:54:47.855: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
Feb 20 21:54:47.855: INFO: stdout: "affinity-clusterip-timeout-d6hr9"
Feb 20 21:54:47.855: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9874, will wait for the garbage collector to delete the pods 02/20/23 21:54:47.916
Feb 20 21:54:47.989: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.509141ms
Feb 20 21:54:48.090: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.326789ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 21:54:51.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9874" for this suite. 02/20/23 21:54:51.147
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":20,"skipped":422,"failed":0}
------------------------------
• [SLOW TEST] [53.878 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:53:57.294
    Feb 20 21:53:57.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 21:53:57.299
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:53:57.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:53:57.373
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-9874 02/20/23 21:53:57.387
    Feb 20 21:53:57.454: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9874" to be "running and ready"
    Feb 20 21:53:57.468: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 13.654994ms
    Feb 20 21:53:57.468: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:53:59.481: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.026891248s
    Feb 20 21:53:59.481: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 20 21:53:59.481: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 20 21:53:59.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 20 21:53:59.834: INFO: rc: 7
    Feb 20 21:53:59.874: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 20 21:53:59.885: INFO: Pod kube-proxy-mode-detector no longer exists
    Feb 20 21:53:59.885: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-clusterip-timeout in namespace services-9874 02/20/23 21:53:59.885
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-9874 02/20/23 21:53:59.928
    I0220 21:53:59.939142      20 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9874, replica count: 3
    I0220 21:54:02.990501      20 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 21:54:03.018: INFO: Creating new exec pod
    Feb 20 21:54:03.055: INFO: Waiting up to 5m0s for pod "execpod-affinity9w9rc" in namespace "services-9874" to be "running"
    Feb 20 21:54:03.074: INFO: Pod "execpod-affinity9w9rc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.611087ms
    Feb 20 21:54:05.085: INFO: Pod "execpod-affinity9w9rc": Phase="Running", Reason="", readiness=true. Elapsed: 2.029664123s
    Feb 20 21:54:05.085: INFO: Pod "execpod-affinity9w9rc" satisfied condition "running"
    Feb 20 21:54:06.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Feb 20 21:54:06.348: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Feb 20 21:54:06.348: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 21:54:06.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.201.33 80'
    Feb 20 21:54:06.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.201.33 80\nConnection to 172.21.201.33 80 port [tcp/http] succeeded!\n"
    Feb 20 21:54:06.639: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 21:54:06.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.201.33:80/ ; done'
    Feb 20 21:54:07.009: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
    Feb 20 21:54:07.009: INFO: stdout: "\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5\naffinity-clusterip-timeout-jwsf5"
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Received response from host: affinity-clusterip-timeout-jwsf5
    Feb 20 21:54:07.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.201.33:80/'
    Feb 20 21:54:07.287: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
    Feb 20 21:54:07.288: INFO: stdout: "affinity-clusterip-timeout-jwsf5"
    Feb 20 21:54:27.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.201.33:80/'
    Feb 20 21:54:27.555: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
    Feb 20 21:54:27.555: INFO: stdout: "affinity-clusterip-timeout-jwsf5"
    Feb 20 21:54:47.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-9874 exec execpod-affinity9w9rc -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.201.33:80/'
    Feb 20 21:54:47.855: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.201.33:80/\n"
    Feb 20 21:54:47.855: INFO: stdout: "affinity-clusterip-timeout-d6hr9"
    Feb 20 21:54:47.855: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9874, will wait for the garbage collector to delete the pods 02/20/23 21:54:47.916
    Feb 20 21:54:47.989: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.509141ms
    Feb 20 21:54:48.090: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 101.326789ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 21:54:51.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9874" for this suite. 02/20/23 21:54:51.147
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:54:51.177
Feb 20 21:54:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 21:54:51.18
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:54:51.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:54:51.246
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 02/20/23 21:54:51.258
Feb 20 21:54:51.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92" in namespace "projected-659" to be "Succeeded or Failed"
Feb 20 21:54:51.337: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Pending", Reason="", readiness=false. Elapsed: 11.481642ms
Feb 20 21:54:53.350: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024517549s
Feb 20 21:54:55.349: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023864588s
Feb 20 21:54:57.348: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022706276s
STEP: Saw pod success 02/20/23 21:54:57.348
Feb 20 21:54:57.348: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92" satisfied condition "Succeeded or Failed"
Feb 20 21:54:57.360: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92 container client-container: <nil>
STEP: delete the pod 02/20/23 21:54:57.382
Feb 20 21:54:57.414: INFO: Waiting for pod downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92 to disappear
Feb 20 21:54:57.424: INFO: Pod downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 21:54:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-659" for this suite. 02/20/23 21:54:57.443
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":21,"skipped":430,"failed":0}
------------------------------
• [SLOW TEST] [6.290 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:54:51.177
    Feb 20 21:54:51.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 21:54:51.18
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:54:51.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:54:51.246
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 02/20/23 21:54:51.258
    Feb 20 21:54:51.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92" in namespace "projected-659" to be "Succeeded or Failed"
    Feb 20 21:54:51.337: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Pending", Reason="", readiness=false. Elapsed: 11.481642ms
    Feb 20 21:54:53.350: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024517549s
    Feb 20 21:54:55.349: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023864588s
    Feb 20 21:54:57.348: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022706276s
    STEP: Saw pod success 02/20/23 21:54:57.348
    Feb 20 21:54:57.348: INFO: Pod "downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92" satisfied condition "Succeeded or Failed"
    Feb 20 21:54:57.360: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92 container client-container: <nil>
    STEP: delete the pod 02/20/23 21:54:57.382
    Feb 20 21:54:57.414: INFO: Waiting for pod downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92 to disappear
    Feb 20 21:54:57.424: INFO: Pod downwardapi-volume-5d5d7d4a-acd8-4896-a4ee-e40c490aaf92 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 21:54:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-659" for this suite. 02/20/23 21:54:57.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:54:57.469
Feb 20 21:54:57.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 21:54:57.471
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:54:57.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:54:57.567
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-576da986-979f-4bdd-94ba-f42110f60482 02/20/23 21:54:57.581
STEP: Creating a pod to test consume configMaps 02/20/23 21:54:57.597
Feb 20 21:54:57.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3" in namespace "configmap-7120" to be "Succeeded or Failed"
Feb 20 21:54:57.675: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.069259ms
Feb 20 21:54:59.688: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025081162s
Feb 20 21:55:01.707: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044158039s
STEP: Saw pod success 02/20/23 21:55:01.707
Feb 20 21:55:01.707: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3" satisfied condition "Succeeded or Failed"
Feb 20 21:55:01.728: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 21:55:01.765
Feb 20 21:55:01.826: INFO: Waiting for pod pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3 to disappear
Feb 20 21:55:01.839: INFO: Pod pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 21:55:01.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7120" for this suite. 02/20/23 21:55:01.858
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":22,"skipped":437,"failed":0}
------------------------------
• [4.434 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:54:57.469
    Feb 20 21:54:57.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 21:54:57.471
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:54:57.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:54:57.567
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-576da986-979f-4bdd-94ba-f42110f60482 02/20/23 21:54:57.581
    STEP: Creating a pod to test consume configMaps 02/20/23 21:54:57.597
    Feb 20 21:54:57.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3" in namespace "configmap-7120" to be "Succeeded or Failed"
    Feb 20 21:54:57.675: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.069259ms
    Feb 20 21:54:59.688: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025081162s
    Feb 20 21:55:01.707: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044158039s
    STEP: Saw pod success 02/20/23 21:55:01.707
    Feb 20 21:55:01.707: INFO: Pod "pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3" satisfied condition "Succeeded or Failed"
    Feb 20 21:55:01.728: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 21:55:01.765
    Feb 20 21:55:01.826: INFO: Waiting for pod pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3 to disappear
    Feb 20 21:55:01.839: INFO: Pod pod-configmaps-0837234c-a2e3-44e7-bee7-1cd46ad81de3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 21:55:01.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7120" for this suite. 02/20/23 21:55:01.858
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:55:01.903
Feb 20 21:55:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename containers 02/20/23 21:55:01.904
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:01.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:01.991
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 02/20/23 21:55:02.007
Feb 20 21:55:02.104: INFO: Waiting up to 5m0s for pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e" in namespace "containers-7874" to be "Succeeded or Failed"
Feb 20 21:55:02.116: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.792366ms
Feb 20 21:55:04.134: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030006502s
Feb 20 21:55:06.153: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049180991s
Feb 20 21:55:08.132: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027588062s
STEP: Saw pod success 02/20/23 21:55:08.132
Feb 20 21:55:08.132: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e" satisfied condition "Succeeded or Failed"
Feb 20 21:55:08.146: INFO: Trying to get logs from node 10.8.38.66 pod client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e container agnhost-container: <nil>
STEP: delete the pod 02/20/23 21:55:08.168
Feb 20 21:55:08.230: INFO: Waiting for pod client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e to disappear
Feb 20 21:55:08.241: INFO: Pod client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 20 21:55:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7874" for this suite. 02/20/23 21:55:08.258
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":23,"skipped":441,"failed":0}
------------------------------
• [SLOW TEST] [6.424 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:55:01.903
    Feb 20 21:55:01.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename containers 02/20/23 21:55:01.904
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:01.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:01.991
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 02/20/23 21:55:02.007
    Feb 20 21:55:02.104: INFO: Waiting up to 5m0s for pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e" in namespace "containers-7874" to be "Succeeded or Failed"
    Feb 20 21:55:02.116: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.792366ms
    Feb 20 21:55:04.134: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030006502s
    Feb 20 21:55:06.153: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049180991s
    Feb 20 21:55:08.132: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027588062s
    STEP: Saw pod success 02/20/23 21:55:08.132
    Feb 20 21:55:08.132: INFO: Pod "client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e" satisfied condition "Succeeded or Failed"
    Feb 20 21:55:08.146: INFO: Trying to get logs from node 10.8.38.66 pod client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 21:55:08.168
    Feb 20 21:55:08.230: INFO: Waiting for pod client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e to disappear
    Feb 20 21:55:08.241: INFO: Pod client-containers-7ca0f8f6-6a04-442b-adf3-77dd2202781e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 20 21:55:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7874" for this suite. 02/20/23 21:55:08.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:55:08.335
Feb 20 21:55:08.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename subpath 02/20/23 21:55:08.336
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:08.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:08.439
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/20/23 21:55:08.45
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-k5nq 02/20/23 21:55:08.519
STEP: Creating a pod to test atomic-volume-subpath 02/20/23 21:55:08.52
Feb 20 21:55:08.597: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-k5nq" in namespace "subpath-7234" to be "Succeeded or Failed"
Feb 20 21:55:08.612: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Pending", Reason="", readiness=false. Elapsed: 14.496839ms
Feb 20 21:55:10.628: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031135726s
Feb 20 21:55:12.624: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 4.026315929s
Feb 20 21:55:14.642: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 6.044465914s
Feb 20 21:55:16.624: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 8.02705174s
Feb 20 21:55:18.625: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 10.028050056s
Feb 20 21:55:20.626: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 12.02850283s
Feb 20 21:55:22.629: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 14.031972436s
Feb 20 21:55:24.626: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 16.029007933s
Feb 20 21:55:26.624: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 18.027169774s
Feb 20 21:55:28.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 20.029472477s
Feb 20 21:55:30.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 22.029530439s
Feb 20 21:55:32.632: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=false. Elapsed: 24.034698661s
Feb 20 21:55:34.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.029672346s
STEP: Saw pod success 02/20/23 21:55:34.627
Feb 20 21:55:34.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq" satisfied condition "Succeeded or Failed"
Feb 20 21:55:34.638: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-downwardapi-k5nq container test-container-subpath-downwardapi-k5nq: <nil>
STEP: delete the pod 02/20/23 21:55:34.66
Feb 20 21:55:34.692: INFO: Waiting for pod pod-subpath-test-downwardapi-k5nq to disappear
Feb 20 21:55:34.705: INFO: Pod pod-subpath-test-downwardapi-k5nq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-k5nq 02/20/23 21:55:34.706
Feb 20 21:55:34.706: INFO: Deleting pod "pod-subpath-test-downwardapi-k5nq" in namespace "subpath-7234"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 20 21:55:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7234" for this suite. 02/20/23 21:55:34.742
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":24,"skipped":545,"failed":0}
------------------------------
• [SLOW TEST] [26.435 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:55:08.335
    Feb 20 21:55:08.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename subpath 02/20/23 21:55:08.336
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:08.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:08.439
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/20/23 21:55:08.45
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-k5nq 02/20/23 21:55:08.519
    STEP: Creating a pod to test atomic-volume-subpath 02/20/23 21:55:08.52
    Feb 20 21:55:08.597: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-k5nq" in namespace "subpath-7234" to be "Succeeded or Failed"
    Feb 20 21:55:08.612: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Pending", Reason="", readiness=false. Elapsed: 14.496839ms
    Feb 20 21:55:10.628: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031135726s
    Feb 20 21:55:12.624: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 4.026315929s
    Feb 20 21:55:14.642: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 6.044465914s
    Feb 20 21:55:16.624: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 8.02705174s
    Feb 20 21:55:18.625: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 10.028050056s
    Feb 20 21:55:20.626: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 12.02850283s
    Feb 20 21:55:22.629: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 14.031972436s
    Feb 20 21:55:24.626: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 16.029007933s
    Feb 20 21:55:26.624: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 18.027169774s
    Feb 20 21:55:28.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 20.029472477s
    Feb 20 21:55:30.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=true. Elapsed: 22.029530439s
    Feb 20 21:55:32.632: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Running", Reason="", readiness=false. Elapsed: 24.034698661s
    Feb 20 21:55:34.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.029672346s
    STEP: Saw pod success 02/20/23 21:55:34.627
    Feb 20 21:55:34.627: INFO: Pod "pod-subpath-test-downwardapi-k5nq" satisfied condition "Succeeded or Failed"
    Feb 20 21:55:34.638: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-downwardapi-k5nq container test-container-subpath-downwardapi-k5nq: <nil>
    STEP: delete the pod 02/20/23 21:55:34.66
    Feb 20 21:55:34.692: INFO: Waiting for pod pod-subpath-test-downwardapi-k5nq to disappear
    Feb 20 21:55:34.705: INFO: Pod pod-subpath-test-downwardapi-k5nq no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-k5nq 02/20/23 21:55:34.706
    Feb 20 21:55:34.706: INFO: Deleting pod "pod-subpath-test-downwardapi-k5nq" in namespace "subpath-7234"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 20 21:55:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7234" for this suite. 02/20/23 21:55:34.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:55:34.771
Feb 20 21:55:34.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 21:55:34.772
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:34.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:34.879
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1921 02/20/23 21:55:34.889
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/20/23 21:55:34.946
STEP: creating service externalsvc in namespace services-1921 02/20/23 21:55:34.946
STEP: creating replication controller externalsvc in namespace services-1921 02/20/23 21:55:35.016
I0220 21:55:35.035117      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1921, replica count: 2
I0220 21:55:38.092413      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/20/23 21:55:38.118
Feb 20 21:55:38.171: INFO: Creating new exec pod
Feb 20 21:55:38.216: INFO: Waiting up to 5m0s for pod "execpodnhhzv" in namespace "services-1921" to be "running"
Feb 20 21:55:38.228: INFO: Pod "execpodnhhzv": Phase="Pending", Reason="", readiness=false. Elapsed: 11.581645ms
Feb 20 21:55:40.240: INFO: Pod "execpodnhhzv": Phase="Running", Reason="", readiness=true. Elapsed: 2.024122677s
Feb 20 21:55:40.241: INFO: Pod "execpodnhhzv" satisfied condition "running"
Feb 20 21:55:40.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1921 exec execpodnhhzv -- /bin/sh -x -c nslookup nodeport-service.services-1921.svc.cluster.local'
Feb 20 21:55:40.565: INFO: stderr: "+ nslookup nodeport-service.services-1921.svc.cluster.local\n"
Feb 20 21:55:40.565: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-1921.svc.cluster.local\tcanonical name = externalsvc.services-1921.svc.cluster.local.\nName:\texternalsvc.services-1921.svc.cluster.local\nAddress: 172.21.174.244\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1921, will wait for the garbage collector to delete the pods 02/20/23 21:55:40.565
Feb 20 21:55:40.652: INFO: Deleting ReplicationController externalsvc took: 28.455155ms
Feb 20 21:55:40.754: INFO: Terminating ReplicationController externalsvc pods took: 101.115771ms
Feb 20 21:55:43.297: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 21:55:43.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1921" for this suite. 02/20/23 21:55:43.352
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":25,"skipped":559,"failed":0}
------------------------------
• [SLOW TEST] [8.605 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:55:34.771
    Feb 20 21:55:34.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 21:55:34.772
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:34.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:34.879
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1921 02/20/23 21:55:34.889
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/20/23 21:55:34.946
    STEP: creating service externalsvc in namespace services-1921 02/20/23 21:55:34.946
    STEP: creating replication controller externalsvc in namespace services-1921 02/20/23 21:55:35.016
    I0220 21:55:35.035117      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1921, replica count: 2
    I0220 21:55:38.092413      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/20/23 21:55:38.118
    Feb 20 21:55:38.171: INFO: Creating new exec pod
    Feb 20 21:55:38.216: INFO: Waiting up to 5m0s for pod "execpodnhhzv" in namespace "services-1921" to be "running"
    Feb 20 21:55:38.228: INFO: Pod "execpodnhhzv": Phase="Pending", Reason="", readiness=false. Elapsed: 11.581645ms
    Feb 20 21:55:40.240: INFO: Pod "execpodnhhzv": Phase="Running", Reason="", readiness=true. Elapsed: 2.024122677s
    Feb 20 21:55:40.241: INFO: Pod "execpodnhhzv" satisfied condition "running"
    Feb 20 21:55:40.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1921 exec execpodnhhzv -- /bin/sh -x -c nslookup nodeport-service.services-1921.svc.cluster.local'
    Feb 20 21:55:40.565: INFO: stderr: "+ nslookup nodeport-service.services-1921.svc.cluster.local\n"
    Feb 20 21:55:40.565: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-1921.svc.cluster.local\tcanonical name = externalsvc.services-1921.svc.cluster.local.\nName:\texternalsvc.services-1921.svc.cluster.local\nAddress: 172.21.174.244\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1921, will wait for the garbage collector to delete the pods 02/20/23 21:55:40.565
    Feb 20 21:55:40.652: INFO: Deleting ReplicationController externalsvc took: 28.455155ms
    Feb 20 21:55:40.754: INFO: Terminating ReplicationController externalsvc pods took: 101.115771ms
    Feb 20 21:55:43.297: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 21:55:43.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1921" for this suite. 02/20/23 21:55:43.352
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:55:43.378
Feb 20 21:55:43.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 21:55:43.379
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:43.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:43.43
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/20/23 21:55:43.442
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/20/23 21:55:43.442
STEP: creating a pod to probe DNS 02/20/23 21:55:43.442
STEP: submitting the pod to kubernetes 02/20/23 21:55:43.443
Feb 20 21:55:43.514: INFO: Waiting up to 15m0s for pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06" in namespace "dns-9618" to be "running"
Feb 20 21:55:43.535: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 20.915943ms
Feb 20 21:55:45.547: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032935321s
Feb 20 21:55:47.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033683306s
Feb 20 21:55:49.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034073977s
Feb 20 21:55:51.555: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040707502s
Feb 20 21:55:53.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034251447s
Feb 20 21:55:55.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Running", Reason="", readiness=true. Elapsed: 12.034188969s
Feb 20 21:55:55.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06" satisfied condition "running"
STEP: retrieving the pod 02/20/23 21:55:55.548
STEP: looking for the results for each expected name from probers 02/20/23 21:55:55.561
Feb 20 21:55:55.640: INFO: DNS probes using dns-9618/dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06 succeeded

STEP: deleting the pod 02/20/23 21:55:55.64
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 21:55:55.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9618" for this suite. 02/20/23 21:55:55.69
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":26,"skipped":596,"failed":0}
------------------------------
• [SLOW TEST] [12.338 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:55:43.378
    Feb 20 21:55:43.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 21:55:43.379
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:43.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:43.43
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/20/23 21:55:43.442
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/20/23 21:55:43.442
    STEP: creating a pod to probe DNS 02/20/23 21:55:43.442
    STEP: submitting the pod to kubernetes 02/20/23 21:55:43.443
    Feb 20 21:55:43.514: INFO: Waiting up to 15m0s for pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06" in namespace "dns-9618" to be "running"
    Feb 20 21:55:43.535: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 20.915943ms
    Feb 20 21:55:45.547: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032935321s
    Feb 20 21:55:47.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033683306s
    Feb 20 21:55:49.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034073977s
    Feb 20 21:55:51.555: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 8.040707502s
    Feb 20 21:55:53.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034251447s
    Feb 20 21:55:55.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06": Phase="Running", Reason="", readiness=true. Elapsed: 12.034188969s
    Feb 20 21:55:55.548: INFO: Pod "dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 21:55:55.548
    STEP: looking for the results for each expected name from probers 02/20/23 21:55:55.561
    Feb 20 21:55:55.640: INFO: DNS probes using dns-9618/dns-test-7937c69a-b714-4d90-b1c1-922d6be55c06 succeeded

    STEP: deleting the pod 02/20/23 21:55:55.64
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 21:55:55.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9618" for this suite. 02/20/23 21:55:55.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:55:55.72
Feb 20 21:55:55.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename subpath 02/20/23 21:55:55.728
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:55.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:55.779
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/20/23 21:55:55.789
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-w5qf 02/20/23 21:55:55.821
STEP: Creating a pod to test atomic-volume-subpath 02/20/23 21:55:55.821
Feb 20 21:55:55.867: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-w5qf" in namespace "subpath-4313" to be "Succeeded or Failed"
Feb 20 21:55:55.878: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.220075ms
Feb 20 21:55:57.892: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025107133s
Feb 20 21:55:59.900: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 4.033141584s
Feb 20 21:56:01.890: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 6.023556866s
Feb 20 21:56:03.891: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 8.023871271s
Feb 20 21:56:05.894: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 10.026808465s
Feb 20 21:56:07.893: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 12.025968615s
Feb 20 21:56:09.891: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 14.023762091s
Feb 20 21:56:11.892: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 16.024990628s
Feb 20 21:56:13.891: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 18.024001455s
Feb 20 21:56:15.901: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 20.033908846s
Feb 20 21:56:17.890: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 22.023125167s
Feb 20 21:56:19.902: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=false. Elapsed: 24.035096844s
Feb 20 21:56:21.890: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.023230488s
STEP: Saw pod success 02/20/23 21:56:21.89
Feb 20 21:56:21.891: INFO: Pod "pod-subpath-test-secret-w5qf" satisfied condition "Succeeded or Failed"
Feb 20 21:56:21.902: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-secret-w5qf container test-container-subpath-secret-w5qf: <nil>
STEP: delete the pod 02/20/23 21:56:21.923
Feb 20 21:56:21.972: INFO: Waiting for pod pod-subpath-test-secret-w5qf to disappear
Feb 20 21:56:21.983: INFO: Pod pod-subpath-test-secret-w5qf no longer exists
STEP: Deleting pod pod-subpath-test-secret-w5qf 02/20/23 21:56:21.983
Feb 20 21:56:21.984: INFO: Deleting pod "pod-subpath-test-secret-w5qf" in namespace "subpath-4313"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 20 21:56:21.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4313" for this suite. 02/20/23 21:56:22.051
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":27,"skipped":663,"failed":0}
------------------------------
• [SLOW TEST] [26.354 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:55:55.72
    Feb 20 21:55:55.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename subpath 02/20/23 21:55:55.728
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:55:55.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:55:55.779
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/20/23 21:55:55.789
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-w5qf 02/20/23 21:55:55.821
    STEP: Creating a pod to test atomic-volume-subpath 02/20/23 21:55:55.821
    Feb 20 21:55:55.867: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-w5qf" in namespace "subpath-4313" to be "Succeeded or Failed"
    Feb 20 21:55:55.878: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.220075ms
    Feb 20 21:55:57.892: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025107133s
    Feb 20 21:55:59.900: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 4.033141584s
    Feb 20 21:56:01.890: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 6.023556866s
    Feb 20 21:56:03.891: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 8.023871271s
    Feb 20 21:56:05.894: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 10.026808465s
    Feb 20 21:56:07.893: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 12.025968615s
    Feb 20 21:56:09.891: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 14.023762091s
    Feb 20 21:56:11.892: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 16.024990628s
    Feb 20 21:56:13.891: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 18.024001455s
    Feb 20 21:56:15.901: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 20.033908846s
    Feb 20 21:56:17.890: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=true. Elapsed: 22.023125167s
    Feb 20 21:56:19.902: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Running", Reason="", readiness=false. Elapsed: 24.035096844s
    Feb 20 21:56:21.890: INFO: Pod "pod-subpath-test-secret-w5qf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.023230488s
    STEP: Saw pod success 02/20/23 21:56:21.89
    Feb 20 21:56:21.891: INFO: Pod "pod-subpath-test-secret-w5qf" satisfied condition "Succeeded or Failed"
    Feb 20 21:56:21.902: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-secret-w5qf container test-container-subpath-secret-w5qf: <nil>
    STEP: delete the pod 02/20/23 21:56:21.923
    Feb 20 21:56:21.972: INFO: Waiting for pod pod-subpath-test-secret-w5qf to disappear
    Feb 20 21:56:21.983: INFO: Pod pod-subpath-test-secret-w5qf no longer exists
    STEP: Deleting pod pod-subpath-test-secret-w5qf 02/20/23 21:56:21.983
    Feb 20 21:56:21.984: INFO: Deleting pod "pod-subpath-test-secret-w5qf" in namespace "subpath-4313"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 20 21:56:21.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4313" for this suite. 02/20/23 21:56:22.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:56:22.08
Feb 20 21:56:22.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename subpath 02/20/23 21:56:22.082
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:56:22.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:56:22.161
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/20/23 21:56:22.177
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-qqg5 02/20/23 21:56:22.217
STEP: Creating a pod to test atomic-volume-subpath 02/20/23 21:56:22.217
Feb 20 21:56:22.306: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qqg5" in namespace "subpath-1629" to be "Succeeded or Failed"
Feb 20 21:56:22.323: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.699537ms
Feb 20 21:56:24.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.030259615s
Feb 20 21:56:26.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.029582595s
Feb 20 21:56:28.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.030235438s
Feb 20 21:56:30.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.02931824s
Feb 20 21:56:32.344: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.037659409s
Feb 20 21:56:34.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.030453811s
Feb 20 21:56:36.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.030011985s
Feb 20 21:56:38.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.02977727s
Feb 20 21:56:40.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.029844307s
Feb 20 21:56:42.335: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.028476884s
Feb 20 21:56:44.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=false. Elapsed: 22.03098238s
Feb 20 21:56:46.338: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031966567s
STEP: Saw pod success 02/20/23 21:56:46.338
Feb 20 21:56:46.339: INFO: Pod "pod-subpath-test-configmap-qqg5" satisfied condition "Succeeded or Failed"
Feb 20 21:56:46.355: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-configmap-qqg5 container test-container-subpath-configmap-qqg5: <nil>
STEP: delete the pod 02/20/23 21:56:46.379
Feb 20 21:56:46.411: INFO: Waiting for pod pod-subpath-test-configmap-qqg5 to disappear
Feb 20 21:56:46.424: INFO: Pod pod-subpath-test-configmap-qqg5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qqg5 02/20/23 21:56:46.424
Feb 20 21:56:46.425: INFO: Deleting pod "pod-subpath-test-configmap-qqg5" in namespace "subpath-1629"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 20 21:56:46.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1629" for this suite. 02/20/23 21:56:46.456
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":28,"skipped":686,"failed":0}
------------------------------
• [SLOW TEST] [24.399 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:56:22.08
    Feb 20 21:56:22.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename subpath 02/20/23 21:56:22.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:56:22.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:56:22.161
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/20/23 21:56:22.177
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-qqg5 02/20/23 21:56:22.217
    STEP: Creating a pod to test atomic-volume-subpath 02/20/23 21:56:22.217
    Feb 20 21:56:22.306: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qqg5" in namespace "subpath-1629" to be "Succeeded or Failed"
    Feb 20 21:56:22.323: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.699537ms
    Feb 20 21:56:24.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.030259615s
    Feb 20 21:56:26.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.029582595s
    Feb 20 21:56:28.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 6.030235438s
    Feb 20 21:56:30.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 8.02931824s
    Feb 20 21:56:32.344: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 10.037659409s
    Feb 20 21:56:34.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 12.030453811s
    Feb 20 21:56:36.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 14.030011985s
    Feb 20 21:56:38.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 16.02977727s
    Feb 20 21:56:40.336: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 18.029844307s
    Feb 20 21:56:42.335: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=true. Elapsed: 20.028476884s
    Feb 20 21:56:44.337: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Running", Reason="", readiness=false. Elapsed: 22.03098238s
    Feb 20 21:56:46.338: INFO: Pod "pod-subpath-test-configmap-qqg5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031966567s
    STEP: Saw pod success 02/20/23 21:56:46.338
    Feb 20 21:56:46.339: INFO: Pod "pod-subpath-test-configmap-qqg5" satisfied condition "Succeeded or Failed"
    Feb 20 21:56:46.355: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-configmap-qqg5 container test-container-subpath-configmap-qqg5: <nil>
    STEP: delete the pod 02/20/23 21:56:46.379
    Feb 20 21:56:46.411: INFO: Waiting for pod pod-subpath-test-configmap-qqg5 to disappear
    Feb 20 21:56:46.424: INFO: Pod pod-subpath-test-configmap-qqg5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-qqg5 02/20/23 21:56:46.424
    Feb 20 21:56:46.425: INFO: Deleting pod "pod-subpath-test-configmap-qqg5" in namespace "subpath-1629"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 20 21:56:46.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1629" for this suite. 02/20/23 21:56:46.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:56:46.48
Feb 20 21:56:46.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 21:56:46.481
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:56:46.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:56:46.584
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/20/23 21:56:46.626
Feb 20 21:56:46.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 21:56:55.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 21:57:30.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8473" for this suite. 02/20/23 21:57:30.945
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":29,"skipped":698,"failed":0}
------------------------------
• [SLOW TEST] [44.480 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:56:46.48
    Feb 20 21:56:46.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 21:56:46.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:56:46.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:56:46.584
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/20/23 21:56:46.626
    Feb 20 21:56:46.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 21:56:55.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 21:57:30.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8473" for this suite. 02/20/23 21:57:30.945
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:57:30.966
Feb 20 21:57:30.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 21:57:30.968
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:57:31.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:57:31.022
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
W0220 21:57:31.061388      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 21:58:31.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4680" for this suite. 02/20/23 21:58:31.088
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":30,"skipped":701,"failed":0}
------------------------------
• [SLOW TEST] [60.137 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:57:30.966
    Feb 20 21:57:30.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 21:57:30.968
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:57:31.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:57:31.022
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    W0220 21:57:31.061388      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 21:58:31.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4680" for this suite. 02/20/23 21:58:31.088
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:31.111
Feb 20 21:58:31.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename discovery 02/20/23 21:58:31.113
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:31.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:31.161
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/20/23 21:58:31.172
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 20 21:58:31.505: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 20 21:58:31.508: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 20 21:58:31.509: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 20 21:58:31.509: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 20 21:58:31.509: INFO: Checking APIGroup: apps
Feb 20 21:58:31.512: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 20 21:58:31.512: INFO: Versions found [{apps/v1 v1}]
Feb 20 21:58:31.512: INFO: apps/v1 matches apps/v1
Feb 20 21:58:31.512: INFO: Checking APIGroup: events.k8s.io
Feb 20 21:58:31.515: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 20 21:58:31.515: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 20 21:58:31.515: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 20 21:58:31.515: INFO: Checking APIGroup: authentication.k8s.io
Feb 20 21:58:31.518: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 20 21:58:31.519: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 20 21:58:31.519: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 20 21:58:31.519: INFO: Checking APIGroup: authorization.k8s.io
Feb 20 21:58:31.522: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 20 21:58:31.522: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 20 21:58:31.522: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 20 21:58:31.522: INFO: Checking APIGroup: autoscaling
Feb 20 21:58:31.525: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 20 21:58:31.525: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Feb 20 21:58:31.525: INFO: autoscaling/v2 matches autoscaling/v2
Feb 20 21:58:31.525: INFO: Checking APIGroup: batch
Feb 20 21:58:31.529: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 20 21:58:31.529: INFO: Versions found [{batch/v1 v1}]
Feb 20 21:58:31.529: INFO: batch/v1 matches batch/v1
Feb 20 21:58:31.529: INFO: Checking APIGroup: certificates.k8s.io
Feb 20 21:58:31.533: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 20 21:58:31.533: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 20 21:58:31.533: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 20 21:58:31.533: INFO: Checking APIGroup: networking.k8s.io
Feb 20 21:58:31.538: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 20 21:58:31.538: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 20 21:58:31.538: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 20 21:58:31.538: INFO: Checking APIGroup: policy
Feb 20 21:58:31.541: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 20 21:58:31.541: INFO: Versions found [{policy/v1 v1}]
Feb 20 21:58:31.541: INFO: policy/v1 matches policy/v1
Feb 20 21:58:31.541: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 20 21:58:31.544: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 20 21:58:31.544: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 20 21:58:31.544: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 20 21:58:31.545: INFO: Checking APIGroup: storage.k8s.io
Feb 20 21:58:31.548: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 20 21:58:31.548: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 20 21:58:31.548: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 20 21:58:31.548: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 20 21:58:31.581: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 20 21:58:31.581: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 20 21:58:31.581: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 20 21:58:31.581: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 20 21:58:31.584: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 20 21:58:31.585: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 20 21:58:31.585: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 20 21:58:31.585: INFO: Checking APIGroup: scheduling.k8s.io
Feb 20 21:58:31.588: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 20 21:58:31.588: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 20 21:58:31.588: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 20 21:58:31.589: INFO: Checking APIGroup: coordination.k8s.io
Feb 20 21:58:31.592: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 20 21:58:31.592: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 20 21:58:31.592: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 20 21:58:31.592: INFO: Checking APIGroup: node.k8s.io
Feb 20 21:58:31.596: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 20 21:58:31.596: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 20 21:58:31.596: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 20 21:58:31.596: INFO: Checking APIGroup: discovery.k8s.io
Feb 20 21:58:31.600: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 20 21:58:31.600: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 20 21:58:31.600: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 20 21:58:31.600: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 20 21:58:31.603: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Feb 20 21:58:31.603: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 20 21:58:31.603: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Feb 20 21:58:31.604: INFO: Checking APIGroup: apps.openshift.io
Feb 20 21:58:31.607: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Feb 20 21:58:31.607: INFO: Versions found [{apps.openshift.io/v1 v1}]
Feb 20 21:58:31.607: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Feb 20 21:58:31.607: INFO: Checking APIGroup: authorization.openshift.io
Feb 20 21:58:31.611: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Feb 20 21:58:31.611: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Feb 20 21:58:31.611: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Feb 20 21:58:31.611: INFO: Checking APIGroup: build.openshift.io
Feb 20 21:58:31.614: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Feb 20 21:58:31.614: INFO: Versions found [{build.openshift.io/v1 v1}]
Feb 20 21:58:31.614: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Feb 20 21:58:31.615: INFO: Checking APIGroup: image.openshift.io
Feb 20 21:58:31.620: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Feb 20 21:58:31.620: INFO: Versions found [{image.openshift.io/v1 v1}]
Feb 20 21:58:31.620: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Feb 20 21:58:31.620: INFO: Checking APIGroup: oauth.openshift.io
Feb 20 21:58:31.623: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Feb 20 21:58:31.623: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Feb 20 21:58:31.623: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Feb 20 21:58:31.623: INFO: Checking APIGroup: project.openshift.io
Feb 20 21:58:31.628: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Feb 20 21:58:31.628: INFO: Versions found [{project.openshift.io/v1 v1}]
Feb 20 21:58:31.628: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Feb 20 21:58:31.629: INFO: Checking APIGroup: quota.openshift.io
Feb 20 21:58:31.632: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Feb 20 21:58:31.633: INFO: Versions found [{quota.openshift.io/v1 v1}]
Feb 20 21:58:31.633: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Feb 20 21:58:31.633: INFO: Checking APIGroup: route.openshift.io
Feb 20 21:58:31.636: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Feb 20 21:58:31.636: INFO: Versions found [{route.openshift.io/v1 v1}]
Feb 20 21:58:31.636: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Feb 20 21:58:31.636: INFO: Checking APIGroup: security.openshift.io
Feb 20 21:58:31.639: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Feb 20 21:58:31.640: INFO: Versions found [{security.openshift.io/v1 v1}]
Feb 20 21:58:31.640: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Feb 20 21:58:31.640: INFO: Checking APIGroup: template.openshift.io
Feb 20 21:58:31.643: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Feb 20 21:58:31.644: INFO: Versions found [{template.openshift.io/v1 v1}]
Feb 20 21:58:31.644: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Feb 20 21:58:31.644: INFO: Checking APIGroup: user.openshift.io
Feb 20 21:58:31.647: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Feb 20 21:58:31.647: INFO: Versions found [{user.openshift.io/v1 v1}]
Feb 20 21:58:31.647: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Feb 20 21:58:31.647: INFO: Checking APIGroup: packages.operators.coreos.com
Feb 20 21:58:31.651: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Feb 20 21:58:31.651: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Feb 20 21:58:31.652: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Feb 20 21:58:31.652: INFO: Checking APIGroup: config.openshift.io
Feb 20 21:58:31.655: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Feb 20 21:58:31.655: INFO: Versions found [{config.openshift.io/v1 v1}]
Feb 20 21:58:31.655: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Feb 20 21:58:31.655: INFO: Checking APIGroup: operator.openshift.io
Feb 20 21:58:31.659: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Feb 20 21:58:31.659: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Feb 20 21:58:31.659: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Feb 20 21:58:31.659: INFO: Checking APIGroup: apiserver.openshift.io
Feb 20 21:58:31.663: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Feb 20 21:58:31.663: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Feb 20 21:58:31.663: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Feb 20 21:58:31.663: INFO: Checking APIGroup: cloudcredential.openshift.io
Feb 20 21:58:31.667: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Feb 20 21:58:31.667: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Feb 20 21:58:31.667: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Feb 20 21:58:31.667: INFO: Checking APIGroup: console.openshift.io
Feb 20 21:58:31.670: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Feb 20 21:58:31.670: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Feb 20 21:58:31.670: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Feb 20 21:58:31.670: INFO: Checking APIGroup: crd.projectcalico.org
Feb 20 21:58:31.674: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 20 21:58:31.674: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 20 21:58:31.674: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 20 21:58:31.674: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Feb 20 21:58:31.677: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Feb 20 21:58:31.677: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Feb 20 21:58:31.678: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Feb 20 21:58:31.678: INFO: Checking APIGroup: ingress.operator.openshift.io
Feb 20 21:58:31.681: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Feb 20 21:58:31.681: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Feb 20 21:58:31.681: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Feb 20 21:58:31.681: INFO: Checking APIGroup: k8s.cni.cncf.io
Feb 20 21:58:31.684: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Feb 20 21:58:31.684: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Feb 20 21:58:31.684: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Feb 20 21:58:31.684: INFO: Checking APIGroup: machineconfiguration.openshift.io
Feb 20 21:58:31.688: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Feb 20 21:58:31.688: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Feb 20 21:58:31.688: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Feb 20 21:58:31.688: INFO: Checking APIGroup: monitoring.coreos.com
Feb 20 21:58:31.691: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Feb 20 21:58:31.692: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Feb 20 21:58:31.692: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Feb 20 21:58:31.692: INFO: Checking APIGroup: network.operator.openshift.io
Feb 20 21:58:31.705: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Feb 20 21:58:31.705: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Feb 20 21:58:31.705: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Feb 20 21:58:31.705: INFO: Checking APIGroup: operator.tigera.io
Feb 20 21:58:31.712: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Feb 20 21:58:31.713: INFO: Versions found [{operator.tigera.io/v1 v1}]
Feb 20 21:58:31.713: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Feb 20 21:58:31.713: INFO: Checking APIGroup: operators.coreos.com
Feb 20 21:58:31.716: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Feb 20 21:58:31.716: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Feb 20 21:58:31.716: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Feb 20 21:58:31.716: INFO: Checking APIGroup: performance.openshift.io
Feb 20 21:58:31.719: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Feb 20 21:58:31.720: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Feb 20 21:58:31.720: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Feb 20 21:58:31.720: INFO: Checking APIGroup: samples.operator.openshift.io
Feb 20 21:58:31.723: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Feb 20 21:58:31.723: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Feb 20 21:58:31.723: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Feb 20 21:58:31.723: INFO: Checking APIGroup: security.internal.openshift.io
Feb 20 21:58:31.726: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Feb 20 21:58:31.727: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Feb 20 21:58:31.727: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Feb 20 21:58:31.727: INFO: Checking APIGroup: snapshot.storage.k8s.io
Feb 20 21:58:31.731: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Feb 20 21:58:31.732: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Feb 20 21:58:31.732: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Feb 20 21:58:31.732: INFO: Checking APIGroup: tuned.openshift.io
Feb 20 21:58:31.735: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Feb 20 21:58:31.735: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Feb 20 21:58:31.735: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Feb 20 21:58:31.735: INFO: Checking APIGroup: controlplane.operator.openshift.io
Feb 20 21:58:31.739: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Feb 20 21:58:31.739: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Feb 20 21:58:31.739: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Feb 20 21:58:31.739: INFO: Checking APIGroup: ibm.com
Feb 20 21:58:31.742: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Feb 20 21:58:31.742: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Feb 20 21:58:31.742: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Feb 20 21:58:31.742: INFO: Checking APIGroup: migration.k8s.io
Feb 20 21:58:31.745: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Feb 20 21:58:31.745: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Feb 20 21:58:31.746: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Feb 20 21:58:31.746: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Feb 20 21:58:31.754: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Feb 20 21:58:31.754: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Feb 20 21:58:31.754: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Feb 20 21:58:31.755: INFO: Checking APIGroup: helm.openshift.io
Feb 20 21:58:31.805: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Feb 20 21:58:31.805: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Feb 20 21:58:31.805: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Feb 20 21:58:31.806: INFO: Checking APIGroup: metrics.k8s.io
Feb 20 21:58:31.855: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 20 21:58:31.855: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 20 21:58:31.855: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Feb 20 21:58:31.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2399" for this suite. 02/20/23 21:58:31.915
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":31,"skipped":704,"failed":0}
------------------------------
• [0.871 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:31.111
    Feb 20 21:58:31.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename discovery 02/20/23 21:58:31.113
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:31.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:31.161
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/20/23 21:58:31.172
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 20 21:58:31.505: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 20 21:58:31.508: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 20 21:58:31.509: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 20 21:58:31.509: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 20 21:58:31.509: INFO: Checking APIGroup: apps
    Feb 20 21:58:31.512: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 20 21:58:31.512: INFO: Versions found [{apps/v1 v1}]
    Feb 20 21:58:31.512: INFO: apps/v1 matches apps/v1
    Feb 20 21:58:31.512: INFO: Checking APIGroup: events.k8s.io
    Feb 20 21:58:31.515: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 20 21:58:31.515: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 20 21:58:31.515: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 20 21:58:31.515: INFO: Checking APIGroup: authentication.k8s.io
    Feb 20 21:58:31.518: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 20 21:58:31.519: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 20 21:58:31.519: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 20 21:58:31.519: INFO: Checking APIGroup: authorization.k8s.io
    Feb 20 21:58:31.522: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 20 21:58:31.522: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 20 21:58:31.522: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 20 21:58:31.522: INFO: Checking APIGroup: autoscaling
    Feb 20 21:58:31.525: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 20 21:58:31.525: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Feb 20 21:58:31.525: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 20 21:58:31.525: INFO: Checking APIGroup: batch
    Feb 20 21:58:31.529: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 20 21:58:31.529: INFO: Versions found [{batch/v1 v1}]
    Feb 20 21:58:31.529: INFO: batch/v1 matches batch/v1
    Feb 20 21:58:31.529: INFO: Checking APIGroup: certificates.k8s.io
    Feb 20 21:58:31.533: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 20 21:58:31.533: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 20 21:58:31.533: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 20 21:58:31.533: INFO: Checking APIGroup: networking.k8s.io
    Feb 20 21:58:31.538: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 20 21:58:31.538: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 20 21:58:31.538: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 20 21:58:31.538: INFO: Checking APIGroup: policy
    Feb 20 21:58:31.541: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 20 21:58:31.541: INFO: Versions found [{policy/v1 v1}]
    Feb 20 21:58:31.541: INFO: policy/v1 matches policy/v1
    Feb 20 21:58:31.541: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 20 21:58:31.544: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 20 21:58:31.544: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 20 21:58:31.544: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 20 21:58:31.545: INFO: Checking APIGroup: storage.k8s.io
    Feb 20 21:58:31.548: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 20 21:58:31.548: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 20 21:58:31.548: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 20 21:58:31.548: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 20 21:58:31.581: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 20 21:58:31.581: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 20 21:58:31.581: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 20 21:58:31.581: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 20 21:58:31.584: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 20 21:58:31.585: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 20 21:58:31.585: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 20 21:58:31.585: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 20 21:58:31.588: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 20 21:58:31.588: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 20 21:58:31.588: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 20 21:58:31.589: INFO: Checking APIGroup: coordination.k8s.io
    Feb 20 21:58:31.592: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 20 21:58:31.592: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 20 21:58:31.592: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 20 21:58:31.592: INFO: Checking APIGroup: node.k8s.io
    Feb 20 21:58:31.596: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 20 21:58:31.596: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 20 21:58:31.596: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 20 21:58:31.596: INFO: Checking APIGroup: discovery.k8s.io
    Feb 20 21:58:31.600: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 20 21:58:31.600: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 20 21:58:31.600: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 20 21:58:31.600: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 20 21:58:31.603: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Feb 20 21:58:31.603: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Feb 20 21:58:31.603: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Feb 20 21:58:31.604: INFO: Checking APIGroup: apps.openshift.io
    Feb 20 21:58:31.607: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Feb 20 21:58:31.607: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Feb 20 21:58:31.607: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Feb 20 21:58:31.607: INFO: Checking APIGroup: authorization.openshift.io
    Feb 20 21:58:31.611: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Feb 20 21:58:31.611: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Feb 20 21:58:31.611: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Feb 20 21:58:31.611: INFO: Checking APIGroup: build.openshift.io
    Feb 20 21:58:31.614: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Feb 20 21:58:31.614: INFO: Versions found [{build.openshift.io/v1 v1}]
    Feb 20 21:58:31.614: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Feb 20 21:58:31.615: INFO: Checking APIGroup: image.openshift.io
    Feb 20 21:58:31.620: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Feb 20 21:58:31.620: INFO: Versions found [{image.openshift.io/v1 v1}]
    Feb 20 21:58:31.620: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Feb 20 21:58:31.620: INFO: Checking APIGroup: oauth.openshift.io
    Feb 20 21:58:31.623: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Feb 20 21:58:31.623: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Feb 20 21:58:31.623: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Feb 20 21:58:31.623: INFO: Checking APIGroup: project.openshift.io
    Feb 20 21:58:31.628: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Feb 20 21:58:31.628: INFO: Versions found [{project.openshift.io/v1 v1}]
    Feb 20 21:58:31.628: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Feb 20 21:58:31.629: INFO: Checking APIGroup: quota.openshift.io
    Feb 20 21:58:31.632: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Feb 20 21:58:31.633: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Feb 20 21:58:31.633: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Feb 20 21:58:31.633: INFO: Checking APIGroup: route.openshift.io
    Feb 20 21:58:31.636: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Feb 20 21:58:31.636: INFO: Versions found [{route.openshift.io/v1 v1}]
    Feb 20 21:58:31.636: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Feb 20 21:58:31.636: INFO: Checking APIGroup: security.openshift.io
    Feb 20 21:58:31.639: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Feb 20 21:58:31.640: INFO: Versions found [{security.openshift.io/v1 v1}]
    Feb 20 21:58:31.640: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Feb 20 21:58:31.640: INFO: Checking APIGroup: template.openshift.io
    Feb 20 21:58:31.643: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Feb 20 21:58:31.644: INFO: Versions found [{template.openshift.io/v1 v1}]
    Feb 20 21:58:31.644: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Feb 20 21:58:31.644: INFO: Checking APIGroup: user.openshift.io
    Feb 20 21:58:31.647: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Feb 20 21:58:31.647: INFO: Versions found [{user.openshift.io/v1 v1}]
    Feb 20 21:58:31.647: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Feb 20 21:58:31.647: INFO: Checking APIGroup: packages.operators.coreos.com
    Feb 20 21:58:31.651: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Feb 20 21:58:31.651: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Feb 20 21:58:31.652: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Feb 20 21:58:31.652: INFO: Checking APIGroup: config.openshift.io
    Feb 20 21:58:31.655: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Feb 20 21:58:31.655: INFO: Versions found [{config.openshift.io/v1 v1}]
    Feb 20 21:58:31.655: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Feb 20 21:58:31.655: INFO: Checking APIGroup: operator.openshift.io
    Feb 20 21:58:31.659: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Feb 20 21:58:31.659: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.659: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Feb 20 21:58:31.659: INFO: Checking APIGroup: apiserver.openshift.io
    Feb 20 21:58:31.663: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Feb 20 21:58:31.663: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Feb 20 21:58:31.663: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Feb 20 21:58:31.663: INFO: Checking APIGroup: cloudcredential.openshift.io
    Feb 20 21:58:31.667: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Feb 20 21:58:31.667: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Feb 20 21:58:31.667: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Feb 20 21:58:31.667: INFO: Checking APIGroup: console.openshift.io
    Feb 20 21:58:31.670: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Feb 20 21:58:31.670: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.670: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Feb 20 21:58:31.670: INFO: Checking APIGroup: crd.projectcalico.org
    Feb 20 21:58:31.674: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Feb 20 21:58:31.674: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Feb 20 21:58:31.674: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Feb 20 21:58:31.674: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Feb 20 21:58:31.677: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Feb 20 21:58:31.677: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Feb 20 21:58:31.678: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Feb 20 21:58:31.678: INFO: Checking APIGroup: ingress.operator.openshift.io
    Feb 20 21:58:31.681: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Feb 20 21:58:31.681: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Feb 20 21:58:31.681: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Feb 20 21:58:31.681: INFO: Checking APIGroup: k8s.cni.cncf.io
    Feb 20 21:58:31.684: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Feb 20 21:58:31.684: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Feb 20 21:58:31.684: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Feb 20 21:58:31.684: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Feb 20 21:58:31.688: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Feb 20 21:58:31.688: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Feb 20 21:58:31.688: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Feb 20 21:58:31.688: INFO: Checking APIGroup: monitoring.coreos.com
    Feb 20 21:58:31.691: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Feb 20 21:58:31.692: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.692: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Feb 20 21:58:31.692: INFO: Checking APIGroup: network.operator.openshift.io
    Feb 20 21:58:31.705: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Feb 20 21:58:31.705: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Feb 20 21:58:31.705: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Feb 20 21:58:31.705: INFO: Checking APIGroup: operator.tigera.io
    Feb 20 21:58:31.712: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Feb 20 21:58:31.713: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Feb 20 21:58:31.713: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Feb 20 21:58:31.713: INFO: Checking APIGroup: operators.coreos.com
    Feb 20 21:58:31.716: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Feb 20 21:58:31.716: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.716: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Feb 20 21:58:31.716: INFO: Checking APIGroup: performance.openshift.io
    Feb 20 21:58:31.719: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Feb 20 21:58:31.720: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.720: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Feb 20 21:58:31.720: INFO: Checking APIGroup: samples.operator.openshift.io
    Feb 20 21:58:31.723: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Feb 20 21:58:31.723: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Feb 20 21:58:31.723: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Feb 20 21:58:31.723: INFO: Checking APIGroup: security.internal.openshift.io
    Feb 20 21:58:31.726: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Feb 20 21:58:31.727: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Feb 20 21:58:31.727: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Feb 20 21:58:31.727: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Feb 20 21:58:31.731: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Feb 20 21:58:31.732: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Feb 20 21:58:31.732: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Feb 20 21:58:31.732: INFO: Checking APIGroup: tuned.openshift.io
    Feb 20 21:58:31.735: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Feb 20 21:58:31.735: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Feb 20 21:58:31.735: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Feb 20 21:58:31.735: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Feb 20 21:58:31.739: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Feb 20 21:58:31.739: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.739: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Feb 20 21:58:31.739: INFO: Checking APIGroup: ibm.com
    Feb 20 21:58:31.742: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Feb 20 21:58:31.742: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.742: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Feb 20 21:58:31.742: INFO: Checking APIGroup: migration.k8s.io
    Feb 20 21:58:31.745: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Feb 20 21:58:31.745: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.746: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Feb 20 21:58:31.746: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Feb 20 21:58:31.754: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Feb 20 21:58:31.754: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Feb 20 21:58:31.754: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Feb 20 21:58:31.755: INFO: Checking APIGroup: helm.openshift.io
    Feb 20 21:58:31.805: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Feb 20 21:58:31.805: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Feb 20 21:58:31.805: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Feb 20 21:58:31.806: INFO: Checking APIGroup: metrics.k8s.io
    Feb 20 21:58:31.855: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Feb 20 21:58:31.855: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Feb 20 21:58:31.855: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Feb 20 21:58:31.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-2399" for this suite. 02/20/23 21:58:31.915
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:31.983
Feb 20 21:58:31.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 21:58:31.985
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:32.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:32.089
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 21:58:32.156
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 21:58:32.647
STEP: Deploying the webhook pod 02/20/23 21:58:32.674
STEP: Wait for the deployment to be ready 02/20/23 21:58:32.698
Feb 20 21:58:32.719: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 21:58:34.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 21:58:36.756
STEP: Verifying the service has paired with the endpoint 02/20/23 21:58:36.783
Feb 20 21:58:37.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/20/23 21:58:37.793
STEP: create a pod that should be updated by the webhook 02/20/23 21:58:37.835
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 21:58:37.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6034" for this suite. 02/20/23 21:58:37.952
STEP: Destroying namespace "webhook-6034-markers" for this suite. 02/20/23 21:58:37.972
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":32,"skipped":704,"failed":0}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:31.983
    Feb 20 21:58:31.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 21:58:31.985
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:32.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:32.089
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 21:58:32.156
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 21:58:32.647
    STEP: Deploying the webhook pod 02/20/23 21:58:32.674
    STEP: Wait for the deployment to be ready 02/20/23 21:58:32.698
    Feb 20 21:58:32.719: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 21:58:34.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 21, 58, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 21:58:36.756
    STEP: Verifying the service has paired with the endpoint 02/20/23 21:58:36.783
    Feb 20 21:58:37.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/20/23 21:58:37.793
    STEP: create a pod that should be updated by the webhook 02/20/23 21:58:37.835
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 21:58:37.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6034" for this suite. 02/20/23 21:58:37.952
    STEP: Destroying namespace "webhook-6034-markers" for this suite. 02/20/23 21:58:37.972
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:38.08
Feb 20 21:58:38.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename certificates 02/20/23 21:58:38.081
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:38.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:38.124
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/20/23 21:58:38.803
STEP: getting /apis/certificates.k8s.io 02/20/23 21:58:38.82
STEP: getting /apis/certificates.k8s.io/v1 02/20/23 21:58:38.823
STEP: creating 02/20/23 21:58:38.826
STEP: getting 02/20/23 21:58:38.881
STEP: listing 02/20/23 21:58:38.889
STEP: watching 02/20/23 21:58:38.9
Feb 20 21:58:38.900: INFO: starting watch
STEP: patching 02/20/23 21:58:38.904
STEP: updating 02/20/23 21:58:38.916
Feb 20 21:58:38.926: INFO: waiting for watch events with expected annotations
Feb 20 21:58:38.926: INFO: saw patched and updated annotations
STEP: getting /approval 02/20/23 21:58:38.926
STEP: patching /approval 02/20/23 21:58:38.936
STEP: updating /approval 02/20/23 21:58:38.948
STEP: getting /status 02/20/23 21:58:38.96
STEP: patching /status 02/20/23 21:58:38.969
STEP: updating /status 02/20/23 21:58:38.981
STEP: deleting 02/20/23 21:58:38.992
STEP: deleting a collection 02/20/23 21:58:39.022
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 21:58:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2505" for this suite. 02/20/23 21:58:39.073
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":33,"skipped":712,"failed":0}
------------------------------
• [1.009 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:38.08
    Feb 20 21:58:38.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename certificates 02/20/23 21:58:38.081
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:38.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:38.124
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/20/23 21:58:38.803
    STEP: getting /apis/certificates.k8s.io 02/20/23 21:58:38.82
    STEP: getting /apis/certificates.k8s.io/v1 02/20/23 21:58:38.823
    STEP: creating 02/20/23 21:58:38.826
    STEP: getting 02/20/23 21:58:38.881
    STEP: listing 02/20/23 21:58:38.889
    STEP: watching 02/20/23 21:58:38.9
    Feb 20 21:58:38.900: INFO: starting watch
    STEP: patching 02/20/23 21:58:38.904
    STEP: updating 02/20/23 21:58:38.916
    Feb 20 21:58:38.926: INFO: waiting for watch events with expected annotations
    Feb 20 21:58:38.926: INFO: saw patched and updated annotations
    STEP: getting /approval 02/20/23 21:58:38.926
    STEP: patching /approval 02/20/23 21:58:38.936
    STEP: updating /approval 02/20/23 21:58:38.948
    STEP: getting /status 02/20/23 21:58:38.96
    STEP: patching /status 02/20/23 21:58:38.969
    STEP: updating /status 02/20/23 21:58:38.981
    STEP: deleting 02/20/23 21:58:38.992
    STEP: deleting a collection 02/20/23 21:58:39.022
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 21:58:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-2505" for this suite. 02/20/23 21:58:39.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:39.096
Feb 20 21:58:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename events 02/20/23 21:58:39.097
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:39.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:39.147
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/20/23 21:58:39.155
STEP: listing events in all namespaces 02/20/23 21:58:39.22
STEP: listing events in test namespace 02/20/23 21:58:39.317
STEP: listing events with field selection filtering on source 02/20/23 21:58:39.349
STEP: listing events with field selection filtering on reportingController 02/20/23 21:58:39.358
STEP: getting the test event 02/20/23 21:58:39.37
STEP: patching the test event 02/20/23 21:58:39.405
STEP: getting the test event 02/20/23 21:58:39.436
STEP: updating the test event 02/20/23 21:58:39.448
STEP: getting the test event 02/20/23 21:58:39.471
STEP: deleting the test event 02/20/23 21:58:39.509
STEP: listing events in all namespaces 02/20/23 21:58:39.537
STEP: listing events in test namespace 02/20/23 21:58:39.615
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 20 21:58:39.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2108" for this suite. 02/20/23 21:58:39.663
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":34,"skipped":730,"failed":0}
------------------------------
• [0.590 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:39.096
    Feb 20 21:58:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename events 02/20/23 21:58:39.097
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:39.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:39.147
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/20/23 21:58:39.155
    STEP: listing events in all namespaces 02/20/23 21:58:39.22
    STEP: listing events in test namespace 02/20/23 21:58:39.317
    STEP: listing events with field selection filtering on source 02/20/23 21:58:39.349
    STEP: listing events with field selection filtering on reportingController 02/20/23 21:58:39.358
    STEP: getting the test event 02/20/23 21:58:39.37
    STEP: patching the test event 02/20/23 21:58:39.405
    STEP: getting the test event 02/20/23 21:58:39.436
    STEP: updating the test event 02/20/23 21:58:39.448
    STEP: getting the test event 02/20/23 21:58:39.471
    STEP: deleting the test event 02/20/23 21:58:39.509
    STEP: listing events in all namespaces 02/20/23 21:58:39.537
    STEP: listing events in test namespace 02/20/23 21:58:39.615
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 20 21:58:39.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2108" for this suite. 02/20/23 21:58:39.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:39.687
Feb 20 21:58:39.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 21:58:39.689
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:39.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:39.744
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 02/20/23 21:58:39.828
Feb 20 21:58:39.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 create -f -'
Feb 20 21:58:41.328: INFO: stderr: ""
Feb 20 21:58:41.328: INFO: stdout: "pod/pause created\n"
Feb 20 21:58:41.328: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 20 21:58:41.328: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6744" to be "running and ready"
Feb 20 21:58:41.337: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.634759ms
Feb 20 21:58:41.337: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.8.38.66' to be 'Running' but was 'Pending'
Feb 20 21:58:43.348: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019726003s
Feb 20 21:58:43.348: INFO: Pod "pause" satisfied condition "running and ready"
Feb 20 21:58:43.348: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 02/20/23 21:58:43.349
Feb 20 21:58:43.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 label pods pause testing-label=testing-label-value'
Feb 20 21:58:43.524: INFO: stderr: ""
Feb 20 21:58:43.524: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/20/23 21:58:43.524
Feb 20 21:58:43.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get pod pause -L testing-label'
Feb 20 21:58:43.658: INFO: stderr: ""
Feb 20 21:58:43.659: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/20/23 21:58:43.659
Feb 20 21:58:43.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 label pods pause testing-label-'
Feb 20 21:58:43.820: INFO: stderr: ""
Feb 20 21:58:43.820: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/20/23 21:58:43.82
Feb 20 21:58:43.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get pod pause -L testing-label'
Feb 20 21:58:43.941: INFO: stderr: ""
Feb 20 21:58:43.941: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 02/20/23 21:58:43.941
Feb 20 21:58:43.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 delete --grace-period=0 --force -f -'
Feb 20 21:58:44.127: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 21:58:44.127: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 20 21:58:44.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get rc,svc -l name=pause --no-headers'
Feb 20 21:58:44.269: INFO: stderr: "No resources found in kubectl-6744 namespace.\n"
Feb 20 21:58:44.269: INFO: stdout: ""
Feb 20 21:58:44.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 21:58:44.379: INFO: stderr: ""
Feb 20 21:58:44.379: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 21:58:44.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6744" for this suite. 02/20/23 21:58:44.394
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":35,"skipped":747,"failed":0}
------------------------------
• [4.723 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:39.687
    Feb 20 21:58:39.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 21:58:39.689
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:39.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:39.744
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 02/20/23 21:58:39.828
    Feb 20 21:58:39.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 create -f -'
    Feb 20 21:58:41.328: INFO: stderr: ""
    Feb 20 21:58:41.328: INFO: stdout: "pod/pause created\n"
    Feb 20 21:58:41.328: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 20 21:58:41.328: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6744" to be "running and ready"
    Feb 20 21:58:41.337: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.634759ms
    Feb 20 21:58:41.337: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.8.38.66' to be 'Running' but was 'Pending'
    Feb 20 21:58:43.348: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019726003s
    Feb 20 21:58:43.348: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 20 21:58:43.348: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 02/20/23 21:58:43.349
    Feb 20 21:58:43.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 label pods pause testing-label=testing-label-value'
    Feb 20 21:58:43.524: INFO: stderr: ""
    Feb 20 21:58:43.524: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/20/23 21:58:43.524
    Feb 20 21:58:43.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get pod pause -L testing-label'
    Feb 20 21:58:43.658: INFO: stderr: ""
    Feb 20 21:58:43.659: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/20/23 21:58:43.659
    Feb 20 21:58:43.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 label pods pause testing-label-'
    Feb 20 21:58:43.820: INFO: stderr: ""
    Feb 20 21:58:43.820: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/20/23 21:58:43.82
    Feb 20 21:58:43.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get pod pause -L testing-label'
    Feb 20 21:58:43.941: INFO: stderr: ""
    Feb 20 21:58:43.941: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 02/20/23 21:58:43.941
    Feb 20 21:58:43.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 delete --grace-period=0 --force -f -'
    Feb 20 21:58:44.127: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 21:58:44.127: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 20 21:58:44.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get rc,svc -l name=pause --no-headers'
    Feb 20 21:58:44.269: INFO: stderr: "No resources found in kubectl-6744 namespace.\n"
    Feb 20 21:58:44.269: INFO: stdout: ""
    Feb 20 21:58:44.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6744 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 20 21:58:44.379: INFO: stderr: ""
    Feb 20 21:58:44.379: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 21:58:44.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6744" for this suite. 02/20/23 21:58:44.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:44.414
Feb 20 21:58:44.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 21:58:44.416
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:44.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:44.46
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/20/23 21:58:44.47
W0220 21:58:44.483220      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 02/20/23 21:58:44.483
STEP: delete the deployment 02/20/23 21:58:45.01
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/20/23 21:58:45.024
STEP: Gathering metrics 02/20/23 21:58:45.579
W0220 21:58:45.597568      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 20 21:58:45.597: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 21:58:45.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5663" for this suite. 02/20/23 21:58:45.611
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":36,"skipped":760,"failed":0}
------------------------------
• [1.211 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:44.414
    Feb 20 21:58:44.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 21:58:44.416
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:44.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:44.46
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/20/23 21:58:44.47
    W0220 21:58:44.483220      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 02/20/23 21:58:44.483
    STEP: delete the deployment 02/20/23 21:58:45.01
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/20/23 21:58:45.024
    STEP: Gathering metrics 02/20/23 21:58:45.579
    W0220 21:58:45.597568      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 20 21:58:45.597: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 21:58:45.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5663" for this suite. 02/20/23 21:58:45.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:45.632
Feb 20 21:58:45.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename proxy 02/20/23 21:58:45.635
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:45.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:45.685
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/20/23 21:58:45.718
STEP: creating replication controller proxy-service-qqqk8 in namespace proxy-1857 02/20/23 21:58:45.719
I0220 21:58:45.737131      20 runners.go:193] Created replication controller with name: proxy-service-qqqk8, namespace: proxy-1857, replica count: 1
I0220 21:58:46.791363      20 runners.go:193] proxy-service-qqqk8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 21:58:47.792270      20 runners.go:193] proxy-service-qqqk8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 21:58:48.792454      20 runners.go:193] proxy-service-qqqk8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 21:58:48.800: INFO: setup took 3.104456064s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/20/23 21:58:48.8
Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 31.111038ms)
Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 31.770581ms)
Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 31.753662ms)
Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 32.100337ms)
Feb 20 21:58:48.869: INFO: (0) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 68.194301ms)
Feb 20 21:58:48.869: INFO: (0) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 68.270951ms)
Feb 20 21:58:48.869: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 68.381605ms)
Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 68.710635ms)
Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 68.054085ms)
Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 67.89387ms)
Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 67.980836ms)
Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 68.347505ms)
Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 68.243399ms)
Feb 20 21:58:48.881: INFO: (0) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 80.124031ms)
Feb 20 21:58:48.881: INFO: (0) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 79.290691ms)
Feb 20 21:58:48.881: INFO: (0) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 79.601542ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 30.090936ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 29.935743ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 29.874161ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 30.372432ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 30.203134ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 30.895607ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 30.586631ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 30.009839ms)
Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 30.242791ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 48.643184ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 48.599988ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 49.217062ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 49.205251ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 48.981785ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 49.529282ms)
Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 49.2595ms)
Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.078778ms)
Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.065306ms)
Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.386803ms)
Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 14.220284ms)
Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 14.956606ms)
Feb 20 21:58:48.947: INFO: (2) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.611624ms)
Feb 20 21:58:48.948: INFO: (2) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.872225ms)
Feb 20 21:58:48.948: INFO: (2) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.070998ms)
Feb 20 21:58:48.950: INFO: (2) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 18.156677ms)
Feb 20 21:58:48.950: INFO: (2) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 18.334569ms)
Feb 20 21:58:48.950: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 18.324614ms)
Feb 20 21:58:48.951: INFO: (2) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 19.316159ms)
Feb 20 21:58:48.952: INFO: (2) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 19.849714ms)
Feb 20 21:58:48.952: INFO: (2) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.534127ms)
Feb 20 21:58:48.953: INFO: (2) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.697048ms)
Feb 20 21:58:48.953: INFO: (2) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 20.951978ms)
Feb 20 21:58:48.972: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 17.724271ms)
Feb 20 21:58:48.972: INFO: (3) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 18.233894ms)
Feb 20 21:58:48.972: INFO: (3) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 18.628369ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 20.335482ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 20.274978ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 21.148706ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 19.930411ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 21.15753ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 20.280411ms)
Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 21.342506ms)
Feb 20 21:58:48.976: INFO: (3) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 21.953803ms)
Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 25.313775ms)
Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 25.503584ms)
Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 26.036985ms)
Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 25.517511ms)
Feb 20 21:58:48.980: INFO: (3) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 26.47312ms)
Feb 20 21:58:49.012: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 29.676894ms)
Feb 20 21:58:49.013: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 30.725525ms)
Feb 20 21:58:49.013: INFO: (4) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 29.326581ms)
Feb 20 21:58:49.030: INFO: (4) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 46.580738ms)
Feb 20 21:58:49.033: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 52.134126ms)
Feb 20 21:58:49.033: INFO: (4) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 51.342165ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 57.724587ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 56.657245ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 57.598635ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 54.617444ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 55.707019ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 56.746296ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 55.48286ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 54.879094ms)
Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 56.242425ms)
Feb 20 21:58:49.048: INFO: (4) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 66.9768ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 17.10736ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.976344ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.539283ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.012851ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 16.604884ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.523095ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.681281ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 16.650319ms)
Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.400867ms)
Feb 20 21:58:49.068: INFO: (5) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 18.628969ms)
Feb 20 21:58:49.074: INFO: (5) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 25.016606ms)
Feb 20 21:58:49.075: INFO: (5) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 25.842964ms)
Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 26.549626ms)
Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 26.286523ms)
Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 26.796813ms)
Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 27.227675ms)
Feb 20 21:58:49.088: INFO: (6) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 11.699757ms)
Feb 20 21:58:49.092: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.95922ms)
Feb 20 21:58:49.092: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.316953ms)
Feb 20 21:58:49.092: INFO: (6) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.405063ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 15.373946ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.999756ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 16.281124ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 16.159219ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.543166ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.735185ms)
Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.1601ms)
Feb 20 21:58:49.098: INFO: (6) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 20.813688ms)
Feb 20 21:58:49.098: INFO: (6) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.938617ms)
Feb 20 21:58:49.099: INFO: (6) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 22.558536ms)
Feb 20 21:58:49.099: INFO: (6) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.638226ms)
Feb 20 21:58:49.100: INFO: (6) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 22.5579ms)
Feb 20 21:58:49.111: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 11.303317ms)
Feb 20 21:58:49.114: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.866711ms)
Feb 20 21:58:49.115: INFO: (7) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.098511ms)
Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 15.321189ms)
Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 15.835374ms)
Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.829167ms)
Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 15.98265ms)
Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.975515ms)
Feb 20 21:58:49.117: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.854649ms)
Feb 20 21:58:49.117: INFO: (7) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.05514ms)
Feb 20 21:58:49.123: INFO: (7) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.688404ms)
Feb 20 21:58:49.125: INFO: (7) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 24.348875ms)
Feb 20 21:58:49.125: INFO: (7) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 24.073404ms)
Feb 20 21:58:49.125: INFO: (7) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 24.634919ms)
Feb 20 21:58:49.126: INFO: (7) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 25.047245ms)
Feb 20 21:58:49.126: INFO: (7) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 24.993693ms)
Feb 20 21:58:49.137: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 10.842549ms)
Feb 20 21:58:49.141: INFO: (8) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.170791ms)
Feb 20 21:58:49.141: INFO: (8) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.238322ms)
Feb 20 21:58:49.142: INFO: (8) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 15.525786ms)
Feb 20 21:58:49.142: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.21508ms)
Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.765262ms)
Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.189624ms)
Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.31742ms)
Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 16.639639ms)
Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.004879ms)
Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 23.713927ms)
Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 24.942747ms)
Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 23.971096ms)
Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 24.937702ms)
Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 24.613585ms)
Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 24.941514ms)
Feb 20 21:58:49.163: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 12.087281ms)
Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.093484ms)
Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 12.441249ms)
Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 14.212884ms)
Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.308794ms)
Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 12.848105ms)
Feb 20 21:58:49.166: INFO: (9) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.894255ms)
Feb 20 21:58:49.166: INFO: (9) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 12.795589ms)
Feb 20 21:58:49.167: INFO: (9) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.349038ms)
Feb 20 21:58:49.167: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.414967ms)
Feb 20 21:58:49.169: INFO: (9) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 16.800474ms)
Feb 20 21:58:49.171: INFO: (9) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 17.27733ms)
Feb 20 21:58:49.172: INFO: (9) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 18.914431ms)
Feb 20 21:58:49.173: INFO: (9) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.596761ms)
Feb 20 21:58:49.173: INFO: (9) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 19.693996ms)
Feb 20 21:58:49.174: INFO: (9) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.543121ms)
Feb 20 21:58:49.185: INFO: (10) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 10.962596ms)
Feb 20 21:58:49.187: INFO: (10) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 12.344435ms)
Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 13.236636ms)
Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.419287ms)
Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.65178ms)
Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.07398ms)
Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.92603ms)
Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.313228ms)
Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.692124ms)
Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.429433ms)
Feb 20 21:58:49.190: INFO: (10) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 16.00903ms)
Feb 20 21:58:49.193: INFO: (10) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 18.42574ms)
Feb 20 21:58:49.195: INFO: (10) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.55578ms)
Feb 20 21:58:49.196: INFO: (10) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 21.108331ms)
Feb 20 21:58:49.196: INFO: (10) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.011102ms)
Feb 20 21:58:49.196: INFO: (10) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.985154ms)
Feb 20 21:58:49.207: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 11.175351ms)
Feb 20 21:58:49.209: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.368823ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 13.881443ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.740122ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.460505ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.713291ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 13.968501ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.988246ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.274687ms)
Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.158667ms)
Feb 20 21:58:49.211: INFO: (11) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 15.539967ms)
Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.998799ms)
Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 20.85283ms)
Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.690869ms)
Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 20.531646ms)
Feb 20 21:58:49.223: INFO: (11) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 27.117622ms)
Feb 20 21:58:49.237: INFO: (12) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 12.784913ms)
Feb 20 21:58:49.238: INFO: (12) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.096546ms)
Feb 20 21:58:49.239: INFO: (12) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.70501ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 15.63834ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.867029ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.639203ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 15.730957ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.125223ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.93546ms)
Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.665885ms)
Feb 20 21:58:49.242: INFO: (12) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.071943ms)
Feb 20 21:58:49.245: INFO: (12) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.965097ms)
Feb 20 21:58:49.246: INFO: (12) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 21.995771ms)
Feb 20 21:58:49.246: INFO: (12) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 22.520271ms)
Feb 20 21:58:49.247: INFO: (12) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 22.12232ms)
Feb 20 21:58:49.248: INFO: (12) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 24.416999ms)
Feb 20 21:58:49.261: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 12.680796ms)
Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.178076ms)
Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.887939ms)
Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 15.513673ms)
Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 15.693263ms)
Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.564871ms)
Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 17.706394ms)
Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 16.904202ms)
Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 18.049325ms)
Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 17.635337ms)
Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 17.935298ms)
Feb 20 21:58:49.268: INFO: (13) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.328401ms)
Feb 20 21:58:49.269: INFO: (13) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 19.030272ms)
Feb 20 21:58:49.270: INFO: (13) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 19.602207ms)
Feb 20 21:58:49.271: INFO: (13) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 19.891136ms)
Feb 20 21:58:49.271: INFO: (13) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 19.852233ms)
Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.347662ms)
Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 12.882463ms)
Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 13.757552ms)
Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.250759ms)
Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.139324ms)
Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.681234ms)
Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 16.053657ms)
Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 17.394055ms)
Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 16.462781ms)
Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.251823ms)
Feb 20 21:58:49.289: INFO: (14) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 17.31711ms)
Feb 20 21:58:49.290: INFO: (14) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.616026ms)
Feb 20 21:58:49.292: INFO: (14) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 20.039645ms)
Feb 20 21:58:49.293: INFO: (14) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.099106ms)
Feb 20 21:58:49.293: INFO: (14) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.721645ms)
Feb 20 21:58:49.293: INFO: (14) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 19.828578ms)
Feb 20 21:58:49.306: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.118311ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.924293ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 13.74213ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.038693ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.625526ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.310162ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.90447ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.435735ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.609082ms)
Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.88656ms)
Feb 20 21:58:49.311: INFO: (15) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.042143ms)
Feb 20 21:58:49.312: INFO: (15) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 18.003775ms)
Feb 20 21:58:49.314: INFO: (15) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 20.425396ms)
Feb 20 21:58:49.315: INFO: (15) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.235687ms)
Feb 20 21:58:49.315: INFO: (15) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 20.709475ms)
Feb 20 21:58:49.315: INFO: (15) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.65479ms)
Feb 20 21:58:49.326: INFO: (16) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 11.407214ms)
Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.706163ms)
Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.411726ms)
Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 13.833112ms)
Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.859199ms)
Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 14.478226ms)
Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.746157ms)
Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 13.695686ms)
Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.614428ms)
Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.048306ms)
Feb 20 21:58:49.331: INFO: (16) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 16.246864ms)
Feb 20 21:58:49.336: INFO: (16) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.358493ms)
Feb 20 21:58:49.338: INFO: (16) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.469455ms)
Feb 20 21:58:49.338: INFO: (16) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 22.869823ms)
Feb 20 21:58:49.338: INFO: (16) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 22.789078ms)
Feb 20 21:58:49.352: INFO: (16) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 36.51232ms)
Feb 20 21:58:49.367: INFO: (17) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.868849ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.398656ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.572501ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 15.134558ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.911446ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 14.735497ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.580668ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.269896ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 14.505003ms)
Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.912054ms)
Feb 20 21:58:49.369: INFO: (17) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 16.26008ms)
Feb 20 21:58:49.372: INFO: (17) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 18.649999ms)
Feb 20 21:58:49.373: INFO: (17) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 19.147377ms)
Feb 20 21:58:49.373: INFO: (17) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 19.108782ms)
Feb 20 21:58:49.373: INFO: (17) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 19.144507ms)
Feb 20 21:58:49.383: INFO: (17) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 29.317047ms)
Feb 20 21:58:49.400: INFO: (18) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 17.175265ms)
Feb 20 21:58:49.400: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 17.238059ms)
Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 17.568938ms)
Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.409662ms)
Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.013783ms)
Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 17.59642ms)
Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 17.582647ms)
Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 17.654887ms)
Feb 20 21:58:49.402: INFO: (18) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 18.5076ms)
Feb 20 21:58:49.402: INFO: (18) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 17.904552ms)
Feb 20 21:58:49.405: INFO: (18) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.487136ms)
Feb 20 21:58:49.406: INFO: (18) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 21.96777ms)
Feb 20 21:58:49.406: INFO: (18) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.129607ms)
Feb 20 21:58:49.408: INFO: (18) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 23.913556ms)
Feb 20 21:58:49.409: INFO: (18) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 24.911839ms)
Feb 20 21:58:49.409: INFO: (18) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 25.346153ms)
Feb 20 21:58:49.421: INFO: (19) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 12.06268ms)
Feb 20 21:58:49.423: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.607598ms)
Feb 20 21:58:49.424: INFO: (19) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 13.939756ms)
Feb 20 21:58:49.424: INFO: (19) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.162327ms)
Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 16.916163ms)
Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 16.535065ms)
Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.537575ms)
Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.57921ms)
Feb 20 21:58:49.427: INFO: (19) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 17.18608ms)
Feb 20 21:58:49.427: INFO: (19) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 17.54299ms)
Feb 20 21:58:49.427: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.459922ms)
Feb 20 21:58:49.430: INFO: (19) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.978999ms)
Feb 20 21:58:49.430: INFO: (19) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.880919ms)
Feb 20 21:58:49.431: INFO: (19) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.805516ms)
Feb 20 21:58:49.431: INFO: (19) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.260508ms)
Feb 20 21:58:49.431: INFO: (19) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 21.693867ms)
STEP: deleting ReplicationController proxy-service-qqqk8 in namespace proxy-1857, will wait for the garbage collector to delete the pods 02/20/23 21:58:49.432
Feb 20 21:58:49.504: INFO: Deleting ReplicationController proxy-service-qqqk8 took: 13.364135ms
Feb 20 21:58:49.605: INFO: Terminating ReplicationController proxy-service-qqqk8 pods took: 100.742027ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 20 21:58:52.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1857" for this suite. 02/20/23 21:58:52.12
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":37,"skipped":775,"failed":0}
------------------------------
• [SLOW TEST] [6.506 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:45.632
    Feb 20 21:58:45.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename proxy 02/20/23 21:58:45.635
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:45.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:45.685
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/20/23 21:58:45.718
    STEP: creating replication controller proxy-service-qqqk8 in namespace proxy-1857 02/20/23 21:58:45.719
    I0220 21:58:45.737131      20 runners.go:193] Created replication controller with name: proxy-service-qqqk8, namespace: proxy-1857, replica count: 1
    I0220 21:58:46.791363      20 runners.go:193] proxy-service-qqqk8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0220 21:58:47.792270      20 runners.go:193] proxy-service-qqqk8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0220 21:58:48.792454      20 runners.go:193] proxy-service-qqqk8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 21:58:48.800: INFO: setup took 3.104456064s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/20/23 21:58:48.8
    Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 31.111038ms)
    Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 31.770581ms)
    Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 31.753662ms)
    Feb 20 21:58:48.833: INFO: (0) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 32.100337ms)
    Feb 20 21:58:48.869: INFO: (0) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 68.194301ms)
    Feb 20 21:58:48.869: INFO: (0) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 68.270951ms)
    Feb 20 21:58:48.869: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 68.381605ms)
    Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 68.710635ms)
    Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 68.054085ms)
    Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 67.89387ms)
    Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 67.980836ms)
    Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 68.347505ms)
    Feb 20 21:58:48.870: INFO: (0) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 68.243399ms)
    Feb 20 21:58:48.881: INFO: (0) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 80.124031ms)
    Feb 20 21:58:48.881: INFO: (0) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 79.290691ms)
    Feb 20 21:58:48.881: INFO: (0) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 79.601542ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 30.090936ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 29.935743ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 29.874161ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 30.372432ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 30.203134ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 30.895607ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 30.586631ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 30.009839ms)
    Feb 20 21:58:48.912: INFO: (1) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 30.242791ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 48.643184ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 48.599988ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 49.217062ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 49.205251ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 48.981785ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 49.529282ms)
    Feb 20 21:58:48.931: INFO: (1) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 49.2595ms)
    Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.078778ms)
    Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.065306ms)
    Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.386803ms)
    Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 14.220284ms)
    Feb 20 21:58:48.946: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 14.956606ms)
    Feb 20 21:58:48.947: INFO: (2) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.611624ms)
    Feb 20 21:58:48.948: INFO: (2) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.872225ms)
    Feb 20 21:58:48.948: INFO: (2) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.070998ms)
    Feb 20 21:58:48.950: INFO: (2) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 18.156677ms)
    Feb 20 21:58:48.950: INFO: (2) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 18.334569ms)
    Feb 20 21:58:48.950: INFO: (2) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 18.324614ms)
    Feb 20 21:58:48.951: INFO: (2) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 19.316159ms)
    Feb 20 21:58:48.952: INFO: (2) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 19.849714ms)
    Feb 20 21:58:48.952: INFO: (2) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.534127ms)
    Feb 20 21:58:48.953: INFO: (2) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.697048ms)
    Feb 20 21:58:48.953: INFO: (2) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 20.951978ms)
    Feb 20 21:58:48.972: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 17.724271ms)
    Feb 20 21:58:48.972: INFO: (3) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 18.233894ms)
    Feb 20 21:58:48.972: INFO: (3) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 18.628369ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 20.335482ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 20.274978ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 21.148706ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 19.930411ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 21.15753ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 20.280411ms)
    Feb 20 21:58:48.974: INFO: (3) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 21.342506ms)
    Feb 20 21:58:48.976: INFO: (3) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 21.953803ms)
    Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 25.313775ms)
    Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 25.503584ms)
    Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 26.036985ms)
    Feb 20 21:58:48.979: INFO: (3) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 25.517511ms)
    Feb 20 21:58:48.980: INFO: (3) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 26.47312ms)
    Feb 20 21:58:49.012: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 29.676894ms)
    Feb 20 21:58:49.013: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 30.725525ms)
    Feb 20 21:58:49.013: INFO: (4) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 29.326581ms)
    Feb 20 21:58:49.030: INFO: (4) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 46.580738ms)
    Feb 20 21:58:49.033: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 52.134126ms)
    Feb 20 21:58:49.033: INFO: (4) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 51.342165ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 57.724587ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 56.657245ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 57.598635ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 54.617444ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 55.707019ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 56.746296ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 55.48286ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 54.879094ms)
    Feb 20 21:58:49.038: INFO: (4) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 56.242425ms)
    Feb 20 21:58:49.048: INFO: (4) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 66.9768ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 17.10736ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.976344ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.539283ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.012851ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 16.604884ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.523095ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.681281ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 16.650319ms)
    Feb 20 21:58:49.065: INFO: (5) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.400867ms)
    Feb 20 21:58:49.068: INFO: (5) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 18.628969ms)
    Feb 20 21:58:49.074: INFO: (5) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 25.016606ms)
    Feb 20 21:58:49.075: INFO: (5) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 25.842964ms)
    Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 26.549626ms)
    Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 26.286523ms)
    Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 26.796813ms)
    Feb 20 21:58:49.076: INFO: (5) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 27.227675ms)
    Feb 20 21:58:49.088: INFO: (6) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 11.699757ms)
    Feb 20 21:58:49.092: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.95922ms)
    Feb 20 21:58:49.092: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.316953ms)
    Feb 20 21:58:49.092: INFO: (6) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.405063ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 15.373946ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.999756ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 16.281124ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 16.159219ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.543166ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.735185ms)
    Feb 20 21:58:49.093: INFO: (6) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.1601ms)
    Feb 20 21:58:49.098: INFO: (6) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 20.813688ms)
    Feb 20 21:58:49.098: INFO: (6) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.938617ms)
    Feb 20 21:58:49.099: INFO: (6) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 22.558536ms)
    Feb 20 21:58:49.099: INFO: (6) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.638226ms)
    Feb 20 21:58:49.100: INFO: (6) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 22.5579ms)
    Feb 20 21:58:49.111: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 11.303317ms)
    Feb 20 21:58:49.114: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.866711ms)
    Feb 20 21:58:49.115: INFO: (7) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.098511ms)
    Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 15.321189ms)
    Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 15.835374ms)
    Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.829167ms)
    Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 15.98265ms)
    Feb 20 21:58:49.116: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.975515ms)
    Feb 20 21:58:49.117: INFO: (7) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.854649ms)
    Feb 20 21:58:49.117: INFO: (7) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.05514ms)
    Feb 20 21:58:49.123: INFO: (7) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.688404ms)
    Feb 20 21:58:49.125: INFO: (7) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 24.348875ms)
    Feb 20 21:58:49.125: INFO: (7) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 24.073404ms)
    Feb 20 21:58:49.125: INFO: (7) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 24.634919ms)
    Feb 20 21:58:49.126: INFO: (7) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 25.047245ms)
    Feb 20 21:58:49.126: INFO: (7) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 24.993693ms)
    Feb 20 21:58:49.137: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 10.842549ms)
    Feb 20 21:58:49.141: INFO: (8) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.170791ms)
    Feb 20 21:58:49.141: INFO: (8) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.238322ms)
    Feb 20 21:58:49.142: INFO: (8) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 15.525786ms)
    Feb 20 21:58:49.142: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.21508ms)
    Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.765262ms)
    Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 16.189624ms)
    Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.31742ms)
    Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 16.639639ms)
    Feb 20 21:58:49.143: INFO: (8) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.004879ms)
    Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 23.713927ms)
    Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 24.942747ms)
    Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 23.971096ms)
    Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 24.937702ms)
    Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 24.613585ms)
    Feb 20 21:58:49.151: INFO: (8) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 24.941514ms)
    Feb 20 21:58:49.163: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 12.087281ms)
    Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.093484ms)
    Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 12.441249ms)
    Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 14.212884ms)
    Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.308794ms)
    Feb 20 21:58:49.165: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 12.848105ms)
    Feb 20 21:58:49.166: INFO: (9) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.894255ms)
    Feb 20 21:58:49.166: INFO: (9) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 12.795589ms)
    Feb 20 21:58:49.167: INFO: (9) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.349038ms)
    Feb 20 21:58:49.167: INFO: (9) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.414967ms)
    Feb 20 21:58:49.169: INFO: (9) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 16.800474ms)
    Feb 20 21:58:49.171: INFO: (9) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 17.27733ms)
    Feb 20 21:58:49.172: INFO: (9) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 18.914431ms)
    Feb 20 21:58:49.173: INFO: (9) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.596761ms)
    Feb 20 21:58:49.173: INFO: (9) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 19.693996ms)
    Feb 20 21:58:49.174: INFO: (9) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.543121ms)
    Feb 20 21:58:49.185: INFO: (10) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 10.962596ms)
    Feb 20 21:58:49.187: INFO: (10) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 12.344435ms)
    Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 13.236636ms)
    Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.419287ms)
    Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.65178ms)
    Feb 20 21:58:49.188: INFO: (10) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.07398ms)
    Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.92603ms)
    Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.313228ms)
    Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.692124ms)
    Feb 20 21:58:49.189: INFO: (10) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.429433ms)
    Feb 20 21:58:49.190: INFO: (10) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 16.00903ms)
    Feb 20 21:58:49.193: INFO: (10) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 18.42574ms)
    Feb 20 21:58:49.195: INFO: (10) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.55578ms)
    Feb 20 21:58:49.196: INFO: (10) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 21.108331ms)
    Feb 20 21:58:49.196: INFO: (10) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.011102ms)
    Feb 20 21:58:49.196: INFO: (10) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.985154ms)
    Feb 20 21:58:49.207: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 11.175351ms)
    Feb 20 21:58:49.209: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.368823ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 13.881443ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.740122ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.460505ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.713291ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 13.968501ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.988246ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.274687ms)
    Feb 20 21:58:49.210: INFO: (11) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.158667ms)
    Feb 20 21:58:49.211: INFO: (11) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 15.539967ms)
    Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.998799ms)
    Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 20.85283ms)
    Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.690869ms)
    Feb 20 21:58:49.217: INFO: (11) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 20.531646ms)
    Feb 20 21:58:49.223: INFO: (11) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 27.117622ms)
    Feb 20 21:58:49.237: INFO: (12) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 12.784913ms)
    Feb 20 21:58:49.238: INFO: (12) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.096546ms)
    Feb 20 21:58:49.239: INFO: (12) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.70501ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 15.63834ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.867029ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.639203ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 15.730957ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.125223ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.93546ms)
    Feb 20 21:58:49.240: INFO: (12) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.665885ms)
    Feb 20 21:58:49.242: INFO: (12) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.071943ms)
    Feb 20 21:58:49.245: INFO: (12) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.965097ms)
    Feb 20 21:58:49.246: INFO: (12) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 21.995771ms)
    Feb 20 21:58:49.246: INFO: (12) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 22.520271ms)
    Feb 20 21:58:49.247: INFO: (12) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 22.12232ms)
    Feb 20 21:58:49.248: INFO: (12) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 24.416999ms)
    Feb 20 21:58:49.261: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 12.680796ms)
    Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 15.178076ms)
    Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 15.887939ms)
    Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 15.513673ms)
    Feb 20 21:58:49.266: INFO: (13) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 15.693263ms)
    Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 15.564871ms)
    Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 17.706394ms)
    Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 16.904202ms)
    Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 18.049325ms)
    Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 17.635337ms)
    Feb 20 21:58:49.267: INFO: (13) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 17.935298ms)
    Feb 20 21:58:49.268: INFO: (13) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.328401ms)
    Feb 20 21:58:49.269: INFO: (13) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 19.030272ms)
    Feb 20 21:58:49.270: INFO: (13) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 19.602207ms)
    Feb 20 21:58:49.271: INFO: (13) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 19.891136ms)
    Feb 20 21:58:49.271: INFO: (13) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 19.852233ms)
    Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.347662ms)
    Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 12.882463ms)
    Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 13.757552ms)
    Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.250759ms)
    Feb 20 21:58:49.286: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.139324ms)
    Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.681234ms)
    Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 16.053657ms)
    Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 17.394055ms)
    Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 16.462781ms)
    Feb 20 21:58:49.288: INFO: (14) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 15.251823ms)
    Feb 20 21:58:49.289: INFO: (14) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 17.31711ms)
    Feb 20 21:58:49.290: INFO: (14) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.616026ms)
    Feb 20 21:58:49.292: INFO: (14) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 20.039645ms)
    Feb 20 21:58:49.293: INFO: (14) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.099106ms)
    Feb 20 21:58:49.293: INFO: (14) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.721645ms)
    Feb 20 21:58:49.293: INFO: (14) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 19.828578ms)
    Feb 20 21:58:49.306: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 13.118311ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.924293ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 13.74213ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.038693ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.625526ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.310162ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.90447ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.435735ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.609082ms)
    Feb 20 21:58:49.308: INFO: (15) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.88656ms)
    Feb 20 21:58:49.311: INFO: (15) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 18.042143ms)
    Feb 20 21:58:49.312: INFO: (15) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 18.003775ms)
    Feb 20 21:58:49.314: INFO: (15) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 20.425396ms)
    Feb 20 21:58:49.315: INFO: (15) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.235687ms)
    Feb 20 21:58:49.315: INFO: (15) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 20.709475ms)
    Feb 20 21:58:49.315: INFO: (15) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.65479ms)
    Feb 20 21:58:49.326: INFO: (16) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 11.407214ms)
    Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 13.706163ms)
    Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.411726ms)
    Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 13.833112ms)
    Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.859199ms)
    Feb 20 21:58:49.329: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 14.478226ms)
    Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.746157ms)
    Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 13.695686ms)
    Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 13.614428ms)
    Feb 20 21:58:49.330: INFO: (16) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.048306ms)
    Feb 20 21:58:49.331: INFO: (16) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 16.246864ms)
    Feb 20 21:58:49.336: INFO: (16) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 20.358493ms)
    Feb 20 21:58:49.338: INFO: (16) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.469455ms)
    Feb 20 21:58:49.338: INFO: (16) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 22.869823ms)
    Feb 20 21:58:49.338: INFO: (16) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 22.789078ms)
    Feb 20 21:58:49.352: INFO: (16) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 36.51232ms)
    Feb 20 21:58:49.367: INFO: (17) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 14.868849ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.398656ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.572501ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 15.134558ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.911446ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 14.735497ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 14.580668ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 14.269896ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 14.505003ms)
    Feb 20 21:58:49.368: INFO: (17) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 14.912054ms)
    Feb 20 21:58:49.369: INFO: (17) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 16.26008ms)
    Feb 20 21:58:49.372: INFO: (17) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 18.649999ms)
    Feb 20 21:58:49.373: INFO: (17) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 19.147377ms)
    Feb 20 21:58:49.373: INFO: (17) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 19.108782ms)
    Feb 20 21:58:49.373: INFO: (17) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 19.144507ms)
    Feb 20 21:58:49.383: INFO: (17) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 29.317047ms)
    Feb 20 21:58:49.400: INFO: (18) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 17.175265ms)
    Feb 20 21:58:49.400: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 17.238059ms)
    Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 17.568938ms)
    Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.409662ms)
    Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.013783ms)
    Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 17.59642ms)
    Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 17.582647ms)
    Feb 20 21:58:49.401: INFO: (18) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 17.654887ms)
    Feb 20 21:58:49.402: INFO: (18) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 18.5076ms)
    Feb 20 21:58:49.402: INFO: (18) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 17.904552ms)
    Feb 20 21:58:49.405: INFO: (18) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.487136ms)
    Feb 20 21:58:49.406: INFO: (18) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 21.96777ms)
    Feb 20 21:58:49.406: INFO: (18) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.129607ms)
    Feb 20 21:58:49.408: INFO: (18) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 23.913556ms)
    Feb 20 21:58:49.409: INFO: (18) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 24.911839ms)
    Feb 20 21:58:49.409: INFO: (18) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 25.346153ms)
    Feb 20 21:58:49.421: INFO: (19) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 12.06268ms)
    Feb 20 21:58:49.423: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 13.607598ms)
    Feb 20 21:58:49.424: INFO: (19) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">... (200; 13.939756ms)
    Feb 20 21:58:49.424: INFO: (19) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:460/proxy/: tls baz (200; 14.162327ms)
    Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:1080/proxy/rewriteme">test<... (200; 16.916163ms)
    Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq/proxy/rewriteme">test</a> (200; 16.535065ms)
    Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/http:proxy-service-qqqk8-b2zxq:162/proxy/: bar (200; 16.537575ms)
    Feb 20 21:58:49.426: INFO: (19) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:462/proxy/: tls qux (200; 16.57921ms)
    Feb 20 21:58:49.427: INFO: (19) /api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/: <a href="/api/v1/namespaces/proxy-1857/pods/https:proxy-service-qqqk8-b2zxq:443/proxy/tlsrewritem... (200; 17.18608ms)
    Feb 20 21:58:49.427: INFO: (19) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname2/proxy/: tls qux (200; 17.54299ms)
    Feb 20 21:58:49.427: INFO: (19) /api/v1/namespaces/proxy-1857/pods/proxy-service-qqqk8-b2zxq:160/proxy/: foo (200; 17.459922ms)
    Feb 20 21:58:49.430: INFO: (19) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname1/proxy/: foo (200; 20.978999ms)
    Feb 20 21:58:49.430: INFO: (19) /api/v1/namespaces/proxy-1857/services/https:proxy-service-qqqk8:tlsportname1/proxy/: tls baz (200; 20.880919ms)
    Feb 20 21:58:49.431: INFO: (19) /api/v1/namespaces/proxy-1857/services/proxy-service-qqqk8:portname2/proxy/: bar (200; 21.805516ms)
    Feb 20 21:58:49.431: INFO: (19) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname2/proxy/: bar (200; 22.260508ms)
    Feb 20 21:58:49.431: INFO: (19) /api/v1/namespaces/proxy-1857/services/http:proxy-service-qqqk8:portname1/proxy/: foo (200; 21.693867ms)
    STEP: deleting ReplicationController proxy-service-qqqk8 in namespace proxy-1857, will wait for the garbage collector to delete the pods 02/20/23 21:58:49.432
    Feb 20 21:58:49.504: INFO: Deleting ReplicationController proxy-service-qqqk8 took: 13.364135ms
    Feb 20 21:58:49.605: INFO: Terminating ReplicationController proxy-service-qqqk8 pods took: 100.742027ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 20 21:58:52.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1857" for this suite. 02/20/23 21:58:52.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:58:52.139
Feb 20 21:58:52.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replicaset 02/20/23 21:58:52.14
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:52.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:52.226
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 20 21:58:52.265: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 20 21:58:57.277: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/20/23 21:58:57.278
STEP: Scaling up "test-rs" replicaset  02/20/23 21:58:57.278
Feb 20 21:58:57.300: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/20/23 21:58:57.3
W0220 21:58:57.329157      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 20 21:58:57.335: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
Feb 20 21:58:57.444: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
Feb 20 21:58:57.522: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
Feb 20 21:58:57.542: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
Feb 20 21:59:05.012: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 2, AvailableReplicas 2
Feb 20 21:59:07.846: INFO: observed Replicaset test-rs in namespace replicaset-6383 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 20 21:59:07.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6383" for this suite. 02/20/23 21:59:07.859
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":38,"skipped":780,"failed":0}
------------------------------
• [SLOW TEST] [15.734 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:58:52.139
    Feb 20 21:58:52.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replicaset 02/20/23 21:58:52.14
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:58:52.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:58:52.226
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 20 21:58:52.265: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 20 21:58:57.277: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/20/23 21:58:57.278
    STEP: Scaling up "test-rs" replicaset  02/20/23 21:58:57.278
    Feb 20 21:58:57.300: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/20/23 21:58:57.3
    W0220 21:58:57.329157      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 20 21:58:57.335: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
    Feb 20 21:58:57.444: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
    Feb 20 21:58:57.522: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
    Feb 20 21:58:57.542: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 1, AvailableReplicas 1
    Feb 20 21:59:05.012: INFO: observed ReplicaSet test-rs in namespace replicaset-6383 with ReadyReplicas 2, AvailableReplicas 2
    Feb 20 21:59:07.846: INFO: observed Replicaset test-rs in namespace replicaset-6383 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 20 21:59:07.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6383" for this suite. 02/20/23 21:59:07.859
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:59:07.878
Feb 20 21:59:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 21:59:07.881
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:07.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:07.971
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 02/20/23 21:59:07.985
STEP: fetching the ConfigMap 02/20/23 21:59:08.005
STEP: patching the ConfigMap 02/20/23 21:59:08.016
STEP: listing all ConfigMaps in all namespaces with a label selector 02/20/23 21:59:08.031
STEP: deleting the ConfigMap by collection with a label selector 02/20/23 21:59:08.132
STEP: listing all ConfigMaps in test namespace 02/20/23 21:59:08.161
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 21:59:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4029" for this suite. 02/20/23 21:59:08.188
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":39,"skipped":782,"failed":0}
------------------------------
• [0.326 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:59:07.878
    Feb 20 21:59:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 21:59:07.881
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:07.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:07.971
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 02/20/23 21:59:07.985
    STEP: fetching the ConfigMap 02/20/23 21:59:08.005
    STEP: patching the ConfigMap 02/20/23 21:59:08.016
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/20/23 21:59:08.031
    STEP: deleting the ConfigMap by collection with a label selector 02/20/23 21:59:08.132
    STEP: listing all ConfigMaps in test namespace 02/20/23 21:59:08.161
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 21:59:08.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4029" for this suite. 02/20/23 21:59:08.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:59:08.214
Feb 20 21:59:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 21:59:08.226
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:08.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:08.296
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/20/23 21:59:08.305
Feb 20 21:59:08.340: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8776  33239db5-a8e8-45c5-a949-80db1826ff2d 70260 0 2023-02-20 21:59:08 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-02-20 21:59:08 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v24zj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v24zj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 21:59:08.341: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8776" to be "running and ready"
Feb 20 21:59:08.350: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.399367ms
Feb 20 21:59:08.350: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:59:10.384: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.043155266s
Feb 20 21:59:10.384: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 20 21:59:10.384: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/20/23 21:59:10.384
Feb 20 21:59:10.385: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 21:59:10.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 21:59:10.386: INFO: ExecWithOptions: Clientset creation
Feb 20 21:59:10.386: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/20/23 21:59:10.816
Feb 20 21:59:10.816: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 21:59:10.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 21:59:10.817: INFO: ExecWithOptions: Clientset creation
Feb 20 21:59:10.817: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 21:59:11.054: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 21:59:11.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8776" for this suite. 02/20/23 21:59:11.089
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":40,"skipped":824,"failed":0}
------------------------------
• [2.891 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:59:08.214
    Feb 20 21:59:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 21:59:08.226
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:08.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:08.296
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/20/23 21:59:08.305
    Feb 20 21:59:08.340: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8776  33239db5-a8e8-45c5-a949-80db1826ff2d 70260 0 2023-02-20 21:59:08 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-02-20 21:59:08 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v24zj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v24zj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 21:59:08.341: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8776" to be "running and ready"
    Feb 20 21:59:08.350: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.399367ms
    Feb 20 21:59:08.350: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:59:10.384: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.043155266s
    Feb 20 21:59:10.384: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 20 21:59:10.384: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/20/23 21:59:10.384
    Feb 20 21:59:10.385: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 21:59:10.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 21:59:10.386: INFO: ExecWithOptions: Clientset creation
    Feb 20 21:59:10.386: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/20/23 21:59:10.816
    Feb 20 21:59:10.816: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8776 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 21:59:10.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 21:59:10.817: INFO: ExecWithOptions: Clientset creation
    Feb 20 21:59:10.817: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-8776/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 21:59:11.054: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 21:59:11.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8776" for this suite. 02/20/23 21:59:11.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:59:11.111
Feb 20 21:59:11.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 21:59:11.113
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:11.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:11.21
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 02/20/23 21:59:11.22
Feb 20 21:59:11.260: INFO: Waiting up to 5m0s for pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b" in namespace "var-expansion-1897" to be "Succeeded or Failed"
Feb 20 21:59:11.270: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.918833ms
Feb 20 21:59:13.280: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019866004s
Feb 20 21:59:15.280: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019498661s
STEP: Saw pod success 02/20/23 21:59:15.28
Feb 20 21:59:15.280: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b" satisfied condition "Succeeded or Failed"
Feb 20 21:59:15.289: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b container dapi-container: <nil>
STEP: delete the pod 02/20/23 21:59:15.331
Feb 20 21:59:15.355: INFO: Waiting for pod var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b to disappear
Feb 20 21:59:15.363: INFO: Pod var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 21:59:15.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1897" for this suite. 02/20/23 21:59:15.376
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":41,"skipped":836,"failed":0}
------------------------------
• [4.279 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:59:11.111
    Feb 20 21:59:11.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 21:59:11.113
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:11.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:11.21
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 02/20/23 21:59:11.22
    Feb 20 21:59:11.260: INFO: Waiting up to 5m0s for pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b" in namespace "var-expansion-1897" to be "Succeeded or Failed"
    Feb 20 21:59:11.270: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.918833ms
    Feb 20 21:59:13.280: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019866004s
    Feb 20 21:59:15.280: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019498661s
    STEP: Saw pod success 02/20/23 21:59:15.28
    Feb 20 21:59:15.280: INFO: Pod "var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b" satisfied condition "Succeeded or Failed"
    Feb 20 21:59:15.289: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b container dapi-container: <nil>
    STEP: delete the pod 02/20/23 21:59:15.331
    Feb 20 21:59:15.355: INFO: Waiting for pod var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b to disappear
    Feb 20 21:59:15.363: INFO: Pod var-expansion-c4b95b23-eb93-46b9-88ab-4de1c462064b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 21:59:15.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1897" for this suite. 02/20/23 21:59:15.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 21:59:15.392
Feb 20 21:59:15.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 21:59:15.395
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:15.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:15.478
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
Feb 20 21:59:15.503: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-689b88ea-fdaf-4c91-9645-5c01bdbfa646 02/20/23 21:59:15.503
STEP: Creating secret with name s-test-opt-upd-e7580b45-e898-47da-a1a8-d1dd96ed8b46 02/20/23 21:59:15.539
STEP: Creating the pod 02/20/23 21:59:15.556
Feb 20 21:59:15.595: INFO: Waiting up to 5m0s for pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650" in namespace "secrets-9840" to be "running and ready"
Feb 20 21:59:15.605: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650": Phase="Pending", Reason="", readiness=false. Elapsed: 9.702588ms
Feb 20 21:59:15.605: INFO: The phase of Pod pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:59:17.615: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019594993s
Feb 20 21:59:17.615: INFO: The phase of Pod pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 21:59:19.614: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650": Phase="Running", Reason="", readiness=true. Elapsed: 4.018921891s
Feb 20 21:59:19.614: INFO: The phase of Pod pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650 is Running (Ready = true)
Feb 20 21:59:19.614: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-689b88ea-fdaf-4c91-9645-5c01bdbfa646 02/20/23 21:59:19.675
STEP: Updating secret s-test-opt-upd-e7580b45-e898-47da-a1a8-d1dd96ed8b46 02/20/23 21:59:19.688
STEP: Creating secret with name s-test-opt-create-6a8ae1b5-6695-45a9-bbd1-47e031541bc2 02/20/23 21:59:19.698
STEP: waiting to observe update in volume 02/20/23 21:59:19.709
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:00:28.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9840" for this suite. 02/20/23 22:00:28.611
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":42,"skipped":849,"failed":0}
------------------------------
• [SLOW TEST] [73.233 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 21:59:15.392
    Feb 20 21:59:15.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 21:59:15.395
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 21:59:15.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 21:59:15.478
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    Feb 20 21:59:15.503: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-689b88ea-fdaf-4c91-9645-5c01bdbfa646 02/20/23 21:59:15.503
    STEP: Creating secret with name s-test-opt-upd-e7580b45-e898-47da-a1a8-d1dd96ed8b46 02/20/23 21:59:15.539
    STEP: Creating the pod 02/20/23 21:59:15.556
    Feb 20 21:59:15.595: INFO: Waiting up to 5m0s for pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650" in namespace "secrets-9840" to be "running and ready"
    Feb 20 21:59:15.605: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650": Phase="Pending", Reason="", readiness=false. Elapsed: 9.702588ms
    Feb 20 21:59:15.605: INFO: The phase of Pod pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:59:17.615: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019594993s
    Feb 20 21:59:17.615: INFO: The phase of Pod pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 21:59:19.614: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650": Phase="Running", Reason="", readiness=true. Elapsed: 4.018921891s
    Feb 20 21:59:19.614: INFO: The phase of Pod pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650 is Running (Ready = true)
    Feb 20 21:59:19.614: INFO: Pod "pod-secrets-14c17746-ff82-40ce-be16-6ad474c92650" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-689b88ea-fdaf-4c91-9645-5c01bdbfa646 02/20/23 21:59:19.675
    STEP: Updating secret s-test-opt-upd-e7580b45-e898-47da-a1a8-d1dd96ed8b46 02/20/23 21:59:19.688
    STEP: Creating secret with name s-test-opt-create-6a8ae1b5-6695-45a9-bbd1-47e031541bc2 02/20/23 21:59:19.698
    STEP: waiting to observe update in volume 02/20/23 21:59:19.709
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:00:28.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9840" for this suite. 02/20/23 22:00:28.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:28.637
Feb 20 22:00:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:00:28.639
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:28.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:28.695
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-1d96e632-b032-4316-b994-4a2a96c74598 02/20/23 22:00:28.791
STEP: Creating a pod to test consume secrets 02/20/23 22:00:28.808
Feb 20 22:00:28.860: INFO: Waiting up to 5m0s for pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14" in namespace "secrets-2940" to be "Succeeded or Failed"
Feb 20 22:00:28.871: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.760379ms
Feb 20 22:00:30.881: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02106591s
Feb 20 22:00:32.882: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02113791s
STEP: Saw pod success 02/20/23 22:00:32.882
Feb 20 22:00:32.882: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14" satisfied condition "Succeeded or Failed"
Feb 20 22:00:32.920: INFO: Trying to get logs from node 10.8.38.70 pod pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:00:32.961
Feb 20 22:00:32.985: INFO: Waiting for pod pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14 to disappear
Feb 20 22:00:32.993: INFO: Pod pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:00:32.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2940" for this suite. 02/20/23 22:00:33.006
STEP: Destroying namespace "secret-namespace-9655" for this suite. 02/20/23 22:00:33.022
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":43,"skipped":891,"failed":0}
------------------------------
• [4.402 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:28.637
    Feb 20 22:00:28.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:00:28.639
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:28.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:28.695
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-1d96e632-b032-4316-b994-4a2a96c74598 02/20/23 22:00:28.791
    STEP: Creating a pod to test consume secrets 02/20/23 22:00:28.808
    Feb 20 22:00:28.860: INFO: Waiting up to 5m0s for pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14" in namespace "secrets-2940" to be "Succeeded or Failed"
    Feb 20 22:00:28.871: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.760379ms
    Feb 20 22:00:30.881: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02106591s
    Feb 20 22:00:32.882: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02113791s
    STEP: Saw pod success 02/20/23 22:00:32.882
    Feb 20 22:00:32.882: INFO: Pod "pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14" satisfied condition "Succeeded or Failed"
    Feb 20 22:00:32.920: INFO: Trying to get logs from node 10.8.38.70 pod pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:00:32.961
    Feb 20 22:00:32.985: INFO: Waiting for pod pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14 to disappear
    Feb 20 22:00:32.993: INFO: Pod pod-secrets-a23a44d8-a8b7-433f-8652-13128caf7e14 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:00:32.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2940" for this suite. 02/20/23 22:00:33.006
    STEP: Destroying namespace "secret-namespace-9655" for this suite. 02/20/23 22:00:33.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:33.04
Feb 20 22:00:33.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:00:33.043
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:33.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:33.11
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-5f7aac01-d2a6-473f-ad01-f3ec0948bedb 02/20/23 22:00:33.118
STEP: Creating a pod to test consume configMaps 02/20/23 22:00:33.139
Feb 20 22:00:33.170: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7" in namespace "projected-3493" to be "Succeeded or Failed"
Feb 20 22:00:33.179: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.607457ms
Feb 20 22:00:35.191: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020421323s
Feb 20 22:00:37.189: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018596819s
Feb 20 22:00:39.190: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019270995s
STEP: Saw pod success 02/20/23 22:00:39.19
Feb 20 22:00:39.190: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7" satisfied condition "Succeeded or Failed"
Feb 20 22:00:39.199: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:00:39.217
Feb 20 22:00:39.255: INFO: Waiting for pod pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7 to disappear
Feb 20 22:00:39.265: INFO: Pod pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 22:00:39.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3493" for this suite. 02/20/23 22:00:39.278
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":44,"skipped":896,"failed":0}
------------------------------
• [SLOW TEST] [6.252 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:33.04
    Feb 20 22:00:33.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:00:33.043
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:33.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:33.11
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-5f7aac01-d2a6-473f-ad01-f3ec0948bedb 02/20/23 22:00:33.118
    STEP: Creating a pod to test consume configMaps 02/20/23 22:00:33.139
    Feb 20 22:00:33.170: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7" in namespace "projected-3493" to be "Succeeded or Failed"
    Feb 20 22:00:33.179: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.607457ms
    Feb 20 22:00:35.191: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020421323s
    Feb 20 22:00:37.189: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018596819s
    Feb 20 22:00:39.190: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019270995s
    STEP: Saw pod success 02/20/23 22:00:39.19
    Feb 20 22:00:39.190: INFO: Pod "pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7" satisfied condition "Succeeded or Failed"
    Feb 20 22:00:39.199: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:00:39.217
    Feb 20 22:00:39.255: INFO: Waiting for pod pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7 to disappear
    Feb 20 22:00:39.265: INFO: Pod pod-projected-configmaps-df442d22-f419-4257-94ce-ae69de8217c7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 22:00:39.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3493" for this suite. 02/20/23 22:00:39.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:39.295
Feb 20 22:00:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:00:39.299
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:39.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:39.361
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:00:39.37
Feb 20 22:00:39.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43" in namespace "downward-api-2651" to be "Succeeded or Failed"
Feb 20 22:00:39.413: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231536ms
Feb 20 22:00:41.426: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022272167s
Feb 20 22:00:43.421: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017338481s
STEP: Saw pod success 02/20/23 22:00:43.421
Feb 20 22:00:43.421: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43" satisfied condition "Succeeded or Failed"
Feb 20 22:00:43.428: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43 container client-container: <nil>
STEP: delete the pod 02/20/23 22:00:43.445
Feb 20 22:00:43.470: INFO: Waiting for pod downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43 to disappear
Feb 20 22:00:43.478: INFO: Pod downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:00:43.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2651" for this suite. 02/20/23 22:00:43.494
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":907,"failed":0}
------------------------------
• [4.216 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:39.295
    Feb 20 22:00:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:00:39.299
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:39.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:39.361
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:00:39.37
    Feb 20 22:00:39.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43" in namespace "downward-api-2651" to be "Succeeded or Failed"
    Feb 20 22:00:39.413: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231536ms
    Feb 20 22:00:41.426: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022272167s
    Feb 20 22:00:43.421: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017338481s
    STEP: Saw pod success 02/20/23 22:00:43.421
    Feb 20 22:00:43.421: INFO: Pod "downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43" satisfied condition "Succeeded or Failed"
    Feb 20 22:00:43.428: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:00:43.445
    Feb 20 22:00:43.470: INFO: Waiting for pod downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43 to disappear
    Feb 20 22:00:43.478: INFO: Pod downwardapi-volume-48fe822c-ad35-4051-a573-962dd31dbf43 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:00:43.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2651" for this suite. 02/20/23 22:00:43.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:43.515
Feb 20 22:00:43.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename watch 02/20/23 22:00:43.517
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:43.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:43.567
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/20/23 22:00:43.579
STEP: starting a background goroutine to produce watch events 02/20/23 22:00:43.596
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/20/23 22:00:43.596
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 20 22:00:46.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6087" for this suite. 02/20/23 22:00:46.403
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":46,"skipped":920,"failed":0}
------------------------------
• [2.918 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:43.515
    Feb 20 22:00:43.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename watch 02/20/23 22:00:43.517
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:43.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:43.567
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/20/23 22:00:43.579
    STEP: starting a background goroutine to produce watch events 02/20/23 22:00:43.596
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/20/23 22:00:43.596
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 20 22:00:46.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6087" for this suite. 02/20/23 22:00:46.403
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:46.434
Feb 20 22:00:46.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename ephemeral-containers-test 02/20/23 22:00:46.436
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:46.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:46.486
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/20/23 22:00:46.495
Feb 20 22:00:46.535: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8068" to be "running and ready"
Feb 20 22:00:46.548: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.788599ms
Feb 20 22:00:46.548: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:00:48.557: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021685919s
Feb 20 22:00:48.557: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 20 22:00:48.557: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/20/23 22:00:48.567
Feb 20 22:00:48.596: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8068" to be "container debugger running"
Feb 20 22:00:48.605: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.886374ms
Feb 20 22:00:50.615: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018826038s
Feb 20 22:00:52.615: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018717784s
Feb 20 22:00:52.615: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/20/23 22:00:52.615
Feb 20 22:00:52.615: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8068 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:00:52.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:00:52.617: INFO: ExecWithOptions: Clientset creation
Feb 20 22:00:52.617: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-8068/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 20 22:00:52.757: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 22:00:52.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-8068" for this suite. 02/20/23 22:00:52.794
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":47,"skipped":920,"failed":0}
------------------------------
• [SLOW TEST] [6.374 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:46.434
    Feb 20 22:00:46.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/20/23 22:00:46.436
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:46.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:46.486
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/20/23 22:00:46.495
    Feb 20 22:00:46.535: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8068" to be "running and ready"
    Feb 20 22:00:46.548: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.788599ms
    Feb 20 22:00:46.548: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:00:48.557: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021685919s
    Feb 20 22:00:48.557: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 20 22:00:48.557: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/20/23 22:00:48.567
    Feb 20 22:00:48.596: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8068" to be "container debugger running"
    Feb 20 22:00:48.605: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.886374ms
    Feb 20 22:00:50.615: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.018826038s
    Feb 20 22:00:52.615: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018717784s
    Feb 20 22:00:52.615: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/20/23 22:00:52.615
    Feb 20 22:00:52.615: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8068 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:00:52.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:00:52.617: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:00:52.617: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-8068/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 20 22:00:52.757: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 22:00:52.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-8068" for this suite. 02/20/23 22:00:52.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:52.813
Feb 20 22:00:52.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 22:00:52.815
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:52.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:52.878
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/20/23 22:00:52.898
W0220 22:00:52.910137      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 02/20/23 22:00:57.928
STEP: wait for the rc to be deleted 02/20/23 22:00:57.955
STEP: Gathering metrics 02/20/23 22:00:58.975
W0220 22:00:58.991662      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 20 22:00:58.991: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 22:00:58.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-816" for this suite. 02/20/23 22:00:59.017
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":48,"skipped":959,"failed":0}
------------------------------
• [SLOW TEST] [6.248 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:52.813
    Feb 20 22:00:52.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 22:00:52.815
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:52.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:52.878
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/20/23 22:00:52.898
    W0220 22:00:52.910137      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 02/20/23 22:00:57.928
    STEP: wait for the rc to be deleted 02/20/23 22:00:57.955
    STEP: Gathering metrics 02/20/23 22:00:58.975
    W0220 22:00:58.991662      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 20 22:00:58.991: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 22:00:58.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-816" for this suite. 02/20/23 22:00:59.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:00:59.068
Feb 20 22:00:59.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:00:59.069
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:59.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:59.168
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 02/20/23 22:00:59.176
Feb 20 22:00:59.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: rename a version 02/20/23 22:01:20.878
STEP: check the new version name is served 02/20/23 22:01:20.902
STEP: check the old version name is removed 02/20/23 22:01:29.913
STEP: check the other version is not changed 02/20/23 22:01:33.407
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:01:49.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3073" for this suite. 02/20/23 22:01:49.685
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":49,"skipped":994,"failed":0}
------------------------------
• [SLOW TEST] [50.647 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:00:59.068
    Feb 20 22:00:59.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:00:59.069
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:00:59.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:00:59.168
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 02/20/23 22:00:59.176
    Feb 20 22:00:59.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: rename a version 02/20/23 22:01:20.878
    STEP: check the new version name is served 02/20/23 22:01:20.902
    STEP: check the old version name is removed 02/20/23 22:01:29.913
    STEP: check the other version is not changed 02/20/23 22:01:33.407
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:01:49.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3073" for this suite. 02/20/23 22:01:49.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:01:49.712
Feb 20 22:01:49.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 22:01:49.715
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:01:49.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:01:49.772
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 02/20/23 22:01:49.784
Feb 20 22:01:49.835: INFO: Waiting up to 5m0s for pod "pod-vgp4h" in namespace "pods-5571" to be "running"
Feb 20 22:01:49.846: INFO: Pod "pod-vgp4h": Phase="Pending", Reason="", readiness=false. Elapsed: 10.248433ms
Feb 20 22:01:51.858: INFO: Pod "pod-vgp4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.022972078s
Feb 20 22:01:51.858: INFO: Pod "pod-vgp4h" satisfied condition "running"
STEP: patching /status 02/20/23 22:01:51.858
Feb 20 22:01:51.877: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 22:01:51.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5571" for this suite. 02/20/23 22:01:51.894
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":50,"skipped":1016,"failed":0}
------------------------------
• [2.204 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:01:49.712
    Feb 20 22:01:49.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 22:01:49.715
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:01:49.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:01:49.772
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 02/20/23 22:01:49.784
    Feb 20 22:01:49.835: INFO: Waiting up to 5m0s for pod "pod-vgp4h" in namespace "pods-5571" to be "running"
    Feb 20 22:01:49.846: INFO: Pod "pod-vgp4h": Phase="Pending", Reason="", readiness=false. Elapsed: 10.248433ms
    Feb 20 22:01:51.858: INFO: Pod "pod-vgp4h": Phase="Running", Reason="", readiness=true. Elapsed: 2.022972078s
    Feb 20 22:01:51.858: INFO: Pod "pod-vgp4h" satisfied condition "running"
    STEP: patching /status 02/20/23 22:01:51.858
    Feb 20 22:01:51.877: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 22:01:51.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5571" for this suite. 02/20/23 22:01:51.894
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:01:51.918
Feb 20 22:01:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:01:51.921
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:01:51.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:01:51.972
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
Feb 20 22:01:52.001: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-302649fe-29ad-4a10-a49b-97b0d2e5e96a 02/20/23 22:01:52.001
STEP: Creating the pod 02/20/23 22:01:52.016
Feb 20 22:01:52.085: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561" in namespace "projected-4110" to be "running and ready"
Feb 20 22:01:52.097: INFO: Pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561": Phase="Pending", Reason="", readiness=false. Elapsed: 11.428494ms
Feb 20 22:01:52.097: INFO: The phase of Pod pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:01:54.110: INFO: Pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561": Phase="Running", Reason="", readiness=true. Elapsed: 2.02447957s
Feb 20 22:01:54.110: INFO: The phase of Pod pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561 is Running (Ready = true)
Feb 20 22:01:54.110: INFO: Pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-302649fe-29ad-4a10-a49b-97b0d2e5e96a 02/20/23 22:01:54.154
STEP: waiting to observe update in volume 02/20/23 22:01:54.17
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 22:01:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4110" for this suite. 02/20/23 22:01:56.234
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":51,"skipped":1018,"failed":0}
------------------------------
• [4.339 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:01:51.918
    Feb 20 22:01:51.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:01:51.921
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:01:51.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:01:51.972
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    Feb 20 22:01:52.001: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-302649fe-29ad-4a10-a49b-97b0d2e5e96a 02/20/23 22:01:52.001
    STEP: Creating the pod 02/20/23 22:01:52.016
    Feb 20 22:01:52.085: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561" in namespace "projected-4110" to be "running and ready"
    Feb 20 22:01:52.097: INFO: Pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561": Phase="Pending", Reason="", readiness=false. Elapsed: 11.428494ms
    Feb 20 22:01:52.097: INFO: The phase of Pod pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:01:54.110: INFO: Pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561": Phase="Running", Reason="", readiness=true. Elapsed: 2.02447957s
    Feb 20 22:01:54.110: INFO: The phase of Pod pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561 is Running (Ready = true)
    Feb 20 22:01:54.110: INFO: Pod "pod-projected-configmaps-e58dfa86-d87b-4170-abc3-75a5d9a74561" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-302649fe-29ad-4a10-a49b-97b0d2e5e96a 02/20/23 22:01:54.154
    STEP: waiting to observe update in volume 02/20/23 22:01:54.17
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 22:01:56.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4110" for this suite. 02/20/23 22:01:56.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:01:56.261
Feb 20 22:01:56.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 22:01:56.263
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:01:56.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:01:56.326
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1695 02/20/23 22:01:56.338
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-1695 02/20/23 22:01:56.374
Feb 20 22:01:56.405: INFO: Found 0 stateful pods, waiting for 1
Feb 20 22:02:06.417: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/20/23 22:02:06.436
STEP: Getting /status 02/20/23 22:02:06.452
Feb 20 22:02:06.462: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/20/23 22:02:06.462
Feb 20 22:02:06.480: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/20/23 22:02:06.48
Feb 20 22:02:06.490: INFO: Observed &StatefulSet event: ADDED
Feb 20 22:02:06.490: INFO: Found Statefulset ss in namespace statefulset-1695 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 20 22:02:06.490: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/20/23 22:02:06.491
Feb 20 22:02:06.491: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 20 22:02:06.503: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/20/23 22:02:06.503
Feb 20 22:02:06.510: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 22:02:06.510: INFO: Deleting all statefulset in ns statefulset-1695
Feb 20 22:02:06.519: INFO: Scaling statefulset ss to 0
Feb 20 22:02:16.567: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:02:16.576: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 22:02:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1695" for this suite. 02/20/23 22:02:16.621
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":52,"skipped":1027,"failed":0}
------------------------------
• [SLOW TEST] [20.385 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:01:56.261
    Feb 20 22:01:56.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 22:01:56.263
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:01:56.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:01:56.326
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1695 02/20/23 22:01:56.338
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-1695 02/20/23 22:01:56.374
    Feb 20 22:01:56.405: INFO: Found 0 stateful pods, waiting for 1
    Feb 20 22:02:06.417: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/20/23 22:02:06.436
    STEP: Getting /status 02/20/23 22:02:06.452
    Feb 20 22:02:06.462: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/20/23 22:02:06.462
    Feb 20 22:02:06.480: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/20/23 22:02:06.48
    Feb 20 22:02:06.490: INFO: Observed &StatefulSet event: ADDED
    Feb 20 22:02:06.490: INFO: Found Statefulset ss in namespace statefulset-1695 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 20 22:02:06.490: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/20/23 22:02:06.491
    Feb 20 22:02:06.491: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 20 22:02:06.503: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/20/23 22:02:06.503
    Feb 20 22:02:06.510: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 22:02:06.510: INFO: Deleting all statefulset in ns statefulset-1695
    Feb 20 22:02:06.519: INFO: Scaling statefulset ss to 0
    Feb 20 22:02:16.567: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 22:02:16.576: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 22:02:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1695" for this suite. 02/20/23 22:02:16.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:02:16.649
Feb 20 22:02:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 22:02:16.651
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:02:16.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:02:16.711
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/20/23 22:02:16.746
Feb 20 22:02:16.803: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9251" to be "running and ready"
Feb 20 22:02:16.815: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.581928ms
Feb 20 22:02:16.815: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:02:18.836: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033044521s
Feb 20 22:02:18.836: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:02:20.828: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.025755934s
Feb 20 22:02:20.828: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 20 22:02:20.828: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 02/20/23 22:02:20.84
Feb 20 22:02:20.877: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9251" to be "running and ready"
Feb 20 22:02:20.894: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962987ms
Feb 20 22:02:20.894: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:02:22.935: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057359515s
Feb 20 22:02:22.935: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:02:24.906: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.028772316s
Feb 20 22:02:24.906: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 20 22:02:24.906: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/20/23 22:02:24.917
STEP: delete the pod with lifecycle hook 02/20/23 22:02:24.967
Feb 20 22:02:24.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 22:02:25.008: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 22:02:27.009: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 22:02:27.023: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 20 22:02:27.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9251" for this suite. 02/20/23 22:02:27.049
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":53,"skipped":1040,"failed":0}
------------------------------
• [SLOW TEST] [10.424 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:02:16.649
    Feb 20 22:02:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 22:02:16.651
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:02:16.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:02:16.711
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/20/23 22:02:16.746
    Feb 20 22:02:16.803: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9251" to be "running and ready"
    Feb 20 22:02:16.815: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.581928ms
    Feb 20 22:02:16.815: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:02:18.836: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033044521s
    Feb 20 22:02:18.836: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:02:20.828: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.025755934s
    Feb 20 22:02:20.828: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 20 22:02:20.828: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 02/20/23 22:02:20.84
    Feb 20 22:02:20.877: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9251" to be "running and ready"
    Feb 20 22:02:20.894: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962987ms
    Feb 20 22:02:20.894: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:02:22.935: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057359515s
    Feb 20 22:02:22.935: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:02:24.906: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.028772316s
    Feb 20 22:02:24.906: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 20 22:02:24.906: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/20/23 22:02:24.917
    STEP: delete the pod with lifecycle hook 02/20/23 22:02:24.967
    Feb 20 22:02:24.996: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 20 22:02:25.008: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 20 22:02:27.009: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 20 22:02:27.023: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 20 22:02:27.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9251" for this suite. 02/20/23 22:02:27.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:02:27.076
Feb 20 22:02:27.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename subpath 02/20/23 22:02:27.078
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:02:27.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:02:27.146
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/20/23 22:02:27.156
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-xp6r 02/20/23 22:02:27.193
STEP: Creating a pod to test atomic-volume-subpath 02/20/23 22:02:27.193
Feb 20 22:02:27.244: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xp6r" in namespace "subpath-2190" to be "Succeeded or Failed"
Feb 20 22:02:27.260: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Pending", Reason="", readiness=false. Elapsed: 16.415238ms
Feb 20 22:02:29.272: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028276471s
Feb 20 22:02:31.274: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 4.029866216s
Feb 20 22:02:33.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 6.028772568s
Feb 20 22:02:35.300: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 8.055763672s
Feb 20 22:02:37.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 10.029467685s
Feb 20 22:02:39.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 12.029604763s
Feb 20 22:02:41.274: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 14.030163459s
Feb 20 22:02:43.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 16.029710631s
Feb 20 22:02:45.272: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 18.028447293s
Feb 20 22:02:47.274: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 20.030510375s
Feb 20 22:02:49.272: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=false. Elapsed: 22.028274807s
Feb 20 22:02:51.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02878381s
STEP: Saw pod success 02/20/23 22:02:51.273
Feb 20 22:02:51.273: INFO: Pod "pod-subpath-test-projected-xp6r" satisfied condition "Succeeded or Failed"
Feb 20 22:02:51.284: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-projected-xp6r container test-container-subpath-projected-xp6r: <nil>
STEP: delete the pod 02/20/23 22:02:51.32
Feb 20 22:02:51.345: INFO: Waiting for pod pod-subpath-test-projected-xp6r to disappear
Feb 20 22:02:51.356: INFO: Pod pod-subpath-test-projected-xp6r no longer exists
STEP: Deleting pod pod-subpath-test-projected-xp6r 02/20/23 22:02:51.356
Feb 20 22:02:51.356: INFO: Deleting pod "pod-subpath-test-projected-xp6r" in namespace "subpath-2190"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 20 22:02:51.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2190" for this suite. 02/20/23 22:02:51.387
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":54,"skipped":1045,"failed":0}
------------------------------
• [SLOW TEST] [24.336 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:02:27.076
    Feb 20 22:02:27.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename subpath 02/20/23 22:02:27.078
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:02:27.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:02:27.146
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/20/23 22:02:27.156
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-xp6r 02/20/23 22:02:27.193
    STEP: Creating a pod to test atomic-volume-subpath 02/20/23 22:02:27.193
    Feb 20 22:02:27.244: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xp6r" in namespace "subpath-2190" to be "Succeeded or Failed"
    Feb 20 22:02:27.260: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Pending", Reason="", readiness=false. Elapsed: 16.415238ms
    Feb 20 22:02:29.272: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028276471s
    Feb 20 22:02:31.274: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 4.029866216s
    Feb 20 22:02:33.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 6.028772568s
    Feb 20 22:02:35.300: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 8.055763672s
    Feb 20 22:02:37.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 10.029467685s
    Feb 20 22:02:39.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 12.029604763s
    Feb 20 22:02:41.274: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 14.030163459s
    Feb 20 22:02:43.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 16.029710631s
    Feb 20 22:02:45.272: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 18.028447293s
    Feb 20 22:02:47.274: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=true. Elapsed: 20.030510375s
    Feb 20 22:02:49.272: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Running", Reason="", readiness=false. Elapsed: 22.028274807s
    Feb 20 22:02:51.273: INFO: Pod "pod-subpath-test-projected-xp6r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02878381s
    STEP: Saw pod success 02/20/23 22:02:51.273
    Feb 20 22:02:51.273: INFO: Pod "pod-subpath-test-projected-xp6r" satisfied condition "Succeeded or Failed"
    Feb 20 22:02:51.284: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-projected-xp6r container test-container-subpath-projected-xp6r: <nil>
    STEP: delete the pod 02/20/23 22:02:51.32
    Feb 20 22:02:51.345: INFO: Waiting for pod pod-subpath-test-projected-xp6r to disappear
    Feb 20 22:02:51.356: INFO: Pod pod-subpath-test-projected-xp6r no longer exists
    STEP: Deleting pod pod-subpath-test-projected-xp6r 02/20/23 22:02:51.356
    Feb 20 22:02:51.356: INFO: Deleting pod "pod-subpath-test-projected-xp6r" in namespace "subpath-2190"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 20 22:02:51.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2190" for this suite. 02/20/23 22:02:51.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:02:51.419
Feb 20 22:02:51.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replication-controller 02/20/23 22:02:51.421
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:02:51.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:02:51.491
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6 02/20/23 22:02:51.505
Feb 20 22:02:51.528: INFO: Pod name my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6: Found 0 pods out of 1
Feb 20 22:02:56.565: INFO: Pod name my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6: Found 1 pods out of 1
Feb 20 22:02:56.565: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6" are running
Feb 20 22:02:56.565: INFO: Waiting up to 5m0s for pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb" in namespace "replication-controller-9923" to be "running"
Feb 20 22:02:56.576: INFO: Pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb": Phase="Running", Reason="", readiness=true. Elapsed: 11.23784ms
Feb 20 22:02:56.577: INFO: Pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb" satisfied condition "running"
Feb 20 22:02:56.577: INFO: Pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:51 +0000 UTC Reason: Message:}])
Feb 20 22:02:56.577: INFO: Trying to dial the pod
Feb 20 22:03:01.648: INFO: Controller my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6: Got expected result from replica 1 [my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb]: "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 20 22:03:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9923" for this suite. 02/20/23 22:03:01.671
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":55,"skipped":1057,"failed":0}
------------------------------
• [SLOW TEST] [10.285 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:02:51.419
    Feb 20 22:02:51.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replication-controller 02/20/23 22:02:51.421
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:02:51.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:02:51.491
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6 02/20/23 22:02:51.505
    Feb 20 22:02:51.528: INFO: Pod name my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6: Found 0 pods out of 1
    Feb 20 22:02:56.565: INFO: Pod name my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6: Found 1 pods out of 1
    Feb 20 22:02:56.565: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6" are running
    Feb 20 22:02:56.565: INFO: Waiting up to 5m0s for pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb" in namespace "replication-controller-9923" to be "running"
    Feb 20 22:02:56.576: INFO: Pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb": Phase="Running", Reason="", readiness=true. Elapsed: 11.23784ms
    Feb 20 22:02:56.577: INFO: Pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb" satisfied condition "running"
    Feb 20 22:02:56.577: INFO: Pod "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:02:51 +0000 UTC Reason: Message:}])
    Feb 20 22:02:56.577: INFO: Trying to dial the pod
    Feb 20 22:03:01.648: INFO: Controller my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6: Got expected result from replica 1 [my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb]: "my-hostname-basic-28e8bd25-b9a0-4a27-ad7b-e21341457aa6-jt4jb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 20 22:03:01.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9923" for this suite. 02/20/23 22:03:01.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:01.709
Feb 20 22:03:01.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:03:01.711
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:01.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:01.806
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 02/20/23 22:03:01.818
STEP: Creating a ResourceQuota 02/20/23 22:03:06.827
STEP: Ensuring resource quota status is calculated 02/20/23 22:03:06.84
STEP: Creating a Pod that fits quota 02/20/23 22:03:08.85
STEP: Ensuring ResourceQuota status captures the pod usage 02/20/23 22:03:08.901
STEP: Not allowing a pod to be created that exceeds remaining quota 02/20/23 22:03:10.911
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/20/23 22:03:10.938
STEP: Ensuring a pod cannot update its resource requirements 02/20/23 22:03:10.985
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/20/23 22:03:11.013
STEP: Deleting the pod 02/20/23 22:03:13.026
STEP: Ensuring resource quota status released the pod usage 02/20/23 22:03:13.058
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:03:15.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4216" for this suite. 02/20/23 22:03:15.089
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":56,"skipped":1074,"failed":0}
------------------------------
• [SLOW TEST] [13.406 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:01.709
    Feb 20 22:03:01.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:03:01.711
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:01.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:01.806
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 02/20/23 22:03:01.818
    STEP: Creating a ResourceQuota 02/20/23 22:03:06.827
    STEP: Ensuring resource quota status is calculated 02/20/23 22:03:06.84
    STEP: Creating a Pod that fits quota 02/20/23 22:03:08.85
    STEP: Ensuring ResourceQuota status captures the pod usage 02/20/23 22:03:08.901
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/20/23 22:03:10.911
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/20/23 22:03:10.938
    STEP: Ensuring a pod cannot update its resource requirements 02/20/23 22:03:10.985
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/20/23 22:03:11.013
    STEP: Deleting the pod 02/20/23 22:03:13.026
    STEP: Ensuring resource quota status released the pod usage 02/20/23 22:03:13.058
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:03:15.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4216" for this suite. 02/20/23 22:03:15.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:15.117
Feb 20 22:03:15.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:03:15.119
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:15.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:15.182
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:03:15.252
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:03:15.641
STEP: Deploying the webhook pod 02/20/23 22:03:15.745
STEP: Wait for the deployment to be ready 02/20/23 22:03:15.772
Feb 20 22:03:15.794: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 20 22:03:17.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:03:19.834
STEP: Verifying the service has paired with the endpoint 02/20/23 22:03:19.86
Feb 20 22:03:20.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/20/23 22:03:20.873
STEP: create a namespace for the webhook 02/20/23 22:03:20.909
STEP: create a configmap should be unconditionally rejected by the webhook 02/20/23 22:03:20.931
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:03:21.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4007" for this suite. 02/20/23 22:03:21.047
STEP: Destroying namespace "webhook-4007-markers" for this suite. 02/20/23 22:03:21.069
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":57,"skipped":1092,"failed":0}
------------------------------
• [SLOW TEST] [6.080 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:15.117
    Feb 20 22:03:15.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:03:15.119
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:15.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:15.182
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:03:15.252
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:03:15.641
    STEP: Deploying the webhook pod 02/20/23 22:03:15.745
    STEP: Wait for the deployment to be ready 02/20/23 22:03:15.772
    Feb 20 22:03:15.794: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Feb 20 22:03:17.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 3, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:03:19.834
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:03:19.86
    Feb 20 22:03:20.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/20/23 22:03:20.873
    STEP: create a namespace for the webhook 02/20/23 22:03:20.909
    STEP: create a configmap should be unconditionally rejected by the webhook 02/20/23 22:03:20.931
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:03:21.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4007" for this suite. 02/20/23 22:03:21.047
    STEP: Destroying namespace "webhook-4007-markers" for this suite. 02/20/23 22:03:21.069
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:21.197
Feb 20 22:03:21.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:03:21.198
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:21.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:21.288
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7462 02/20/23 22:03:21.298
STEP: changing the ExternalName service to type=NodePort 02/20/23 22:03:21.32
STEP: creating replication controller externalname-service in namespace services-7462 02/20/23 22:03:21.377
I0220 22:03:21.389154      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7462, replica count: 2
I0220 22:03:24.439932      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:03:24.439: INFO: Creating new exec pod
Feb 20 22:03:24.481: INFO: Waiting up to 5m0s for pod "execpodvc8z2" in namespace "services-7462" to be "running"
Feb 20 22:03:24.504: INFO: Pod "execpodvc8z2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.028086ms
Feb 20 22:03:26.515: INFO: Pod "execpodvc8z2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033690456s
Feb 20 22:03:28.519: INFO: Pod "execpodvc8z2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037732394s
Feb 20 22:03:28.519: INFO: Pod "execpodvc8z2" satisfied condition "running"
Feb 20 22:03:29.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 22:03:29.900: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 22:03:29.900: INFO: stdout: ""
Feb 20 22:03:30.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 22:03:31.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 22:03:31.275: INFO: stdout: ""
Feb 20 22:03:31.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 22:03:32.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 22:03:32.219: INFO: stdout: ""
Feb 20 22:03:32.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 20 22:03:33.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 20 22:03:33.187: INFO: stdout: "externalname-service-p2nsx"
Feb 20 22:03:33.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.162.154 80'
Feb 20 22:03:33.452: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.162.154 80\nConnection to 172.21.162.154 80 port [tcp/http] succeeded!\n"
Feb 20 22:03:33.452: INFO: stdout: "externalname-service-p2nsx"
Feb 20 22:03:33.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32682'
Feb 20 22:03:33.750: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32682\nConnection to 10.8.38.70 32682 port [tcp/*] succeeded!\n"
Feb 20 22:03:33.750: INFO: stdout: "externalname-service-p2nsx"
Feb 20 22:03:33.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 32682'
Feb 20 22:03:34.081: INFO: stderr: "+ nc -v -t -w 2 10.8.38.66 32682\n+ echo hostName\nConnection to 10.8.38.66 32682 port [tcp/*] succeeded!\n"
Feb 20 22:03:34.081: INFO: stdout: ""
Feb 20 22:03:35.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 32682'
Feb 20 22:03:35.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 32682\nConnection to 10.8.38.66 32682 port [tcp/*] succeeded!\n"
Feb 20 22:03:35.368: INFO: stdout: ""
Feb 20 22:03:36.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 32682'
Feb 20 22:03:36.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 32682\nConnection to 10.8.38.66 32682 port [tcp/*] succeeded!\n"
Feb 20 22:03:36.383: INFO: stdout: "externalname-service-p2nsx"
Feb 20 22:03:36.383: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:03:36.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7462" for this suite. 02/20/23 22:03:36.45
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":58,"skipped":1092,"failed":0}
------------------------------
• [SLOW TEST] [15.276 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:21.197
    Feb 20 22:03:21.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:03:21.198
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:21.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:21.288
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7462 02/20/23 22:03:21.298
    STEP: changing the ExternalName service to type=NodePort 02/20/23 22:03:21.32
    STEP: creating replication controller externalname-service in namespace services-7462 02/20/23 22:03:21.377
    I0220 22:03:21.389154      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7462, replica count: 2
    I0220 22:03:24.439932      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:03:24.439: INFO: Creating new exec pod
    Feb 20 22:03:24.481: INFO: Waiting up to 5m0s for pod "execpodvc8z2" in namespace "services-7462" to be "running"
    Feb 20 22:03:24.504: INFO: Pod "execpodvc8z2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.028086ms
    Feb 20 22:03:26.515: INFO: Pod "execpodvc8z2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033690456s
    Feb 20 22:03:28.519: INFO: Pod "execpodvc8z2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037732394s
    Feb 20 22:03:28.519: INFO: Pod "execpodvc8z2" satisfied condition "running"
    Feb 20 22:03:29.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 22:03:29.900: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 22:03:29.900: INFO: stdout: ""
    Feb 20 22:03:30.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 22:03:31.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 22:03:31.275: INFO: stdout: ""
    Feb 20 22:03:31.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 22:03:32.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 22:03:32.219: INFO: stdout: ""
    Feb 20 22:03:32.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 20 22:03:33.186: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 20 22:03:33.187: INFO: stdout: "externalname-service-p2nsx"
    Feb 20 22:03:33.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.162.154 80'
    Feb 20 22:03:33.452: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.162.154 80\nConnection to 172.21.162.154 80 port [tcp/http] succeeded!\n"
    Feb 20 22:03:33.452: INFO: stdout: "externalname-service-p2nsx"
    Feb 20 22:03:33.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32682'
    Feb 20 22:03:33.750: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32682\nConnection to 10.8.38.70 32682 port [tcp/*] succeeded!\n"
    Feb 20 22:03:33.750: INFO: stdout: "externalname-service-p2nsx"
    Feb 20 22:03:33.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 32682'
    Feb 20 22:03:34.081: INFO: stderr: "+ nc -v -t -w 2 10.8.38.66 32682\n+ echo hostName\nConnection to 10.8.38.66 32682 port [tcp/*] succeeded!\n"
    Feb 20 22:03:34.081: INFO: stdout: ""
    Feb 20 22:03:35.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 32682'
    Feb 20 22:03:35.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 32682\nConnection to 10.8.38.66 32682 port [tcp/*] succeeded!\n"
    Feb 20 22:03:35.368: INFO: stdout: ""
    Feb 20 22:03:36.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7462 exec execpodvc8z2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 32682'
    Feb 20 22:03:36.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 32682\nConnection to 10.8.38.66 32682 port [tcp/*] succeeded!\n"
    Feb 20 22:03:36.383: INFO: stdout: "externalname-service-p2nsx"
    Feb 20 22:03:36.383: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:03:36.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7462" for this suite. 02/20/23 22:03:36.45
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:36.474
Feb 20 22:03:36.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubelet-test 02/20/23 22:03:36.475
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:36.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:36.529
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/20/23 22:03:36.617
Feb 20 22:03:36.618: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f" in namespace "kubelet-test-6031" to be "completed"
Feb 20 22:03:36.629: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.41884ms
Feb 20 22:03:38.643: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024787749s
Feb 20 22:03:40.644: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026719984s
Feb 20 22:03:42.646: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028693259s
Feb 20 22:03:42.647: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 20 22:03:42.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6031" for this suite. 02/20/23 22:03:42.699
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":59,"skipped":1094,"failed":0}
------------------------------
• [SLOW TEST] [6.247 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:36.474
    Feb 20 22:03:36.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubelet-test 02/20/23 22:03:36.475
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:36.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:36.529
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/20/23 22:03:36.617
    Feb 20 22:03:36.618: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f" in namespace "kubelet-test-6031" to be "completed"
    Feb 20 22:03:36.629: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.41884ms
    Feb 20 22:03:38.643: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024787749s
    Feb 20 22:03:40.644: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026719984s
    Feb 20 22:03:42.646: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028693259s
    Feb 20 22:03:42.647: INFO: Pod "agnhost-host-aliasesb08a68d8-97a4-4505-a408-fa324e8c4a5f" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 20 22:03:42.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6031" for this suite. 02/20/23 22:03:42.699
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:42.725
Feb 20 22:03:42.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename proxy 02/20/23 22:03:42.728
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:42.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:42.804
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 20 22:03:42.814: INFO: Creating pod...
Feb 20 22:03:42.877: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1300" to be "running"
Feb 20 22:03:42.890: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.945715ms
Feb 20 22:03:44.907: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.029457918s
Feb 20 22:03:44.907: INFO: Pod "agnhost" satisfied condition "running"
Feb 20 22:03:44.907: INFO: Creating service...
Feb 20 22:03:44.931: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/DELETE
Feb 20 22:03:44.951: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 20 22:03:44.951: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/GET
Feb 20 22:03:44.967: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 20 22:03:44.967: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/HEAD
Feb 20 22:03:44.983: INFO: http.Client request:HEAD | StatusCode:200
Feb 20 22:03:44.983: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 20 22:03:44.996: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 20 22:03:44.996: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/PATCH
Feb 20 22:03:45.009: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 20 22:03:45.009: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/POST
Feb 20 22:03:45.030: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 20 22:03:45.030: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/PUT
Feb 20 22:03:45.044: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 20 22:03:45.045: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/DELETE
Feb 20 22:03:45.064: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 20 22:03:45.064: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/GET
Feb 20 22:03:45.081: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 20 22:03:45.081: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/HEAD
Feb 20 22:03:45.102: INFO: http.Client request:HEAD | StatusCode:200
Feb 20 22:03:45.102: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/OPTIONS
Feb 20 22:03:45.121: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 20 22:03:45.121: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/PATCH
Feb 20 22:03:45.141: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 20 22:03:45.141: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/POST
Feb 20 22:03:45.164: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 20 22:03:45.165: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/PUT
Feb 20 22:03:45.185: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 20 22:03:45.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1300" for this suite. 02/20/23 22:03:45.211
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":60,"skipped":1096,"failed":0}
------------------------------
• [2.512 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:42.725
    Feb 20 22:03:42.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename proxy 02/20/23 22:03:42.728
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:42.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:42.804
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 20 22:03:42.814: INFO: Creating pod...
    Feb 20 22:03:42.877: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1300" to be "running"
    Feb 20 22:03:42.890: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.945715ms
    Feb 20 22:03:44.907: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.029457918s
    Feb 20 22:03:44.907: INFO: Pod "agnhost" satisfied condition "running"
    Feb 20 22:03:44.907: INFO: Creating service...
    Feb 20 22:03:44.931: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/DELETE
    Feb 20 22:03:44.951: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 20 22:03:44.951: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/GET
    Feb 20 22:03:44.967: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 20 22:03:44.967: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/HEAD
    Feb 20 22:03:44.983: INFO: http.Client request:HEAD | StatusCode:200
    Feb 20 22:03:44.983: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 20 22:03:44.996: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 20 22:03:44.996: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/PATCH
    Feb 20 22:03:45.009: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 20 22:03:45.009: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/POST
    Feb 20 22:03:45.030: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 20 22:03:45.030: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/pods/agnhost/proxy/some/path/with/PUT
    Feb 20 22:03:45.044: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 20 22:03:45.045: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/DELETE
    Feb 20 22:03:45.064: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 20 22:03:45.064: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/GET
    Feb 20 22:03:45.081: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 20 22:03:45.081: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/HEAD
    Feb 20 22:03:45.102: INFO: http.Client request:HEAD | StatusCode:200
    Feb 20 22:03:45.102: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/OPTIONS
    Feb 20 22:03:45.121: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 20 22:03:45.121: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/PATCH
    Feb 20 22:03:45.141: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 20 22:03:45.141: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/POST
    Feb 20 22:03:45.164: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 20 22:03:45.165: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1300/services/test-service/proxy/some/path/with/PUT
    Feb 20 22:03:45.185: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 20 22:03:45.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1300" for this suite. 02/20/23 22:03:45.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:45.24
Feb 20 22:03:45.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:03:45.241
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:45.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:45.296
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 02/20/23 22:03:45.309
Feb 20 22:03:45.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 create -f -'
Feb 20 22:03:46.869: INFO: stderr: ""
Feb 20 22:03:46.869: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:03:46.87
Feb 20 22:03:46.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:03:46.998: INFO: stderr: ""
Feb 20 22:03:46.998: INFO: stdout: "update-demo-nautilus-cpx4z update-demo-nautilus-n5gmx "
Feb 20 22:03:46.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:03:47.110: INFO: stderr: ""
Feb 20 22:03:47.110: INFO: stdout: ""
Feb 20 22:03:47.110: INFO: update-demo-nautilus-cpx4z is created but not running
Feb 20 22:03:52.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:03:52.281: INFO: stderr: ""
Feb 20 22:03:52.281: INFO: stdout: "update-demo-nautilus-cpx4z update-demo-nautilus-n5gmx "
Feb 20 22:03:52.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:03:52.408: INFO: stderr: ""
Feb 20 22:03:52.408: INFO: stdout: ""
Feb 20 22:03:52.408: INFO: update-demo-nautilus-cpx4z is created but not running
Feb 20 22:03:57.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:03:57.520: INFO: stderr: ""
Feb 20 22:03:57.520: INFO: stdout: "update-demo-nautilus-cpx4z update-demo-nautilus-n5gmx "
Feb 20 22:03:57.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:03:57.634: INFO: stderr: ""
Feb 20 22:03:57.634: INFO: stdout: "true"
Feb 20 22:03:57.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:03:57.753: INFO: stderr: ""
Feb 20 22:03:57.753: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:03:57.753: INFO: validating pod update-demo-nautilus-cpx4z
Feb 20 22:03:57.772: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:03:57.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:03:57.772: INFO: update-demo-nautilus-cpx4z is verified up and running
Feb 20 22:03:57.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-n5gmx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:03:57.880: INFO: stderr: ""
Feb 20 22:03:57.880: INFO: stdout: "true"
Feb 20 22:03:57.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-n5gmx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:03:57.984: INFO: stderr: ""
Feb 20 22:03:57.984: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:03:57.984: INFO: validating pod update-demo-nautilus-n5gmx
Feb 20 22:03:58.004: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:03:58.005: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:03:58.005: INFO: update-demo-nautilus-n5gmx is verified up and running
STEP: using delete to clean up resources 02/20/23 22:03:58.005
Feb 20 22:03:58.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 delete --grace-period=0 --force -f -'
Feb 20 22:03:58.138: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:03:58.138: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 22:03:58.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get rc,svc -l name=update-demo --no-headers'
Feb 20 22:03:58.277: INFO: stderr: "No resources found in kubectl-4982 namespace.\n"
Feb 20 22:03:58.277: INFO: stdout: ""
Feb 20 22:03:58.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 22:03:58.442: INFO: stderr: ""
Feb 20 22:03:58.442: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:03:58.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4982" for this suite. 02/20/23 22:03:58.46
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":61,"skipped":1119,"failed":0}
------------------------------
• [SLOW TEST] [13.277 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:45.24
    Feb 20 22:03:45.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:03:45.241
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:45.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:45.296
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 02/20/23 22:03:45.309
    Feb 20 22:03:45.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 create -f -'
    Feb 20 22:03:46.869: INFO: stderr: ""
    Feb 20 22:03:46.869: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:03:46.87
    Feb 20 22:03:46.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:03:46.998: INFO: stderr: ""
    Feb 20 22:03:46.998: INFO: stdout: "update-demo-nautilus-cpx4z update-demo-nautilus-n5gmx "
    Feb 20 22:03:46.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:03:47.110: INFO: stderr: ""
    Feb 20 22:03:47.110: INFO: stdout: ""
    Feb 20 22:03:47.110: INFO: update-demo-nautilus-cpx4z is created but not running
    Feb 20 22:03:52.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:03:52.281: INFO: stderr: ""
    Feb 20 22:03:52.281: INFO: stdout: "update-demo-nautilus-cpx4z update-demo-nautilus-n5gmx "
    Feb 20 22:03:52.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:03:52.408: INFO: stderr: ""
    Feb 20 22:03:52.408: INFO: stdout: ""
    Feb 20 22:03:52.408: INFO: update-demo-nautilus-cpx4z is created but not running
    Feb 20 22:03:57.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:03:57.520: INFO: stderr: ""
    Feb 20 22:03:57.520: INFO: stdout: "update-demo-nautilus-cpx4z update-demo-nautilus-n5gmx "
    Feb 20 22:03:57.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:03:57.634: INFO: stderr: ""
    Feb 20 22:03:57.634: INFO: stdout: "true"
    Feb 20 22:03:57.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-cpx4z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:03:57.753: INFO: stderr: ""
    Feb 20 22:03:57.753: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:03:57.753: INFO: validating pod update-demo-nautilus-cpx4z
    Feb 20 22:03:57.772: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:03:57.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:03:57.772: INFO: update-demo-nautilus-cpx4z is verified up and running
    Feb 20 22:03:57.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-n5gmx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:03:57.880: INFO: stderr: ""
    Feb 20 22:03:57.880: INFO: stdout: "true"
    Feb 20 22:03:57.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods update-demo-nautilus-n5gmx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:03:57.984: INFO: stderr: ""
    Feb 20 22:03:57.984: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:03:57.984: INFO: validating pod update-demo-nautilus-n5gmx
    Feb 20 22:03:58.004: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:03:58.005: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:03:58.005: INFO: update-demo-nautilus-n5gmx is verified up and running
    STEP: using delete to clean up resources 02/20/23 22:03:58.005
    Feb 20 22:03:58.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 delete --grace-period=0 --force -f -'
    Feb 20 22:03:58.138: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 22:03:58.138: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 20 22:03:58.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get rc,svc -l name=update-demo --no-headers'
    Feb 20 22:03:58.277: INFO: stderr: "No resources found in kubectl-4982 namespace.\n"
    Feb 20 22:03:58.277: INFO: stdout: ""
    Feb 20 22:03:58.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4982 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 20 22:03:58.442: INFO: stderr: ""
    Feb 20 22:03:58.442: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:03:58.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4982" for this suite. 02/20/23 22:03:58.46
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:03:58.518
Feb 20 22:03:58.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 22:03:58.52
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:58.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:58.628
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b in namespace container-probe-7929 02/20/23 22:03:58.646
Feb 20 22:03:58.759: INFO: Waiting up to 5m0s for pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b" in namespace "container-probe-7929" to be "not pending"
Feb 20 22:03:58.772: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.958807ms
Feb 20 22:04:00.784: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024603818s
Feb 20 22:04:02.784: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b": Phase="Running", Reason="", readiness=true. Elapsed: 4.025292645s
Feb 20 22:04:02.784: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b" satisfied condition "not pending"
Feb 20 22:04:02.785: INFO: Started pod busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b in namespace container-probe-7929
STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 22:04:02.785
Feb 20 22:04:02.796: INFO: Initial restart count of pod busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b is 0
Feb 20 22:04:51.146: INFO: Restart count of pod container-probe-7929/busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b is now 1 (48.349725512s elapsed)
STEP: deleting the pod 02/20/23 22:04:51.146
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 22:04:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7929" for this suite. 02/20/23 22:04:51.2
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":62,"skipped":1122,"failed":0}
------------------------------
• [SLOW TEST] [52.707 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:03:58.518
    Feb 20 22:03:58.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 22:03:58.52
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:03:58.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:03:58.628
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b in namespace container-probe-7929 02/20/23 22:03:58.646
    Feb 20 22:03:58.759: INFO: Waiting up to 5m0s for pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b" in namespace "container-probe-7929" to be "not pending"
    Feb 20 22:03:58.772: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.958807ms
    Feb 20 22:04:00.784: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024603818s
    Feb 20 22:04:02.784: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b": Phase="Running", Reason="", readiness=true. Elapsed: 4.025292645s
    Feb 20 22:04:02.784: INFO: Pod "busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b" satisfied condition "not pending"
    Feb 20 22:04:02.785: INFO: Started pod busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b in namespace container-probe-7929
    STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 22:04:02.785
    Feb 20 22:04:02.796: INFO: Initial restart count of pod busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b is 0
    Feb 20 22:04:51.146: INFO: Restart count of pod container-probe-7929/busybox-a02b7397-ae66-47aa-ad0a-aa89bc01207b is now 1 (48.349725512s elapsed)
    STEP: deleting the pod 02/20/23 22:04:51.146
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 22:04:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7929" for this suite. 02/20/23 22:04:51.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:04:51.234
Feb 20 22:04:51.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 22:04:51.236
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:04:51.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:04:51.296
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-4936dbd3-2d39-4700-bb0c-4c6f51ecd52c 02/20/23 22:04:51.311
STEP: Creating a pod to test consume configMaps 02/20/23 22:04:51.327
Feb 20 22:04:51.385: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81" in namespace "configmap-1782" to be "Succeeded or Failed"
Feb 20 22:04:51.397: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81": Phase="Pending", Reason="", readiness=false. Elapsed: 11.56135ms
Feb 20 22:04:53.410: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024298519s
Feb 20 22:04:55.411: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026004363s
STEP: Saw pod success 02/20/23 22:04:55.412
Feb 20 22:04:55.412: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81" satisfied condition "Succeeded or Failed"
Feb 20 22:04:55.423: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:04:55.455
Feb 20 22:04:55.509: INFO: Waiting for pod pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81 to disappear
Feb 20 22:04:55.522: INFO: Pod pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 22:04:55.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1782" for this suite. 02/20/23 22:04:55.54
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":63,"skipped":1144,"failed":0}
------------------------------
• [4.332 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:04:51.234
    Feb 20 22:04:51.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 22:04:51.236
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:04:51.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:04:51.296
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-4936dbd3-2d39-4700-bb0c-4c6f51ecd52c 02/20/23 22:04:51.311
    STEP: Creating a pod to test consume configMaps 02/20/23 22:04:51.327
    Feb 20 22:04:51.385: INFO: Waiting up to 5m0s for pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81" in namespace "configmap-1782" to be "Succeeded or Failed"
    Feb 20 22:04:51.397: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81": Phase="Pending", Reason="", readiness=false. Elapsed: 11.56135ms
    Feb 20 22:04:53.410: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024298519s
    Feb 20 22:04:55.411: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026004363s
    STEP: Saw pod success 02/20/23 22:04:55.412
    Feb 20 22:04:55.412: INFO: Pod "pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81" satisfied condition "Succeeded or Failed"
    Feb 20 22:04:55.423: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:04:55.455
    Feb 20 22:04:55.509: INFO: Waiting for pod pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81 to disappear
    Feb 20 22:04:55.522: INFO: Pod pod-configmaps-3d1d56ed-1902-4aee-8964-8e9c45550c81 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 22:04:55.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1782" for this suite. 02/20/23 22:04:55.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:04:55.571
Feb 20 22:04:55.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replication-controller 02/20/23 22:04:55.573
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:04:55.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:04:55.68
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 02/20/23 22:04:55.705
STEP: waiting for RC to be added 02/20/23 22:04:55.716
STEP: waiting for available Replicas 02/20/23 22:04:55.717
STEP: patching ReplicationController 02/20/23 22:04:58.172
STEP: waiting for RC to be modified 02/20/23 22:04:58.187
STEP: patching ReplicationController status 02/20/23 22:04:58.187
STEP: waiting for RC to be modified 02/20/23 22:04:58.212
STEP: waiting for available Replicas 02/20/23 22:04:58.213
STEP: fetching ReplicationController status 02/20/23 22:04:58.214
STEP: patching ReplicationController scale 02/20/23 22:04:58.222
STEP: waiting for RC to be modified 02/20/23 22:04:58.233
STEP: waiting for ReplicationController's scale to be the max amount 02/20/23 22:04:58.233
STEP: fetching ReplicationController; ensuring that it's patched 02/20/23 22:04:59.646
STEP: updating ReplicationController status 02/20/23 22:04:59.655
STEP: waiting for RC to be modified 02/20/23 22:04:59.667
STEP: listing all ReplicationControllers 02/20/23 22:04:59.667
STEP: checking that ReplicationController has expected values 02/20/23 22:04:59.676
STEP: deleting ReplicationControllers by collection 02/20/23 22:04:59.676
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/20/23 22:04:59.697
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 20 22:04:59.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6415" for this suite. 02/20/23 22:04:59.883
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":64,"skipped":1156,"failed":0}
------------------------------
• [4.338 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:04:55.571
    Feb 20 22:04:55.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replication-controller 02/20/23 22:04:55.573
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:04:55.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:04:55.68
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 02/20/23 22:04:55.705
    STEP: waiting for RC to be added 02/20/23 22:04:55.716
    STEP: waiting for available Replicas 02/20/23 22:04:55.717
    STEP: patching ReplicationController 02/20/23 22:04:58.172
    STEP: waiting for RC to be modified 02/20/23 22:04:58.187
    STEP: patching ReplicationController status 02/20/23 22:04:58.187
    STEP: waiting for RC to be modified 02/20/23 22:04:58.212
    STEP: waiting for available Replicas 02/20/23 22:04:58.213
    STEP: fetching ReplicationController status 02/20/23 22:04:58.214
    STEP: patching ReplicationController scale 02/20/23 22:04:58.222
    STEP: waiting for RC to be modified 02/20/23 22:04:58.233
    STEP: waiting for ReplicationController's scale to be the max amount 02/20/23 22:04:58.233
    STEP: fetching ReplicationController; ensuring that it's patched 02/20/23 22:04:59.646
    STEP: updating ReplicationController status 02/20/23 22:04:59.655
    STEP: waiting for RC to be modified 02/20/23 22:04:59.667
    STEP: listing all ReplicationControllers 02/20/23 22:04:59.667
    STEP: checking that ReplicationController has expected values 02/20/23 22:04:59.676
    STEP: deleting ReplicationControllers by collection 02/20/23 22:04:59.676
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/20/23 22:04:59.697
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 20 22:04:59.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6415" for this suite. 02/20/23 22:04:59.883
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:04:59.911
Feb 20 22:04:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:04:59.913
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:04:59.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:00.012
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:05:00.026
Feb 20 22:05:00.112: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2" in namespace "downward-api-5334" to be "Succeeded or Failed"
Feb 20 22:05:00.132: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.263345ms
Feb 20 22:05:02.144: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031563488s
Feb 20 22:05:04.174: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061129528s
Feb 20 22:05:06.152: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039931408s
STEP: Saw pod success 02/20/23 22:05:06.153
Feb 20 22:05:06.153: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2" satisfied condition "Succeeded or Failed"
Feb 20 22:05:06.202: INFO: Trying to get logs from node 10.8.38.69 pod downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2 container client-container: <nil>
STEP: delete the pod 02/20/23 22:05:06.298
Feb 20 22:05:06.348: INFO: Waiting for pod downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2 to disappear
Feb 20 22:05:06.367: INFO: Pod downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:05:06.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5334" for this suite. 02/20/23 22:05:06.405
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":65,"skipped":1159,"failed":0}
------------------------------
• [SLOW TEST] [6.540 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:04:59.911
    Feb 20 22:04:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:04:59.913
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:04:59.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:00.012
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:05:00.026
    Feb 20 22:05:00.112: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2" in namespace "downward-api-5334" to be "Succeeded or Failed"
    Feb 20 22:05:00.132: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.263345ms
    Feb 20 22:05:02.144: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031563488s
    Feb 20 22:05:04.174: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061129528s
    Feb 20 22:05:06.152: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039931408s
    STEP: Saw pod success 02/20/23 22:05:06.153
    Feb 20 22:05:06.153: INFO: Pod "downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2" satisfied condition "Succeeded or Failed"
    Feb 20 22:05:06.202: INFO: Trying to get logs from node 10.8.38.69 pod downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:05:06.298
    Feb 20 22:05:06.348: INFO: Waiting for pod downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2 to disappear
    Feb 20 22:05:06.367: INFO: Pod downwardapi-volume-8c2e33f9-2be9-4418-9d55-789b8f0f64a2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:05:06.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5334" for this suite. 02/20/23 22:05:06.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:05:06.513
Feb 20 22:05:06.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replicaset 02/20/23 22:05:06.514
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:05:06.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:06.66
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/20/23 22:05:06.697
STEP: Verify that the required pods have come up. 02/20/23 22:05:06.718
Feb 20 22:05:06.733: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 20 22:05:11.765: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/20/23 22:05:11.765
STEP: Getting /status 02/20/23 22:05:11.765
Feb 20 22:05:11.795: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/20/23 22:05:11.795
Feb 20 22:05:11.857: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/20/23 22:05:11.857
Feb 20 22:05:11.863: INFO: Observed &ReplicaSet event: ADDED
Feb 20 22:05:11.863: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.864: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.864: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.864: INFO: Found replicaset test-rs in namespace replicaset-6965 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 20 22:05:11.864: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/20/23 22:05:11.864
Feb 20 22:05:11.864: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 20 22:05:11.881: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/20/23 22:05:11.881
Feb 20 22:05:11.886: INFO: Observed &ReplicaSet event: ADDED
Feb 20 22:05:11.887: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.887: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.887: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.887: INFO: Observed replicaset test-rs in namespace replicaset-6965 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 20 22:05:11.888: INFO: Observed &ReplicaSet event: MODIFIED
Feb 20 22:05:11.888: INFO: Found replicaset test-rs in namespace replicaset-6965 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 20 22:05:11.888: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 20 22:05:11.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6965" for this suite. 02/20/23 22:05:11.909
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":66,"skipped":1169,"failed":0}
------------------------------
• [SLOW TEST] [5.420 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:05:06.513
    Feb 20 22:05:06.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replicaset 02/20/23 22:05:06.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:05:06.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:06.66
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/20/23 22:05:06.697
    STEP: Verify that the required pods have come up. 02/20/23 22:05:06.718
    Feb 20 22:05:06.733: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 20 22:05:11.765: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/20/23 22:05:11.765
    STEP: Getting /status 02/20/23 22:05:11.765
    Feb 20 22:05:11.795: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/20/23 22:05:11.795
    Feb 20 22:05:11.857: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/20/23 22:05:11.857
    Feb 20 22:05:11.863: INFO: Observed &ReplicaSet event: ADDED
    Feb 20 22:05:11.863: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.864: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.864: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.864: INFO: Found replicaset test-rs in namespace replicaset-6965 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 20 22:05:11.864: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/20/23 22:05:11.864
    Feb 20 22:05:11.864: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 20 22:05:11.881: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/20/23 22:05:11.881
    Feb 20 22:05:11.886: INFO: Observed &ReplicaSet event: ADDED
    Feb 20 22:05:11.887: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.887: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.887: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.887: INFO: Observed replicaset test-rs in namespace replicaset-6965 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 20 22:05:11.888: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 20 22:05:11.888: INFO: Found replicaset test-rs in namespace replicaset-6965 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 20 22:05:11.888: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 20 22:05:11.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6965" for this suite. 02/20/23 22:05:11.909
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:05:11.939
Feb 20 22:05:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir-wrapper 02/20/23 22:05:11.943
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:05:11.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:12
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/20/23 22:05:12.011
STEP: Creating RC which spawns configmap-volume pods 02/20/23 22:05:12.766
Feb 20 22:05:12.798: INFO: Pod name wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390: Found 0 pods out of 5
Feb 20 22:05:17.826: INFO: Pod name wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/20/23 22:05:17.826
Feb 20 22:05:17.827: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-27dwr" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:17.840: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-27dwr": Phase="Running", Reason="", readiness=true. Elapsed: 12.991209ms
Feb 20 22:05:17.840: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-27dwr" satisfied condition "running"
Feb 20 22:05:17.840: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-2whhz" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:17.857: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-2whhz": Phase="Running", Reason="", readiness=true. Elapsed: 17.146655ms
Feb 20 22:05:17.857: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-2whhz" satisfied condition "running"
Feb 20 22:05:17.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-q4mb8" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:17.869: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-q4mb8": Phase="Running", Reason="", readiness=true. Elapsed: 11.563783ms
Feb 20 22:05:17.869: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-q4mb8" satisfied condition "running"
Feb 20 22:05:17.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-skfhc" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:17.882: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-skfhc": Phase="Running", Reason="", readiness=true. Elapsed: 12.396548ms
Feb 20 22:05:17.882: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-skfhc" satisfied condition "running"
Feb 20 22:05:17.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-wbtjk" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:17.895: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-wbtjk": Phase="Running", Reason="", readiness=true. Elapsed: 13.396533ms
Feb 20 22:05:17.895: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-wbtjk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390 in namespace emptydir-wrapper-6661, will wait for the garbage collector to delete the pods 02/20/23 22:05:17.895
Feb 20 22:05:17.969: INFO: Deleting ReplicationController wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390 took: 13.883294ms
Feb 20 22:05:18.183: INFO: Terminating ReplicationController wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390 pods took: 214.090449ms
STEP: Creating RC which spawns configmap-volume pods 02/20/23 22:05:21.903
Feb 20 22:05:21.930: INFO: Pod name wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790: Found 0 pods out of 5
Feb 20 22:05:26.980: INFO: Pod name wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/20/23 22:05:26.98
Feb 20 22:05:26.980: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:26.992: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.627265ms
Feb 20 22:05:29.013: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032348568s
Feb 20 22:05:31.047: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066966913s
Feb 20 22:05:33.035: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05481538s
Feb 20 22:05:35.010: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03011729s
Feb 20 22:05:37.012: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031786316s
Feb 20 22:05:39.012: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031442963s
Feb 20 22:05:41.011: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030501183s
Feb 20 22:05:43.011: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Running", Reason="", readiness=true. Elapsed: 16.030767707s
Feb 20 22:05:43.011: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4" satisfied condition "running"
Feb 20 22:05:43.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-6dmpm" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:43.025: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-6dmpm": Phase="Running", Reason="", readiness=true. Elapsed: 13.839898ms
Feb 20 22:05:43.025: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-6dmpm" satisfied condition "running"
Feb 20 22:05:43.025: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-hpmxn" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:43.037: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-hpmxn": Phase="Running", Reason="", readiness=true. Elapsed: 12.363249ms
Feb 20 22:05:43.038: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-hpmxn" satisfied condition "running"
Feb 20 22:05:43.038: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-j5jvw" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:43.051: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-j5jvw": Phase="Running", Reason="", readiness=true. Elapsed: 13.149014ms
Feb 20 22:05:43.051: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-j5jvw" satisfied condition "running"
Feb 20 22:05:43.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-pbnq5" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:43.063: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-pbnq5": Phase="Running", Reason="", readiness=true. Elapsed: 12.555163ms
Feb 20 22:05:43.063: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-pbnq5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790 in namespace emptydir-wrapper-6661, will wait for the garbage collector to delete the pods 02/20/23 22:05:43.063
Feb 20 22:05:43.167: INFO: Deleting ReplicationController wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790 took: 13.578445ms
Feb 20 22:05:43.267: INFO: Terminating ReplicationController wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790 pods took: 100.121294ms
STEP: Creating RC which spawns configmap-volume pods 02/20/23 22:05:46.208
Feb 20 22:05:46.247: INFO: Pod name wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a: Found 0 pods out of 5
Feb 20 22:05:51.280: INFO: Pod name wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/20/23 22:05:51.28
Feb 20 22:05:51.280: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-bg4b7" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:51.292: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-bg4b7": Phase="Running", Reason="", readiness=true. Elapsed: 12.618175ms
Feb 20 22:05:51.293: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-bg4b7" satisfied condition "running"
Feb 20 22:05:51.293: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-dm66s" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:51.304: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-dm66s": Phase="Running", Reason="", readiness=true. Elapsed: 11.4823ms
Feb 20 22:05:51.304: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-dm66s" satisfied condition "running"
Feb 20 22:05:51.305: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-jv9qd" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:51.318: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-jv9qd": Phase="Running", Reason="", readiness=true. Elapsed: 13.282933ms
Feb 20 22:05:51.318: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-jv9qd" satisfied condition "running"
Feb 20 22:05:51.319: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-m78b4" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:51.330: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-m78b4": Phase="Running", Reason="", readiness=true. Elapsed: 11.406979ms
Feb 20 22:05:51.330: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-m78b4" satisfied condition "running"
Feb 20 22:05:51.330: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-wrjm2" in namespace "emptydir-wrapper-6661" to be "running"
Feb 20 22:05:51.344: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-wrjm2": Phase="Running", Reason="", readiness=true. Elapsed: 13.409941ms
Feb 20 22:05:51.344: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-wrjm2" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a in namespace emptydir-wrapper-6661, will wait for the garbage collector to delete the pods 02/20/23 22:05:51.344
Feb 20 22:05:51.423: INFO: Deleting ReplicationController wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a took: 19.453694ms
Feb 20 22:05:51.524: INFO: Terminating ReplicationController wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a pods took: 101.094752ms
STEP: Cleaning up the configMaps 02/20/23 22:05:54.426
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 20 22:05:55.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6661" for this suite. 02/20/23 22:05:55.419
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":67,"skipped":1169,"failed":0}
------------------------------
• [SLOW TEST] [43.502 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:05:11.939
    Feb 20 22:05:11.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir-wrapper 02/20/23 22:05:11.943
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:05:11.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:12
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/20/23 22:05:12.011
    STEP: Creating RC which spawns configmap-volume pods 02/20/23 22:05:12.766
    Feb 20 22:05:12.798: INFO: Pod name wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390: Found 0 pods out of 5
    Feb 20 22:05:17.826: INFO: Pod name wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/20/23 22:05:17.826
    Feb 20 22:05:17.827: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-27dwr" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:17.840: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-27dwr": Phase="Running", Reason="", readiness=true. Elapsed: 12.991209ms
    Feb 20 22:05:17.840: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-27dwr" satisfied condition "running"
    Feb 20 22:05:17.840: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-2whhz" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:17.857: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-2whhz": Phase="Running", Reason="", readiness=true. Elapsed: 17.146655ms
    Feb 20 22:05:17.857: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-2whhz" satisfied condition "running"
    Feb 20 22:05:17.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-q4mb8" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:17.869: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-q4mb8": Phase="Running", Reason="", readiness=true. Elapsed: 11.563783ms
    Feb 20 22:05:17.869: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-q4mb8" satisfied condition "running"
    Feb 20 22:05:17.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-skfhc" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:17.882: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-skfhc": Phase="Running", Reason="", readiness=true. Elapsed: 12.396548ms
    Feb 20 22:05:17.882: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-skfhc" satisfied condition "running"
    Feb 20 22:05:17.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-wbtjk" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:17.895: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-wbtjk": Phase="Running", Reason="", readiness=true. Elapsed: 13.396533ms
    Feb 20 22:05:17.895: INFO: Pod "wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390-wbtjk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390 in namespace emptydir-wrapper-6661, will wait for the garbage collector to delete the pods 02/20/23 22:05:17.895
    Feb 20 22:05:17.969: INFO: Deleting ReplicationController wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390 took: 13.883294ms
    Feb 20 22:05:18.183: INFO: Terminating ReplicationController wrapped-volume-race-f5b8eef3-9d5e-4a75-bb74-1b13e54f3390 pods took: 214.090449ms
    STEP: Creating RC which spawns configmap-volume pods 02/20/23 22:05:21.903
    Feb 20 22:05:21.930: INFO: Pod name wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790: Found 0 pods out of 5
    Feb 20 22:05:26.980: INFO: Pod name wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/20/23 22:05:26.98
    Feb 20 22:05:26.980: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:26.992: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.627265ms
    Feb 20 22:05:29.013: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032348568s
    Feb 20 22:05:31.047: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066966913s
    Feb 20 22:05:33.035: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05481538s
    Feb 20 22:05:35.010: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03011729s
    Feb 20 22:05:37.012: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031786316s
    Feb 20 22:05:39.012: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.031442963s
    Feb 20 22:05:41.011: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030501183s
    Feb 20 22:05:43.011: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4": Phase="Running", Reason="", readiness=true. Elapsed: 16.030767707s
    Feb 20 22:05:43.011: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-2rvj4" satisfied condition "running"
    Feb 20 22:05:43.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-6dmpm" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:43.025: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-6dmpm": Phase="Running", Reason="", readiness=true. Elapsed: 13.839898ms
    Feb 20 22:05:43.025: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-6dmpm" satisfied condition "running"
    Feb 20 22:05:43.025: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-hpmxn" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:43.037: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-hpmxn": Phase="Running", Reason="", readiness=true. Elapsed: 12.363249ms
    Feb 20 22:05:43.038: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-hpmxn" satisfied condition "running"
    Feb 20 22:05:43.038: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-j5jvw" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:43.051: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-j5jvw": Phase="Running", Reason="", readiness=true. Elapsed: 13.149014ms
    Feb 20 22:05:43.051: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-j5jvw" satisfied condition "running"
    Feb 20 22:05:43.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-pbnq5" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:43.063: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-pbnq5": Phase="Running", Reason="", readiness=true. Elapsed: 12.555163ms
    Feb 20 22:05:43.063: INFO: Pod "wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790-pbnq5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790 in namespace emptydir-wrapper-6661, will wait for the garbage collector to delete the pods 02/20/23 22:05:43.063
    Feb 20 22:05:43.167: INFO: Deleting ReplicationController wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790 took: 13.578445ms
    Feb 20 22:05:43.267: INFO: Terminating ReplicationController wrapped-volume-race-23c0b72c-cafc-42c8-8a5b-79db44e13790 pods took: 100.121294ms
    STEP: Creating RC which spawns configmap-volume pods 02/20/23 22:05:46.208
    Feb 20 22:05:46.247: INFO: Pod name wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a: Found 0 pods out of 5
    Feb 20 22:05:51.280: INFO: Pod name wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/20/23 22:05:51.28
    Feb 20 22:05:51.280: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-bg4b7" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:51.292: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-bg4b7": Phase="Running", Reason="", readiness=true. Elapsed: 12.618175ms
    Feb 20 22:05:51.293: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-bg4b7" satisfied condition "running"
    Feb 20 22:05:51.293: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-dm66s" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:51.304: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-dm66s": Phase="Running", Reason="", readiness=true. Elapsed: 11.4823ms
    Feb 20 22:05:51.304: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-dm66s" satisfied condition "running"
    Feb 20 22:05:51.305: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-jv9qd" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:51.318: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-jv9qd": Phase="Running", Reason="", readiness=true. Elapsed: 13.282933ms
    Feb 20 22:05:51.318: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-jv9qd" satisfied condition "running"
    Feb 20 22:05:51.319: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-m78b4" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:51.330: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-m78b4": Phase="Running", Reason="", readiness=true. Elapsed: 11.406979ms
    Feb 20 22:05:51.330: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-m78b4" satisfied condition "running"
    Feb 20 22:05:51.330: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-wrjm2" in namespace "emptydir-wrapper-6661" to be "running"
    Feb 20 22:05:51.344: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-wrjm2": Phase="Running", Reason="", readiness=true. Elapsed: 13.409941ms
    Feb 20 22:05:51.344: INFO: Pod "wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a-wrjm2" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a in namespace emptydir-wrapper-6661, will wait for the garbage collector to delete the pods 02/20/23 22:05:51.344
    Feb 20 22:05:51.423: INFO: Deleting ReplicationController wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a took: 19.453694ms
    Feb 20 22:05:51.524: INFO: Terminating ReplicationController wrapped-volume-race-290a5214-9aad-4e95-862d-ddf4d8c0ac6a pods took: 101.094752ms
    STEP: Cleaning up the configMaps 02/20/23 22:05:54.426
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:05:55.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6661" for this suite. 02/20/23 22:05:55.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:05:55.448
Feb 20 22:05:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:05:55.45
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:05:55.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:55.536
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:05:55.547
Feb 20 22:05:55.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71" in namespace "projected-9182" to be "Succeeded or Failed"
Feb 20 22:05:55.643: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Pending", Reason="", readiness=false. Elapsed: 13.450869ms
Feb 20 22:05:57.656: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026125211s
Feb 20 22:05:59.691: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061750237s
Feb 20 22:06:01.660: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030618315s
STEP: Saw pod success 02/20/23 22:06:01.66
Feb 20 22:06:01.660: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71" satisfied condition "Succeeded or Failed"
Feb 20 22:06:01.691: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71 container client-container: <nil>
STEP: delete the pod 02/20/23 22:06:01.741
Feb 20 22:06:01.799: INFO: Waiting for pod downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71 to disappear
Feb 20 22:06:01.811: INFO: Pod downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 22:06:01.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9182" for this suite. 02/20/23 22:06:01.828
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":68,"skipped":1188,"failed":0}
------------------------------
• [SLOW TEST] [6.418 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:05:55.448
    Feb 20 22:05:55.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:05:55.45
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:05:55.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:05:55.536
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:05:55.547
    Feb 20 22:05:55.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71" in namespace "projected-9182" to be "Succeeded or Failed"
    Feb 20 22:05:55.643: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Pending", Reason="", readiness=false. Elapsed: 13.450869ms
    Feb 20 22:05:57.656: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026125211s
    Feb 20 22:05:59.691: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061750237s
    Feb 20 22:06:01.660: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030618315s
    STEP: Saw pod success 02/20/23 22:06:01.66
    Feb 20 22:06:01.660: INFO: Pod "downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71" satisfied condition "Succeeded or Failed"
    Feb 20 22:06:01.691: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:06:01.741
    Feb 20 22:06:01.799: INFO: Waiting for pod downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71 to disappear
    Feb 20 22:06:01.811: INFO: Pod downwardapi-volume-137ddc74-d628-476a-b82a-19c9f5d91b71 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 22:06:01.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9182" for this suite. 02/20/23 22:06:01.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:06:01.867
Feb 20 22:06:01.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:06:01.878
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:01.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:01.958
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:06:02.018
Feb 20 22:06:02.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52" in namespace "projected-4035" to be "Succeeded or Failed"
Feb 20 22:06:02.148: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Pending", Reason="", readiness=false. Elapsed: 24.88346ms
Feb 20 22:06:04.164: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041085444s
Feb 20 22:06:06.162: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039329941s
Feb 20 22:06:08.162: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038819217s
STEP: Saw pod success 02/20/23 22:06:08.162
Feb 20 22:06:08.162: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52" satisfied condition "Succeeded or Failed"
Feb 20 22:06:08.174: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52 container client-container: <nil>
STEP: delete the pod 02/20/23 22:06:08.196
Feb 20 22:06:08.230: INFO: Waiting for pod downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52 to disappear
Feb 20 22:06:08.242: INFO: Pod downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 22:06:08.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4035" for this suite. 02/20/23 22:06:08.262
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":69,"skipped":1198,"failed":0}
------------------------------
• [SLOW TEST] [6.421 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:06:01.867
    Feb 20 22:06:01.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:06:01.878
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:01.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:01.958
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:06:02.018
    Feb 20 22:06:02.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52" in namespace "projected-4035" to be "Succeeded or Failed"
    Feb 20 22:06:02.148: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Pending", Reason="", readiness=false. Elapsed: 24.88346ms
    Feb 20 22:06:04.164: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041085444s
    Feb 20 22:06:06.162: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039329941s
    Feb 20 22:06:08.162: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038819217s
    STEP: Saw pod success 02/20/23 22:06:08.162
    Feb 20 22:06:08.162: INFO: Pod "downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52" satisfied condition "Succeeded or Failed"
    Feb 20 22:06:08.174: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:06:08.196
    Feb 20 22:06:08.230: INFO: Waiting for pod downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52 to disappear
    Feb 20 22:06:08.242: INFO: Pod downwardapi-volume-01360a78-bc2a-476c-acfd-06fd8ce0ec52 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 22:06:08.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4035" for this suite. 02/20/23 22:06:08.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:06:08.299
Feb 20 22:06:08.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sysctl 02/20/23 22:06:08.301
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:08.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:08.381
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/20/23 22:06:08.395
W0220 22:06:08.448673      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for error events or started pod 02/20/23 22:06:08.448
STEP: Waiting for pod completion 02/20/23 22:06:10.461
Feb 20 22:06:10.462: INFO: Waiting up to 3m0s for pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26" in namespace "sysctl-9259" to be "completed"
Feb 20 22:06:10.474: INFO: Pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26": Phase="Pending", Reason="", readiness=false. Elapsed: 12.282589ms
Feb 20 22:06:12.490: INFO: Pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02818381s
Feb 20 22:06:12.490: INFO: Pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/20/23 22:06:12.501
STEP: Getting logs from the pod 02/20/23 22:06:12.501
STEP: Checking that the sysctl is actually updated 02/20/23 22:06:12.523
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 22:06:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9259" for this suite. 02/20/23 22:06:12.542
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":70,"skipped":1256,"failed":0}
------------------------------
• [4.268 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:06:08.299
    Feb 20 22:06:08.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sysctl 02/20/23 22:06:08.301
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:08.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:08.381
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/20/23 22:06:08.395
    W0220 22:06:08.448673      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for error events or started pod 02/20/23 22:06:08.448
    STEP: Waiting for pod completion 02/20/23 22:06:10.461
    Feb 20 22:06:10.462: INFO: Waiting up to 3m0s for pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26" in namespace "sysctl-9259" to be "completed"
    Feb 20 22:06:10.474: INFO: Pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26": Phase="Pending", Reason="", readiness=false. Elapsed: 12.282589ms
    Feb 20 22:06:12.490: INFO: Pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02818381s
    Feb 20 22:06:12.490: INFO: Pod "sysctl-ea5b4a53-5606-4cfd-8a5e-2d0dc4053f26" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/20/23 22:06:12.501
    STEP: Getting logs from the pod 02/20/23 22:06:12.501
    STEP: Checking that the sysctl is actually updated 02/20/23 22:06:12.523
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 22:06:12.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9259" for this suite. 02/20/23 22:06:12.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:06:12.572
Feb 20 22:06:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 22:06:12.575
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:12.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:12.638
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 02/20/23 22:06:12.72
STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:06:12.732
Feb 20 22:06:12.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:06:12.758: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:06:13.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:06:13.797: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:06:14.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 22:06:14.790: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 22:06:15.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 22:06:15.791: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/20/23 22:06:15.799
STEP: DeleteCollection of the DaemonSets 02/20/23 22:06:15.821
STEP: Verify that ReplicaSets have been deleted 02/20/23 22:06:15.835
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Feb 20 22:06:15.858: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77534"},"items":null}

Feb 20 22:06:15.877: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77536"},"items":[{"metadata":{"name":"daemon-set-fxcdj","generateName":"daemon-set-","namespace":"daemonsets-1703","uid":"6665e6a4-588b-435f-ae3a-de7550e71e7e","resourceVersion":"77535","creationTimestamp":"2023-02-20T22:06:12Z","deletionTimestamp":"2023-02-20T22:06:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"31b0b4cc89998baa050f455473433168c4810f3629e810a903541e238c9c7efd","cni.projectcalico.org/podIP":"172.30.144.220/32","cni.projectcalico.org/podIPs":"172.30.144.220/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.144.220\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.144.220\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"94b07d2f-6ddd-4a97-a82b-eed85d62c08a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94b07d2f-6ddd-4a97-a82b-eed85d62c08a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fg6np","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fg6np","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.8.38.69","securityContext":{"seLinuxOptions":{"level":"s0:c40,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-kmvzl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.8.38.69"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"}],"hostIP":"10.8.38.69","podIP":"172.30.144.220","podIPs":[{"ip":"172.30.144.220"}],"startTime":"2023-02-20T22:06:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-20T22:06:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b01bf5f00279519304e57094ee4a00a78e4a19a1ff24f989dbac8c2c6c605a54","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-l65pc","generateName":"daemon-set-","namespace":"daemonsets-1703","uid":"633e89b2-1bb6-4c19-8dfa-9f5df513834f","resourceVersion":"77520","creationTimestamp":"2023-02-20T22:06:12Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"500748ebb8f04d3047b4f14f8343792760a9f5bc4e68a72ba3feb0eb0c7770e0","cni.projectcalico.org/podIP":"172.30.31.191/32","cni.projectcalico.org/podIPs":"172.30.31.191/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.31.191\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.31.191\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"94b07d2f-6ddd-4a97-a82b-eed85d62c08a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94b07d2f-6ddd-4a97-a82b-eed85d62c08a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cgl6h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cgl6h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.8.38.70","securityContext":{"seLinuxOptions":{"level":"s0:c40,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-kmvzl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.8.38.70"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"}],"hostIP":"10.8.38.70","podIP":"172.30.31.191","podIPs":[{"ip":"172.30.31.191"}],"startTime":"2023-02-20T22:06:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-20T22:06:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://76c7092eca769c13d5bf9148af72db0d464507142a6a1aad8837f5a7b0d9c302","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tdgrc","generateName":"daemon-set-","namespace":"daemonsets-1703","uid":"95a8d701-9ea0-455b-bac3-8f15c8cae843","resourceVersion":"77504","creationTimestamp":"2023-02-20T22:06:12Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"551d1395ed02b756a315f89bdd333eef42e553e1b955ccd6148c3545524f4386","cni.projectcalico.org/podIP":"172.30.181.236/32","cni.projectcalico.org/podIPs":"172.30.181.236/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.236\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.236\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"94b07d2f-6ddd-4a97-a82b-eed85d62c08a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94b07d2f-6ddd-4a97-a82b-eed85d62c08a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pcs6g","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pcs6g","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.8.38.66","securityContext":{"seLinuxOptions":{"level":"s0:c40,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-kmvzl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.8.38.66"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"}],"hostIP":"10.8.38.66","podIP":"172.30.181.236","podIPs":[{"ip":"172.30.181.236"}],"startTime":"2023-02-20T22:06:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-20T22:06:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://02d94f70bb7a99615a63bacd44181fb784ea77c48ecee463e63d8f42866c566b","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:06:15.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1703" for this suite. 02/20/23 22:06:15.953
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":71,"skipped":1301,"failed":0}
------------------------------
• [3.407 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:06:12.572
    Feb 20 22:06:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 22:06:12.575
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:12.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:12.638
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 02/20/23 22:06:12.72
    STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:06:12.732
    Feb 20 22:06:12.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:06:12.758: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:06:13.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:06:13.797: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:06:14.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 22:06:14.790: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 22:06:15.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 22:06:15.791: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/20/23 22:06:15.799
    STEP: DeleteCollection of the DaemonSets 02/20/23 22:06:15.821
    STEP: Verify that ReplicaSets have been deleted 02/20/23 22:06:15.835
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Feb 20 22:06:15.858: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77534"},"items":null}

    Feb 20 22:06:15.877: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77536"},"items":[{"metadata":{"name":"daemon-set-fxcdj","generateName":"daemon-set-","namespace":"daemonsets-1703","uid":"6665e6a4-588b-435f-ae3a-de7550e71e7e","resourceVersion":"77535","creationTimestamp":"2023-02-20T22:06:12Z","deletionTimestamp":"2023-02-20T22:06:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"31b0b4cc89998baa050f455473433168c4810f3629e810a903541e238c9c7efd","cni.projectcalico.org/podIP":"172.30.144.220/32","cni.projectcalico.org/podIPs":"172.30.144.220/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.144.220\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.144.220\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"94b07d2f-6ddd-4a97-a82b-eed85d62c08a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94b07d2f-6ddd-4a97-a82b-eed85d62c08a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fg6np","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fg6np","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.8.38.69","securityContext":{"seLinuxOptions":{"level":"s0:c40,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-kmvzl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.8.38.69"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"}],"hostIP":"10.8.38.69","podIP":"172.30.144.220","podIPs":[{"ip":"172.30.144.220"}],"startTime":"2023-02-20T22:06:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-20T22:06:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b01bf5f00279519304e57094ee4a00a78e4a19a1ff24f989dbac8c2c6c605a54","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-l65pc","generateName":"daemon-set-","namespace":"daemonsets-1703","uid":"633e89b2-1bb6-4c19-8dfa-9f5df513834f","resourceVersion":"77520","creationTimestamp":"2023-02-20T22:06:12Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"500748ebb8f04d3047b4f14f8343792760a9f5bc4e68a72ba3feb0eb0c7770e0","cni.projectcalico.org/podIP":"172.30.31.191/32","cni.projectcalico.org/podIPs":"172.30.31.191/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.31.191\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.31.191\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"94b07d2f-6ddd-4a97-a82b-eed85d62c08a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94b07d2f-6ddd-4a97-a82b-eed85d62c08a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cgl6h","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cgl6h","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.8.38.70","securityContext":{"seLinuxOptions":{"level":"s0:c40,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-kmvzl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.8.38.70"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"}],"hostIP":"10.8.38.70","podIP":"172.30.31.191","podIPs":[{"ip":"172.30.31.191"}],"startTime":"2023-02-20T22:06:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-20T22:06:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://76c7092eca769c13d5bf9148af72db0d464507142a6a1aad8837f5a7b0d9c302","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tdgrc","generateName":"daemon-set-","namespace":"daemonsets-1703","uid":"95a8d701-9ea0-455b-bac3-8f15c8cae843","resourceVersion":"77504","creationTimestamp":"2023-02-20T22:06:12Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"551d1395ed02b756a315f89bdd333eef42e553e1b955ccd6148c3545524f4386","cni.projectcalico.org/podIP":"172.30.181.236/32","cni.projectcalico.org/podIPs":"172.30.181.236/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.236\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.236\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"94b07d2f-6ddd-4a97-a82b-eed85d62c08a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94b07d2f-6ddd-4a97-a82b-eed85d62c08a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-20T22:06:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pcs6g","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pcs6g","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.8.38.66","securityContext":{"seLinuxOptions":{"level":"s0:c40,c25"}},"imagePullSecrets":[{"name":"default-dockercfg-kmvzl"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.8.38.66"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-20T22:06:12Z"}],"hostIP":"10.8.38.66","podIP":"172.30.181.236","podIPs":[{"ip":"172.30.181.236"}],"startTime":"2023-02-20T22:06:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-20T22:06:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://02d94f70bb7a99615a63bacd44181fb784ea77c48ecee463e63d8f42866c566b","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:06:15.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1703" for this suite. 02/20/23 22:06:15.953
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:06:15.982
Feb 20 22:06:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:06:15.986
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:16.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:16.062
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:06:16.12
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:06:16.813
STEP: Deploying the webhook pod 02/20/23 22:06:16.842
STEP: Wait for the deployment to be ready 02/20/23 22:06:16.869
Feb 20 22:06:16.892: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/20/23 22:06:18.926
STEP: Verifying the service has paired with the endpoint 02/20/23 22:06:18.957
Feb 20 22:06:19.957: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Feb 20 22:06:19.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9534-crds.webhook.example.com via the AdmissionRegistration API 02/20/23 22:06:20.509
STEP: Creating a custom resource that should be mutated by the webhook 02/20/23 22:06:20.55
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:06:23.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7564" for this suite. 02/20/23 22:06:23.355
STEP: Destroying namespace "webhook-7564-markers" for this suite. 02/20/23 22:06:23.383
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":72,"skipped":1305,"failed":0}
------------------------------
• [SLOW TEST] [7.552 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:06:15.982
    Feb 20 22:06:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:06:15.986
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:16.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:16.062
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:06:16.12
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:06:16.813
    STEP: Deploying the webhook pod 02/20/23 22:06:16.842
    STEP: Wait for the deployment to be ready 02/20/23 22:06:16.869
    Feb 20 22:06:16.892: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/20/23 22:06:18.926
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:06:18.957
    Feb 20 22:06:19.957: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Feb 20 22:06:19.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9534-crds.webhook.example.com via the AdmissionRegistration API 02/20/23 22:06:20.509
    STEP: Creating a custom resource that should be mutated by the webhook 02/20/23 22:06:20.55
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:06:23.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7564" for this suite. 02/20/23 22:06:23.355
    STEP: Destroying namespace "webhook-7564-markers" for this suite. 02/20/23 22:06:23.383
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:06:23.535
Feb 20 22:06:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:06:23.537
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:23.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:23.611
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 02/20/23 22:06:23.623
Feb 20 22:06:23.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9072 create -f -'
Feb 20 22:06:26.308: INFO: stderr: ""
Feb 20 22:06:26.308: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/20/23 22:06:26.308
Feb 20 22:06:27.321: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:06:27.321: INFO: Found 0 / 1
Feb 20 22:06:28.321: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:06:28.321: INFO: Found 0 / 1
Feb 20 22:06:29.321: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:06:29.321: INFO: Found 1 / 1
Feb 20 22:06:29.321: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/20/23 22:06:29.321
Feb 20 22:06:29.333: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:06:29.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 22:06:29.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9072 patch pod agnhost-primary-bmlrw -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 20 22:06:29.545: INFO: stderr: ""
Feb 20 22:06:29.545: INFO: stdout: "pod/agnhost-primary-bmlrw patched\n"
STEP: checking annotations 02/20/23 22:06:29.545
Feb 20 22:06:29.560: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:06:29.560: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:06:29.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9072" for this suite. 02/20/23 22:06:29.584
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":73,"skipped":1314,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:06:23.535
    Feb 20 22:06:23.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:06:23.537
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:23.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:23.611
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 02/20/23 22:06:23.623
    Feb 20 22:06:23.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9072 create -f -'
    Feb 20 22:06:26.308: INFO: stderr: ""
    Feb 20 22:06:26.308: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/20/23 22:06:26.308
    Feb 20 22:06:27.321: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:06:27.321: INFO: Found 0 / 1
    Feb 20 22:06:28.321: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:06:28.321: INFO: Found 0 / 1
    Feb 20 22:06:29.321: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:06:29.321: INFO: Found 1 / 1
    Feb 20 22:06:29.321: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/20/23 22:06:29.321
    Feb 20 22:06:29.333: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:06:29.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 20 22:06:29.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9072 patch pod agnhost-primary-bmlrw -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 20 22:06:29.545: INFO: stderr: ""
    Feb 20 22:06:29.545: INFO: stdout: "pod/agnhost-primary-bmlrw patched\n"
    STEP: checking annotations 02/20/23 22:06:29.545
    Feb 20 22:06:29.560: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:06:29.560: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:06:29.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9072" for this suite. 02/20/23 22:06:29.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:06:29.631
Feb 20 22:06:29.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename endpointslice 02/20/23 22:06:29.633
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:29.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:29.709
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 02/20/23 22:06:34.996
STEP: referencing matching pods with named port 02/20/23 22:06:40.019
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/20/23 22:06:45.042
STEP: recreating EndpointSlices after they've been deleted 02/20/23 22:06:50.064
Feb 20 22:06:50.107: INFO: EndpointSlice for Service endpointslice-4024/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 20 22:07:00.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4024" for this suite. 02/20/23 22:07:00.166
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":74,"skipped":1349,"failed":0}
------------------------------
• [SLOW TEST] [30.572 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:06:29.631
    Feb 20 22:06:29.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename endpointslice 02/20/23 22:06:29.633
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:06:29.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:06:29.709
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 02/20/23 22:06:34.996
    STEP: referencing matching pods with named port 02/20/23 22:06:40.019
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/20/23 22:06:45.042
    STEP: recreating EndpointSlices after they've been deleted 02/20/23 22:06:50.064
    Feb 20 22:06:50.107: INFO: EndpointSlice for Service endpointslice-4024/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 20 22:07:00.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4024" for this suite. 02/20/23 22:07:00.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:07:00.215
Feb 20 22:07:00.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 22:07:00.217
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:07:00.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:07:00.31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db in namespace container-probe-6968 02/20/23 22:07:00.323
Feb 20 22:07:00.413: INFO: Waiting up to 5m0s for pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db" in namespace "container-probe-6968" to be "not pending"
Feb 20 22:07:00.430: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db": Phase="Pending", Reason="", readiness=false. Elapsed: 17.000011ms
Feb 20 22:07:02.445: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032094819s
Feb 20 22:07:04.444: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db": Phase="Running", Reason="", readiness=true. Elapsed: 4.030752861s
Feb 20 22:07:04.444: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db" satisfied condition "not pending"
Feb 20 22:07:04.444: INFO: Started pod test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db in namespace container-probe-6968
STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 22:07:04.444
Feb 20 22:07:04.456: INFO: Initial restart count of pod test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db is 0
STEP: deleting the pod 02/20/23 22:11:06.208
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 22:11:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6968" for this suite. 02/20/23 22:11:06.267
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":75,"skipped":1427,"failed":0}
------------------------------
• [SLOW TEST] [246.081 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:07:00.215
    Feb 20 22:07:00.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 22:07:00.217
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:07:00.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:07:00.31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db in namespace container-probe-6968 02/20/23 22:07:00.323
    Feb 20 22:07:00.413: INFO: Waiting up to 5m0s for pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db" in namespace "container-probe-6968" to be "not pending"
    Feb 20 22:07:00.430: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db": Phase="Pending", Reason="", readiness=false. Elapsed: 17.000011ms
    Feb 20 22:07:02.445: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032094819s
    Feb 20 22:07:04.444: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db": Phase="Running", Reason="", readiness=true. Elapsed: 4.030752861s
    Feb 20 22:07:04.444: INFO: Pod "test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db" satisfied condition "not pending"
    Feb 20 22:07:04.444: INFO: Started pod test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db in namespace container-probe-6968
    STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 22:07:04.444
    Feb 20 22:07:04.456: INFO: Initial restart count of pod test-webserver-d2e6cf3b-8c7a-440f-a15a-5c9a29bda2db is 0
    STEP: deleting the pod 02/20/23 22:11:06.208
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 22:11:06.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6968" for this suite. 02/20/23 22:11:06.267
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:06.304
Feb 20 22:11:06.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-pred 02/20/23 22:11:06.306
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:06.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:06.37
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 20 22:11:06.381: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 22:11:06.410: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 22:11:06.503: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.66 before test
Feb 20 22:11:06.563: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:11:06.563: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:11:06.563: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:11:06.563: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:11:06.563: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:11:06.563: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:11:06.563: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:11:06.563: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:11:06.563: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:11:06.563: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:11:06.563: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:11:06.563: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:11:06.563: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 20 22:11:06.563: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:11:06.563: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container router ready: true, restart count 0
Feb 20 22:11:06.563: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container migrator ready: true, restart count 0
Feb 20 22:11:06.563: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:11:06.563: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:11:06.563: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:11:06.563: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:11:06.563: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.563: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:11:06.563: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:11:06.564: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:11:06.564: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 20 22:11:06.564: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:11:06.564: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:11:06.564: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:11:06.564: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:11:06.564: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:11:06.564: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:11:06.564: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 20 22:11:06.564: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:11:06.564: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.69 before test
Feb 20 22:11:06.607: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 20 22:11:06.607: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:11:06.607: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 20 22:11:06.607: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:11:06.607: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:11:06.607: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:11:06.607: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 20 22:11:06.607: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:11:06.607: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 20 22:11:06.607: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container vpn ready: true, restart count 0
Feb 20 22:11:06.607: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 20 22:11:06.607: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:11:06.607: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 20 22:11:06.607: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 20 22:11:06.607: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Feb 20 22:11:06.607: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Feb 20 22:11:06.607: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container console-operator ready: true, restart count 1
Feb 20 22:11:06.607: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Feb 20 22:11:06.607: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container console ready: true, restart count 0
Feb 20 22:11:06.607: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container dns-operator ready: true, restart count 0
Feb 20 22:11:06.607: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.607: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.607: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:11:06.607: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:11:06.608: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 20 22:11:06.608: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:11:06.608: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:11:06.608: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container insights-operator ready: true, restart count 1
Feb 20 22:11:06.608: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Feb 20 22:11:06.608: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 20 22:11:06.608: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:11:06.608: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:11:06.608: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:11:06.608: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:11:06.608: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:11:06.608: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:11:06.608: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container check-endpoints ready: true, restart count 0
Feb 20 22:11:06.608: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:11:06.608: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container network-operator ready: true, restart count 1
Feb 20 22:11:06.608: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 20 22:11:06.608: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:11:06.608: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:11:06.608: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container olm-operator ready: true, restart count 0
Feb 20 22:11:06.608: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container package-server-manager ready: true, restart count 0
Feb 20 22:11:06.608: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:11:06.608: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container metrics ready: true, restart count 1
Feb 20 22:11:06.608: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container push-gateway ready: true, restart count 0
Feb 20 22:11:06.608: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container service-ca-operator ready: true, restart count 1
Feb 20 22:11:06.608: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container e2e ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:11:06.608: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:11:06.608: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:11:06.608: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.609: INFO: 	Container tigera-operator ready: true, restart count 2
Feb 20 22:11:06.609: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.70 before test
Feb 20 22:11:06.667: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:11:06.667: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:11:06.667: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:11:06.667: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:11:06.667: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:11:06.667: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:11:06.667: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:11:06.667: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:11:06.667: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:11:06.667: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container console ready: true, restart count 0
Feb 20 22:11:06.667: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:11:06.667: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:11:06.667: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container registry ready: true, restart count 0
Feb 20 22:11:06.667: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:11:06.667: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:11:06.667: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container router ready: true, restart count 0
Feb 20 22:11:06.667: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:11:06.667: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:11:06.667: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 20 22:11:06.667: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:11:06.667: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 20 22:11:06.667: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:11:06.667: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:11:06.667: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container reload ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 20 22:11:06.667: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:11:06.667: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:11:06.667: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:11:06.667: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:11:06.667: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:11:06.667: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:11:06.667: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:11:06.667: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:11:06.667: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.667: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:11:06.667: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
Feb 20 22:11:06.668: INFO: 	Container service-ca-controller ready: true, restart count 0
Feb 20 22:11:06.668: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:11:06.668: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:11:06.668: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/20/23 22:11:06.668
Feb 20 22:11:06.994: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1542" to be "running"
Feb 20 22:11:07.014: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.275951ms
Feb 20 22:11:09.027: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.033101667s
Feb 20 22:11:09.027: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/20/23 22:11:09.038
STEP: Trying to apply a random label on the found node. 02/20/23 22:11:09.062
STEP: verifying the node has the label kubernetes.io/e2e-1683451f-00b4-4e7a-a7ed-3805f0d09301 42 02/20/23 22:11:09.1
STEP: Trying to relaunch the pod, now with labels. 02/20/23 22:11:09.115
Feb 20 22:11:09.146: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1542" to be "not pending"
Feb 20 22:11:09.157: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 10.979655ms
Feb 20 22:11:11.170: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.023749985s
Feb 20 22:11:11.170: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-1683451f-00b4-4e7a-a7ed-3805f0d09301 off the node 10.8.38.66 02/20/23 22:11:11.181
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1683451f-00b4-4e7a-a7ed-3805f0d09301 02/20/23 22:11:11.22
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:11:11.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1542" for this suite. 02/20/23 22:11:11.258
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":76,"skipped":1430,"failed":0}
------------------------------
• [SLOW TEST] [5.040 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:06.304
    Feb 20 22:11:06.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-pred 02/20/23 22:11:06.306
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:06.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:06.37
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 20 22:11:06.381: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 20 22:11:06.410: INFO: Waiting for terminating namespaces to be deleted...
    Feb 20 22:11:06.503: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.66 before test
    Feb 20 22:11:06.563: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container pvc-permissions ready: false, restart count 0
    Feb 20 22:11:06.563: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container migrator ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:11:06.563: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.563: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:11:06.563: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container prometheus-operator ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:11:06.564: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.69 before test
    Feb 20 22:11:06.607: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container vpn ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Feb 20 22:11:06.607: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container console-operator ready: true, restart count 1
    Feb 20 22:11:06.607: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Feb 20 22:11:06.607: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container dns-operator ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.607: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:11:06.607: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container ingress-operator ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container insights-operator ready: true, restart count 1
    Feb 20 22:11:06.608: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Feb 20 22:11:06.608: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container marketplace-operator ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container check-endpoints ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container network-operator ready: true, restart count 1
    Feb 20 22:11:06.608: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container catalog-operator ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:11:06.608: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:11:06.608: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container olm-operator ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container package-server-manager ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container metrics ready: true, restart count 1
    Feb 20 22:11:06.608: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container push-gateway ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container service-ca-operator ready: true, restart count 1
    Feb 20 22:11:06.608: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container e2e ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:11:06.608: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.609: INFO: 	Container tigera-operator ready: true, restart count 2
    Feb 20 22:11:06.609: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.70 before test
    Feb 20 22:11:06.667: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container registry ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:11:06.667: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container reload ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container telemeter-client ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:11:06.667: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.667: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:11:06.667: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
    Feb 20 22:11:06.668: INFO: 	Container service-ca-controller ready: true, restart count 0
    Feb 20 22:11:06.668: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:11:06.668: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:11:06.668: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/20/23 22:11:06.668
    Feb 20 22:11:06.994: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1542" to be "running"
    Feb 20 22:11:07.014: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 20.275951ms
    Feb 20 22:11:09.027: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.033101667s
    Feb 20 22:11:09.027: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/20/23 22:11:09.038
    STEP: Trying to apply a random label on the found node. 02/20/23 22:11:09.062
    STEP: verifying the node has the label kubernetes.io/e2e-1683451f-00b4-4e7a-a7ed-3805f0d09301 42 02/20/23 22:11:09.1
    STEP: Trying to relaunch the pod, now with labels. 02/20/23 22:11:09.115
    Feb 20 22:11:09.146: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-1542" to be "not pending"
    Feb 20 22:11:09.157: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 10.979655ms
    Feb 20 22:11:11.170: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.023749985s
    Feb 20 22:11:11.170: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-1683451f-00b4-4e7a-a7ed-3805f0d09301 off the node 10.8.38.66 02/20/23 22:11:11.181
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1683451f-00b4-4e7a-a7ed-3805f0d09301 02/20/23 22:11:11.22
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:11:11.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1542" for this suite. 02/20/23 22:11:11.258
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:11.353
Feb 20 22:11:11.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename containers 02/20/23 22:11:11.355
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:11.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:11.474
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 02/20/23 22:11:11.488
Feb 20 22:11:11.593: INFO: Waiting up to 5m0s for pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817" in namespace "containers-339" to be "Succeeded or Failed"
Feb 20 22:11:11.610: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817": Phase="Pending", Reason="", readiness=false. Elapsed: 16.088088ms
Feb 20 22:11:13.624: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030527734s
Feb 20 22:11:15.624: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030296817s
STEP: Saw pod success 02/20/23 22:11:15.624
Feb 20 22:11:15.624: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817" satisfied condition "Succeeded or Failed"
Feb 20 22:11:15.635: INFO: Trying to get logs from node 10.8.38.70 pod client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:11:15.677
Feb 20 22:11:15.705: INFO: Waiting for pod client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817 to disappear
Feb 20 22:11:15.718: INFO: Pod client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 20 22:11:15.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-339" for this suite. 02/20/23 22:11:15.737
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":77,"skipped":1436,"failed":0}
------------------------------
• [4.411 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:11.353
    Feb 20 22:11:11.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename containers 02/20/23 22:11:11.355
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:11.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:11.474
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 02/20/23 22:11:11.488
    Feb 20 22:11:11.593: INFO: Waiting up to 5m0s for pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817" in namespace "containers-339" to be "Succeeded or Failed"
    Feb 20 22:11:11.610: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817": Phase="Pending", Reason="", readiness=false. Elapsed: 16.088088ms
    Feb 20 22:11:13.624: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030527734s
    Feb 20 22:11:15.624: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030296817s
    STEP: Saw pod success 02/20/23 22:11:15.624
    Feb 20 22:11:15.624: INFO: Pod "client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817" satisfied condition "Succeeded or Failed"
    Feb 20 22:11:15.635: INFO: Trying to get logs from node 10.8.38.70 pod client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:11:15.677
    Feb 20 22:11:15.705: INFO: Waiting for pod client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817 to disappear
    Feb 20 22:11:15.718: INFO: Pod client-containers-f39882d4-6332-4e8f-93b0-8846a1bfa817 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 20 22:11:15.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-339" for this suite. 02/20/23 22:11:15.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:15.769
Feb 20 22:11:15.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 22:11:15.771
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:15.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:15.854
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-87de507c-929d-4448-bab2-c712f37da10a 02/20/23 22:11:15.866
STEP: Creating a pod to test consume configMaps 02/20/23 22:11:15.881
Feb 20 22:11:15.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c" in namespace "configmap-8527" to be "Succeeded or Failed"
Feb 20 22:11:15.961: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.1371ms
Feb 20 22:11:17.974: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024262758s
Feb 20 22:11:19.974: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024171362s
STEP: Saw pod success 02/20/23 22:11:19.974
Feb 20 22:11:19.975: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c" satisfied condition "Succeeded or Failed"
Feb 20 22:11:19.986: INFO: Trying to get logs from node 10.8.38.70 pod pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:11:20.011
Feb 20 22:11:20.048: INFO: Waiting for pod pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c to disappear
Feb 20 22:11:20.058: INFO: Pod pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 22:11:20.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8527" for this suite. 02/20/23 22:11:20.077
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":78,"skipped":1450,"failed":0}
------------------------------
• [4.330 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:15.769
    Feb 20 22:11:15.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 22:11:15.771
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:15.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:15.854
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-87de507c-929d-4448-bab2-c712f37da10a 02/20/23 22:11:15.866
    STEP: Creating a pod to test consume configMaps 02/20/23 22:11:15.881
    Feb 20 22:11:15.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c" in namespace "configmap-8527" to be "Succeeded or Failed"
    Feb 20 22:11:15.961: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.1371ms
    Feb 20 22:11:17.974: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024262758s
    Feb 20 22:11:19.974: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024171362s
    STEP: Saw pod success 02/20/23 22:11:19.974
    Feb 20 22:11:19.975: INFO: Pod "pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c" satisfied condition "Succeeded or Failed"
    Feb 20 22:11:19.986: INFO: Trying to get logs from node 10.8.38.70 pod pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:11:20.011
    Feb 20 22:11:20.048: INFO: Waiting for pod pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c to disappear
    Feb 20 22:11:20.058: INFO: Pod pod-configmaps-b6b17a27-5934-474b-9e13-87710919969c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 22:11:20.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8527" for this suite. 02/20/23 22:11:20.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:20.103
Feb 20 22:11:20.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:11:20.108
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:20.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:20.213
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:11:20.225
Feb 20 22:11:20.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99" in namespace "projected-2692" to be "Succeeded or Failed"
Feb 20 22:11:20.385: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99": Phase="Pending", Reason="", readiness=false. Elapsed: 69.586553ms
Feb 20 22:11:22.397: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081763394s
Feb 20 22:11:24.399: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083416736s
STEP: Saw pod success 02/20/23 22:11:24.399
Feb 20 22:11:24.399: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99" satisfied condition "Succeeded or Failed"
Feb 20 22:11:24.412: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99 container client-container: <nil>
STEP: delete the pod 02/20/23 22:11:24.493
Feb 20 22:11:24.527: INFO: Waiting for pod downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99 to disappear
Feb 20 22:11:24.538: INFO: Pod downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 22:11:24.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2692" for this suite. 02/20/23 22:11:24.563
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":79,"skipped":1468,"failed":0}
------------------------------
• [4.517 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:20.103
    Feb 20 22:11:20.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:11:20.108
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:20.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:20.213
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:11:20.225
    Feb 20 22:11:20.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99" in namespace "projected-2692" to be "Succeeded or Failed"
    Feb 20 22:11:20.385: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99": Phase="Pending", Reason="", readiness=false. Elapsed: 69.586553ms
    Feb 20 22:11:22.397: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081763394s
    Feb 20 22:11:24.399: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083416736s
    STEP: Saw pod success 02/20/23 22:11:24.399
    Feb 20 22:11:24.399: INFO: Pod "downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99" satisfied condition "Succeeded or Failed"
    Feb 20 22:11:24.412: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:11:24.493
    Feb 20 22:11:24.527: INFO: Waiting for pod downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99 to disappear
    Feb 20 22:11:24.538: INFO: Pod downwardapi-volume-d16b1f8a-efd1-4fd6-abc4-37aabb6bef99 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 22:11:24.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2692" for this suite. 02/20/23 22:11:24.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:24.621
Feb 20 22:11:24.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename tables 02/20/23 22:11:24.627
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:24.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:24.691
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Feb 20 22:11:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6356" for this suite. 02/20/23 22:11:24.731
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":80,"skipped":1474,"failed":0}
------------------------------
• [0.153 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:24.621
    Feb 20 22:11:24.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename tables 02/20/23 22:11:24.627
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:24.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:24.691
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Feb 20 22:11:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-6356" for this suite. 02/20/23 22:11:24.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:24.778
Feb 20 22:11:24.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:11:24.78
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:24.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:24.84
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
Feb 20 22:11:24.876: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-202eb4a1-41ae-4d3a-96f3-cd48e8fe339e 02/20/23 22:11:24.876
STEP: Creating secret with name s-test-opt-upd-5b1b642f-2b4d-4150-8dd1-ce4bba187d37 02/20/23 22:11:24.89
STEP: Creating the pod 02/20/23 22:11:24.912
Feb 20 22:11:24.963: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb" in namespace "projected-4891" to be "running and ready"
Feb 20 22:11:24.977: INFO: Pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.776597ms
Feb 20 22:11:24.977: INFO: The phase of Pod pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:11:26.993: INFO: Pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb": Phase="Running", Reason="", readiness=true. Elapsed: 2.030622334s
Feb 20 22:11:26.994: INFO: The phase of Pod pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb is Running (Ready = true)
Feb 20 22:11:26.994: INFO: Pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-202eb4a1-41ae-4d3a-96f3-cd48e8fe339e 02/20/23 22:11:27.073
STEP: Updating secret s-test-opt-upd-5b1b642f-2b4d-4150-8dd1-ce4bba187d37 02/20/23 22:11:27.092
STEP: Creating secret with name s-test-opt-create-0a9e6b51-6c97-4908-860a-fc86fb866231 02/20/23 22:11:27.108
STEP: waiting to observe update in volume 02/20/23 22:11:27.123
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 22:11:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4891" for this suite. 02/20/23 22:11:29.231
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":81,"skipped":1495,"failed":0}
------------------------------
• [4.492 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:24.778
    Feb 20 22:11:24.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:11:24.78
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:24.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:24.84
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    Feb 20 22:11:24.876: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-202eb4a1-41ae-4d3a-96f3-cd48e8fe339e 02/20/23 22:11:24.876
    STEP: Creating secret with name s-test-opt-upd-5b1b642f-2b4d-4150-8dd1-ce4bba187d37 02/20/23 22:11:24.89
    STEP: Creating the pod 02/20/23 22:11:24.912
    Feb 20 22:11:24.963: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb" in namespace "projected-4891" to be "running and ready"
    Feb 20 22:11:24.977: INFO: Pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.776597ms
    Feb 20 22:11:24.977: INFO: The phase of Pod pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:11:26.993: INFO: Pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb": Phase="Running", Reason="", readiness=true. Elapsed: 2.030622334s
    Feb 20 22:11:26.994: INFO: The phase of Pod pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb is Running (Ready = true)
    Feb 20 22:11:26.994: INFO: Pod "pod-projected-secrets-6a4c20da-844e-4227-961e-1c3dbb1ceedb" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-202eb4a1-41ae-4d3a-96f3-cd48e8fe339e 02/20/23 22:11:27.073
    STEP: Updating secret s-test-opt-upd-5b1b642f-2b4d-4150-8dd1-ce4bba187d37 02/20/23 22:11:27.092
    STEP: Creating secret with name s-test-opt-create-0a9e6b51-6c97-4908-860a-fc86fb866231 02/20/23 22:11:27.108
    STEP: waiting to observe update in volume 02/20/23 22:11:27.123
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 22:11:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4891" for this suite. 02/20/23 22:11:29.231
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:29.27
Feb 20 22:11:29.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-runtime 02/20/23 22:11:29.274
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:29.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:29.384
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 02/20/23 22:11:29.398
STEP: wait for the container to reach Succeeded 02/20/23 22:11:29.457
STEP: get the container status 02/20/23 22:11:34.534
STEP: the container should be terminated 02/20/23 22:11:34.545
STEP: the termination message should be set 02/20/23 22:11:34.546
Feb 20 22:11:34.546: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/20/23 22:11:34.547
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 20 22:11:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7378" for this suite. 02/20/23 22:11:34.609
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":82,"skipped":1499,"failed":0}
------------------------------
• [SLOW TEST] [5.371 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:29.27
    Feb 20 22:11:29.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-runtime 02/20/23 22:11:29.274
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:29.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:29.384
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 02/20/23 22:11:29.398
    STEP: wait for the container to reach Succeeded 02/20/23 22:11:29.457
    STEP: get the container status 02/20/23 22:11:34.534
    STEP: the container should be terminated 02/20/23 22:11:34.545
    STEP: the termination message should be set 02/20/23 22:11:34.546
    Feb 20 22:11:34.546: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/20/23 22:11:34.547
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 20 22:11:34.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7378" for this suite. 02/20/23 22:11:34.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:34.644
Feb 20 22:11:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 22:11:34.646
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:34.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:34.708
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Feb 20 22:11:34.800: INFO: Waiting up to 5m0s for pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90" in namespace "pods-8435" to be "running and ready"
Feb 20 22:11:34.812: INFO: Pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90": Phase="Pending", Reason="", readiness=false. Elapsed: 10.912219ms
Feb 20 22:11:34.812: INFO: The phase of Pod server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:11:36.824: INFO: Pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90": Phase="Running", Reason="", readiness=true. Elapsed: 2.023138669s
Feb 20 22:11:36.824: INFO: The phase of Pod server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90 is Running (Ready = true)
Feb 20 22:11:36.824: INFO: Pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90" satisfied condition "running and ready"
Feb 20 22:11:36.941: INFO: Waiting up to 5m0s for pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0" in namespace "pods-8435" to be "Succeeded or Failed"
Feb 20 22:11:36.952: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.497185ms
Feb 20 22:11:38.965: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024759223s
Feb 20 22:11:40.980: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039355866s
Feb 20 22:11:42.964: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023071837s
STEP: Saw pod success 02/20/23 22:11:42.964
Feb 20 22:11:42.964: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0" satisfied condition "Succeeded or Failed"
Feb 20 22:11:42.978: INFO: Trying to get logs from node 10.8.38.69 pod client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0 container env3cont: <nil>
STEP: delete the pod 02/20/23 22:11:43.031
Feb 20 22:11:43.064: INFO: Waiting for pod client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0 to disappear
Feb 20 22:11:43.079: INFO: Pod client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 22:11:43.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8435" for this suite. 02/20/23 22:11:43.096
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":83,"skipped":1505,"failed":0}
------------------------------
• [SLOW TEST] [8.473 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:34.644
    Feb 20 22:11:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 22:11:34.646
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:34.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:34.708
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Feb 20 22:11:34.800: INFO: Waiting up to 5m0s for pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90" in namespace "pods-8435" to be "running and ready"
    Feb 20 22:11:34.812: INFO: Pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90": Phase="Pending", Reason="", readiness=false. Elapsed: 10.912219ms
    Feb 20 22:11:34.812: INFO: The phase of Pod server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:11:36.824: INFO: Pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90": Phase="Running", Reason="", readiness=true. Elapsed: 2.023138669s
    Feb 20 22:11:36.824: INFO: The phase of Pod server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90 is Running (Ready = true)
    Feb 20 22:11:36.824: INFO: Pod "server-envvars-66ba89a2-1e62-4902-9bd0-3a5cfe145e90" satisfied condition "running and ready"
    Feb 20 22:11:36.941: INFO: Waiting up to 5m0s for pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0" in namespace "pods-8435" to be "Succeeded or Failed"
    Feb 20 22:11:36.952: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.497185ms
    Feb 20 22:11:38.965: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024759223s
    Feb 20 22:11:40.980: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039355866s
    Feb 20 22:11:42.964: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023071837s
    STEP: Saw pod success 02/20/23 22:11:42.964
    Feb 20 22:11:42.964: INFO: Pod "client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0" satisfied condition "Succeeded or Failed"
    Feb 20 22:11:42.978: INFO: Trying to get logs from node 10.8.38.69 pod client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0 container env3cont: <nil>
    STEP: delete the pod 02/20/23 22:11:43.031
    Feb 20 22:11:43.064: INFO: Waiting for pod client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0 to disappear
    Feb 20 22:11:43.079: INFO: Pod client-envvars-a8a6102a-76c3-41cf-8396-6dffa9efb1d0 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 22:11:43.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8435" for this suite. 02/20/23 22:11:43.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:43.12
Feb 20 22:11:43.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-webhook 02/20/23 22:11:43.123
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:43.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:43.216
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/20/23 22:11:43.225
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/20/23 22:11:43.656
STEP: Deploying the custom resource conversion webhook pod 02/20/23 22:11:43.681
STEP: Wait for the deployment to be ready 02/20/23 22:11:43.717
Feb 20 22:11:43.744: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Feb 20 22:11:45.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:11:47.784
STEP: Verifying the service has paired with the endpoint 02/20/23 22:11:47.808
Feb 20 22:11:48.809: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 20 22:11:48.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Creating a v1 custom resource 02/20/23 22:11:51.518
STEP: v2 custom resource should be converted 02/20/23 22:11:51.53
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:11:52.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1322" for this suite. 02/20/23 22:11:52.16
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":84,"skipped":1522,"failed":0}
------------------------------
• [SLOW TEST] [9.185 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:43.12
    Feb 20 22:11:43.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-webhook 02/20/23 22:11:43.123
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:43.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:43.216
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/20/23 22:11:43.225
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/20/23 22:11:43.656
    STEP: Deploying the custom resource conversion webhook pod 02/20/23 22:11:43.681
    STEP: Wait for the deployment to be ready 02/20/23 22:11:43.717
    Feb 20 22:11:43.744: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Feb 20 22:11:45.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 11, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:11:47.784
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:11:47.808
    Feb 20 22:11:48.809: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 20 22:11:48.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Creating a v1 custom resource 02/20/23 22:11:51.518
    STEP: v2 custom resource should be converted 02/20/23 22:11:51.53
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:11:52.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-1322" for this suite. 02/20/23 22:11:52.16
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:52.306
Feb 20 22:11:52.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 22:11:52.307
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:52.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:52.409
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 22:11:52.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1710" for this suite. 02/20/23 22:11:52.581
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":85,"skipped":1536,"failed":0}
------------------------------
• [0.300 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:52.306
    Feb 20 22:11:52.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 22:11:52.307
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:52.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:52.409
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 22:11:52.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1710" for this suite. 02/20/23 22:11:52.581
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:52.606
Feb 20 22:11:52.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:11:52.609
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:52.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:52.729
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Feb 20 22:11:52.755: INFO: Got root ca configmap in namespace "svcaccounts-865"
Feb 20 22:11:52.774: INFO: Deleted root ca configmap in namespace "svcaccounts-865"
STEP: waiting for a new root ca configmap created 02/20/23 22:11:53.275
Feb 20 22:11:53.292: INFO: Recreated root ca configmap in namespace "svcaccounts-865"
Feb 20 22:11:53.307: INFO: Updated root ca configmap in namespace "svcaccounts-865"
STEP: waiting for the root ca configmap reconciled 02/20/23 22:11:53.808
Feb 20 22:11:53.848: INFO: Reconciled root ca configmap in namespace "svcaccounts-865"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 20 22:11:53.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-865" for this suite. 02/20/23 22:11:53.886
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":86,"skipped":1537,"failed":0}
------------------------------
• [1.305 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:52.606
    Feb 20 22:11:52.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:11:52.609
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:52.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:52.729
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Feb 20 22:11:52.755: INFO: Got root ca configmap in namespace "svcaccounts-865"
    Feb 20 22:11:52.774: INFO: Deleted root ca configmap in namespace "svcaccounts-865"
    STEP: waiting for a new root ca configmap created 02/20/23 22:11:53.275
    Feb 20 22:11:53.292: INFO: Recreated root ca configmap in namespace "svcaccounts-865"
    Feb 20 22:11:53.307: INFO: Updated root ca configmap in namespace "svcaccounts-865"
    STEP: waiting for the root ca configmap reconciled 02/20/23 22:11:53.808
    Feb 20 22:11:53.848: INFO: Reconciled root ca configmap in namespace "svcaccounts-865"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 20 22:11:53.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-865" for this suite. 02/20/23 22:11:53.886
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:11:53.913
Feb 20 22:11:53.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 22:11:53.916
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:54.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:54.035
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/20/23 22:11:54.061
STEP: waiting for Deployment to be created 02/20/23 22:11:54.074
STEP: waiting for all Replicas to be Ready 02/20/23 22:11:54.081
Feb 20 22:11:54.087: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.087: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.105: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.105: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.135: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.135: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.261: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:54.261: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 20 22:11:55.771: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 20 22:11:55.771: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 20 22:11:56.083: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/20/23 22:11:56.083
W0220 22:11:56.097504      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 20 22:11:56.103: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/20/23 22:11:56.103
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.151: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.151: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.186: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.186: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:56.228: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:11:56.228: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:11:56.262: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:11:56.262: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:11:57.698: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:57.698: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:11:57.740: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
STEP: listing Deployments 02/20/23 22:11:57.74
Feb 20 22:11:57.757: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/20/23 22:11:57.757
Feb 20 22:11:57.784: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/20/23 22:11:57.784
Feb 20 22:11:57.802: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:57.802: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:57.844: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:57.884: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:57.917: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:57.939: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:59.781: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:59.812: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:59.825: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:59.844: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:11:59.895: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 20 22:12:02.264: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/20/23 22:12:02.332
STEP: fetching the DeploymentStatus 02/20/23 22:12:02.352
Feb 20 22:12:02.368: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
Feb 20 22:12:02.387: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 3
STEP: deleting the Deployment 02/20/23 22:12:02.387
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.417: INFO: observed event type MODIFIED
Feb 20 22:12:02.418: INFO: observed event type MODIFIED
Feb 20 22:12:02.418: INFO: observed event type MODIFIED
Feb 20 22:12:02.418: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 22:12:02.432: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 22:12:02.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7154" for this suite. 02/20/23 22:12:02.46
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":87,"skipped":1538,"failed":0}
------------------------------
• [SLOW TEST] [8.574 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:11:53.913
    Feb 20 22:11:53.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 22:11:53.916
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:11:54.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:11:54.035
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/20/23 22:11:54.061
    STEP: waiting for Deployment to be created 02/20/23 22:11:54.074
    STEP: waiting for all Replicas to be Ready 02/20/23 22:11:54.081
    Feb 20 22:11:54.087: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.087: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.105: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.105: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.135: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.135: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.261: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:54.261: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 20 22:11:55.771: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 20 22:11:55.771: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 20 22:11:56.083: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/20/23 22:11:56.083
    W0220 22:11:56.097504      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 20 22:11:56.103: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/20/23 22:11:56.103
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 0
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:11:56.110: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.111: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.151: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.151: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.186: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.186: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:56.228: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:11:56.228: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:11:56.262: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:11:56.262: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:11:57.698: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:57.698: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:11:57.740: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    STEP: listing Deployments 02/20/23 22:11:57.74
    Feb 20 22:11:57.757: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/20/23 22:11:57.757
    Feb 20 22:11:57.784: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/20/23 22:11:57.784
    Feb 20 22:11:57.802: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:57.802: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:57.844: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:57.884: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:57.917: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:57.939: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:59.781: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:59.812: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:59.825: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:59.844: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:11:59.895: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 20 22:12:02.264: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/20/23 22:12:02.332
    STEP: fetching the DeploymentStatus 02/20/23 22:12:02.352
    Feb 20 22:12:02.368: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:12:02.369: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 1
    Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:12:02.386: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 2
    Feb 20 22:12:02.387: INFO: observed Deployment test-deployment in namespace deployment-7154 with ReadyReplicas 3
    STEP: deleting the Deployment 02/20/23 22:12:02.387
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.417: INFO: observed event type MODIFIED
    Feb 20 22:12:02.418: INFO: observed event type MODIFIED
    Feb 20 22:12:02.418: INFO: observed event type MODIFIED
    Feb 20 22:12:02.418: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 22:12:02.432: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 22:12:02.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7154" for this suite. 02/20/23 22:12:02.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:02.487
Feb 20 22:12:02.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-pred 02/20/23 22:12:02.489
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:02.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:02.558
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 20 22:12:02.569: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 22:12:02.600: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 22:12:02.623: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.66 before test
Feb 20 22:12:02.688: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.688: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:12:02.688: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.688: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:12:02.689: INFO: test-deployment-7c7d8d58c8-6vwlc from deployment-7154 started at 2023-02-20 22:11:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container test-deployment ready: true, restart count 0
Feb 20 22:12:02.689: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:12:02.689: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:12:02.689: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:12:02.689: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:12:02.689: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:12:02.689: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:12:02.689: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:12:02.689: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:12:02.689: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:12:02.689: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:12:02.689: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.689: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:12:02.689: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:12:02.689: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 20 22:12:02.689: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:12:02.689: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.689: INFO: 	Container router ready: true, restart count 0
Feb 20 22:12:02.689: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container migrator ready: true, restart count 0
Feb 20 22:12:02.690: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:02.690: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:02.690: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:02.690: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:12:02.690: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:12:02.690: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:12:02.690: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:12:02.690: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.690: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 20 22:12:02.690: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.690: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:12:02.690: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:12:02.691: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:12:02.691: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:12:02.691: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:12:02.691: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:12:02.691: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 20 22:12:02.691: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.691: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:12:02.691: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.69 before test
Feb 20 22:12:02.733: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.733: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 20 22:12:02.733: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.733: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:12:02.733: INFO: test-deployment-54cc775c4b-l9bk5 from deployment-7154 started at 2023-02-20 22:11:56 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container test-deployment ready: true, restart count 0
Feb 20 22:12:02.734: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 20 22:12:02.734: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:12:02.734: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:12:02.734: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:12:02.734: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 20 22:12:02.734: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:12:02.734: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 20 22:12:02.734: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container vpn ready: true, restart count 0
Feb 20 22:12:02.734: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 20 22:12:02.734: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:12:02.734: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.734: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 20 22:12:02.734: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 20 22:12:02.735: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Feb 20 22:12:02.735: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Feb 20 22:12:02.735: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container console-operator ready: true, restart count 1
Feb 20 22:12:02.735: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Feb 20 22:12:02.735: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container console ready: true, restart count 0
Feb 20 22:12:02.735: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container dns-operator ready: true, restart count 0
Feb 20 22:12:02.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.735: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:12:02.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.735: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:12:02.735: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 20 22:12:02.735: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:12:02.735: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.735: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:12:02.736: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.736: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container insights-operator ready: true, restart count 1
Feb 20 22:12:02.736: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.736: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Feb 20 22:12:02.736: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 20 22:12:02.736: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.736: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.736: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:12:02.736: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:12:02.736: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:12:02.736: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.736: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:12:02.737: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:12:02.737: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.737: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:12:02.737: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container check-endpoints ready: true, restart count 0
Feb 20 22:12:02.737: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:12:02.737: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container network-operator ready: true, restart count 1
Feb 20 22:12:02.737: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 20 22:12:02.737: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:12:02.737: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:12:02.737: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container olm-operator ready: true, restart count 0
Feb 20 22:12:02.737: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container package-server-manager ready: true, restart count 0
Feb 20 22:12:02.737: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:12:02.737: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container metrics ready: true, restart count 1
Feb 20 22:12:02.737: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container push-gateway ready: true, restart count 0
Feb 20 22:12:02.737: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container service-ca-operator ready: true, restart count 1
Feb 20 22:12:02.737: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container e2e ready: true, restart count 0
Feb 20 22:12:02.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:02.737: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:02.737: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:12:02.738: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.738: INFO: 	Container tigera-operator ready: true, restart count 2
Feb 20 22:12:02.738: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.70 before test
Feb 20 22:12:02.780: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.780: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:12:02.780: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:12:02.781: INFO: test-deployment-7c7d8d58c8-nft2q from deployment-7154 started at 2023-02-20 22:11:57 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container test-deployment ready: true, restart count 0
Feb 20 22:12:02.781: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:12:02.781: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:12:02.781: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:12:02.781: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:12:02.781: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:12:02.781: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.781: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:12:02.782: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:12:02.782: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:12:02.782: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container console ready: true, restart count 0
Feb 20 22:12:02.782: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:12:02.782: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:12:02.782: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.782: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:12:02.782: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.782: INFO: 	Container registry ready: true, restart count 0
Feb 20 22:12:02.782: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.783: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:12:02.783: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.783: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:12:02.783: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.783: INFO: 	Container router ready: true, restart count 0
Feb 20 22:12:02.783: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.783: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:12:02.783: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.783: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.783: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:02.783: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:02.783: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:12:02.783: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:12:02.783: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:02.784: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 20 22:12:02.784: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:12:02.784: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 20 22:12:02.784: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:02.784: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.785: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:12:02.785: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:12:02.785: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:12:02.785: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:12:02.785: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.785: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:12:02.785: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
Feb 20 22:12:02.785: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.788: INFO: 	Container reload ready: true, restart count 0
Feb 20 22:12:02.788: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 20 22:12:02.788: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:12:02.789: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.789: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:12:02.789: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.789: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:12:02.789: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:12:02.789: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:02.789: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:12:02.790: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.790: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:12:02.790: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.790: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:12:02.790: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.790: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:12:02.790: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:02.790: INFO: 	Container service-ca-controller ready: true, restart count 0
Feb 20 22:12:02.790: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:02.790: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:02.790: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/20/23 22:12:02.79
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1745a7f8baa0a449], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 02/20/23 22:12:03.007
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:12:04.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5222" for this suite. 02/20/23 22:12:04.025
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":88,"skipped":1547,"failed":0}
------------------------------
• [1.573 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:02.487
    Feb 20 22:12:02.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-pred 02/20/23 22:12:02.489
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:02.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:02.558
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 20 22:12:02.569: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 20 22:12:02.600: INFO: Waiting for terminating namespaces to be deleted...
    Feb 20 22:12:02.623: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.66 before test
    Feb 20 22:12:02.688: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.688: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:12:02.688: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.688: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: test-deployment-7c7d8d58c8-6vwlc from deployment-7154 started at 2023-02-20 22:11:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container test-deployment ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container pvc-permissions ready: false, restart count 0
    Feb 20 22:12:02.689: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.689: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:12:02.689: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container migrator ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:12:02.690: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: 	Container prometheus-operator ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.690: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:12:02.690: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.691: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:12:02.691: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.69 before test
    Feb 20 22:12:02.733: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.733: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 20 22:12:02.733: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.733: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:12:02.733: INFO: test-deployment-54cc775c4b-l9bk5 from deployment-7154 started at 2023-02-20 22:11:56 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container test-deployment ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container vpn ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.734: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Feb 20 22:12:02.734: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Feb 20 22:12:02.735: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container console-operator ready: true, restart count 1
    Feb 20 22:12:02.735: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Feb 20 22:12:02.735: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container dns-operator ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:12:02.735: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.735: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container ingress-operator ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container insights-operator ready: true, restart count 1
    Feb 20 22:12:02.736: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Feb 20 22:12:02.736: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container marketplace-operator ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.736: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.736: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container check-endpoints ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container network-operator ready: true, restart count 1
    Feb 20 22:12:02.737: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container catalog-operator ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:12:02.737: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:12:02.737: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container olm-operator ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container package-server-manager ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container metrics ready: true, restart count 1
    Feb 20 22:12:02.737: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container push-gateway ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container service-ca-operator ready: true, restart count 1
    Feb 20 22:12:02.737: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container e2e ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:02.737: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:12:02.738: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.738: INFO: 	Container tigera-operator ready: true, restart count 2
    Feb 20 22:12:02.738: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.70 before test
    Feb 20 22:12:02.780: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.780: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:12:02.780: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: test-deployment-7c7d8d58c8-nft2q from deployment-7154 started at 2023-02-20 22:11:57 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container test-deployment ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:12:02.781: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.781: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.782: INFO: 	Container registry ready: true, restart count 0
    Feb 20 22:12:02.782: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.783: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.783: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.783: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.783: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.783: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:02.783: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:12:02.783: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:12:02.783: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:02.784: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:02.784: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.785: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:12:02.785: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:12:02.785: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:12:02.785: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:12:02.785: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.785: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:12:02.785: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
    Feb 20 22:12:02.785: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.788: INFO: 	Container reload ready: true, restart count 0
    Feb 20 22:12:02.788: INFO: 	Container telemeter-client ready: true, restart count 0
    Feb 20 22:12:02.788: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.789: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.789: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.789: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:02.789: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:12:02.790: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.790: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:12:02.790: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.790: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:12:02.790: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.790: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:12:02.790: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:02.790: INFO: 	Container service-ca-controller ready: true, restart count 0
    Feb 20 22:12:02.790: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:02.790: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:02.790: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/20/23 22:12:02.79
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1745a7f8baa0a449], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 02/20/23 22:12:03.007
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:12:04.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5222" for this suite. 02/20/23 22:12:04.025
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:04.066
Feb 20 22:12:04.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename namespaces 02/20/23 22:12:04.069
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:04.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:04.133
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 02/20/23 22:12:04.151
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:04.322
STEP: Creating a pod in the namespace 02/20/23 22:12:04.339
STEP: Waiting for the pod to have running status 02/20/23 22:12:04.389
Feb 20 22:12:04.389: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3647" to be "running"
Feb 20 22:12:04.400: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.003097ms
Feb 20 22:12:06.414: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024576927s
Feb 20 22:12:08.414: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.024953917s
Feb 20 22:12:08.414: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/20/23 22:12:08.414
STEP: Waiting for the namespace to be removed. 02/20/23 22:12:08.441
STEP: Recreating the namespace 02/20/23 22:12:22.454
STEP: Verifying there are no pods in the namespace 02/20/23 22:12:22.499
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:12:22.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5765" for this suite. 02/20/23 22:12:22.536
STEP: Destroying namespace "nsdeletetest-3647" for this suite. 02/20/23 22:12:22.565
Feb 20 22:12:22.579: INFO: Namespace nsdeletetest-3647 was already deleted
STEP: Destroying namespace "nsdeletetest-1003" for this suite. 02/20/23 22:12:22.579
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":89,"skipped":1583,"failed":0}
------------------------------
• [SLOW TEST] [18.542 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:04.066
    Feb 20 22:12:04.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename namespaces 02/20/23 22:12:04.069
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:04.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:04.133
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 02/20/23 22:12:04.151
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:04.322
    STEP: Creating a pod in the namespace 02/20/23 22:12:04.339
    STEP: Waiting for the pod to have running status 02/20/23 22:12:04.389
    Feb 20 22:12:04.389: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3647" to be "running"
    Feb 20 22:12:04.400: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.003097ms
    Feb 20 22:12:06.414: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024576927s
    Feb 20 22:12:08.414: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.024953917s
    Feb 20 22:12:08.414: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/20/23 22:12:08.414
    STEP: Waiting for the namespace to be removed. 02/20/23 22:12:08.441
    STEP: Recreating the namespace 02/20/23 22:12:22.454
    STEP: Verifying there are no pods in the namespace 02/20/23 22:12:22.499
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:12:22.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5765" for this suite. 02/20/23 22:12:22.536
    STEP: Destroying namespace "nsdeletetest-3647" for this suite. 02/20/23 22:12:22.565
    Feb 20 22:12:22.579: INFO: Namespace nsdeletetest-3647 was already deleted
    STEP: Destroying namespace "nsdeletetest-1003" for this suite. 02/20/23 22:12:22.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:22.614
Feb 20 22:12:22.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:22.616
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:22.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:22.669
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 02/20/23 22:12:22.682
Feb 20 22:12:22.745: INFO: Waiting up to 5m0s for pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d" in namespace "emptydir-3795" to be "Succeeded or Failed"
Feb 20 22:12:22.761: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.710192ms
Feb 20 22:12:24.773: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028057489s
Feb 20 22:12:26.773: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028201928s
STEP: Saw pod success 02/20/23 22:12:26.773
Feb 20 22:12:26.774: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d" satisfied condition "Succeeded or Failed"
Feb 20 22:12:26.785: INFO: Trying to get logs from node 10.8.38.66 pod pod-cdda8d9b-c9ae-4da1-b275-00858020168d container test-container: <nil>
STEP: delete the pod 02/20/23 22:12:26.807
Feb 20 22:12:26.849: INFO: Waiting for pod pod-cdda8d9b-c9ae-4da1-b275-00858020168d to disappear
Feb 20 22:12:26.881: INFO: Pod pod-cdda8d9b-c9ae-4da1-b275-00858020168d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:12:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3795" for this suite. 02/20/23 22:12:26.91
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":90,"skipped":1590,"failed":0}
------------------------------
• [4.318 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:22.614
    Feb 20 22:12:22.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:22.616
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:22.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:22.669
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 02/20/23 22:12:22.682
    Feb 20 22:12:22.745: INFO: Waiting up to 5m0s for pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d" in namespace "emptydir-3795" to be "Succeeded or Failed"
    Feb 20 22:12:22.761: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.710192ms
    Feb 20 22:12:24.773: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028057489s
    Feb 20 22:12:26.773: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028201928s
    STEP: Saw pod success 02/20/23 22:12:26.773
    Feb 20 22:12:26.774: INFO: Pod "pod-cdda8d9b-c9ae-4da1-b275-00858020168d" satisfied condition "Succeeded or Failed"
    Feb 20 22:12:26.785: INFO: Trying to get logs from node 10.8.38.66 pod pod-cdda8d9b-c9ae-4da1-b275-00858020168d container test-container: <nil>
    STEP: delete the pod 02/20/23 22:12:26.807
    Feb 20 22:12:26.849: INFO: Waiting for pod pod-cdda8d9b-c9ae-4da1-b275-00858020168d to disappear
    Feb 20 22:12:26.881: INFO: Pod pod-cdda8d9b-c9ae-4da1-b275-00858020168d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:12:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3795" for this suite. 02/20/23 22:12:26.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:26.938
Feb 20 22:12:26.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:26.94
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:27.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:27.032
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 02/20/23 22:12:27.043
Feb 20 22:12:27.100: INFO: Waiting up to 5m0s for pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844" in namespace "emptydir-1006" to be "Succeeded or Failed"
Feb 20 22:12:27.120: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Pending", Reason="", readiness=false. Elapsed: 19.315753ms
Feb 20 22:12:29.133: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032514589s
Feb 20 22:12:31.144: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043349955s
Feb 20 22:12:33.131: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031077135s
STEP: Saw pod success 02/20/23 22:12:33.131
Feb 20 22:12:33.138: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844" satisfied condition "Succeeded or Failed"
Feb 20 22:12:33.151: INFO: Trying to get logs from node 10.8.38.66 pod pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844 container test-container: <nil>
STEP: delete the pod 02/20/23 22:12:33.174
Feb 20 22:12:33.207: INFO: Waiting for pod pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844 to disappear
Feb 20 22:12:33.218: INFO: Pod pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:12:33.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1006" for this suite. 02/20/23 22:12:33.234
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":91,"skipped":1611,"failed":0}
------------------------------
• [SLOW TEST] [6.326 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:26.938
    Feb 20 22:12:26.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:26.94
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:27.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:27.032
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/20/23 22:12:27.043
    Feb 20 22:12:27.100: INFO: Waiting up to 5m0s for pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844" in namespace "emptydir-1006" to be "Succeeded or Failed"
    Feb 20 22:12:27.120: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Pending", Reason="", readiness=false. Elapsed: 19.315753ms
    Feb 20 22:12:29.133: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032514589s
    Feb 20 22:12:31.144: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043349955s
    Feb 20 22:12:33.131: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031077135s
    STEP: Saw pod success 02/20/23 22:12:33.131
    Feb 20 22:12:33.138: INFO: Pod "pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844" satisfied condition "Succeeded or Failed"
    Feb 20 22:12:33.151: INFO: Trying to get logs from node 10.8.38.66 pod pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844 container test-container: <nil>
    STEP: delete the pod 02/20/23 22:12:33.174
    Feb 20 22:12:33.207: INFO: Waiting for pod pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844 to disappear
    Feb 20 22:12:33.218: INFO: Pod pod-7a1acd4d-a05e-4e56-bd70-7dfe38f0e844 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:12:33.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1006" for this suite. 02/20/23 22:12:33.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:33.267
Feb 20 22:12:33.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:12:33.271
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:33.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:33.336
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 22:12:33.357
Feb 20 22:12:33.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8680 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 20 22:12:33.494: INFO: stderr: ""
Feb 20 22:12:33.494: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/20/23 22:12:33.494
Feb 20 22:12:33.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8680 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Feb 20 22:12:36.141: INFO: stderr: ""
Feb 20 22:12:36.142: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 22:12:36.142
Feb 20 22:12:36.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8680 delete pods e2e-test-httpd-pod'
Feb 20 22:12:38.376: INFO: stderr: ""
Feb 20 22:12:38.376: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:12:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8680" for this suite. 02/20/23 22:12:38.396
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":92,"skipped":1625,"failed":0}
------------------------------
• [SLOW TEST] [5.152 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:33.267
    Feb 20 22:12:33.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:12:33.271
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:33.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:33.336
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 22:12:33.357
    Feb 20 22:12:33.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8680 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 20 22:12:33.494: INFO: stderr: ""
    Feb 20 22:12:33.494: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/20/23 22:12:33.494
    Feb 20 22:12:33.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8680 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Feb 20 22:12:36.141: INFO: stderr: ""
    Feb 20 22:12:36.142: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 22:12:36.142
    Feb 20 22:12:36.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8680 delete pods e2e-test-httpd-pod'
    Feb 20 22:12:38.376: INFO: stderr: ""
    Feb 20 22:12:38.376: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:12:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8680" for this suite. 02/20/23 22:12:38.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:38.423
Feb 20 22:12:38.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:12:38.425
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:38.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:38.482
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:12:38.555
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:12:39.076
STEP: Deploying the webhook pod 02/20/23 22:12:39.102
STEP: Wait for the deployment to be ready 02/20/23 22:12:39.128
Feb 20 22:12:39.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 22:12:41.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:12:43.197
STEP: Verifying the service has paired with the endpoint 02/20/23 22:12:43.219
Feb 20 22:12:44.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Feb 20 22:12:44.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-12-crds.webhook.example.com via the AdmissionRegistration API 02/20/23 22:12:44.797
STEP: Creating a custom resource while v1 is storage version 02/20/23 22:12:44.859
STEP: Patching Custom Resource Definition to set v2 as storage 02/20/23 22:12:46.963
STEP: Patching the custom resource while v2 is storage version 02/20/23 22:12:46.99
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:12:47.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6826" for this suite. 02/20/23 22:12:47.649
STEP: Destroying namespace "webhook-6826-markers" for this suite. 02/20/23 22:12:47.674
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":93,"skipped":1649,"failed":0}
------------------------------
• [SLOW TEST] [9.389 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:38.423
    Feb 20 22:12:38.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:12:38.425
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:38.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:38.482
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:12:38.555
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:12:39.076
    STEP: Deploying the webhook pod 02/20/23 22:12:39.102
    STEP: Wait for the deployment to be ready 02/20/23 22:12:39.128
    Feb 20 22:12:39.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 22:12:41.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 12, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:12:43.197
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:12:43.219
    Feb 20 22:12:44.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Feb 20 22:12:44.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-12-crds.webhook.example.com via the AdmissionRegistration API 02/20/23 22:12:44.797
    STEP: Creating a custom resource while v1 is storage version 02/20/23 22:12:44.859
    STEP: Patching Custom Resource Definition to set v2 as storage 02/20/23 22:12:46.963
    STEP: Patching the custom resource while v2 is storage version 02/20/23 22:12:46.99
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:12:47.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6826" for this suite. 02/20/23 22:12:47.649
    STEP: Destroying namespace "webhook-6826-markers" for this suite. 02/20/23 22:12:47.674
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:47.815
Feb 20 22:12:47.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-pred 02/20/23 22:12:47.816
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:47.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:47.868
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 20 22:12:47.892: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 22:12:47.938: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 22:12:48.012: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.66 before test
Feb 20 22:12:48.051: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:12:48.051: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:12:48.051: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:12:48.051: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:12:48.051: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:12:48.051: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:12:48.051: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:12:48.051: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:12:48.051: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:12:48.051: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:12:48.051: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:12:48.051: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:12:48.051: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 20 22:12:48.051: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:12:48.051: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container router ready: true, restart count 0
Feb 20 22:12:48.051: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container migrator ready: true, restart count 0
Feb 20 22:12:48.051: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:48.051: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:48.051: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:48.051: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:12:48.051: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:12:48.051: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:12:48.051: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:48.051: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:12:48.052: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 20 22:12:48.052: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:12:48.052: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:12:48.052: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:12:48.052: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:12:48.052: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:12:48.052: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:12:48.052: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 20 22:12:48.052: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.052: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:12:48.052: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.69 before test
Feb 20 22:12:48.105: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 20 22:12:48.105: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:12:48.105: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 20 22:12:48.105: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:12:48.105: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:12:48.105: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:12:48.105: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 20 22:12:48.105: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:12:48.105: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 20 22:12:48.105: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container vpn ready: true, restart count 0
Feb 20 22:12:48.105: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 20 22:12:48.105: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:12:48.105: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 20 22:12:48.105: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 20 22:12:48.105: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Feb 20 22:12:48.105: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Feb 20 22:12:48.105: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container console-operator ready: true, restart count 1
Feb 20 22:12:48.105: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Feb 20 22:12:48.105: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container console ready: true, restart count 0
Feb 20 22:12:48.105: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.105: INFO: 	Container dns-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:12:48.106: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:12:48.106: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:12:48.106: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container insights-operator ready: true, restart count 1
Feb 20 22:12:48.106: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Feb 20 22:12:48.106: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:12:48.106: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:12:48.106: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:12:48.106: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:12:48.106: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:12:48.106: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:12:48.106: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container check-endpoints ready: true, restart count 0
Feb 20 22:12:48.106: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:12:48.106: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container network-operator ready: true, restart count 1
Feb 20 22:12:48.106: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:12:48.106: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:12:48.106: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container olm-operator ready: true, restart count 0
Feb 20 22:12:48.106: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container package-server-manager ready: true, restart count 0
Feb 20 22:12:48.106: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:12:48.106: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container metrics ready: true, restart count 1
Feb 20 22:12:48.106: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container push-gateway ready: true, restart count 0
Feb 20 22:12:48.106: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container service-ca-operator ready: true, restart count 1
Feb 20 22:12:48.106: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container e2e ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:48.106: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:48.106: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:12:48.106: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.106: INFO: 	Container tigera-operator ready: true, restart count 2
Feb 20 22:12:48.106: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.70 before test
Feb 20 22:12:48.142: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.142: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:12:48.142: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.142: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:12:48.142: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.142: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:12:48.142: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.142: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:12:48.142: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.142: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:12:48.143: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:12:48.143: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:12:48.143: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:12:48.143: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:12:48.143: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container console ready: true, restart count 0
Feb 20 22:12:48.143: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:12:48.143: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:12:48.143: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container registry ready: true, restart count 0
Feb 20 22:12:48.143: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:12:48.143: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:12:48.143: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container router ready: true, restart count 0
Feb 20 22:12:48.143: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:12:48.143: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:12:48.143: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 20 22:12:48.143: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:12:48.143: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 20 22:12:48.143: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:12:48.143: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:12:48.143: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container reload ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 20 22:12:48.143: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:12:48.143: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:12:48.143: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:12:48.143: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:12:48.143: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:12:48.143: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:12:48.143: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:12:48.143: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:12:48.143: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container service-ca-controller ready: true, restart count 0
Feb 20 22:12:48.143: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:12:48.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:12:48.143: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node 10.8.38.66 02/20/23 22:12:48.225
STEP: verifying the node has the label node 10.8.38.69 02/20/23 22:12:48.27
STEP: verifying the node has the label node 10.8.38.70 02/20/23 22:12:48.342
Feb 20 22:12:48.417: INFO: Pod calico-kube-controllers-5b6b964466-ljmhd requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod calico-node-6l54d requesting resource cpu=250m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod calico-node-9djxs requesting resource cpu=250m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod calico-node-jj7lx requesting resource cpu=250m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod calico-typha-65d7b689d4-97f55 requesting resource cpu=250m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod calico-typha-65d7b689d4-kpn77 requesting resource cpu=250m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 requesting resource cpu=5m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s requesting resource cpu=5m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod ibm-file-plugin-d4b9d9dd5-fjk8x requesting resource cpu=50m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod ibm-keepalived-watcher-jnq8d requesting resource cpu=5m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod ibm-keepalived-watcher-n8zf8 requesting resource cpu=5m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod ibm-keepalived-watcher-snrrr requesting resource cpu=5m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ibm-master-proxy-static-10.8.38.66 requesting resource cpu=26m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ibm-master-proxy-static-10.8.38.69 requesting resource cpu=26m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod ibm-master-proxy-static-10.8.38.70 requesting resource cpu=26m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod ibm-storage-watcher-86f4b88d84-sb4dd requesting resource cpu=50m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-driver-5s8m7 requesting resource cpu=50m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-driver-9s9rq requesting resource cpu=50m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-driver-dcm4m requesting resource cpu=50m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv requesting resource cpu=50m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod vpn-77468d4d47-xrvvz requesting resource cpu=5m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod cluster-node-tuning-operator-65bd8f65fb-z2xnp requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod tuned-57bz4 requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod tuned-jt9sm requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod tuned-sxlgc requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod cluster-samples-operator-585c69687-52df4 requesting resource cpu=20m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod cluster-storage-operator-66bf7c9c5f-jhz4m requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod csi-snapshot-controller-5bfddff4c-tqhzz requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod csi-snapshot-controller-5bfddff4c-whkcp requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod csi-snapshot-controller-operator-557b547dc4-dz4vh requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod csi-snapshot-webhook-544b96bbcd-bxjcg requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod csi-snapshot-webhook-544b96bbcd-rjs5h requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod console-operator-8578f747-4fpjs requesting resource cpu=20m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod console-57dd6cb75b-lgjgl requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod console-57dd6cb75b-lzmnl requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod downloads-8f679847b-nbfcm requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod downloads-8f679847b-r829z requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod dns-operator-c96b75bc5-66nj2 requesting resource cpu=20m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod dns-default-6gh9k requesting resource cpu=60m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod dns-default-7wnbj requesting resource cpu=60m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod dns-default-snlvx requesting resource cpu=60m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod node-resolver-2lz8p requesting resource cpu=5m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod node-resolver-9wzh6 requesting resource cpu=5m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod node-resolver-9zmwj requesting resource cpu=5m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod cluster-image-registry-operator-6cc666c6b4-gwz8b requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod image-registry-f445fb4b9-qprbx requesting resource cpu=100m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod node-ca-cq7v2 requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod node-ca-jmgpg requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod node-ca-k9szq requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ingress-canary-c2h9v requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod ingress-canary-cvlvh requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod ingress-canary-s48w8 requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod ingress-operator-75d985687c-2cff5 requesting resource cpu=20m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod router-default-f9ffd57f7-rc68j requesting resource cpu=100m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod router-default-f9ffd57f7-vxv45 requesting resource cpu=100m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod insights-operator-76dd9549c4-zpvkl requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod openshift-kube-proxy-46d5j requesting resource cpu=110m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod openshift-kube-proxy-4bz4j requesting resource cpu=110m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod openshift-kube-proxy-mjvmc requesting resource cpu=110m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod migrator-5c54d8d69d-jm5pm requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod certified-operators-mswgj requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod community-operators-rkvlk requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod marketplace-operator-57868d7ff-tn59b requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod redhat-marketplace-qnk27 requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod redhat-operators-8zq49 requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod cluster-monitoring-operator-57c54cf78c-stk6t requesting resource cpu=11m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod kube-state-metrics-75455b796c-nfnxt requesting resource cpu=4m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod node-exporter-857fp requesting resource cpu=9m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod node-exporter-9kjkf requesting resource cpu=9m on Node 10.8.38.69
Feb 20 22:12:48.417: INFO: Pod node-exporter-zkcjf requesting resource cpu=9m on Node 10.8.38.66
Feb 20 22:12:48.417: INFO: Pod openshift-state-metrics-5ff95d844f-nw74k requesting resource cpu=3m on Node 10.8.38.70
Feb 20 22:12:48.417: INFO: Pod prometheus-adapter-685b6bd76d-7dn46 requesting resource cpu=1m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod prometheus-adapter-685b6bd76d-xrdf9 requesting resource cpu=1m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod prometheus-operator-688459b4f4-4cv2n requesting resource cpu=6m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-vqmvd requesting resource cpu=5m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-z26qc requesting resource cpu=5m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod telemeter-client-7c87b5c5c6-lpcrj requesting resource cpu=3m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod thanos-querier-cb675b7f5-8hjjt requesting resource cpu=15m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod thanos-querier-cb675b7f5-qlcwd requesting resource cpu=15m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod multus-6c8tx requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod multus-8cpsm requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod multus-additional-cni-plugins-5sr6s requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod multus-additional-cni-plugins-bn22f requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod multus-additional-cni-plugins-kcltt requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod multus-admission-controller-56545f7655-2v9n2 requesting resource cpu=20m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod multus-admission-controller-56545f7655-cx8nb requesting resource cpu=20m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod multus-s2ck6 requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod network-metrics-daemon-bmgpj requesting resource cpu=20m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod network-metrics-daemon-c9fn7 requesting resource cpu=20m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod network-metrics-daemon-hz5x4 requesting resource cpu=20m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod network-check-source-b94cc7564-h2vbb requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod network-check-target-gbl7m requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod network-check-target-glmj8 requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod network-check-target-rbbb9 requesting resource cpu=10m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod network-operator-55dd4b48f9-2sfhw requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod catalog-operator-dbc6c488b-ddnlm requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod olm-operator-6b9698f878-l84v6 requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod package-server-manager-7cb4d78bc5-h2ptd requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod packageserver-588f486fcc-84nv4 requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod packageserver-588f486fcc-bkqw6 requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod metrics-745b98dffd-mhfpv requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod push-gateway-96bdb5f74-n2czq requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod service-ca-operator-59cdc768f4-hq9b9 requesting resource cpu=10m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod service-ca-869668698d-t2x9f requesting resource cpu=10m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod sonobuoy-e2e-job-6fcfb70347cc4f0d requesting resource cpu=0m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh requesting resource cpu=0m on Node 10.8.38.70
Feb 20 22:12:48.418: INFO: Pod sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj requesting resource cpu=0m on Node 10.8.38.69
Feb 20 22:12:48.418: INFO: Pod sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz requesting resource cpu=0m on Node 10.8.38.66
Feb 20 22:12:48.418: INFO: Pod tigera-operator-84f4f4565b-qjg88 requesting resource cpu=100m on Node 10.8.38.69
STEP: Starting Pods to consume most of the cluster CPU. 02/20/23 22:12:48.418
Feb 20 22:12:48.418: INFO: Creating a pod which consumes cpu=1937m on Node 10.8.38.69
Feb 20 22:12:48.481: INFO: Creating a pod which consumes cpu=1859m on Node 10.8.38.70
Feb 20 22:12:48.557: INFO: Creating a pod which consumes cpu=1945m on Node 10.8.38.66
Feb 20 22:12:48.653: INFO: Waiting up to 5m0s for pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17" in namespace "sched-pred-9580" to be "running"
Feb 20 22:12:48.668: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17": Phase="Pending", Reason="", readiness=false. Elapsed: 15.162015ms
Feb 20 22:12:50.681: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028118368s
Feb 20 22:12:52.681: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17": Phase="Running", Reason="", readiness=true. Elapsed: 4.027972691s
Feb 20 22:12:52.681: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17" satisfied condition "running"
Feb 20 22:12:52.681: INFO: Waiting up to 5m0s for pod "filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09" in namespace "sched-pred-9580" to be "running"
Feb 20 22:12:52.692: INFO: Pod "filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09": Phase="Running", Reason="", readiness=true. Elapsed: 10.948225ms
Feb 20 22:12:52.692: INFO: Pod "filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09" satisfied condition "running"
Feb 20 22:12:52.692: INFO: Waiting up to 5m0s for pod "filler-pod-f3910999-0684-43c6-900b-69488b12d79a" in namespace "sched-pred-9580" to be "running"
Feb 20 22:12:52.706: INFO: Pod "filler-pod-f3910999-0684-43c6-900b-69488b12d79a": Phase="Running", Reason="", readiness=true. Elapsed: 13.661751ms
Feb 20 22:12:52.706: INFO: Pod "filler-pod-f3910999-0684-43c6-900b-69488b12d79a" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/20/23 22:12:52.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a80352818e30], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9580/filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17 to 10.8.38.69] 02/20/23 22:12:52.725
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a803891cc490], Reason = [AddedInterface], Message = [Add eth0 [172.30.144.231/32] from k8s-pod-network] 02/20/23 22:12:52.725
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a8039af89a74], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/20/23 22:12:52.726
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a803aca4f93b], Reason = [Created], Message = [Created container filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17] 02/20/23 22:12:52.726
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a803aec165cc], Reason = [Started], Message = [Started container filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17] 02/20/23 22:12:52.727
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a80356c250dd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9580/filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09 to 10.8.38.70] 02/20/23 22:12:52.727
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a8038b011fa2], Reason = [AddedInterface], Message = [Add eth0 [172.30.31.174/32] from k8s-pod-network] 02/20/23 22:12:52.728
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a80399783d07], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/20/23 22:12:52.728
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a803a7bf3338], Reason = [Created], Message = [Created container filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09] 02/20/23 22:12:52.728
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a803a9829cbd], Reason = [Started], Message = [Started container filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09] 02/20/23 22:12:52.729
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a8035d450db8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9580/filler-pod-f3910999-0684-43c6-900b-69488b12d79a to 10.8.38.66] 02/20/23 22:12:52.729
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a8039359b0c9], Reason = [AddedInterface], Message = [Add eth0 [172.30.181.245/32] from k8s-pod-network] 02/20/23 22:12:52.729
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a803a19ef07b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/20/23 22:12:52.729
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a803afca84e2], Reason = [Created], Message = [Created container filler-pod-f3910999-0684-43c6-900b-69488b12d79a] 02/20/23 22:12:52.73
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a803b2628766], Reason = [Started], Message = [Started container filler-pod-f3910999-0684-43c6-900b-69488b12d79a] 02/20/23 22:12:52.731
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1745a8045160babe], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 02/20/23 22:12:52.777
STEP: removing the label node off the node 10.8.38.70 02/20/23 22:12:53.787
STEP: verifying the node doesn't have the label node 02/20/23 22:12:53.824
STEP: removing the label node off the node 10.8.38.66 02/20/23 22:12:53.843
STEP: verifying the node doesn't have the label node 02/20/23 22:12:53.897
STEP: removing the label node off the node 10.8.38.69 02/20/23 22:12:53.937
STEP: verifying the node doesn't have the label node 02/20/23 22:12:54.01
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:12:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9580" for this suite. 02/20/23 22:12:54.098
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":94,"skipped":1663,"failed":0}
------------------------------
• [SLOW TEST] [6.318 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:47.815
    Feb 20 22:12:47.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-pred 02/20/23 22:12:47.816
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:47.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:47.868
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 20 22:12:47.892: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 20 22:12:47.938: INFO: Waiting for terminating namespaces to be deleted...
    Feb 20 22:12:48.012: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.66 before test
    Feb 20 22:12:48.051: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container pvc-permissions ready: false, restart count 0
    Feb 20 22:12:48.051: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container migrator ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:12:48.051: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:48.051: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:48.051: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container prometheus-operator ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.052: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:12:48.052: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.69 before test
    Feb 20 22:12:48.105: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container vpn ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Feb 20 22:12:48.105: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container console-operator ready: true, restart count 1
    Feb 20 22:12:48.105: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Feb 20 22:12:48.105: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:12:48.105: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.105: INFO: 	Container dns-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container ingress-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container insights-operator ready: true, restart count 1
    Feb 20 22:12:48.106: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Feb 20 22:12:48.106: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container marketplace-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container check-endpoints ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container network-operator ready: true, restart count 1
    Feb 20 22:12:48.106: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container catalog-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:12:48.106: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:12:48.106: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container olm-operator ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container package-server-manager ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container metrics ready: true, restart count 1
    Feb 20 22:12:48.106: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container push-gateway ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container service-ca-operator ready: true, restart count 1
    Feb 20 22:12:48.106: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container e2e ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:12:48.106: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.106: INFO: 	Container tigera-operator ready: true, restart count 2
    Feb 20 22:12:48.106: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.70 before test
    Feb 20 22:12:48.142: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.142: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:12:48.142: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.142: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:12:48.142: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.142: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:12:48.142: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.142: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:12:48.142: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.142: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container registry ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:12:48.143: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container reload ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container telemeter-client ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:12:48.143: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container service-ca-controller ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:12:48.143: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:12:48.143: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node 10.8.38.66 02/20/23 22:12:48.225
    STEP: verifying the node has the label node 10.8.38.69 02/20/23 22:12:48.27
    STEP: verifying the node has the label node 10.8.38.70 02/20/23 22:12:48.342
    Feb 20 22:12:48.417: INFO: Pod calico-kube-controllers-5b6b964466-ljmhd requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod calico-node-6l54d requesting resource cpu=250m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod calico-node-9djxs requesting resource cpu=250m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod calico-node-jj7lx requesting resource cpu=250m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod calico-typha-65d7b689d4-97f55 requesting resource cpu=250m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod calico-typha-65d7b689d4-kpn77 requesting resource cpu=250m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 requesting resource cpu=5m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s requesting resource cpu=5m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod ibm-file-plugin-d4b9d9dd5-fjk8x requesting resource cpu=50m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod ibm-keepalived-watcher-jnq8d requesting resource cpu=5m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod ibm-keepalived-watcher-n8zf8 requesting resource cpu=5m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod ibm-keepalived-watcher-snrrr requesting resource cpu=5m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ibm-master-proxy-static-10.8.38.66 requesting resource cpu=26m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ibm-master-proxy-static-10.8.38.69 requesting resource cpu=26m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod ibm-master-proxy-static-10.8.38.70 requesting resource cpu=26m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod ibm-storage-watcher-86f4b88d84-sb4dd requesting resource cpu=50m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-driver-5s8m7 requesting resource cpu=50m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-driver-9s9rq requesting resource cpu=50m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-driver-dcm4m requesting resource cpu=50m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv requesting resource cpu=50m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod vpn-77468d4d47-xrvvz requesting resource cpu=5m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod cluster-node-tuning-operator-65bd8f65fb-z2xnp requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod tuned-57bz4 requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod tuned-jt9sm requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod tuned-sxlgc requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod cluster-samples-operator-585c69687-52df4 requesting resource cpu=20m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod cluster-storage-operator-66bf7c9c5f-jhz4m requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod csi-snapshot-controller-5bfddff4c-tqhzz requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod csi-snapshot-controller-5bfddff4c-whkcp requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod csi-snapshot-controller-operator-557b547dc4-dz4vh requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod csi-snapshot-webhook-544b96bbcd-bxjcg requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod csi-snapshot-webhook-544b96bbcd-rjs5h requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod console-operator-8578f747-4fpjs requesting resource cpu=20m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod console-57dd6cb75b-lgjgl requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod console-57dd6cb75b-lzmnl requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod downloads-8f679847b-nbfcm requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod downloads-8f679847b-r829z requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod dns-operator-c96b75bc5-66nj2 requesting resource cpu=20m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod dns-default-6gh9k requesting resource cpu=60m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod dns-default-7wnbj requesting resource cpu=60m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod dns-default-snlvx requesting resource cpu=60m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod node-resolver-2lz8p requesting resource cpu=5m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod node-resolver-9wzh6 requesting resource cpu=5m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod node-resolver-9zmwj requesting resource cpu=5m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod cluster-image-registry-operator-6cc666c6b4-gwz8b requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod image-registry-f445fb4b9-qprbx requesting resource cpu=100m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod node-ca-cq7v2 requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod node-ca-jmgpg requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod node-ca-k9szq requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ingress-canary-c2h9v requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod ingress-canary-cvlvh requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod ingress-canary-s48w8 requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod ingress-operator-75d985687c-2cff5 requesting resource cpu=20m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod router-default-f9ffd57f7-rc68j requesting resource cpu=100m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod router-default-f9ffd57f7-vxv45 requesting resource cpu=100m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod insights-operator-76dd9549c4-zpvkl requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod openshift-kube-proxy-46d5j requesting resource cpu=110m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod openshift-kube-proxy-4bz4j requesting resource cpu=110m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod openshift-kube-proxy-mjvmc requesting resource cpu=110m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod migrator-5c54d8d69d-jm5pm requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod certified-operators-mswgj requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod community-operators-rkvlk requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod marketplace-operator-57868d7ff-tn59b requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod redhat-marketplace-qnk27 requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod redhat-operators-8zq49 requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod cluster-monitoring-operator-57c54cf78c-stk6t requesting resource cpu=11m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod kube-state-metrics-75455b796c-nfnxt requesting resource cpu=4m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod node-exporter-857fp requesting resource cpu=9m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod node-exporter-9kjkf requesting resource cpu=9m on Node 10.8.38.69
    Feb 20 22:12:48.417: INFO: Pod node-exporter-zkcjf requesting resource cpu=9m on Node 10.8.38.66
    Feb 20 22:12:48.417: INFO: Pod openshift-state-metrics-5ff95d844f-nw74k requesting resource cpu=3m on Node 10.8.38.70
    Feb 20 22:12:48.417: INFO: Pod prometheus-adapter-685b6bd76d-7dn46 requesting resource cpu=1m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod prometheus-adapter-685b6bd76d-xrdf9 requesting resource cpu=1m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod prometheus-operator-688459b4f4-4cv2n requesting resource cpu=6m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-vqmvd requesting resource cpu=5m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod prometheus-operator-admission-webhook-7d4759d465-z26qc requesting resource cpu=5m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod telemeter-client-7c87b5c5c6-lpcrj requesting resource cpu=3m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod thanos-querier-cb675b7f5-8hjjt requesting resource cpu=15m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod thanos-querier-cb675b7f5-qlcwd requesting resource cpu=15m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod multus-6c8tx requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod multus-8cpsm requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod multus-additional-cni-plugins-5sr6s requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod multus-additional-cni-plugins-bn22f requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod multus-additional-cni-plugins-kcltt requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod multus-admission-controller-56545f7655-2v9n2 requesting resource cpu=20m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod multus-admission-controller-56545f7655-cx8nb requesting resource cpu=20m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod multus-s2ck6 requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod network-metrics-daemon-bmgpj requesting resource cpu=20m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod network-metrics-daemon-c9fn7 requesting resource cpu=20m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod network-metrics-daemon-hz5x4 requesting resource cpu=20m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod network-check-source-b94cc7564-h2vbb requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod network-check-target-gbl7m requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod network-check-target-glmj8 requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod network-check-target-rbbb9 requesting resource cpu=10m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod network-operator-55dd4b48f9-2sfhw requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod catalog-operator-dbc6c488b-ddnlm requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod olm-operator-6b9698f878-l84v6 requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod package-server-manager-7cb4d78bc5-h2ptd requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod packageserver-588f486fcc-84nv4 requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod packageserver-588f486fcc-bkqw6 requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod metrics-745b98dffd-mhfpv requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod push-gateway-96bdb5f74-n2czq requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod service-ca-operator-59cdc768f4-hq9b9 requesting resource cpu=10m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod service-ca-869668698d-t2x9f requesting resource cpu=10m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod sonobuoy-e2e-job-6fcfb70347cc4f0d requesting resource cpu=0m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh requesting resource cpu=0m on Node 10.8.38.70
    Feb 20 22:12:48.418: INFO: Pod sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj requesting resource cpu=0m on Node 10.8.38.69
    Feb 20 22:12:48.418: INFO: Pod sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz requesting resource cpu=0m on Node 10.8.38.66
    Feb 20 22:12:48.418: INFO: Pod tigera-operator-84f4f4565b-qjg88 requesting resource cpu=100m on Node 10.8.38.69
    STEP: Starting Pods to consume most of the cluster CPU. 02/20/23 22:12:48.418
    Feb 20 22:12:48.418: INFO: Creating a pod which consumes cpu=1937m on Node 10.8.38.69
    Feb 20 22:12:48.481: INFO: Creating a pod which consumes cpu=1859m on Node 10.8.38.70
    Feb 20 22:12:48.557: INFO: Creating a pod which consumes cpu=1945m on Node 10.8.38.66
    Feb 20 22:12:48.653: INFO: Waiting up to 5m0s for pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17" in namespace "sched-pred-9580" to be "running"
    Feb 20 22:12:48.668: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17": Phase="Pending", Reason="", readiness=false. Elapsed: 15.162015ms
    Feb 20 22:12:50.681: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028118368s
    Feb 20 22:12:52.681: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17": Phase="Running", Reason="", readiness=true. Elapsed: 4.027972691s
    Feb 20 22:12:52.681: INFO: Pod "filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17" satisfied condition "running"
    Feb 20 22:12:52.681: INFO: Waiting up to 5m0s for pod "filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09" in namespace "sched-pred-9580" to be "running"
    Feb 20 22:12:52.692: INFO: Pod "filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09": Phase="Running", Reason="", readiness=true. Elapsed: 10.948225ms
    Feb 20 22:12:52.692: INFO: Pod "filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09" satisfied condition "running"
    Feb 20 22:12:52.692: INFO: Waiting up to 5m0s for pod "filler-pod-f3910999-0684-43c6-900b-69488b12d79a" in namespace "sched-pred-9580" to be "running"
    Feb 20 22:12:52.706: INFO: Pod "filler-pod-f3910999-0684-43c6-900b-69488b12d79a": Phase="Running", Reason="", readiness=true. Elapsed: 13.661751ms
    Feb 20 22:12:52.706: INFO: Pod "filler-pod-f3910999-0684-43c6-900b-69488b12d79a" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/20/23 22:12:52.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a80352818e30], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9580/filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17 to 10.8.38.69] 02/20/23 22:12:52.725
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a803891cc490], Reason = [AddedInterface], Message = [Add eth0 [172.30.144.231/32] from k8s-pod-network] 02/20/23 22:12:52.725
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a8039af89a74], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/20/23 22:12:52.726
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a803aca4f93b], Reason = [Created], Message = [Created container filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17] 02/20/23 22:12:52.726
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17.1745a803aec165cc], Reason = [Started], Message = [Started container filler-pod-3ba61d93-bdaa-4bb6-87c0-187f2c8f9b17] 02/20/23 22:12:52.727
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a80356c250dd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9580/filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09 to 10.8.38.70] 02/20/23 22:12:52.727
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a8038b011fa2], Reason = [AddedInterface], Message = [Add eth0 [172.30.31.174/32] from k8s-pod-network] 02/20/23 22:12:52.728
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a80399783d07], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/20/23 22:12:52.728
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a803a7bf3338], Reason = [Created], Message = [Created container filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09] 02/20/23 22:12:52.728
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09.1745a803a9829cbd], Reason = [Started], Message = [Started container filler-pod-5aad15f0-5e6d-43bd-8745-9b8b75003e09] 02/20/23 22:12:52.729
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a8035d450db8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9580/filler-pod-f3910999-0684-43c6-900b-69488b12d79a to 10.8.38.66] 02/20/23 22:12:52.729
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a8039359b0c9], Reason = [AddedInterface], Message = [Add eth0 [172.30.181.245/32] from k8s-pod-network] 02/20/23 22:12:52.729
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a803a19ef07b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/20/23 22:12:52.729
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a803afca84e2], Reason = [Created], Message = [Created container filler-pod-f3910999-0684-43c6-900b-69488b12d79a] 02/20/23 22:12:52.73
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f3910999-0684-43c6-900b-69488b12d79a.1745a803b2628766], Reason = [Started], Message = [Started container filler-pod-f3910999-0684-43c6-900b-69488b12d79a] 02/20/23 22:12:52.731
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1745a8045160babe], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 02/20/23 22:12:52.777
    STEP: removing the label node off the node 10.8.38.70 02/20/23 22:12:53.787
    STEP: verifying the node doesn't have the label node 02/20/23 22:12:53.824
    STEP: removing the label node off the node 10.8.38.66 02/20/23 22:12:53.843
    STEP: verifying the node doesn't have the label node 02/20/23 22:12:53.897
    STEP: removing the label node off the node 10.8.38.69 02/20/23 22:12:53.937
    STEP: verifying the node doesn't have the label node 02/20/23 22:12:54.01
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:12:54.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9580" for this suite. 02/20/23 22:12:54.098
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:54.135
Feb 20 22:12:54.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:54.136
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:54.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:54.192
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 02/20/23 22:12:54.203
Feb 20 22:12:54.287: INFO: Waiting up to 5m0s for pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7" in namespace "emptydir-6105" to be "Succeeded or Failed"
Feb 20 22:12:54.298: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.267703ms
Feb 20 22:12:56.312: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024721405s
Feb 20 22:12:58.313: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025993196s
STEP: Saw pod success 02/20/23 22:12:58.313
Feb 20 22:12:58.314: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7" satisfied condition "Succeeded or Failed"
Feb 20 22:12:58.325: INFO: Trying to get logs from node 10.8.38.70 pod pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7 container test-container: <nil>
STEP: delete the pod 02/20/23 22:12:58.36
Feb 20 22:12:58.392: INFO: Waiting for pod pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7 to disappear
Feb 20 22:12:58.407: INFO: Pod pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:12:58.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6105" for this suite. 02/20/23 22:12:58.431
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":95,"skipped":1669,"failed":0}
------------------------------
• [4.329 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:54.135
    Feb 20 22:12:54.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:54.136
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:54.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:54.192
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/20/23 22:12:54.203
    Feb 20 22:12:54.287: INFO: Waiting up to 5m0s for pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7" in namespace "emptydir-6105" to be "Succeeded or Failed"
    Feb 20 22:12:54.298: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.267703ms
    Feb 20 22:12:56.312: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024721405s
    Feb 20 22:12:58.313: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025993196s
    STEP: Saw pod success 02/20/23 22:12:58.313
    Feb 20 22:12:58.314: INFO: Pod "pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7" satisfied condition "Succeeded or Failed"
    Feb 20 22:12:58.325: INFO: Trying to get logs from node 10.8.38.70 pod pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7 container test-container: <nil>
    STEP: delete the pod 02/20/23 22:12:58.36
    Feb 20 22:12:58.392: INFO: Waiting for pod pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7 to disappear
    Feb 20 22:12:58.407: INFO: Pod pod-372bcd8d-d5b0-44f0-b626-d60e99a7a6b7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:12:58.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6105" for this suite. 02/20/23 22:12:58.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:12:58.464
Feb 20 22:12:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:58.465
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:58.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:58.521
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 02/20/23 22:12:58.531
Feb 20 22:12:58.578: INFO: Waiting up to 5m0s for pod "pod-4c870696-1b74-43b8-a550-504ee9564560" in namespace "emptydir-9568" to be "Succeeded or Failed"
Feb 20 22:12:58.591: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560": Phase="Pending", Reason="", readiness=false. Elapsed: 12.383673ms
Feb 20 22:13:00.609: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030782906s
Feb 20 22:13:02.605: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0259894s
STEP: Saw pod success 02/20/23 22:13:02.605
Feb 20 22:13:02.605: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560" satisfied condition "Succeeded or Failed"
Feb 20 22:13:02.616: INFO: Trying to get logs from node 10.8.38.70 pod pod-4c870696-1b74-43b8-a550-504ee9564560 container test-container: <nil>
STEP: delete the pod 02/20/23 22:13:02.686
Feb 20 22:13:02.728: INFO: Waiting for pod pod-4c870696-1b74-43b8-a550-504ee9564560 to disappear
Feb 20 22:13:02.739: INFO: Pod pod-4c870696-1b74-43b8-a550-504ee9564560 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:13:02.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9568" for this suite. 02/20/23 22:13:02.766
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":96,"skipped":1677,"failed":0}
------------------------------
• [4.335 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:12:58.464
    Feb 20 22:12:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:12:58.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:12:58.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:12:58.521
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/20/23 22:12:58.531
    Feb 20 22:12:58.578: INFO: Waiting up to 5m0s for pod "pod-4c870696-1b74-43b8-a550-504ee9564560" in namespace "emptydir-9568" to be "Succeeded or Failed"
    Feb 20 22:12:58.591: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560": Phase="Pending", Reason="", readiness=false. Elapsed: 12.383673ms
    Feb 20 22:13:00.609: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030782906s
    Feb 20 22:13:02.605: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0259894s
    STEP: Saw pod success 02/20/23 22:13:02.605
    Feb 20 22:13:02.605: INFO: Pod "pod-4c870696-1b74-43b8-a550-504ee9564560" satisfied condition "Succeeded or Failed"
    Feb 20 22:13:02.616: INFO: Trying to get logs from node 10.8.38.70 pod pod-4c870696-1b74-43b8-a550-504ee9564560 container test-container: <nil>
    STEP: delete the pod 02/20/23 22:13:02.686
    Feb 20 22:13:02.728: INFO: Waiting for pod pod-4c870696-1b74-43b8-a550-504ee9564560 to disappear
    Feb 20 22:13:02.739: INFO: Pod pod-4c870696-1b74-43b8-a550-504ee9564560 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:13:02.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9568" for this suite. 02/20/23 22:13:02.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:13:02.806
Feb 20 22:13:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename proxy 02/20/23 22:13:02.815
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:02.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:02.894
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 20 22:13:02.912: INFO: Creating pod...
Feb 20 22:13:02.978: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4404" to be "running"
Feb 20 22:13:02.992: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.871222ms
Feb 20 22:13:05.005: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.026406994s
Feb 20 22:13:05.005: INFO: Pod "agnhost" satisfied condition "running"
Feb 20 22:13:05.005: INFO: Creating service...
Feb 20 22:13:05.032: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=DELETE
Feb 20 22:13:05.067: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 20 22:13:05.067: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=OPTIONS
Feb 20 22:13:05.083: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 20 22:13:05.083: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=PATCH
Feb 20 22:13:05.097: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 20 22:13:05.097: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=POST
Feb 20 22:13:05.111: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 20 22:13:05.111: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=PUT
Feb 20 22:13:05.125: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 20 22:13:05.125: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 20 22:13:05.144: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 20 22:13:05.144: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 20 22:13:05.167: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 20 22:13:05.167: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 20 22:13:05.192: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 20 22:13:05.192: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=POST
Feb 20 22:13:05.213: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 20 22:13:05.213: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=PUT
Feb 20 22:13:05.231: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 20 22:13:05.231: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=GET
Feb 20 22:13:05.242: INFO: http.Client request:GET StatusCode:301
Feb 20 22:13:05.243: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=GET
Feb 20 22:13:05.273: INFO: http.Client request:GET StatusCode:301
Feb 20 22:13:05.273: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=HEAD
Feb 20 22:13:05.283: INFO: http.Client request:HEAD StatusCode:301
Feb 20 22:13:05.283: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 20 22:13:05.305: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 20 22:13:05.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4404" for this suite. 02/20/23 22:13:05.324
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":97,"skipped":1727,"failed":0}
------------------------------
• [2.547 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:13:02.806
    Feb 20 22:13:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename proxy 02/20/23 22:13:02.815
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:02.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:02.894
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 20 22:13:02.912: INFO: Creating pod...
    Feb 20 22:13:02.978: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4404" to be "running"
    Feb 20 22:13:02.992: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.871222ms
    Feb 20 22:13:05.005: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.026406994s
    Feb 20 22:13:05.005: INFO: Pod "agnhost" satisfied condition "running"
    Feb 20 22:13:05.005: INFO: Creating service...
    Feb 20 22:13:05.032: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=DELETE
    Feb 20 22:13:05.067: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 20 22:13:05.067: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=OPTIONS
    Feb 20 22:13:05.083: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 20 22:13:05.083: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=PATCH
    Feb 20 22:13:05.097: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 20 22:13:05.097: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=POST
    Feb 20 22:13:05.111: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 20 22:13:05.111: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=PUT
    Feb 20 22:13:05.125: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 20 22:13:05.125: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 20 22:13:05.144: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 20 22:13:05.144: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 20 22:13:05.167: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 20 22:13:05.167: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 20 22:13:05.192: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 20 22:13:05.192: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=POST
    Feb 20 22:13:05.213: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 20 22:13:05.213: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 20 22:13:05.231: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 20 22:13:05.231: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=GET
    Feb 20 22:13:05.242: INFO: http.Client request:GET StatusCode:301
    Feb 20 22:13:05.243: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=GET
    Feb 20 22:13:05.273: INFO: http.Client request:GET StatusCode:301
    Feb 20 22:13:05.273: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/pods/agnhost/proxy?method=HEAD
    Feb 20 22:13:05.283: INFO: http.Client request:HEAD StatusCode:301
    Feb 20 22:13:05.283: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4404/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 20 22:13:05.305: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 20 22:13:05.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4404" for this suite. 02/20/23 22:13:05.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:13:05.355
Feb 20 22:13:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 22:13:05.357
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:05.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:05.443
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
W0220 22:13:05.496481      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:13:05.508: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 20 22:13:10.521: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/20/23 22:13:10.521
Feb 20 22:13:10.522: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 20 22:13:12.538: INFO: Creating deployment "test-rollover-deployment"
Feb 20 22:13:12.557: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 20 22:13:14.575: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 20 22:13:14.595: INFO: Ensure that both replica sets have 1 created replica
Feb 20 22:13:14.627: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 20 22:13:14.648: INFO: Updating deployment test-rollover-deployment
Feb 20 22:13:14.649: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 20 22:13:16.667: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 20 22:13:16.689: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 20 22:13:16.713: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 22:13:16.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:13:18.737: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 22:13:18.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:13:20.739: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 22:13:20.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:13:22.735: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 22:13:22.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:13:24.750: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 22:13:24.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:13:26.754: INFO: 
Feb 20 22:13:26.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:13:28.763: INFO: 
Feb 20 22:13:28.763: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 22:13:28.797: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6877  8c4c5189-118d-47d2-873b-ab40adc0da17 82659 2 2023-02-20 22:13:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cf7e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-20 22:13:12 +0000 UTC,LastTransitionTime:2023-02-20 22:13:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-20 22:13:26 +0000 UTC,LastTransitionTime:2023-02-20 22:13:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 22:13:28.811: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6877  fa6ebdac-ee4b-4059-822f-e5834716ef10 82647 2 2023-02-20 22:13:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8c4c5189-118d-47d2-873b-ab40adc0da17 0xc000d0c617 0xc000d0c618}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c4c5189-118d-47d2-873b-ab40adc0da17\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d0cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:13:28.811: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 20 22:13:28.811: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6877  0742d251-3492-4dc9-84c0-d48068b01187 82658 2 2023-02-20 22:13:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8c4c5189-118d-47d2-873b-ab40adc0da17 0xc000d0c2d7 0xc000d0c2d8}] [] [{e2e.test Update apps/v1 2023-02-20 22:13:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c4c5189-118d-47d2-873b-ab40adc0da17\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000d0c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:13:28.812: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6877  17791c0b-d25a-4fee-a73d-fb1829c3c8e0 82571 2 2023-02-20 22:13:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8c4c5189-118d-47d2-873b-ab40adc0da17 0xc000d0c427 0xc000d0c428}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c4c5189-118d-47d2-873b-ab40adc0da17\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d0c4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:13:28.832: INFO: Pod "test-rollover-deployment-6d45fd857b-vm8qc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-vm8qc test-rollover-deployment-6d45fd857b- deployment-6877  762cc8f0-2c09-4521-8fcf-b3fce8255712 82600 0 2023-02-20 22:13:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:2dc726497d7d4fe7c3967ae3dbabbba25d4a5583b387366a88fc9c857aa513b1 cni.projectcalico.org/podIP:172.30.181.248/32 cni.projectcalico.org/podIPs:172.30.181.248/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.248"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.248"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b fa6ebdac-ee4b-4059-822f-e5834716ef10 0xc000d0de67 0xc000d0de68}] [] [{kube-controller-manager Update v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6ebdac-ee4b-4059-822f-e5834716ef10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:13:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87xmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87xmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-njs5g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.248,StartTime:2023-02-20 22:13:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:13:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://972b1959bb2dce3e02a137b005bcebe362a5e413c67acaad5cbce6cb8d28c5ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 22:13:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6877" for this suite. 02/20/23 22:13:28.881
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":98,"skipped":1752,"failed":0}
------------------------------
• [SLOW TEST] [23.551 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:13:05.355
    Feb 20 22:13:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 22:13:05.357
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:05.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:05.443
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    W0220 22:13:05.496481      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:13:05.508: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 20 22:13:10.521: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/20/23 22:13:10.521
    Feb 20 22:13:10.522: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 20 22:13:12.538: INFO: Creating deployment "test-rollover-deployment"
    Feb 20 22:13:12.557: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 20 22:13:14.575: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 20 22:13:14.595: INFO: Ensure that both replica sets have 1 created replica
    Feb 20 22:13:14.627: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 20 22:13:14.648: INFO: Updating deployment test-rollover-deployment
    Feb 20 22:13:14.649: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 20 22:13:16.667: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 20 22:13:16.689: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 20 22:13:16.713: INFO: all replica sets need to contain the pod-template-hash label
    Feb 20 22:13:16.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:13:18.737: INFO: all replica sets need to contain the pod-template-hash label
    Feb 20 22:13:18.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:13:20.739: INFO: all replica sets need to contain the pod-template-hash label
    Feb 20 22:13:20.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:13:22.735: INFO: all replica sets need to contain the pod-template-hash label
    Feb 20 22:13:22.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:13:24.750: INFO: all replica sets need to contain the pod-template-hash label
    Feb 20 22:13:24.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:13:26.754: INFO: 
    Feb 20 22:13:26.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 13, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 13, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:13:28.763: INFO: 
    Feb 20 22:13:28.763: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 22:13:28.797: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6877  8c4c5189-118d-47d2-873b-ab40adc0da17 82659 2 2023-02-20 22:13:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cf7e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-20 22:13:12 +0000 UTC,LastTransitionTime:2023-02-20 22:13:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-20 22:13:26 +0000 UTC,LastTransitionTime:2023-02-20 22:13:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 20 22:13:28.811: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6877  fa6ebdac-ee4b-4059-822f-e5834716ef10 82647 2 2023-02-20 22:13:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8c4c5189-118d-47d2-873b-ab40adc0da17 0xc000d0c617 0xc000d0c618}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c4c5189-118d-47d2-873b-ab40adc0da17\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d0cb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:13:28.811: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 20 22:13:28.811: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6877  0742d251-3492-4dc9-84c0-d48068b01187 82658 2 2023-02-20 22:13:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8c4c5189-118d-47d2-873b-ab40adc0da17 0xc000d0c2d7 0xc000d0c2d8}] [] [{e2e.test Update apps/v1 2023-02-20 22:13:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c4c5189-118d-47d2-873b-ab40adc0da17\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000d0c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:13:28.812: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6877  17791c0b-d25a-4fee-a73d-fb1829c3c8e0 82571 2 2023-02-20 22:13:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8c4c5189-118d-47d2-873b-ab40adc0da17 0xc000d0c427 0xc000d0c428}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8c4c5189-118d-47d2-873b-ab40adc0da17\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d0c4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:13:28.832: INFO: Pod "test-rollover-deployment-6d45fd857b-vm8qc" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-vm8qc test-rollover-deployment-6d45fd857b- deployment-6877  762cc8f0-2c09-4521-8fcf-b3fce8255712 82600 0 2023-02-20 22:13:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:2dc726497d7d4fe7c3967ae3dbabbba25d4a5583b387366a88fc9c857aa513b1 cni.projectcalico.org/podIP:172.30.181.248/32 cni.projectcalico.org/podIPs:172.30.181.248/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.248"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.248"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b fa6ebdac-ee4b-4059-822f-e5834716ef10 0xc000d0de67 0xc000d0de68}] [] [{kube-controller-manager Update v1 2023-02-20 22:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa6ebdac-ee4b-4059-822f-e5834716ef10\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:13:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.248\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87xmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87xmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-njs5g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:13:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.248,StartTime:2023-02-20 22:13:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:13:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://972b1959bb2dce3e02a137b005bcebe362a5e413c67acaad5cbce6cb8d28c5ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 22:13:28.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6877" for this suite. 02/20/23 22:13:28.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:13:28.92
Feb 20 22:13:28.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:13:28.922
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:28.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:28.982
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:13:29.101
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:13:29.347
STEP: Deploying the webhook pod 02/20/23 22:13:29.376
STEP: Wait for the deployment to be ready 02/20/23 22:13:29.432
Feb 20 22:13:29.472: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/20/23 22:13:31.503
STEP: Verifying the service has paired with the endpoint 02/20/23 22:13:31.528
Feb 20 22:13:32.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 02/20/23 22:13:32.548
STEP: create a pod that should be denied by the webhook 02/20/23 22:13:32.626
STEP: create a pod that causes the webhook to hang 02/20/23 22:13:32.678
STEP: create a configmap that should be denied by the webhook 02/20/23 22:13:42.717
STEP: create a configmap that should be admitted by the webhook 02/20/23 22:13:42.737
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/20/23 22:13:42.76
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/20/23 22:13:42.787
STEP: create a namespace that bypass the webhook 02/20/23 22:13:42.81
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/20/23 22:13:42.83
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:13:42.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1445" for this suite. 02/20/23 22:13:42.985
STEP: Destroying namespace "webhook-1445-markers" for this suite. 02/20/23 22:13:43.01
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":99,"skipped":1784,"failed":0}
------------------------------
• [SLOW TEST] [14.214 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:13:28.92
    Feb 20 22:13:28.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:13:28.922
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:28.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:28.982
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:13:29.101
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:13:29.347
    STEP: Deploying the webhook pod 02/20/23 22:13:29.376
    STEP: Wait for the deployment to be ready 02/20/23 22:13:29.432
    Feb 20 22:13:29.472: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/20/23 22:13:31.503
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:13:31.528
    Feb 20 22:13:32.529: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 02/20/23 22:13:32.548
    STEP: create a pod that should be denied by the webhook 02/20/23 22:13:32.626
    STEP: create a pod that causes the webhook to hang 02/20/23 22:13:32.678
    STEP: create a configmap that should be denied by the webhook 02/20/23 22:13:42.717
    STEP: create a configmap that should be admitted by the webhook 02/20/23 22:13:42.737
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/20/23 22:13:42.76
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/20/23 22:13:42.787
    STEP: create a namespace that bypass the webhook 02/20/23 22:13:42.81
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/20/23 22:13:42.83
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:13:42.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1445" for this suite. 02/20/23 22:13:42.985
    STEP: Destroying namespace "webhook-1445-markers" for this suite. 02/20/23 22:13:43.01
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:13:43.137
Feb 20 22:13:43.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:13:43.139
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:43.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:43.196
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 02/20/23 22:13:43.213
Feb 20 22:13:43.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9352 api-versions'
Feb 20 22:13:43.360: INFO: stderr: ""
Feb 20 22:13:43.360: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:13:43.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9352" for this suite. 02/20/23 22:13:43.376
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":100,"skipped":1793,"failed":0}
------------------------------
• [0.262 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:13:43.137
    Feb 20 22:13:43.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:13:43.139
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:43.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:43.196
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 02/20/23 22:13:43.213
    Feb 20 22:13:43.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9352 api-versions'
    Feb 20 22:13:43.360: INFO: stderr: ""
    Feb 20 22:13:43.360: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:13:43.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9352" for this suite. 02/20/23 22:13:43.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:13:43.402
Feb 20 22:13:43.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 22:13:43.403
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:43.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:43.461
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5188.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/20/23 22:13:43.472
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5188.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/20/23 22:13:43.472
STEP: creating a pod to probe /etc/hosts 02/20/23 22:13:43.472
STEP: submitting the pod to kubernetes 02/20/23 22:13:43.473
Feb 20 22:13:43.584: INFO: Waiting up to 15m0s for pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb" in namespace "dns-5188" to be "running"
Feb 20 22:13:43.598: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.148565ms
Feb 20 22:13:45.612: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028624437s
Feb 20 22:13:47.611: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb": Phase="Running", Reason="", readiness=true. Elapsed: 4.026984781s
Feb 20 22:13:47.611: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb" satisfied condition "running"
STEP: retrieving the pod 02/20/23 22:13:47.611
STEP: looking for the results for each expected name from probers 02/20/23 22:13:47.623
Feb 20 22:13:47.707: INFO: DNS probes using dns-5188/dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb succeeded

STEP: deleting the pod 02/20/23 22:13:47.707
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 22:13:47.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5188" for this suite. 02/20/23 22:13:47.783
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":101,"skipped":1840,"failed":0}
------------------------------
• [4.405 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:13:43.402
    Feb 20 22:13:43.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 22:13:43.403
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:43.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:43.461
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5188.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/20/23 22:13:43.472
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5188.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5188.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/20/23 22:13:43.472
    STEP: creating a pod to probe /etc/hosts 02/20/23 22:13:43.472
    STEP: submitting the pod to kubernetes 02/20/23 22:13:43.473
    Feb 20 22:13:43.584: INFO: Waiting up to 15m0s for pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb" in namespace "dns-5188" to be "running"
    Feb 20 22:13:43.598: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.148565ms
    Feb 20 22:13:45.612: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028624437s
    Feb 20 22:13:47.611: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb": Phase="Running", Reason="", readiness=true. Elapsed: 4.026984781s
    Feb 20 22:13:47.611: INFO: Pod "dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 22:13:47.611
    STEP: looking for the results for each expected name from probers 02/20/23 22:13:47.623
    Feb 20 22:13:47.707: INFO: DNS probes using dns-5188/dns-test-4c7a2902-0f61-41fa-9c5c-e456c31443fb succeeded

    STEP: deleting the pod 02/20/23 22:13:47.707
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 22:13:47.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5188" for this suite. 02/20/23 22:13:47.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:13:47.812
Feb 20 22:13:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-pred 02/20/23 22:13:47.814
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:47.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:47.869
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 20 22:13:47.882: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 22:13:47.946: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 22:13:47.984: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.66 before test
Feb 20 22:13:48.030: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:13:48.030: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:13:48.030: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:13:48.030: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:13:48.030: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:13:48.030: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:13:48.030: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:13:48.030: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:13:48.030: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:13:48.030: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:13:48.030: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:13:48.030: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:13:48.030: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.030: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.030: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:13:48.030: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:13:48.031: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 20 22:13:48.031: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:13:48.031: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container router ready: true, restart count 0
Feb 20 22:13:48.031: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container migrator ready: true, restart count 0
Feb 20 22:13:48.031: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:13:48.031: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:13:48.031: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:13:48.031: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:13:48.031: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:13:48.031: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:13:48.031: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
Feb 20 22:13:48.031: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:13:48.031: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:13:48.032: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 20 22:13:48.032: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:13:48.032: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:13:48.032: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:13:48.032: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:13:48.032: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:13:48.032: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:13:48.032: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 20 22:13:48.032: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.032: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:13:48.032: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.69 before test
Feb 20 22:13:48.080: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 20 22:13:48.080: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:13:48.080: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 20 22:13:48.080: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:13:48.080: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:13:48.080: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:13:48.080: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 20 22:13:48.080: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:13:48.080: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 20 22:13:48.080: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.080: INFO: 	Container vpn ready: true, restart count 0
Feb 20 22:13:48.080: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:13:48.081: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 20 22:13:48.081: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Feb 20 22:13:48.081: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container console-operator ready: true, restart count 1
Feb 20 22:13:48.081: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Feb 20 22:13:48.081: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container console ready: true, restart count 0
Feb 20 22:13:48.081: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container dns-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:13:48.081: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:13:48.081: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:13:48.081: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container insights-operator ready: true, restart count 1
Feb 20 22:13:48.081: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Feb 20 22:13:48.081: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:13:48.081: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 20 22:13:48.081: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:13:48.081: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:13:48.081: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:13:48.081: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:13:48.081: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container check-endpoints ready: true, restart count 0
Feb 20 22:13:48.081: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:13:48.081: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container network-operator ready: true, restart count 1
Feb 20 22:13:48.081: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:13:48.081: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:13:48.081: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container olm-operator ready: true, restart count 0
Feb 20 22:13:48.081: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container package-server-manager ready: true, restart count 0
Feb 20 22:13:48.081: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:13:48.081: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container metrics ready: true, restart count 1
Feb 20 22:13:48.081: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container push-gateway ready: true, restart count 0
Feb 20 22:13:48.081: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container service-ca-operator ready: true, restart count 1
Feb 20 22:13:48.081: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container e2e ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:13:48.081: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:13:48.081: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 20 22:13:48.081: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.081: INFO: 	Container tigera-operator ready: true, restart count 2
Feb 20 22:13:48.081: INFO: 
Logging pods the apiserver thinks is on node 10.8.38.70 before test
Feb 20 22:13:48.119: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 22:13:48.120: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container calico-typha ready: true, restart count 0
Feb 20 22:13:48.120: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
Feb 20 22:13:48.120: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 20 22:13:48.120: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 20 22:13:48.120: INFO: 	Container pause ready: true, restart count 0
Feb 20 22:13:48.120: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 20 22:13:48.120: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.120: INFO: 	Container tuned ready: true, restart count 0
Feb 20 22:13:48.120: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.121: INFO: 	Container snapshot-controller ready: true, restart count 0
Feb 20 22:13:48.121: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.121: INFO: 	Container webhook ready: true, restart count 0
Feb 20 22:13:48.121: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.121: INFO: 	Container console ready: true, restart count 0
Feb 20 22:13:48.121: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.121: INFO: 	Container download-server ready: true, restart count 0
Feb 20 22:13:48.121: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.121: INFO: 	Container dns ready: true, restart count 0
Feb 20 22:13:48.121: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 20 22:13:48.122: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container registry ready: true, restart count 0
Feb 20 22:13:48.122: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container node-ca ready: true, restart count 0
Feb 20 22:13:48.122: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Feb 20 22:13:48.122: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container router ready: true, restart count 0
Feb 20 22:13:48.122: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container registry-server ready: true, restart count 0
Feb 20 22:13:48.122: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container alertmanager ready: true, restart count 1
Feb 20 22:13:48.122: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 20 22:13:48.122: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.122: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 22:13:48.122: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 20 22:13:48.123: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container config-reloader ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container prometheus ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 20 22:13:48.123: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Feb 20 22:13:48.123: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container reload ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 20 22:13:48.123: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container thanos-query ready: true, restart count 0
Feb 20 22:13:48.123: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container kube-multus ready: true, restart count 0
Feb 20 22:13:48.123: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Feb 20 22:13:48.123: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 20 22:13:48.123: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 20 22:13:48.123: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Feb 20 22:13:48.123: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container network-check-target-container ready: true, restart count 0
Feb 20 22:13:48.123: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container collect-profiles ready: false, restart count 0
Feb 20 22:13:48.123: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container packageserver ready: true, restart count 0
Feb 20 22:13:48.123: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container service-ca-controller ready: true, restart count 0
Feb 20 22:13:48.123: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
Feb 20 22:13:48.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 20 22:13:48.124: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/20/23 22:13:48.124
Feb 20 22:13:48.169: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4712" to be "running"
Feb 20 22:13:48.181: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.056952ms
Feb 20 22:13:50.194: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.025063514s
Feb 20 22:13:50.194: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/20/23 22:13:50.209
STEP: Trying to apply a random label on the found node. 02/20/23 22:13:50.314
STEP: verifying the node has the label kubernetes.io/e2e-5fdacada-388c-4a69-8a44-25e1478da90c 95 02/20/23 22:13:50.357
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/20/23 22:13:50.371
Feb 20 22:13:50.415: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4712" to be "not pending"
Feb 20 22:13:50.448: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 33.273517ms
Feb 20 22:13:52.461: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045787547s
Feb 20 22:13:54.460: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.045200608s
Feb 20 22:13:54.460: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.8.38.66 on the node which pod4 resides and expect not scheduled 02/20/23 22:13:54.46
Feb 20 22:13:54.494: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4712" to be "not pending"
Feb 20 22:13:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.090443ms
Feb 20 22:13:56.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024639607s
Feb 20 22:13:58.523: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028518181s
Feb 20 22:14:00.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041372457s
Feb 20 22:14:02.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026537367s
Feb 20 22:14:04.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026808159s
Feb 20 22:14:06.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025582588s
Feb 20 22:14:08.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026672777s
Feb 20 22:14:10.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024873394s
Feb 20 22:14:12.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023885359s
Feb 20 22:14:14.529: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.034911986s
Feb 20 22:14:16.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026223287s
Feb 20 22:14:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.024969755s
Feb 20 22:14:20.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024939532s
Feb 20 22:14:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.024624801s
Feb 20 22:14:24.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.025366205s
Feb 20 22:14:26.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023740948s
Feb 20 22:14:28.524: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029473465s
Feb 20 22:14:30.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025901453s
Feb 20 22:14:32.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023950901s
Feb 20 22:14:34.523: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.028720954s
Feb 20 22:14:36.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.027485499s
Feb 20 22:14:38.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025389368s
Feb 20 22:14:40.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.024903003s
Feb 20 22:14:42.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023697926s
Feb 20 22:14:44.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.024602964s
Feb 20 22:14:46.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.041060856s
Feb 20 22:14:48.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.025520981s
Feb 20 22:14:50.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.026074412s
Feb 20 22:14:52.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024758997s
Feb 20 22:14:54.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.027299515s
Feb 20 22:14:56.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.02425838s
Feb 20 22:14:58.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.024289725s
Feb 20 22:15:00.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024719404s
Feb 20 22:15:02.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025696846s
Feb 20 22:15:04.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025750803s
Feb 20 22:15:06.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.056450205s
Feb 20 22:15:08.527: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.032578355s
Feb 20 22:15:10.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024210519s
Feb 20 22:15:12.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.024220996s
Feb 20 22:15:14.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024289042s
Feb 20 22:15:16.525: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.031142838s
Feb 20 22:15:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024706803s
Feb 20 22:15:20.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.024754117s
Feb 20 22:15:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.024489593s
Feb 20 22:15:24.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.024980088s
Feb 20 22:15:26.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025291925s
Feb 20 22:15:28.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024853621s
Feb 20 22:15:30.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.023571134s
Feb 20 22:15:32.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024090217s
Feb 20 22:15:34.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02523037s
Feb 20 22:15:36.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.024819865s
Feb 20 22:15:38.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.027597523s
Feb 20 22:15:40.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026842916s
Feb 20 22:15:42.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023583526s
Feb 20 22:15:44.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025247245s
Feb 20 22:15:46.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.025286882s
Feb 20 22:15:48.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.056449211s
Feb 20 22:15:50.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025540845s
Feb 20 22:15:52.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.027282068s
Feb 20 22:15:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.036301858s
Feb 20 22:15:56.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.027938448s
Feb 20 22:15:58.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.026414017s
Feb 20 22:16:00.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025480497s
Feb 20 22:16:02.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.027837419s
Feb 20 22:16:04.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.025959882s
Feb 20 22:16:06.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.025606061s
Feb 20 22:16:08.525: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.031015332s
Feb 20 22:16:10.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.025574664s
Feb 20 22:16:12.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.02549327s
Feb 20 22:16:14.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.026796098s
Feb 20 22:16:16.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.035632672s
Feb 20 22:16:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.024792731s
Feb 20 22:16:20.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.041866038s
Feb 20 22:16:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.024577155s
Feb 20 22:16:24.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.025776681s
Feb 20 22:16:26.523: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.028481765s
Feb 20 22:16:28.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.024771318s
Feb 20 22:16:30.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.024776354s
Feb 20 22:16:32.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.024996718s
Feb 20 22:16:34.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.024809698s
Feb 20 22:16:36.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024498789s
Feb 20 22:16:38.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.026764588s
Feb 20 22:16:40.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.025046375s
Feb 20 22:16:42.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.041276574s
Feb 20 22:16:44.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.027313382s
Feb 20 22:16:46.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.027392721s
Feb 20 22:16:48.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.025993185s
Feb 20 22:16:50.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.027260067s
Feb 20 22:16:52.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.02664386s
Feb 20 22:16:54.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.025502165s
Feb 20 22:16:56.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.055050179s
Feb 20 22:16:58.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.024806259s
Feb 20 22:17:00.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025815184s
Feb 20 22:17:02.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.025762739s
Feb 20 22:17:04.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.025931526s
Feb 20 22:17:06.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.024937643s
Feb 20 22:17:08.544: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.049499668s
Feb 20 22:17:10.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02529558s
Feb 20 22:17:12.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.025037915s
Feb 20 22:17:14.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.026279945s
Feb 20 22:17:16.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.025380599s
Feb 20 22:17:18.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.026081857s
Feb 20 22:17:20.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.026066874s
Feb 20 22:17:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.024938702s
Feb 20 22:17:24.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.024797005s
Feb 20 22:17:26.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.024007042s
Feb 20 22:17:28.540: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.045592185s
Feb 20 22:17:30.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.025367361s
Feb 20 22:17:32.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.024813486s
Feb 20 22:17:34.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.024830405s
Feb 20 22:17:36.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.025177659s
Feb 20 22:17:38.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.024633697s
Feb 20 22:17:40.546: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.051729796s
Feb 20 22:17:42.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.02383071s
Feb 20 22:17:44.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.025091965s
Feb 20 22:17:46.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024807328s
Feb 20 22:17:48.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.024785466s
Feb 20 22:17:50.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.025473475s
Feb 20 22:17:52.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.024805506s
Feb 20 22:17:54.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.024677523s
Feb 20 22:17:56.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.024975314s
Feb 20 22:17:58.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.025482225s
Feb 20 22:18:00.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.024984957s
Feb 20 22:18:02.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.025051131s
Feb 20 22:18:04.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.028092222s
Feb 20 22:18:06.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.024816462s
Feb 20 22:18:08.528: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.03349498s
Feb 20 22:18:10.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.025588079s
Feb 20 22:18:12.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.025697563s
Feb 20 22:18:14.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.025436538s
Feb 20 22:18:16.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024888202s
Feb 20 22:18:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.024852206s
Feb 20 22:18:20.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.02475633s
Feb 20 22:18:22.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.025377598s
Feb 20 22:18:24.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.025516875s
Feb 20 22:18:26.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.026537176s
Feb 20 22:18:28.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.025475021s
Feb 20 22:18:30.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.024950855s
Feb 20 22:18:32.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.038912306s
Feb 20 22:18:34.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.025781204s
Feb 20 22:18:36.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.027704289s
Feb 20 22:18:38.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.02580363s
Feb 20 22:18:40.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024649461s
Feb 20 22:18:42.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.028045827s
Feb 20 22:18:44.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.026457518s
Feb 20 22:18:46.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.024807914s
Feb 20 22:18:48.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.024755766s
Feb 20 22:18:50.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.038443673s
Feb 20 22:18:52.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.023548847s
Feb 20 22:18:54.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.024980082s
Feb 20 22:18:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.036930633s
STEP: removing the label kubernetes.io/e2e-5fdacada-388c-4a69-8a44-25e1478da90c off the node 10.8.38.66 02/20/23 22:18:54.531
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5fdacada-388c-4a69-8a44-25e1478da90c 02/20/23 22:18:54.579
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:18:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4712" for this suite. 02/20/23 22:18:54.606
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":102,"skipped":1870,"failed":0}
------------------------------
• [SLOW TEST] [306.815 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:13:47.812
    Feb 20 22:13:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-pred 02/20/23 22:13:47.814
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:13:47.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:13:47.869
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 20 22:13:47.882: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 20 22:13:47.946: INFO: Waiting for terminating namespaces to be deleted...
    Feb 20 22:13:47.984: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.66 before test
    Feb 20 22:13:48.030: INFO: calico-node-jj7lx from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: calico-typha-65d7b689d4-kpn77 from calico-system started at 2023-02-20 19:39:08 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5 from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: ibm-keepalived-watcher-snrrr from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: ibm-master-proxy-static-10.8.38.66 from kube-system started at 2023-02-20 19:37:47 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: ibmcloud-block-storage-driver-9s9rq from kube-system started at 2023-02-20 19:38:01 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: tuned-57bz4 from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: csi-snapshot-controller-5bfddff4c-whkcp from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: csi-snapshot-webhook-544b96bbcd-bxjcg from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: downloads-8f679847b-nbfcm from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: dns-default-6gh9k from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: node-resolver-9wzh6 from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.030: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:13:48.030: INFO: node-ca-k9szq from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: registry-pvc-permissions-2zn2v from openshift-image-registry started at 2023-02-20 19:45:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container pvc-permissions ready: false, restart count 0
    Feb 20 22:13:48.031: INFO: ingress-canary-c2h9v from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: router-default-f9ffd57f7-vxv45 from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: openshift-kube-proxy-4bz4j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: migrator-5c54d8d69d-jm5pm from openshift-kube-storage-version-migrator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container migrator ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: certified-operators-mswgj from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: community-operators-rkvlk from openshift-marketplace started at 2023-02-20 22:09:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: redhat-marketplace-qnk27 from openshift-marketplace started at 2023-02-20 19:41:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-02-20 19:46:29 +0000 UTC (6 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:13:48.031: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: node-exporter-zkcjf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: prometheus-adapter-685b6bd76d-xrdf9 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-02-20 19:46:01 +0000 UTC (6 container statuses recorded)
    Feb 20 22:13:48.031: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:13:48.031: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: prometheus-operator-688459b4f4-4cv2n from openshift-monitoring started at 2023-02-20 19:44:03 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container prometheus-operator ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: prometheus-operator-admission-webhook-7d4759d465-vqmvd from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: thanos-querier-cb675b7f5-8hjjt from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: multus-8cpsm from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: multus-additional-cni-plugins-5sr6s from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: network-metrics-daemon-hz5x4 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: network-check-target-rbbb9 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: sonobuoy from sonobuoy started at 2023-02-20 21:51:30 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.032: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:13:48.032: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.69 before test
    Feb 20 22:13:48.080: INFO: calico-kube-controllers-5b6b964466-ljmhd from calico-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: calico-node-6l54d from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: ibm-file-plugin-d4b9d9dd5-fjk8x from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: ibm-keepalived-watcher-jnq8d from kube-system started at 2023-02-20 19:37:39 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: ibm-master-proxy-static-10.8.38.69 from kube-system started at 2023-02-20 19:37:35 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: ibm-storage-watcher-86f4b88d84-sb4dd from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: ibmcloud-block-storage-driver-dcm4m from kube-system started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: ibmcloud-block-storage-plugin-86f4cbdf95-bn8qv from kube-system started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: vpn-77468d4d47-xrvvz from kube-system started at 2023-02-20 19:48:21 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.080: INFO: 	Container vpn ready: true, restart count 0
    Feb 20 22:13:48.080: INFO: cluster-node-tuning-operator-65bd8f65fb-z2xnp from openshift-cluster-node-tuning-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: tuned-jt9sm from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: cluster-samples-operator-585c69687-52df4 from openshift-cluster-samples-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: cluster-storage-operator-66bf7c9c5f-jhz4m from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: csi-snapshot-controller-operator-557b547dc4-dz4vh from openshift-cluster-storage-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: console-operator-8578f747-4fpjs from openshift-console-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container console-operator ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Feb 20 22:13:48.081: INFO: console-57dd6cb75b-lgjgl from openshift-console started at 2023-02-20 19:47:53 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: dns-operator-c96b75bc5-66nj2 from openshift-dns-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container dns-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: dns-default-snlvx from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: node-resolver-2lz8p from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: cluster-image-registry-operator-6cc666c6b4-gwz8b from openshift-image-registry started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: node-ca-cq7v2 from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: ingress-canary-cvlvh from openshift-ingress-canary started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: ingress-operator-75d985687c-2cff5 from openshift-ingress-operator started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container ingress-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: insights-operator-76dd9549c4-zpvkl from openshift-insights started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container insights-operator ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: openshift-kube-proxy-mjvmc from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: kube-storage-version-migrator-operator-6fd6c8bbc9-mkd4k from openshift-kube-storage-version-migrator-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: marketplace-operator-57868d7ff-tn59b from openshift-marketplace started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container marketplace-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: cluster-monitoring-operator-57c54cf78c-stk6t from openshift-monitoring started at 2023-02-20 19:39:59 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: node-exporter-9kjkf from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: prometheus-adapter-685b6bd76d-7dn46 from openshift-monitoring started at 2023-02-20 19:45:52 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: multus-additional-cni-plugins-bn22f from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: multus-admission-controller-56545f7655-2v9n2 from openshift-multus started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: multus-s2ck6 from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: network-metrics-daemon-bmgpj from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: network-check-source-b94cc7564-h2vbb from openshift-network-diagnostics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container check-endpoints ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: network-check-target-gbl7m from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: network-operator-55dd4b48f9-2sfhw from openshift-network-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container network-operator ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: catalog-operator-dbc6c488b-ddnlm from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container catalog-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: collect-profiles-27948810-c4mbc from openshift-operator-lifecycle-manager started at 2023-02-20 21:30:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:13:48.081: INFO: collect-profiles-27948825-nm62q from openshift-operator-lifecycle-manager started at 2023-02-20 21:45:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:13:48.081: INFO: olm-operator-6b9698f878-l84v6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container olm-operator ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: package-server-manager-7cb4d78bc5-h2ptd from openshift-operator-lifecycle-manager started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container package-server-manager ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: packageserver-588f486fcc-84nv4 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: metrics-745b98dffd-mhfpv from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container metrics ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: push-gateway-96bdb5f74-n2czq from openshift-roks-metrics started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container push-gateway ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: service-ca-operator-59cdc768f4-hq9b9 from openshift-service-ca-operator started at 2023-02-20 19:39:59 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container service-ca-operator ready: true, restart count 1
    Feb 20 22:13:48.081: INFO: sonobuoy-e2e-job-6fcfb70347cc4f0d from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container e2e ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-f24mj from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 20 22:13:48.081: INFO: tigera-operator-84f4f4565b-qjg88 from tigera-operator started at 2023-02-20 19:37:48 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.081: INFO: 	Container tigera-operator ready: true, restart count 2
    Feb 20 22:13:48.081: INFO: 
    Logging pods the apiserver thinks is on node 10.8.38.70 before test
    Feb 20 22:13:48.119: INFO: calico-node-9djxs from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container calico-node ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: calico-typha-65d7b689d4-97f55 from calico-system started at 2023-02-20 19:39:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container calico-typha ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: ibm-cloud-provider-ip-163-68-71-82-fd485669b-ttg4s from ibm-system started at 2023-02-20 19:44:28 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container ibm-cloud-provider-ip-163-68-71-82 ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: ibm-keepalived-watcher-n8zf8 from kube-system started at 2023-02-20 19:37:44 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: ibm-master-proxy-static-10.8.38.70 from kube-system started at 2023-02-20 19:37:36 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: 	Container pause ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: ibmcloud-block-storage-driver-5s8m7 from kube-system started at 2023-02-20 19:37:49 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: tuned-sxlgc from openshift-cluster-node-tuning-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.120: INFO: 	Container tuned ready: true, restart count 0
    Feb 20 22:13:48.120: INFO: csi-snapshot-controller-5bfddff4c-tqhzz from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.121: INFO: 	Container snapshot-controller ready: true, restart count 0
    Feb 20 22:13:48.121: INFO: csi-snapshot-webhook-544b96bbcd-rjs5h from openshift-cluster-storage-operator started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.121: INFO: 	Container webhook ready: true, restart count 0
    Feb 20 22:13:48.121: INFO: console-57dd6cb75b-lzmnl from openshift-console started at 2023-02-20 19:47:26 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.121: INFO: 	Container console ready: true, restart count 0
    Feb 20 22:13:48.121: INFO: downloads-8f679847b-r829z from openshift-console started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.121: INFO: 	Container download-server ready: true, restart count 0
    Feb 20 22:13:48.121: INFO: dns-default-7wnbj from openshift-dns started at 2023-02-20 19:43:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.121: INFO: 	Container dns ready: true, restart count 0
    Feb 20 22:13:48.121: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: node-resolver-9zmwj from openshift-dns started at 2023-02-20 19:43:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: image-registry-f445fb4b9-qprbx from openshift-image-registry started at 2023-02-20 19:45:45 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container registry ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: node-ca-jmgpg from openshift-image-registry started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container node-ca ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: ingress-canary-s48w8 from openshift-ingress-canary started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: router-default-f9ffd57f7-rc68j from openshift-ingress started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container router ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: openshift-kube-proxy-46d5j from openshift-kube-proxy started at 2023-02-20 19:38:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: redhat-operators-8zq49 from openshift-marketplace started at 2023-02-20 19:52:16 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container registry-server ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-02-20 19:45:56 +0000 UTC (6 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container alertmanager ready: true, restart count 1
    Feb 20 22:13:48.122: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: kube-state-metrics-75455b796c-nfnxt from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: node-exporter-857fp from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: 	Container node-exporter ready: true, restart count 0
    Feb 20 22:13:48.122: INFO: openshift-state-metrics-5ff95d844f-nw74k from openshift-monitoring started at 2023-02-20 19:44:21 +0000 UTC (3 container statuses recorded)
    Feb 20 22:13:48.122: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-02-20 19:46:20 +0000 UTC (6 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container config-reloader ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container prometheus ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: prometheus-operator-admission-webhook-7d4759d465-z26qc from openshift-monitoring started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: telemeter-client-7c87b5c5c6-lpcrj from openshift-monitoring started at 2023-02-20 19:44:26 +0000 UTC (3 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container reload ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container telemeter-client ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: thanos-querier-cb675b7f5-qlcwd from openshift-monitoring started at 2023-02-20 19:44:28 +0000 UTC (6 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container oauth-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container thanos-query ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: multus-6c8tx from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: multus-additional-cni-plugins-kcltt from openshift-multus started at 2023-02-20 19:38:32 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: multus-admission-controller-56545f7655-cx8nb from openshift-multus started at 2023-02-20 19:43:37 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: network-metrics-daemon-c9fn7 from openshift-multus started at 2023-02-20 19:38:33 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: network-check-target-glmj8 from openshift-network-diagnostics started at 2023-02-20 19:38:41 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container network-check-target-container ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: collect-profiles-27948840-f4g4g from openshift-operator-lifecycle-manager started at 2023-02-20 22:00:00 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container collect-profiles ready: false, restart count 0
    Feb 20 22:13:48.123: INFO: packageserver-588f486fcc-bkqw6 from openshift-operator-lifecycle-manager started at 2023-02-20 19:43:33 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container packageserver ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: service-ca-869668698d-t2x9f from openshift-service-ca started at 2023-02-20 19:40:19 +0000 UTC (1 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container service-ca-controller ready: true, restart count 0
    Feb 20 22:13:48.123: INFO: sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-7dljh from sonobuoy started at 2023-02-20 21:51:40 +0000 UTC (2 container statuses recorded)
    Feb 20 22:13:48.123: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 20 22:13:48.124: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/20/23 22:13:48.124
    Feb 20 22:13:48.169: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4712" to be "running"
    Feb 20 22:13:48.181: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 12.056952ms
    Feb 20 22:13:50.194: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.025063514s
    Feb 20 22:13:50.194: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/20/23 22:13:50.209
    STEP: Trying to apply a random label on the found node. 02/20/23 22:13:50.314
    STEP: verifying the node has the label kubernetes.io/e2e-5fdacada-388c-4a69-8a44-25e1478da90c 95 02/20/23 22:13:50.357
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/20/23 22:13:50.371
    Feb 20 22:13:50.415: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4712" to be "not pending"
    Feb 20 22:13:50.448: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 33.273517ms
    Feb 20 22:13:52.461: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045787547s
    Feb 20 22:13:54.460: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.045200608s
    Feb 20 22:13:54.460: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.8.38.66 on the node which pod4 resides and expect not scheduled 02/20/23 22:13:54.46
    Feb 20 22:13:54.494: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4712" to be "not pending"
    Feb 20 22:13:54.506: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.090443ms
    Feb 20 22:13:56.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024639607s
    Feb 20 22:13:58.523: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028518181s
    Feb 20 22:14:00.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041372457s
    Feb 20 22:14:02.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026537367s
    Feb 20 22:14:04.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026808159s
    Feb 20 22:14:06.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025582588s
    Feb 20 22:14:08.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026672777s
    Feb 20 22:14:10.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024873394s
    Feb 20 22:14:12.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023885359s
    Feb 20 22:14:14.529: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.034911986s
    Feb 20 22:14:16.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026223287s
    Feb 20 22:14:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.024969755s
    Feb 20 22:14:20.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.024939532s
    Feb 20 22:14:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.024624801s
    Feb 20 22:14:24.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.025366205s
    Feb 20 22:14:26.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023740948s
    Feb 20 22:14:28.524: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029473465s
    Feb 20 22:14:30.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025901453s
    Feb 20 22:14:32.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.023950901s
    Feb 20 22:14:34.523: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.028720954s
    Feb 20 22:14:36.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.027485499s
    Feb 20 22:14:38.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025389368s
    Feb 20 22:14:40.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.024903003s
    Feb 20 22:14:42.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023697926s
    Feb 20 22:14:44.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.024602964s
    Feb 20 22:14:46.535: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.041060856s
    Feb 20 22:14:48.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.025520981s
    Feb 20 22:14:50.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.026074412s
    Feb 20 22:14:52.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024758997s
    Feb 20 22:14:54.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.027299515s
    Feb 20 22:14:56.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.02425838s
    Feb 20 22:14:58.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.024289725s
    Feb 20 22:15:00.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024719404s
    Feb 20 22:15:02.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025696846s
    Feb 20 22:15:04.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.025750803s
    Feb 20 22:15:06.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.056450205s
    Feb 20 22:15:08.527: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.032578355s
    Feb 20 22:15:10.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024210519s
    Feb 20 22:15:12.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.024220996s
    Feb 20 22:15:14.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.024289042s
    Feb 20 22:15:16.525: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.031142838s
    Feb 20 22:15:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024706803s
    Feb 20 22:15:20.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.024754117s
    Feb 20 22:15:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.024489593s
    Feb 20 22:15:24.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.024980088s
    Feb 20 22:15:26.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025291925s
    Feb 20 22:15:28.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024853621s
    Feb 20 22:15:30.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.023571134s
    Feb 20 22:15:32.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024090217s
    Feb 20 22:15:34.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02523037s
    Feb 20 22:15:36.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.024819865s
    Feb 20 22:15:38.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.027597523s
    Feb 20 22:15:40.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026842916s
    Feb 20 22:15:42.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.023583526s
    Feb 20 22:15:44.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025247245s
    Feb 20 22:15:46.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.025286882s
    Feb 20 22:15:48.551: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.056449211s
    Feb 20 22:15:50.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025540845s
    Feb 20 22:15:52.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.027282068s
    Feb 20 22:15:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.036301858s
    Feb 20 22:15:56.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.027938448s
    Feb 20 22:15:58.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.026414017s
    Feb 20 22:16:00.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025480497s
    Feb 20 22:16:02.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.027837419s
    Feb 20 22:16:04.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.025959882s
    Feb 20 22:16:06.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.025606061s
    Feb 20 22:16:08.525: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.031015332s
    Feb 20 22:16:10.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.025574664s
    Feb 20 22:16:12.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.02549327s
    Feb 20 22:16:14.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.026796098s
    Feb 20 22:16:16.530: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.035632672s
    Feb 20 22:16:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.024792731s
    Feb 20 22:16:20.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.041866038s
    Feb 20 22:16:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.024577155s
    Feb 20 22:16:24.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.025776681s
    Feb 20 22:16:26.523: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.028481765s
    Feb 20 22:16:28.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.024771318s
    Feb 20 22:16:30.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.024776354s
    Feb 20 22:16:32.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.024996718s
    Feb 20 22:16:34.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.024809698s
    Feb 20 22:16:36.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.024498789s
    Feb 20 22:16:38.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.026764588s
    Feb 20 22:16:40.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.025046375s
    Feb 20 22:16:42.536: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.041276574s
    Feb 20 22:16:44.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.027313382s
    Feb 20 22:16:46.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.027392721s
    Feb 20 22:16:48.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.025993185s
    Feb 20 22:16:50.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.027260067s
    Feb 20 22:16:52.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.02664386s
    Feb 20 22:16:54.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.025502165s
    Feb 20 22:16:56.549: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.055050179s
    Feb 20 22:16:58.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.024806259s
    Feb 20 22:17:00.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025815184s
    Feb 20 22:17:02.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.025762739s
    Feb 20 22:17:04.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.025931526s
    Feb 20 22:17:06.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.024937643s
    Feb 20 22:17:08.544: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.049499668s
    Feb 20 22:17:10.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02529558s
    Feb 20 22:17:12.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.025037915s
    Feb 20 22:17:14.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.026279945s
    Feb 20 22:17:16.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.025380599s
    Feb 20 22:17:18.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.026081857s
    Feb 20 22:17:20.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.026066874s
    Feb 20 22:17:22.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.024938702s
    Feb 20 22:17:24.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.024797005s
    Feb 20 22:17:26.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.024007042s
    Feb 20 22:17:28.540: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.045592185s
    Feb 20 22:17:30.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.025367361s
    Feb 20 22:17:32.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.024813486s
    Feb 20 22:17:34.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.024830405s
    Feb 20 22:17:36.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.025177659s
    Feb 20 22:17:38.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.024633697s
    Feb 20 22:17:40.546: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.051729796s
    Feb 20 22:17:42.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.02383071s
    Feb 20 22:17:44.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.025091965s
    Feb 20 22:17:46.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.024807328s
    Feb 20 22:17:48.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.024785466s
    Feb 20 22:17:50.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.025473475s
    Feb 20 22:17:52.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.024805506s
    Feb 20 22:17:54.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.024677523s
    Feb 20 22:17:56.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.024975314s
    Feb 20 22:17:58.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.025482225s
    Feb 20 22:18:00.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.024984957s
    Feb 20 22:18:02.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.025051131s
    Feb 20 22:18:04.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.028092222s
    Feb 20 22:18:06.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.024816462s
    Feb 20 22:18:08.528: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.03349498s
    Feb 20 22:18:10.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.025588079s
    Feb 20 22:18:12.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.025697563s
    Feb 20 22:18:14.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.025436538s
    Feb 20 22:18:16.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024888202s
    Feb 20 22:18:18.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.024852206s
    Feb 20 22:18:20.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.02475633s
    Feb 20 22:18:22.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.025377598s
    Feb 20 22:18:24.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.025516875s
    Feb 20 22:18:26.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.026537176s
    Feb 20 22:18:28.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.025475021s
    Feb 20 22:18:30.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.024950855s
    Feb 20 22:18:32.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.038912306s
    Feb 20 22:18:34.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.025781204s
    Feb 20 22:18:36.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.027704289s
    Feb 20 22:18:38.520: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.02580363s
    Feb 20 22:18:40.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024649461s
    Feb 20 22:18:42.522: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.028045827s
    Feb 20 22:18:44.521: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.026457518s
    Feb 20 22:18:46.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.024807914s
    Feb 20 22:18:48.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.024755766s
    Feb 20 22:18:50.533: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.038443673s
    Feb 20 22:18:52.518: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.023548847s
    Feb 20 22:18:54.519: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.024980082s
    Feb 20 22:18:54.531: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.036930633s
    STEP: removing the label kubernetes.io/e2e-5fdacada-388c-4a69-8a44-25e1478da90c off the node 10.8.38.66 02/20/23 22:18:54.531
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-5fdacada-388c-4a69-8a44-25e1478da90c 02/20/23 22:18:54.579
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:18:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4712" for this suite. 02/20/23 22:18:54.606
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:18:54.629
Feb 20 22:18:54.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:18:54.63
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:18:54.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:18:54.699
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 02/20/23 22:18:54.732
STEP: watching for the Service to be added 02/20/23 22:18:54.762
Feb 20 22:18:54.768: INFO: Found Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 20 22:18:54.768: INFO: Service test-service-kld5r created
STEP: Getting /status 02/20/23 22:18:54.768
Feb 20 22:18:54.784: INFO: Service test-service-kld5r has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/20/23 22:18:54.784
STEP: watching for the Service to be patched 02/20/23 22:18:54.801
Feb 20 22:18:54.807: INFO: observed Service test-service-kld5r in namespace services-6118 with annotations: map[] & LoadBalancer: {[]}
Feb 20 22:18:54.807: INFO: Found Service test-service-kld5r in namespace services-6118 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 20 22:18:54.807: INFO: Service test-service-kld5r has service status patched
STEP: updating the ServiceStatus 02/20/23 22:18:54.807
Feb 20 22:18:54.837: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/20/23 22:18:54.838
Feb 20 22:18:54.842: INFO: Observed Service test-service-kld5r in namespace services-6118 with annotations: map[] & Conditions: {[]}
Feb 20 22:18:54.842: INFO: Observed event: &Service{ObjectMeta:{test-service-kld5r  services-6118  bcf05e41-1125-42d4-9b73-a2481afd93d2 84828 0 2023-02-20 22:18:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-20 22:18:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-20 22:18:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.130.241,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.130.241],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 20 22:18:54.843: INFO: Found Service test-service-kld5r in namespace services-6118 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 20 22:18:54.843: INFO: Service test-service-kld5r has service status updated
STEP: patching the service 02/20/23 22:18:54.843
STEP: watching for the Service to be patched 02/20/23 22:18:54.862
Feb 20 22:18:54.867: INFO: observed Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true]
Feb 20 22:18:54.867: INFO: observed Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true]
Feb 20 22:18:54.867: INFO: observed Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true]
Feb 20 22:18:54.867: INFO: Found Service test-service-kld5r in namespace services-6118 with labels: map[test-service:patched test-service-static:true]
Feb 20 22:18:54.867: INFO: Service test-service-kld5r patched
STEP: deleting the service 02/20/23 22:18:54.867
STEP: watching for the Service to be deleted 02/20/23 22:18:54.899
Feb 20 22:18:54.904: INFO: Observed event: ADDED
Feb 20 22:18:54.904: INFO: Observed event: MODIFIED
Feb 20 22:18:54.905: INFO: Observed event: MODIFIED
Feb 20 22:18:54.905: INFO: Observed event: MODIFIED
Feb 20 22:18:54.905: INFO: Found Service test-service-kld5r in namespace services-6118 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 20 22:18:54.905: INFO: Service test-service-kld5r deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:18:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6118" for this suite. 02/20/23 22:18:54.92
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":103,"skipped":1896,"failed":0}
------------------------------
• [0.317 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:18:54.629
    Feb 20 22:18:54.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:18:54.63
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:18:54.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:18:54.699
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 02/20/23 22:18:54.732
    STEP: watching for the Service to be added 02/20/23 22:18:54.762
    Feb 20 22:18:54.768: INFO: Found Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 20 22:18:54.768: INFO: Service test-service-kld5r created
    STEP: Getting /status 02/20/23 22:18:54.768
    Feb 20 22:18:54.784: INFO: Service test-service-kld5r has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/20/23 22:18:54.784
    STEP: watching for the Service to be patched 02/20/23 22:18:54.801
    Feb 20 22:18:54.807: INFO: observed Service test-service-kld5r in namespace services-6118 with annotations: map[] & LoadBalancer: {[]}
    Feb 20 22:18:54.807: INFO: Found Service test-service-kld5r in namespace services-6118 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 20 22:18:54.807: INFO: Service test-service-kld5r has service status patched
    STEP: updating the ServiceStatus 02/20/23 22:18:54.807
    Feb 20 22:18:54.837: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/20/23 22:18:54.838
    Feb 20 22:18:54.842: INFO: Observed Service test-service-kld5r in namespace services-6118 with annotations: map[] & Conditions: {[]}
    Feb 20 22:18:54.842: INFO: Observed event: &Service{ObjectMeta:{test-service-kld5r  services-6118  bcf05e41-1125-42d4-9b73-a2481afd93d2 84828 0 2023-02-20 22:18:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-20 22:18:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-20 22:18:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.130.241,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.130.241],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 20 22:18:54.843: INFO: Found Service test-service-kld5r in namespace services-6118 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 20 22:18:54.843: INFO: Service test-service-kld5r has service status updated
    STEP: patching the service 02/20/23 22:18:54.843
    STEP: watching for the Service to be patched 02/20/23 22:18:54.862
    Feb 20 22:18:54.867: INFO: observed Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true]
    Feb 20 22:18:54.867: INFO: observed Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true]
    Feb 20 22:18:54.867: INFO: observed Service test-service-kld5r in namespace services-6118 with labels: map[test-service-static:true]
    Feb 20 22:18:54.867: INFO: Found Service test-service-kld5r in namespace services-6118 with labels: map[test-service:patched test-service-static:true]
    Feb 20 22:18:54.867: INFO: Service test-service-kld5r patched
    STEP: deleting the service 02/20/23 22:18:54.867
    STEP: watching for the Service to be deleted 02/20/23 22:18:54.899
    Feb 20 22:18:54.904: INFO: Observed event: ADDED
    Feb 20 22:18:54.904: INFO: Observed event: MODIFIED
    Feb 20 22:18:54.905: INFO: Observed event: MODIFIED
    Feb 20 22:18:54.905: INFO: Observed event: MODIFIED
    Feb 20 22:18:54.905: INFO: Found Service test-service-kld5r in namespace services-6118 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 20 22:18:54.905: INFO: Service test-service-kld5r deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:18:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6118" for this suite. 02/20/23 22:18:54.92
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:18:54.952
Feb 20 22:18:54.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 22:18:54.953
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:18:54.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:18:55.004
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 20 22:18:55.018: INFO: Creating deployment "webserver-deployment"
W0220 22:18:55.030427      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:18:55.030: INFO: Waiting for observed generation 1
Feb 20 22:18:57.071: INFO: Waiting for all required pods to come up
Feb 20 22:18:57.089: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/20/23 22:18:57.089
Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-x5js9" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4mhlh" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-96jql" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9brtg" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hcfv6" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kbgrr" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nbdz8" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pqgnt" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-q4fm5" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-vsprs" in namespace "deployment-778" to be "running"
Feb 20 22:18:57.107: INFO: Pod "webserver-deployment-845c8977d9-x5js9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.320272ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-pqgnt": Phase="Pending", Reason="", readiness=false. Elapsed: 35.032195ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-vsprs": Phase="Pending", Reason="", readiness=false. Elapsed: 34.902942ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-kbgrr": Phase="Pending", Reason="", readiness=false. Elapsed: 35.455871ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-hcfv6": Phase="Pending", Reason="", readiness=false. Elapsed: 35.683212ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-nbdz8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.467332ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-9brtg": Phase="Pending", Reason="", readiness=false. Elapsed: 35.990944ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-q4fm5": Phase="Pending", Reason="", readiness=false. Elapsed: 35.451325ms
Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-96jql": Phase="Pending", Reason="", readiness=false. Elapsed: 36.099977ms
Feb 20 22:18:57.127: INFO: Pod "webserver-deployment-845c8977d9-4mhlh": Phase="Pending", Reason="", readiness=false. Elapsed: 37.653587ms
Feb 20 22:18:59.119: INFO: Pod "webserver-deployment-845c8977d9-x5js9": Phase="Running", Reason="", readiness=true. Elapsed: 2.030109274s
Feb 20 22:18:59.119: INFO: Pod "webserver-deployment-845c8977d9-x5js9" satisfied condition "running"
Feb 20 22:18:59.136: INFO: Pod "webserver-deployment-845c8977d9-pqgnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.046514563s
Feb 20 22:18:59.136: INFO: Pod "webserver-deployment-845c8977d9-pqgnt" satisfied condition "running"
Feb 20 22:18:59.139: INFO: Pod "webserver-deployment-845c8977d9-vsprs": Phase="Running", Reason="", readiness=true. Elapsed: 2.049203652s
Feb 20 22:18:59.139: INFO: Pod "webserver-deployment-845c8977d9-vsprs" satisfied condition "running"
Feb 20 22:18:59.141: INFO: Pod "webserver-deployment-845c8977d9-9brtg": Phase="Running", Reason="", readiness=true. Elapsed: 2.051460931s
Feb 20 22:18:59.141: INFO: Pod "webserver-deployment-845c8977d9-9brtg" satisfied condition "running"
Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-kbgrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.053075944s
Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-96jql": Phase="Running", Reason="", readiness=true. Elapsed: 2.053493033s
Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-96jql" satisfied condition "running"
Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-kbgrr" satisfied condition "running"
Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-nbdz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.052957766s
Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-nbdz8" satisfied condition "running"
Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-hcfv6": Phase="Running", Reason="", readiness=true. Elapsed: 2.054306579s
Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-hcfv6" satisfied condition "running"
Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-4mhlh": Phase="Running", Reason="", readiness=true. Elapsed: 2.054739144s
Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-4mhlh" satisfied condition "running"
Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-q4fm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.053851888s
Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-q4fm5" satisfied condition "running"
Feb 20 22:18:59.144: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 20 22:18:59.165: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 20 22:18:59.228: INFO: Updating deployment webserver-deployment
Feb 20 22:18:59.229: INFO: Waiting for observed generation 2
Feb 20 22:19:01.254: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 20 22:19:01.267: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 20 22:19:01.282: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 20 22:19:01.388: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 20 22:19:01.388: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 20 22:19:01.402: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 20 22:19:01.436: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 20 22:19:01.436: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 20 22:19:01.470: INFO: Updating deployment webserver-deployment
Feb 20 22:19:01.470: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 20 22:19:01.532: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 20 22:19:01.606: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 22:19:01.686: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-778  fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8 85215 3 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ee138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-20 22:18:59 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-20 22:19:01 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 20 22:19:01.714: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-778  48f987e6-854d-43c1-a198-7f742c2de0e0 85211 3 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8 0xc00279f727 0xc00279f728}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00279f7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:19:01.714: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 20 22:19:01.714: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-778  cbae57d8-8229-4bfc-a56a-c63f650ebdb2 85209 3 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8 0xc00279f827 0xc00279f828}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00279f8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:19:01.741: INFO: Pod "webserver-deployment-69b7448995-22xt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-22xt8 webserver-deployment-69b7448995- deployment-778  d31fb74a-53ee-46b2-80de-31f6a1df7063 85171 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fa73f04c2291549febcafa01b716d75d8d2c101c3510ba6a08a79dab2042e120 cni.projectcalico.org/podIP:172.30.144.238/32 cni.projectcalico.org/podIPs:172.30.144.238/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.238"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.238"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3167 0xc0040c3168}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-246gv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-246gv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.741: INFO: Pod "webserver-deployment-69b7448995-2s4lh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2s4lh webserver-deployment-69b7448995- deployment-778  4176951f-b1f5-4f88-b80c-d7bd546304cd 85226 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c33d7 0xc0040c33d8}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tsb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tsb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.742: INFO: Pod "webserver-deployment-69b7448995-69rwl" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-69rwl webserver-deployment-69b7448995- deployment-778  9624553f-53df-44a0-b367-a3c4d33d0e34 85189 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5049a99448b3161ed230ac3364f0061746d38bf238a084e7a0ce10a73d6ca1fd cni.projectcalico.org/podIP:172.30.31.178/32 cni.projectcalico.org/podIPs:172.30.31.178/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.178"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.178"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c35b7 0xc0040c35b8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58dkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58dkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.742: INFO: Pod "webserver-deployment-69b7448995-jp55x" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jp55x webserver-deployment-69b7448995- deployment-778  2ca73d0f-7509-4281-b913-69ae4dc218b8 85177 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:95889cc8a27fc804096046f0359e0684b3905aed6db614af8af7cbee10b75c4b cni.projectcalico.org/podIP:172.30.181.243/32 cni.projectcalico.org/podIPs:172.30.181.243/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.243"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.243"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3837 0xc0040c3838}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjj6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjj6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.743: INFO: Pod "webserver-deployment-69b7448995-rpr4t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rpr4t webserver-deployment-69b7448995- deployment-778  41741263-cabe-4691-baaa-3c3eadf6c403 85163 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f95f6849943dc170fed5bedf8594707a0abbdbb79dc01775daefbaf689c34d60 cni.projectcalico.org/podIP:172.30.181.206/32 cni.projectcalico.org/podIPs:172.30.181.206/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.206"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.206"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3ac7 0xc0040c3ac8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh8jb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh8jb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.743: INFO: Pod "webserver-deployment-69b7448995-s7shs" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-s7shs webserver-deployment-69b7448995- deployment-778  61f283ca-547b-4d0a-9163-9b6d453d4845 85161 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a4fe4e9e25350e7401a5da6028c39156ba7dfebaa937bdaf5fc15253eedc7ddd cni.projectcalico.org/podIP:172.30.31.160/32 cni.projectcalico.org/podIPs:172.30.31.160/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.160"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.160"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3d87 0xc0040c3d88}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b25nn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b25nn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.744: INFO: Pod "webserver-deployment-845c8977d9-4mhlh" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4mhlh webserver-deployment-845c8977d9- deployment-778  2f70a7bd-5b22-4a6e-9e24-245ba0accfbc 85030 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7ec357942016072dba7f2d58a801d74354a7948839bb72b7e659e79d548fb313 cni.projectcalico.org/podIP:172.30.181.251/32 cni.projectcalico.org/podIPs:172.30.181.251/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.251"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.251"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc0040c3ff7 0xc0040c3ff8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bshw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bshw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.251,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6a38ff85cf877e7c24eb7ecda9042044d518861941f282c6a120b86f44254576,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.744: INFO: Pod "webserver-deployment-845c8977d9-926qk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-926qk webserver-deployment-845c8977d9- deployment-778  79b3dc42-670b-42d8-9747-3fa26b2ea57c 85224 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c267 0xc00402c268}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhrwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhrwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:,StartTime:2023-02-20 22:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.745: INFO: Pod "webserver-deployment-845c8977d9-96jql" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-96jql webserver-deployment-845c8977d9- deployment-778  a8d514fd-0b79-45fb-bbe1-5d5ffaa9dc19 85032 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:960172f3f79d50bf1244ce23a3545dc490eb059940f09cdc9f1c8cd738099ee1 cni.projectcalico.org/podIP:172.30.144.245/32 cni.projectcalico.org/podIPs:172.30.144.245/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.245"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.245"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c487 0xc00402c488}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2cg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2cg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:172.30.144.245,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://90b73e30748b3b85d16998915a9d4121d725c8cc2846a27b85540a5cae07b8a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.144.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.745: INFO: Pod "webserver-deployment-845c8977d9-9brtg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9brtg webserver-deployment-845c8977d9- deployment-778  fef4200a-7483-4867-a5e8-b259eb742135 85017 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:002820bc6909bdbbd7bb43f8e2633a37ae7938e042cb6b5b8cef04f9d2091bbc cni.projectcalico.org/podIP:172.30.31.181/32 cni.projectcalico.org/podIPs:172.30.31.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.181"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.181"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c727 0xc00402c728}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64z5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64z5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:172.30.31.181,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9f260d6c13fd7c83ddd80e8d75878aeb3ae4a1c422ccf5a2d2489cb2d7953583,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.746: INFO: Pod "webserver-deployment-845c8977d9-hcfv6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hcfv6 webserver-deployment-845c8977d9- deployment-778  cffb0cf8-d813-4777-91e1-21710d8b7dca 85038 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e463bdb5dd58a8de593441d3b2d370065f4830e54e6b7c573f93fbdeb64e5568 cni.projectcalico.org/podIP:172.30.181.247/32 cni.projectcalico.org/podIPs:172.30.181.247/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.247"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.247"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c997 0xc00402c998}] [] [{calico Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rqkr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rqkr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.247,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://65191e8cd6e1a0e94dec0d7e1a24d5708de4c623752359f8dace95fa26c9205a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.746: INFO: Pod "webserver-deployment-845c8977d9-hcrfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hcrfq webserver-deployment-845c8977d9- deployment-778  2f590280-2b88-4d47-991d-4d1530081762 85225 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402cc07 0xc00402cc08}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rs8q7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rs8q7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.747: INFO: Pod "webserver-deployment-845c8977d9-hddsc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hddsc webserver-deployment-845c8977d9- deployment-778  cd7708e0-2517-448c-adc3-fa2ca7e06651 85228 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402cea7 0xc00402cea8}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ml5pb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ml5pb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.747: INFO: Pod "webserver-deployment-845c8977d9-pqgnt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pqgnt webserver-deployment-845c8977d9- deployment-778  1979fa15-33ac-4fdc-bceb-e6ccb4e48ac9 85024 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a4b829f5a0a4a42fb8ed73565125c3caa7ad8d4e11df7ed25becfb425fd98fa0 cni.projectcalico.org/podIP:172.30.144.232/32 cni.projectcalico.org/podIPs:172.30.144.232/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.232"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.232"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402d207 0xc00402d208}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:172.30.144.232,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aac94e863ac373587098dbbe54a4da023cb461fc35d3a124094443e2146dad96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.144.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.747: INFO: Pod "webserver-deployment-845c8977d9-q4fm5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-q4fm5 webserver-deployment-845c8977d9- deployment-778  74bc2d43-f836-4c7a-a1de-c90c25a46ca2 85027 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:de0df7a6e32b6eb96c6958bc8d53d9545c61e3f5f02e6f67dccee0b234b37547 cni.projectcalico.org/podIP:172.30.144.252/32 cni.projectcalico.org/podIPs:172.30.144.252/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.252"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.144.252"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402d5d7 0xc00402d5d8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wrlr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wrlr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:172.30.144.252,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5eb66916147226b429b88d03aa1cf32b1708325b0e89095b21c32b42b44aa97c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.144.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.748: INFO: Pod "webserver-deployment-845c8977d9-vsprs" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vsprs webserver-deployment-845c8977d9- deployment-778  f5791a62-6039-4cd8-b352-0347706dc645 85020 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:72b4ebd121d9f473414a06a5074321acc9463870238b86f50905d757fa591b85 cni.projectcalico.org/podIP:172.30.31.161/32 cni.projectcalico.org/podIPs:172.30.31.161/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.161"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.161"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402d947 0xc00402d948}] [] [{calico Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhzj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhzj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:172.30.31.161,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ee9433e40077a7b88f78d9bd4193d105f7d1762783becc79e0102ab13e99eb40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 20 22:19:01.748: INFO: Pod "webserver-deployment-845c8977d9-x5js9" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x5js9 webserver-deployment-845c8977d9- deployment-778  13ebc3b9-73b1-45ed-8885-3149f58259f3 85035 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:416897ca9a6e3d597da3505cbe485edf6200b6c5f907898d0b2236b87cddb454 cni.projectcalico.org/podIP:172.30.181.246/32 cni.projectcalico.org/podIPs:172.30.181.246/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.246"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.246"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402dd17 0xc00402dd18}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbsc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbsc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.246,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2fa47d9bc3084a7b032a31fbd1f0eb010907d68a5f1b675ca17683cc92414816,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 22:19:01.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-778" for this suite. 02/20/23 22:19:01.776
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":104,"skipped":1921,"failed":0}
------------------------------
• [SLOW TEST] [6.902 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:18:54.952
    Feb 20 22:18:54.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 22:18:54.953
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:18:54.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:18:55.004
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 20 22:18:55.018: INFO: Creating deployment "webserver-deployment"
    W0220 22:18:55.030427      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:18:55.030: INFO: Waiting for observed generation 1
    Feb 20 22:18:57.071: INFO: Waiting for all required pods to come up
    Feb 20 22:18:57.089: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/20/23 22:18:57.089
    Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-x5js9" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4mhlh" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-96jql" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-9brtg" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hcfv6" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.089: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kbgrr" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nbdz8" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pqgnt" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-q4fm5" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.090: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-vsprs" in namespace "deployment-778" to be "running"
    Feb 20 22:18:57.107: INFO: Pod "webserver-deployment-845c8977d9-x5js9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.320272ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-pqgnt": Phase="Pending", Reason="", readiness=false. Elapsed: 35.032195ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-vsprs": Phase="Pending", Reason="", readiness=false. Elapsed: 34.902942ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-kbgrr": Phase="Pending", Reason="", readiness=false. Elapsed: 35.455871ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-hcfv6": Phase="Pending", Reason="", readiness=false. Elapsed: 35.683212ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-nbdz8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.467332ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-9brtg": Phase="Pending", Reason="", readiness=false. Elapsed: 35.990944ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-q4fm5": Phase="Pending", Reason="", readiness=false. Elapsed: 35.451325ms
    Feb 20 22:18:57.125: INFO: Pod "webserver-deployment-845c8977d9-96jql": Phase="Pending", Reason="", readiness=false. Elapsed: 36.099977ms
    Feb 20 22:18:57.127: INFO: Pod "webserver-deployment-845c8977d9-4mhlh": Phase="Pending", Reason="", readiness=false. Elapsed: 37.653587ms
    Feb 20 22:18:59.119: INFO: Pod "webserver-deployment-845c8977d9-x5js9": Phase="Running", Reason="", readiness=true. Elapsed: 2.030109274s
    Feb 20 22:18:59.119: INFO: Pod "webserver-deployment-845c8977d9-x5js9" satisfied condition "running"
    Feb 20 22:18:59.136: INFO: Pod "webserver-deployment-845c8977d9-pqgnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.046514563s
    Feb 20 22:18:59.136: INFO: Pod "webserver-deployment-845c8977d9-pqgnt" satisfied condition "running"
    Feb 20 22:18:59.139: INFO: Pod "webserver-deployment-845c8977d9-vsprs": Phase="Running", Reason="", readiness=true. Elapsed: 2.049203652s
    Feb 20 22:18:59.139: INFO: Pod "webserver-deployment-845c8977d9-vsprs" satisfied condition "running"
    Feb 20 22:18:59.141: INFO: Pod "webserver-deployment-845c8977d9-9brtg": Phase="Running", Reason="", readiness=true. Elapsed: 2.051460931s
    Feb 20 22:18:59.141: INFO: Pod "webserver-deployment-845c8977d9-9brtg" satisfied condition "running"
    Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-kbgrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.053075944s
    Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-96jql": Phase="Running", Reason="", readiness=true. Elapsed: 2.053493033s
    Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-96jql" satisfied condition "running"
    Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-kbgrr" satisfied condition "running"
    Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-nbdz8": Phase="Running", Reason="", readiness=true. Elapsed: 2.052957766s
    Feb 20 22:18:59.143: INFO: Pod "webserver-deployment-845c8977d9-nbdz8" satisfied condition "running"
    Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-hcfv6": Phase="Running", Reason="", readiness=true. Elapsed: 2.054306579s
    Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-hcfv6" satisfied condition "running"
    Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-4mhlh": Phase="Running", Reason="", readiness=true. Elapsed: 2.054739144s
    Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-4mhlh" satisfied condition "running"
    Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-q4fm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.053851888s
    Feb 20 22:18:59.144: INFO: Pod "webserver-deployment-845c8977d9-q4fm5" satisfied condition "running"
    Feb 20 22:18:59.144: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 20 22:18:59.165: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 20 22:18:59.228: INFO: Updating deployment webserver-deployment
    Feb 20 22:18:59.229: INFO: Waiting for observed generation 2
    Feb 20 22:19:01.254: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 20 22:19:01.267: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 20 22:19:01.282: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 20 22:19:01.388: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 20 22:19:01.388: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 20 22:19:01.402: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 20 22:19:01.436: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 20 22:19:01.436: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 20 22:19:01.470: INFO: Updating deployment webserver-deployment
    Feb 20 22:19:01.470: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 20 22:19:01.532: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 20 22:19:01.606: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 22:19:01.686: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-778  fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8 85215 3 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ee138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-20 22:18:59 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-20 22:19:01 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb 20 22:19:01.714: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-778  48f987e6-854d-43c1-a198-7f742c2de0e0 85211 3 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8 0xc00279f727 0xc00279f728}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00279f7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:19:01.714: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 20 22:19:01.714: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-778  cbae57d8-8229-4bfc-a56a-c63f650ebdb2 85209 3 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8 0xc00279f827 0xc00279f828}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc3a2ed4-92e6-4e3a-8f79-79b78275b2e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00279f8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:19:01.741: INFO: Pod "webserver-deployment-69b7448995-22xt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-22xt8 webserver-deployment-69b7448995- deployment-778  d31fb74a-53ee-46b2-80de-31f6a1df7063 85171 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fa73f04c2291549febcafa01b716d75d8d2c101c3510ba6a08a79dab2042e120 cni.projectcalico.org/podIP:172.30.144.238/32 cni.projectcalico.org/podIPs:172.30.144.238/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.238"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.238"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3167 0xc0040c3168}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-246gv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-246gv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.741: INFO: Pod "webserver-deployment-69b7448995-2s4lh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2s4lh webserver-deployment-69b7448995- deployment-778  4176951f-b1f5-4f88-b80c-d7bd546304cd 85226 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c33d7 0xc0040c33d8}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tsb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tsb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.742: INFO: Pod "webserver-deployment-69b7448995-69rwl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-69rwl webserver-deployment-69b7448995- deployment-778  9624553f-53df-44a0-b367-a3c4d33d0e34 85189 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5049a99448b3161ed230ac3364f0061746d38bf238a084e7a0ce10a73d6ca1fd cni.projectcalico.org/podIP:172.30.31.178/32 cni.projectcalico.org/podIPs:172.30.31.178/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.178"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.178"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c35b7 0xc0040c35b8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58dkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58dkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.742: INFO: Pod "webserver-deployment-69b7448995-jp55x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jp55x webserver-deployment-69b7448995- deployment-778  2ca73d0f-7509-4281-b913-69ae4dc218b8 85177 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:95889cc8a27fc804096046f0359e0684b3905aed6db614af8af7cbee10b75c4b cni.projectcalico.org/podIP:172.30.181.243/32 cni.projectcalico.org/podIPs:172.30.181.243/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.243"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.243"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3837 0xc0040c3838}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjj6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjj6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.743: INFO: Pod "webserver-deployment-69b7448995-rpr4t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rpr4t webserver-deployment-69b7448995- deployment-778  41741263-cabe-4691-baaa-3c3eadf6c403 85163 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f95f6849943dc170fed5bedf8594707a0abbdbb79dc01775daefbaf689c34d60 cni.projectcalico.org/podIP:172.30.181.206/32 cni.projectcalico.org/podIPs:172.30.181.206/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.206"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.206"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3ac7 0xc0040c3ac8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh8jb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh8jb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.743: INFO: Pod "webserver-deployment-69b7448995-s7shs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-s7shs webserver-deployment-69b7448995- deployment-778  61f283ca-547b-4d0a-9163-9b6d453d4845 85161 0 2023-02-20 22:18:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a4fe4e9e25350e7401a5da6028c39156ba7dfebaa937bdaf5fc15253eedc7ddd cni.projectcalico.org/podIP:172.30.31.160/32 cni.projectcalico.org/podIPs:172.30.31.160/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.160"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.160"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 48f987e6-854d-43c1-a198-7f742c2de0e0 0xc0040c3d87 0xc0040c3d88}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48f987e6-854d-43c1-a198-7f742c2de0e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:19:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b25nn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b25nn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:,StartTime:2023-02-20 22:18:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.744: INFO: Pod "webserver-deployment-845c8977d9-4mhlh" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4mhlh webserver-deployment-845c8977d9- deployment-778  2f70a7bd-5b22-4a6e-9e24-245ba0accfbc 85030 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7ec357942016072dba7f2d58a801d74354a7948839bb72b7e659e79d548fb313 cni.projectcalico.org/podIP:172.30.181.251/32 cni.projectcalico.org/podIPs:172.30.181.251/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.251"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.251"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc0040c3ff7 0xc0040c3ff8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bshw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bshw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.251,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6a38ff85cf877e7c24eb7ecda9042044d518861941f282c6a120b86f44254576,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.744: INFO: Pod "webserver-deployment-845c8977d9-926qk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-926qk webserver-deployment-845c8977d9- deployment-778  79b3dc42-670b-42d8-9747-3fa26b2ea57c 85224 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c267 0xc00402c268}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhrwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhrwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:,StartTime:2023-02-20 22:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.745: INFO: Pod "webserver-deployment-845c8977d9-96jql" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-96jql webserver-deployment-845c8977d9- deployment-778  a8d514fd-0b79-45fb-bbe1-5d5ffaa9dc19 85032 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:960172f3f79d50bf1244ce23a3545dc490eb059940f09cdc9f1c8cd738099ee1 cni.projectcalico.org/podIP:172.30.144.245/32 cni.projectcalico.org/podIPs:172.30.144.245/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.245"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.245"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c487 0xc00402c488}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2cg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2cg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:172.30.144.245,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://90b73e30748b3b85d16998915a9d4121d725c8cc2846a27b85540a5cae07b8a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.144.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.745: INFO: Pod "webserver-deployment-845c8977d9-9brtg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9brtg webserver-deployment-845c8977d9- deployment-778  fef4200a-7483-4867-a5e8-b259eb742135 85017 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:002820bc6909bdbbd7bb43f8e2633a37ae7938e042cb6b5b8cef04f9d2091bbc cni.projectcalico.org/podIP:172.30.31.181/32 cni.projectcalico.org/podIPs:172.30.31.181/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.181"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.181"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c727 0xc00402c728}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64z5s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64z5s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:172.30.31.181,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9f260d6c13fd7c83ddd80e8d75878aeb3ae4a1c422ccf5a2d2489cb2d7953583,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.746: INFO: Pod "webserver-deployment-845c8977d9-hcfv6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hcfv6 webserver-deployment-845c8977d9- deployment-778  cffb0cf8-d813-4777-91e1-21710d8b7dca 85038 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e463bdb5dd58a8de593441d3b2d370065f4830e54e6b7c573f93fbdeb64e5568 cni.projectcalico.org/podIP:172.30.181.247/32 cni.projectcalico.org/podIPs:172.30.181.247/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.247"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.247"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402c997 0xc00402c998}] [] [{calico Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rqkr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rqkr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.247,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://65191e8cd6e1a0e94dec0d7e1a24d5708de4c623752359f8dace95fa26c9205a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.746: INFO: Pod "webserver-deployment-845c8977d9-hcrfq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hcrfq webserver-deployment-845c8977d9- deployment-778  2f590280-2b88-4d47-991d-4d1530081762 85225 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402cc07 0xc00402cc08}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rs8q7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rs8q7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.747: INFO: Pod "webserver-deployment-845c8977d9-hddsc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hddsc webserver-deployment-845c8977d9- deployment-778  cd7708e0-2517-448c-adc3-fa2ca7e06651 85228 0 2023-02-20 22:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402cea7 0xc00402cea8}] [] [{kube-controller-manager Update v1 2023-02-20 22:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ml5pb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ml5pb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.747: INFO: Pod "webserver-deployment-845c8977d9-pqgnt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pqgnt webserver-deployment-845c8977d9- deployment-778  1979fa15-33ac-4fdc-bceb-e6ccb4e48ac9 85024 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a4b829f5a0a4a42fb8ed73565125c3caa7ad8d4e11df7ed25becfb425fd98fa0 cni.projectcalico.org/podIP:172.30.144.232/32 cni.projectcalico.org/podIPs:172.30.144.232/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.232"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.232"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402d207 0xc00402d208}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:172.30.144.232,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aac94e863ac373587098dbbe54a4da023cb461fc35d3a124094443e2146dad96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.144.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.747: INFO: Pod "webserver-deployment-845c8977d9-q4fm5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-q4fm5 webserver-deployment-845c8977d9- deployment-778  74bc2d43-f836-4c7a-a1de-c90c25a46ca2 85027 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:de0df7a6e32b6eb96c6958bc8d53d9545c61e3f5f02e6f67dccee0b234b37547 cni.projectcalico.org/podIP:172.30.144.252/32 cni.projectcalico.org/podIPs:172.30.144.252/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.252"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.144.252"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402d5d7 0xc00402d5d8}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.144.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4wrlr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4wrlr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.69,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.69,PodIP:172.30.144.252,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5eb66916147226b429b88d03aa1cf32b1708325b0e89095b21c32b42b44aa97c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.144.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.748: INFO: Pod "webserver-deployment-845c8977d9-vsprs" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vsprs webserver-deployment-845c8977d9- deployment-778  f5791a62-6039-4cd8-b352-0347706dc645 85020 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:72b4ebd121d9f473414a06a5074321acc9463870238b86f50905d757fa591b85 cni.projectcalico.org/podIP:172.30.31.161/32 cni.projectcalico.org/podIPs:172.30.31.161/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.161"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.161"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402d947 0xc00402d948}] [] [{calico Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhzj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhzj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:172.30.31.161,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ee9433e40077a7b88f78d9bd4193d105f7d1762783becc79e0102ab13e99eb40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 20 22:19:01.748: INFO: Pod "webserver-deployment-845c8977d9-x5js9" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x5js9 webserver-deployment-845c8977d9- deployment-778  13ebc3b9-73b1-45ed-8885-3149f58259f3 85035 0 2023-02-20 22:18:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:416897ca9a6e3d597da3505cbe485edf6200b6c5f907898d0b2236b87cddb454 cni.projectcalico.org/podIP:172.30.181.246/32 cni.projectcalico.org/podIPs:172.30.181.246/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.246"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.246"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 cbae57d8-8229-4bfc-a56a-c63f650ebdb2 0xc00402dd17 0xc00402dd18}] [] [{kube-controller-manager Update v1 2023-02-20 22:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cbae57d8-8229-4bfc-a56a-c63f650ebdb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zbsc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zbsc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-w7xpv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.246,StartTime:2023-02-20 22:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:18:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2fa47d9bc3084a7b032a31fbd1f0eb010907d68a5f1b675ca17683cc92414816,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 22:19:01.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-778" for this suite. 02/20/23 22:19:01.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:19:01.859
Feb 20 22:19:01.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 22:19:01.86
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:01.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:01.978
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 02/20/23 22:19:02.091
STEP: watching for Pod to be ready 02/20/23 22:19:02.187
Feb 20 22:19:02.197: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 20 22:19:02.197: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
Feb 20 22:19:02.264: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
Feb 20 22:19:03.197: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
Feb 20 22:19:03.286: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
Feb 20 22:19:04.921: INFO: Found Pod pod-test in namespace pods-4079 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/20/23 22:19:04.933
STEP: getting the Pod and ensuring that it's patched 02/20/23 22:19:04.976
STEP: replacing the Pod's status Ready condition to False 02/20/23 22:19:04.987
STEP: check the Pod again to ensure its Ready conditions are False 02/20/23 22:19:05.017
STEP: deleting the Pod via a Collection with a LabelSelector 02/20/23 22:19:05.018
STEP: watching for the Pod to be deleted 02/20/23 22:19:05.044
Feb 20 22:19:05.049: INFO: observed event type MODIFIED
Feb 20 22:19:05.453: INFO: observed event type MODIFIED
Feb 20 22:19:07.377: INFO: observed event type MODIFIED
Feb 20 22:19:09.003: INFO: observed event type MODIFIED
Feb 20 22:19:09.037: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 22:19:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4079" for this suite. 02/20/23 22:19:09.092
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":105,"skipped":1930,"failed":0}
------------------------------
• [SLOW TEST] [7.254 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:19:01.859
    Feb 20 22:19:01.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 22:19:01.86
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:01.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:01.978
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 02/20/23 22:19:02.091
    STEP: watching for Pod to be ready 02/20/23 22:19:02.187
    Feb 20 22:19:02.197: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 20 22:19:02.197: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
    Feb 20 22:19:02.264: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
    Feb 20 22:19:03.197: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
    Feb 20 22:19:03.286: INFO: observed Pod pod-test in namespace pods-4079 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
    Feb 20 22:19:04.921: INFO: Found Pod pod-test in namespace pods-4079 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:19:02 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/20/23 22:19:04.933
    STEP: getting the Pod and ensuring that it's patched 02/20/23 22:19:04.976
    STEP: replacing the Pod's status Ready condition to False 02/20/23 22:19:04.987
    STEP: check the Pod again to ensure its Ready conditions are False 02/20/23 22:19:05.017
    STEP: deleting the Pod via a Collection with a LabelSelector 02/20/23 22:19:05.018
    STEP: watching for the Pod to be deleted 02/20/23 22:19:05.044
    Feb 20 22:19:05.049: INFO: observed event type MODIFIED
    Feb 20 22:19:05.453: INFO: observed event type MODIFIED
    Feb 20 22:19:07.377: INFO: observed event type MODIFIED
    Feb 20 22:19:09.003: INFO: observed event type MODIFIED
    Feb 20 22:19:09.037: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 22:19:09.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4079" for this suite. 02/20/23 22:19:09.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:19:09.125
Feb 20 22:19:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:19:09.126
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:09.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:09.19
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/20/23 22:19:09.201
Feb 20 22:19:09.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:19:18.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:19:52.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2059" for this suite. 02/20/23 22:19:52.305
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":106,"skipped":1996,"failed":0}
------------------------------
• [SLOW TEST] [43.206 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:19:09.125
    Feb 20 22:19:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:19:09.126
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:09.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:09.19
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/20/23 22:19:09.201
    Feb 20 22:19:09.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:19:18.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:19:52.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2059" for this suite. 02/20/23 22:19:52.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:19:52.337
Feb 20 22:19:52.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sysctl 02/20/23 22:19:52.343
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:52.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:52.416
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/20/23 22:19:52.429
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 22:19:52.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3093" for this suite. 02/20/23 22:19:52.486
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":107,"skipped":2014,"failed":0}
------------------------------
• [0.172 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:19:52.337
    Feb 20 22:19:52.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sysctl 02/20/23 22:19:52.343
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:52.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:52.416
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/20/23 22:19:52.429
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 22:19:52.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3093" for this suite. 02/20/23 22:19:52.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:19:52.51
Feb 20 22:19:52.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 22:19:52.512
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:52.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:52.573
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Feb 20 22:19:52.644: INFO: Waiting up to 2m0s for pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" in namespace "var-expansion-7111" to be "container 0 failed with reason CreateContainerConfigError"
Feb 20 22:19:52.654: INFO: Pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.674121ms
Feb 20 22:19:54.667: INFO: Pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023196586s
Feb 20 22:19:54.667: INFO: Pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 20 22:19:54.667: INFO: Deleting pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" in namespace "var-expansion-7111"
Feb 20 22:19:54.693: INFO: Wait up to 5m0s for pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 22:19:58.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7111" for this suite. 02/20/23 22:19:58.744
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":108,"skipped":2022,"failed":0}
------------------------------
• [SLOW TEST] [6.259 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:19:52.51
    Feb 20 22:19:52.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 22:19:52.512
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:52.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:52.573
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Feb 20 22:19:52.644: INFO: Waiting up to 2m0s for pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" in namespace "var-expansion-7111" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 20 22:19:52.654: INFO: Pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.674121ms
    Feb 20 22:19:54.667: INFO: Pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023196586s
    Feb 20 22:19:54.667: INFO: Pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 20 22:19:54.667: INFO: Deleting pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" in namespace "var-expansion-7111"
    Feb 20 22:19:54.693: INFO: Wait up to 5m0s for pod "var-expansion-ff82e0cd-f744-4b9c-bb59-d67f1c3bf65f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 22:19:58.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7111" for this suite. 02/20/23 22:19:58.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:19:58.778
Feb 20 22:19:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 22:19:58.78
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:58.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:58.838
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 20 22:19:58.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:20:05.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4624" for this suite. 02/20/23 22:20:05.54
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":109,"skipped":2046,"failed":0}
------------------------------
• [SLOW TEST] [6.790 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:19:58.778
    Feb 20 22:19:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 22:19:58.78
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:19:58.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:19:58.838
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 20 22:19:58.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:20:05.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4624" for this suite. 02/20/23 22:20:05.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:05.579
Feb 20 22:20:05.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:20:05.581
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:05.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:05.666
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:20:05.744
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:20:06.064
STEP: Deploying the webhook pod 02/20/23 22:20:06.122
STEP: Wait for the deployment to be ready 02/20/23 22:20:06.153
Feb 20 22:20:06.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 22:20:08.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:20:10.237
STEP: Verifying the service has paired with the endpoint 02/20/23 22:20:10.264
Feb 20 22:20:11.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 02/20/23 22:20:11.425
STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:20:11.491
STEP: Deleting the collection of validation webhooks 02/20/23 22:20:11.534
STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:20:11.676
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:20:11.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8973" for this suite. 02/20/23 22:20:11.728
STEP: Destroying namespace "webhook-8973-markers" for this suite. 02/20/23 22:20:11.752
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":110,"skipped":2103,"failed":0}
------------------------------
• [SLOW TEST] [6.306 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:05.579
    Feb 20 22:20:05.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:20:05.581
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:05.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:05.666
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:20:05.744
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:20:06.064
    STEP: Deploying the webhook pod 02/20/23 22:20:06.122
    STEP: Wait for the deployment to be ready 02/20/23 22:20:06.153
    Feb 20 22:20:06.184: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 22:20:08.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 20, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:20:10.237
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:20:10.264
    Feb 20 22:20:11.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 02/20/23 22:20:11.425
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:20:11.491
    STEP: Deleting the collection of validation webhooks 02/20/23 22:20:11.534
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:20:11.676
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:20:11.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8973" for this suite. 02/20/23 22:20:11.728
    STEP: Destroying namespace "webhook-8973-markers" for this suite. 02/20/23 22:20:11.752
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:11.888
Feb 20 22:20:11.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:20:11.889
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:11.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:11.956
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 02/20/23 22:20:11.967
Feb 20 22:20:11.967: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4034 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/20/23 22:20:12.048
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:20:12.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4034" for this suite. 02/20/23 22:20:12.075
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":111,"skipped":2111,"failed":0}
------------------------------
• [0.220 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:11.888
    Feb 20 22:20:11.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:20:11.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:11.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:11.956
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 02/20/23 22:20:11.967
    Feb 20 22:20:11.967: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-4034 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/20/23 22:20:12.048
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:20:12.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4034" for this suite. 02/20/23 22:20:12.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:12.112
Feb 20 22:20:12.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename watch 02/20/23 22:20:12.115
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:12.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:12.22
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/20/23 22:20:12.231
STEP: creating a new configmap 02/20/23 22:20:12.236
STEP: modifying the configmap once 02/20/23 22:20:12.25
STEP: changing the label value of the configmap 02/20/23 22:20:12.28
STEP: Expecting to observe a delete notification for the watched object 02/20/23 22:20:12.307
Feb 20 22:20:12.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86536 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:20:12.308: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86541 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:20:12.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86548 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/20/23 22:20:12.31
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/20/23 22:20:12.338
STEP: changing the label value of the configmap back 02/20/23 22:20:22.339
STEP: modifying the configmap a third time 02/20/23 22:20:22.365
STEP: deleting the configmap 02/20/23 22:20:22.39
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/20/23 22:20:22.407
Feb 20 22:20:22.407: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86684 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:20:22.407: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86685 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:20:22.408: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86686 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 20 22:20:22.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5887" for this suite. 02/20/23 22:20:22.426
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":112,"skipped":2139,"failed":0}
------------------------------
• [SLOW TEST] [10.336 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:12.112
    Feb 20 22:20:12.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename watch 02/20/23 22:20:12.115
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:12.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:12.22
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/20/23 22:20:12.231
    STEP: creating a new configmap 02/20/23 22:20:12.236
    STEP: modifying the configmap once 02/20/23 22:20:12.25
    STEP: changing the label value of the configmap 02/20/23 22:20:12.28
    STEP: Expecting to observe a delete notification for the watched object 02/20/23 22:20:12.307
    Feb 20 22:20:12.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86536 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:20:12.308: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86541 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:20:12.309: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86548 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/20/23 22:20:12.31
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/20/23 22:20:12.338
    STEP: changing the label value of the configmap back 02/20/23 22:20:22.339
    STEP: modifying the configmap a third time 02/20/23 22:20:22.365
    STEP: deleting the configmap 02/20/23 22:20:22.39
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/20/23 22:20:22.407
    Feb 20 22:20:22.407: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86684 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:20:22.407: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86685 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:20:22.408: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5887  eedab9d1-e84d-45aa-b7dd-4a6dd463c99b 86686 0 2023-02-20 22:20:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 20 22:20:22.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5887" for this suite. 02/20/23 22:20:22.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:22.455
Feb 20 22:20:22.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:20:22.459
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:22.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:22.524
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 02/20/23 22:20:22.535
STEP: Creating a ResourceQuota 02/20/23 22:20:27.55
STEP: Ensuring resource quota status is calculated 02/20/23 22:20:27.562
STEP: Creating a Service 02/20/23 22:20:29.573
STEP: Creating a NodePort Service 02/20/23 22:20:29.608
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/20/23 22:20:29.648
STEP: Ensuring resource quota status captures service creation 02/20/23 22:20:29.68
STEP: Deleting Services 02/20/23 22:20:31.692
STEP: Ensuring resource quota status released usage 02/20/23 22:20:31.774
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:20:33.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-449" for this suite. 02/20/23 22:20:33.802
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":113,"skipped":2161,"failed":0}
------------------------------
• [SLOW TEST] [11.371 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:22.455
    Feb 20 22:20:22.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:20:22.459
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:22.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:22.524
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 02/20/23 22:20:22.535
    STEP: Creating a ResourceQuota 02/20/23 22:20:27.55
    STEP: Ensuring resource quota status is calculated 02/20/23 22:20:27.562
    STEP: Creating a Service 02/20/23 22:20:29.573
    STEP: Creating a NodePort Service 02/20/23 22:20:29.608
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/20/23 22:20:29.648
    STEP: Ensuring resource quota status captures service creation 02/20/23 22:20:29.68
    STEP: Deleting Services 02/20/23 22:20:31.692
    STEP: Ensuring resource quota status released usage 02/20/23 22:20:31.774
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:20:33.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-449" for this suite. 02/20/23 22:20:33.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:33.834
Feb 20 22:20:33.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename watch 02/20/23 22:20:33.836
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:33.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:33.92
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/20/23 22:20:33.941
STEP: creating a new configmap 02/20/23 22:20:33.945
STEP: modifying the configmap once 02/20/23 22:20:33.959
STEP: closing the watch once it receives two notifications 02/20/23 22:20:33.986
Feb 20 22:20:33.987: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86849 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:20:33.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86853 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/20/23 22:20:33.988
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/20/23 22:20:34.014
STEP: deleting the configmap 02/20/23 22:20:34.019
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/20/23 22:20:34.037
Feb 20 22:20:34.037: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86854 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:20:34.038: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86856 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 20 22:20:34.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2371" for this suite. 02/20/23 22:20:34.058
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":114,"skipped":2177,"failed":0}
------------------------------
• [0.247 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:33.834
    Feb 20 22:20:33.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename watch 02/20/23 22:20:33.836
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:33.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:33.92
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/20/23 22:20:33.941
    STEP: creating a new configmap 02/20/23 22:20:33.945
    STEP: modifying the configmap once 02/20/23 22:20:33.959
    STEP: closing the watch once it receives two notifications 02/20/23 22:20:33.986
    Feb 20 22:20:33.987: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86849 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:20:33.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86853 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/20/23 22:20:33.988
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/20/23 22:20:34.014
    STEP: deleting the configmap 02/20/23 22:20:34.019
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/20/23 22:20:34.037
    Feb 20 22:20:34.037: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86854 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:20:34.038: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2371  71d9098a-a2fc-4a58-bf7b-dc061b547bc7 86856 0 2023-02-20 22:20:33 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-20 22:20:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 20 22:20:34.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2371" for this suite. 02/20/23 22:20:34.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:34.082
Feb 20 22:20:34.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 22:20:34.083
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:34.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:34.151
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1906 02/20/23 22:20:34.165
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 02/20/23 22:20:34.183
STEP: Creating pod with conflicting port in namespace statefulset-1906 02/20/23 22:20:34.199
STEP: Waiting until pod test-pod will start running in namespace statefulset-1906 02/20/23 22:20:34.241
Feb 20 22:20:34.241: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1906" to be "running"
Feb 20 22:20:34.256: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.466721ms
Feb 20 22:20:36.287: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045519131s
Feb 20 22:20:38.269: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.028032887s
Feb 20 22:20:38.269: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-1906 02/20/23 22:20:38.269
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1906 02/20/23 22:20:38.282
Feb 20 22:20:38.346: INFO: Observed stateful pod in namespace: statefulset-1906, name: ss-0, uid: 57ff3500-9e4f-4156-9703-6a8681621aa2, status phase: Pending. Waiting for statefulset controller to delete.
Feb 20 22:20:38.401: INFO: Observed stateful pod in namespace: statefulset-1906, name: ss-0, uid: 57ff3500-9e4f-4156-9703-6a8681621aa2, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 22:20:38.416: INFO: Observed stateful pod in namespace: statefulset-1906, name: ss-0, uid: 57ff3500-9e4f-4156-9703-6a8681621aa2, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 22:20:38.463: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1906
STEP: Removing pod with conflicting port in namespace statefulset-1906 02/20/23 22:20:38.463
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1906 and will be in running state 02/20/23 22:20:38.495
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 22:20:44.543: INFO: Deleting all statefulset in ns statefulset-1906
Feb 20 22:20:44.551: INFO: Scaling statefulset ss to 0
Feb 20 22:20:54.593: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:20:54.602: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 22:20:54.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1906" for this suite. 02/20/23 22:20:54.645
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":115,"skipped":2186,"failed":0}
------------------------------
• [SLOW TEST] [20.589 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:34.082
    Feb 20 22:20:34.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 22:20:34.083
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:34.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:34.151
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1906 02/20/23 22:20:34.165
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 02/20/23 22:20:34.183
    STEP: Creating pod with conflicting port in namespace statefulset-1906 02/20/23 22:20:34.199
    STEP: Waiting until pod test-pod will start running in namespace statefulset-1906 02/20/23 22:20:34.241
    Feb 20 22:20:34.241: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1906" to be "running"
    Feb 20 22:20:34.256: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.466721ms
    Feb 20 22:20:36.287: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045519131s
    Feb 20 22:20:38.269: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.028032887s
    Feb 20 22:20:38.269: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-1906 02/20/23 22:20:38.269
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1906 02/20/23 22:20:38.282
    Feb 20 22:20:38.346: INFO: Observed stateful pod in namespace: statefulset-1906, name: ss-0, uid: 57ff3500-9e4f-4156-9703-6a8681621aa2, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 20 22:20:38.401: INFO: Observed stateful pod in namespace: statefulset-1906, name: ss-0, uid: 57ff3500-9e4f-4156-9703-6a8681621aa2, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 20 22:20:38.416: INFO: Observed stateful pod in namespace: statefulset-1906, name: ss-0, uid: 57ff3500-9e4f-4156-9703-6a8681621aa2, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 20 22:20:38.463: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1906
    STEP: Removing pod with conflicting port in namespace statefulset-1906 02/20/23 22:20:38.463
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1906 and will be in running state 02/20/23 22:20:38.495
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 22:20:44.543: INFO: Deleting all statefulset in ns statefulset-1906
    Feb 20 22:20:44.551: INFO: Scaling statefulset ss to 0
    Feb 20 22:20:54.593: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 22:20:54.602: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 22:20:54.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1906" for this suite. 02/20/23 22:20:54.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:20:54.673
Feb 20 22:20:54.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 22:20:54.674
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:54.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:54.733
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95 in namespace container-probe-7889 02/20/23 22:20:54.745
Feb 20 22:20:54.816: INFO: Waiting up to 5m0s for pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95" in namespace "container-probe-7889" to be "not pending"
Feb 20 22:20:54.830: INFO: Pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95": Phase="Pending", Reason="", readiness=false. Elapsed: 13.185975ms
Feb 20 22:20:56.843: INFO: Pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95": Phase="Running", Reason="", readiness=true. Elapsed: 2.025989026s
Feb 20 22:20:56.843: INFO: Pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95" satisfied condition "not pending"
Feb 20 22:20:56.843: INFO: Started pod busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95 in namespace container-probe-7889
STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 22:20:56.843
Feb 20 22:20:56.855: INFO: Initial restart count of pod busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95 is 0
STEP: deleting the pod 02/20/23 22:24:58.481
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 22:24:58.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7889" for this suite. 02/20/23 22:24:58.546
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":116,"skipped":2212,"failed":0}
------------------------------
• [SLOW TEST] [243.903 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:20:54.673
    Feb 20 22:20:54.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 22:20:54.674
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:20:54.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:20:54.733
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95 in namespace container-probe-7889 02/20/23 22:20:54.745
    Feb 20 22:20:54.816: INFO: Waiting up to 5m0s for pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95" in namespace "container-probe-7889" to be "not pending"
    Feb 20 22:20:54.830: INFO: Pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95": Phase="Pending", Reason="", readiness=false. Elapsed: 13.185975ms
    Feb 20 22:20:56.843: INFO: Pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95": Phase="Running", Reason="", readiness=true. Elapsed: 2.025989026s
    Feb 20 22:20:56.843: INFO: Pod "busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95" satisfied condition "not pending"
    Feb 20 22:20:56.843: INFO: Started pod busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95 in namespace container-probe-7889
    STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 22:20:56.843
    Feb 20 22:20:56.855: INFO: Initial restart count of pod busybox-a42cb5fd-c948-4715-ab8f-1cec27b2bc95 is 0
    STEP: deleting the pod 02/20/23 22:24:58.481
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 22:24:58.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7889" for this suite. 02/20/23 22:24:58.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:24:58.578
Feb 20 22:24:58.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:24:58.582
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:24:58.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:24:58.639
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:24:58.738
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:24:59.035
STEP: Deploying the webhook pod 02/20/23 22:24:59.064
STEP: Wait for the deployment to be ready 02/20/23 22:24:59.09
Feb 20 22:24:59.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 22:25:01.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:25:03.166
STEP: Verifying the service has paired with the endpoint 02/20/23 22:25:03.194
Feb 20 22:25:04.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Feb 20 22:25:04.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8651-crds.webhook.example.com via the AdmissionRegistration API 02/20/23 22:25:04.743
STEP: Creating a custom resource that should be mutated by the webhook 02/20/23 22:25:04.778
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:25:07.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7341" for this suite. 02/20/23 22:25:07.451
STEP: Destroying namespace "webhook-7341-markers" for this suite. 02/20/23 22:25:07.477
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":117,"skipped":2232,"failed":0}
------------------------------
• [SLOW TEST] [9.032 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:24:58.578
    Feb 20 22:24:58.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:24:58.582
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:24:58.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:24:58.639
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:24:58.738
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:24:59.035
    STEP: Deploying the webhook pod 02/20/23 22:24:59.064
    STEP: Wait for the deployment to be ready 02/20/23 22:24:59.09
    Feb 20 22:24:59.114: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 22:25:01.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 24, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:25:03.166
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:25:03.194
    Feb 20 22:25:04.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Feb 20 22:25:04.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8651-crds.webhook.example.com via the AdmissionRegistration API 02/20/23 22:25:04.743
    STEP: Creating a custom resource that should be mutated by the webhook 02/20/23 22:25:04.778
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:25:07.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7341" for this suite. 02/20/23 22:25:07.451
    STEP: Destroying namespace "webhook-7341-markers" for this suite. 02/20/23 22:25:07.477
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:25:07.625
Feb 20 22:25:07.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:25:07.627
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:25:07.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:25:07.725
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Feb 20 22:25:07.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 create -f -'
Feb 20 22:25:10.197: INFO: stderr: ""
Feb 20 22:25:10.197: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 20 22:25:10.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 create -f -'
Feb 20 22:25:12.874: INFO: stderr: ""
Feb 20 22:25:12.874: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/20/23 22:25:12.874
Feb 20 22:25:13.889: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:25:13.889: INFO: Found 1 / 1
Feb 20 22:25:13.889: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 22:25:13.903: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:25:13.903: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 22:25:13.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe pod agnhost-primary-czp5g'
Feb 20 22:25:14.171: INFO: stderr: ""
Feb 20 22:25:14.171: INFO: stdout: "Name:             agnhost-primary-czp5g\nNamespace:        kubectl-7140\nPriority:         0\nService Account:  default\nNode:             10.8.38.66/10.8.38.66\nStart Time:       Mon, 20 Feb 2023 22:25:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8d06de7b0a5fbf225518d4b796d4e036d9e373ca711ea7e477de5bad7d139646\n                  cni.projectcalico.org/podIP: 172.30.181.228/32\n                  cni.projectcalico.org/podIPs: 172.30.181.228/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.181.228\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.181.228\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.181.228\nIPs:\n  IP:           172.30.181.228\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://13cf7d7f9bec9e3db530ae3a146624d3ca4690db22d0de274ebbc4b4ef11fd5c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 20 Feb 2023 22:25:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4z2j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-t4z2j:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       4s    default-scheduler  Successfully assigned kubectl-7140/agnhost-primary-czp5g to 10.8.38.66\n  Normal  AddedInterface  3s    multus             Add eth0 [172.30.181.228/32] from k8s-pod-network\n  Normal  Pulled          3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         3s    kubelet            Created container agnhost-primary\n  Normal  Started         3s    kubelet            Started container agnhost-primary\n"
Feb 20 22:25:14.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe rc agnhost-primary'
Feb 20 22:25:14.389: INFO: stderr: ""
Feb 20 22:25:14.389: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7140\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-czp5g\n"
Feb 20 22:25:14.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe service agnhost-primary'
Feb 20 22:25:14.561: INFO: stderr: ""
Feb 20 22:25:14.561: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7140\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.74.34\nIPs:               172.21.74.34\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.181.228:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 20 22:25:14.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe node 10.8.38.66'
Feb 20 22:25:14.950: INFO: stderr: ""
Feb 20 22:25:14.950: INFO: stdout: "Name:               10.8.38.66\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-osa\n                    failure-domain.beta.kubernetes.io/zone=osa21\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.68.96.205\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.8.38.66\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=jp-osa\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfpsebio0vo2oj5q8fc0-kubee2epvgm-default-000003a8\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfpsebio0vo2oj5q8fc0-361b2ad\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.2_1526_openshift\n                    ibm-cloud.kubernetes.io/zone=osa21\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.8.38.66\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2981440\n                    publicVLAN=2981442\n                    topology.kubernetes.io/region=jp-osa\n                    topology.kubernetes.io/zone=osa21\nAnnotations:        projectcalico.org/IPv4Address: 10.8.38.66/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.181.192\nCreationTimestamp:  Mon, 20 Feb 2023 19:37:48 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.8.38.66\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 20 Feb 2023 22:25:13 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 20 Feb 2023 19:39:37 +0000   Mon, 20 Feb 2023 19:39:37 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:37:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:37:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:37:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:40:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.8.38.66\n  ExternalIP:  163.68.96.205\n  Hostname:    10.8.38.66\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386536Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13597160Ki\n  pods:               110\nSystem Info:\n  Machine ID:                              62abb638bb14470d9e4e0851550fe15a\n  System UUID:                             76e04005-25af-6e1f-bcda-444b74d02a56\n  Boot ID:                                 78b54df7-ec40-4ffd-aedb-63ddfe3542d1\n  Kernel Version:                          4.18.0-425.10.1.el8_7.x86_64\n  OS Image:                                Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                        linux\n  Architecture:                            amd64\n  Container Runtime Version:               cri-o://1.25.2-4.rhaos4.12.git66af2f6.el8\n  Kubelet Version:                         v1.25.4+a34b9e9\n  Kube-Proxy Version:                      v1.25.4+a34b9e9\nPodCIDR:                                   172.30.2.0/24\nPodCIDRs:                                  172.30.2.0/24\nProviderID:                                ibm://fee034388aa6435883a1f720010ab3a2///cfpsebio0vo2oj5q8fc0/kube-cfpsebio0vo2oj5q8fc0-kubee2epvgm-default-000003a8\nNon-terminated Pods:                       (34 in total)\n  Namespace                                Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                            calico-node-jj7lx                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         166m\n  calico-system                            calico-typha-65d7b689d4-kpn77                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         166m\n  ibm-system                               ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5         5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         160m\n  kube-system                              ibm-keepalived-watcher-snrrr                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         167m\n  kube-system                              ibm-master-proxy-static-10.8.38.66                         26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      167m\n  kube-system                              ibmcloud-block-storage-driver-9s9rq                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     167m\n  kubectl-7140                             agnhost-primary-czp5g                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator   tuned-57bz4                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         161m\n  openshift-cluster-storage-operator       csi-snapshot-controller-5bfddff4c-whkcp                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         161m\n  openshift-cluster-storage-operator       csi-snapshot-webhook-544b96bbcd-bxjcg                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         161m\n  openshift-console                        downloads-8f679847b-nbfcm                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         161m\n  openshift-dns                            dns-default-6gh9k                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         161m\n  openshift-dns                            node-resolver-9wzh6                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         161m\n  openshift-image-registry                 node-ca-k9szq                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         161m\n  openshift-ingress-canary                 ingress-canary-c2h9v                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         161m\n  openshift-ingress                        router-default-f9ffd57f7-vxv45                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         161m\n  openshift-kube-proxy                     openshift-kube-proxy-4bz4j                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         166m\n  openshift-kube-storage-version-migrator  migrator-5c54d8d69d-jm5pm                                  10m (0%)      0 (0%)      200Mi (1%)       0 (0%)         161m\n  openshift-marketplace                    certified-operators-mswgj                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-marketplace                    community-operators-rkvlk                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         15m\n  openshift-marketplace                    redhat-marketplace-qnk27                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-monitoring                     alertmanager-main-0                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         158m\n  openshift-monitoring                     node-exporter-zkcjf                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         160m\n  openshift-monitoring                     prometheus-adapter-685b6bd76d-xrdf9                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         159m\n  openshift-monitoring                     prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         159m\n  openshift-monitoring                     prometheus-operator-688459b4f4-4cv2n                       6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         161m\n  openshift-monitoring                     prometheus-operator-admission-webhook-7d4759d465-vqmvd     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         161m\n  openshift-monitoring                     thanos-querier-cb675b7f5-8hjjt                             15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         160m\n  openshift-multus                         multus-8cpsm                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         166m\n  openshift-multus                         multus-additional-cni-plugins-5sr6s                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         166m\n  openshift-multus                         network-metrics-daemon-hz5x4                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         166m\n  openshift-network-diagnostics            network-check-target-rbbb9                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         166m\n  sonobuoy                                 sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  sonobuoy                                 sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1131m (28%)      600m (15%)\n  memory             3354131Ki (24%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                   Message\n  ----    ------                   ----                 ----                   -------\n  Normal  Starting                 166m                 kube-proxy             \n  Normal  Starting                 167m                 kubelet                Starting kubelet.\n  Normal  NodeAllocatableEnforced  167m                 kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  167m (x7 over 167m)  kubelet                Node 10.8.38.66 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    167m (x7 over 167m)  kubelet                Node 10.8.38.66 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     167m (x7 over 167m)  kubelet                Node 10.8.38.66 status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           167m                 node-controller        Node 10.8.38.66 event: Registered Node 10.8.38.66 in Controller\n  Normal  Synced                   167m                 cloud-node-controller  Node synced successfully\n  Normal  NodeReady                165m                 kubelet                Node 10.8.38.66 status is now: NodeReady\n  Normal  RegisteredNode           161m                 node-controller        Node 10.8.38.66 event: Registered Node 10.8.38.66 in Controller\n"
Feb 20 22:25:14.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe namespace kubectl-7140'
Feb 20 22:25:15.121: INFO: stderr: ""
Feb 20 22:25:15.121: INFO: stdout: "Name:         kubectl-7140\nLabels:       e2e-framework=kubectl\n              e2e-run=24e5f230-9e47-40a6-adcc-763f231ccfc4\n              kubernetes.io/metadata.name=kubectl-7140\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c46,c45\n              openshift.io/sa.scc.supplemental-groups: 1002160000/10000\n              openshift.io/sa.scc.uid-range: 1002160000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:25:15.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7140" for this suite. 02/20/23 22:25:15.142
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":118,"skipped":2260,"failed":0}
------------------------------
• [SLOW TEST] [7.542 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:25:07.625
    Feb 20 22:25:07.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:25:07.627
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:25:07.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:25:07.725
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Feb 20 22:25:07.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 create -f -'
    Feb 20 22:25:10.197: INFO: stderr: ""
    Feb 20 22:25:10.197: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 20 22:25:10.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 create -f -'
    Feb 20 22:25:12.874: INFO: stderr: ""
    Feb 20 22:25:12.874: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/20/23 22:25:12.874
    Feb 20 22:25:13.889: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:25:13.889: INFO: Found 1 / 1
    Feb 20 22:25:13.889: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 20 22:25:13.903: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:25:13.903: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 20 22:25:13.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe pod agnhost-primary-czp5g'
    Feb 20 22:25:14.171: INFO: stderr: ""
    Feb 20 22:25:14.171: INFO: stdout: "Name:             agnhost-primary-czp5g\nNamespace:        kubectl-7140\nPriority:         0\nService Account:  default\nNode:             10.8.38.66/10.8.38.66\nStart Time:       Mon, 20 Feb 2023 22:25:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8d06de7b0a5fbf225518d4b796d4e036d9e373ca711ea7e477de5bad7d139646\n                  cni.projectcalico.org/podIP: 172.30.181.228/32\n                  cni.projectcalico.org/podIPs: 172.30.181.228/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.181.228\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.181.228\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.181.228\nIPs:\n  IP:           172.30.181.228\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://13cf7d7f9bec9e3db530ae3a146624d3ca4690db22d0de274ebbc4b4ef11fd5c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 20 Feb 2023 22:25:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t4z2j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-t4z2j:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       4s    default-scheduler  Successfully assigned kubectl-7140/agnhost-primary-czp5g to 10.8.38.66\n  Normal  AddedInterface  3s    multus             Add eth0 [172.30.181.228/32] from k8s-pod-network\n  Normal  Pulled          3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         3s    kubelet            Created container agnhost-primary\n  Normal  Started         3s    kubelet            Started container agnhost-primary\n"
    Feb 20 22:25:14.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe rc agnhost-primary'
    Feb 20 22:25:14.389: INFO: stderr: ""
    Feb 20 22:25:14.389: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7140\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-czp5g\n"
    Feb 20 22:25:14.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe service agnhost-primary'
    Feb 20 22:25:14.561: INFO: stderr: ""
    Feb 20 22:25:14.561: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7140\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.74.34\nIPs:               172.21.74.34\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.181.228:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 20 22:25:14.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe node 10.8.38.66'
    Feb 20 22:25:14.950: INFO: stderr: ""
    Feb 20 22:25:14.950: INFO: stdout: "Name:               10.8.38.66\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=jp-osa\n                    failure-domain.beta.kubernetes.io/zone=osa21\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=163.68.96.205\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.8.38.66\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=jp-osa\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfpsebio0vo2oj5q8fc0-kubee2epvgm-default-000003a8\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfpsebio0vo2oj5q8fc0-361b2ad\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.2_1526_openshift\n                    ibm-cloud.kubernetes.io/zone=osa21\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.8.38.66\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2981440\n                    publicVLAN=2981442\n                    topology.kubernetes.io/region=jp-osa\n                    topology.kubernetes.io/zone=osa21\nAnnotations:        projectcalico.org/IPv4Address: 10.8.38.66/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.181.192\nCreationTimestamp:  Mon, 20 Feb 2023 19:37:48 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.8.38.66\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 20 Feb 2023 22:25:13 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 20 Feb 2023 19:39:37 +0000   Mon, 20 Feb 2023 19:39:37 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:37:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:37:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:37:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 20 Feb 2023 22:25:10 +0000   Mon, 20 Feb 2023 19:40:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.8.38.66\n  ExternalIP:  163.68.96.205\n  Hostname:    10.8.38.66\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386536Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13597160Ki\n  pods:               110\nSystem Info:\n  Machine ID:                              62abb638bb14470d9e4e0851550fe15a\n  System UUID:                             76e04005-25af-6e1f-bcda-444b74d02a56\n  Boot ID:                                 78b54df7-ec40-4ffd-aedb-63ddfe3542d1\n  Kernel Version:                          4.18.0-425.10.1.el8_7.x86_64\n  OS Image:                                Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                        linux\n  Architecture:                            amd64\n  Container Runtime Version:               cri-o://1.25.2-4.rhaos4.12.git66af2f6.el8\n  Kubelet Version:                         v1.25.4+a34b9e9\n  Kube-Proxy Version:                      v1.25.4+a34b9e9\nPodCIDR:                                   172.30.2.0/24\nPodCIDRs:                                  172.30.2.0/24\nProviderID:                                ibm://fee034388aa6435883a1f720010ab3a2///cfpsebio0vo2oj5q8fc0/kube-cfpsebio0vo2oj5q8fc0-kubee2epvgm-default-000003a8\nNon-terminated Pods:                       (34 in total)\n  Namespace                                Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                            calico-node-jj7lx                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         166m\n  calico-system                            calico-typha-65d7b689d4-kpn77                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         166m\n  ibm-system                               ibm-cloud-provider-ip-163-68-71-82-fd485669b-hgqw5         5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         160m\n  kube-system                              ibm-keepalived-watcher-snrrr                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         167m\n  kube-system                              ibm-master-proxy-static-10.8.38.66                         26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      167m\n  kube-system                              ibmcloud-block-storage-driver-9s9rq                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     167m\n  kubectl-7140                             agnhost-primary-czp5g                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator   tuned-57bz4                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         161m\n  openshift-cluster-storage-operator       csi-snapshot-controller-5bfddff4c-whkcp                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         161m\n  openshift-cluster-storage-operator       csi-snapshot-webhook-544b96bbcd-bxjcg                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         161m\n  openshift-console                        downloads-8f679847b-nbfcm                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         161m\n  openshift-dns                            dns-default-6gh9k                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         161m\n  openshift-dns                            node-resolver-9wzh6                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         161m\n  openshift-image-registry                 node-ca-k9szq                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         161m\n  openshift-ingress-canary                 ingress-canary-c2h9v                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         161m\n  openshift-ingress                        router-default-f9ffd57f7-vxv45                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         161m\n  openshift-kube-proxy                     openshift-kube-proxy-4bz4j                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         166m\n  openshift-kube-storage-version-migrator  migrator-5c54d8d69d-jm5pm                                  10m (0%)      0 (0%)      200Mi (1%)       0 (0%)         161m\n  openshift-marketplace                    certified-operators-mswgj                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-marketplace                    community-operators-rkvlk                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         15m\n  openshift-marketplace                    redhat-marketplace-qnk27                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         163m\n  openshift-monitoring                     alertmanager-main-0                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         158m\n  openshift-monitoring                     node-exporter-zkcjf                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         160m\n  openshift-monitoring                     prometheus-adapter-685b6bd76d-xrdf9                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         159m\n  openshift-monitoring                     prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         159m\n  openshift-monitoring                     prometheus-operator-688459b4f4-4cv2n                       6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         161m\n  openshift-monitoring                     prometheus-operator-admission-webhook-7d4759d465-vqmvd     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         161m\n  openshift-monitoring                     thanos-querier-cb675b7f5-8hjjt                             15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         160m\n  openshift-multus                         multus-8cpsm                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         166m\n  openshift-multus                         multus-additional-cni-plugins-5sr6s                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         166m\n  openshift-multus                         network-metrics-daemon-hz5x4                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         166m\n  openshift-network-diagnostics            network-check-target-rbbb9                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         166m\n  sonobuoy                                 sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  sonobuoy                                 sonobuoy-systemd-logs-daemon-set-cba383a8c39e4f83-x7zhz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1131m (28%)      600m (15%)\n  memory             3354131Ki (24%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                   Message\n  ----    ------                   ----                 ----                   -------\n  Normal  Starting                 166m                 kube-proxy             \n  Normal  Starting                 167m                 kubelet                Starting kubelet.\n  Normal  NodeAllocatableEnforced  167m                 kubelet                Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  167m (x7 over 167m)  kubelet                Node 10.8.38.66 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    167m (x7 over 167m)  kubelet                Node 10.8.38.66 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     167m (x7 over 167m)  kubelet                Node 10.8.38.66 status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           167m                 node-controller        Node 10.8.38.66 event: Registered Node 10.8.38.66 in Controller\n  Normal  Synced                   167m                 cloud-node-controller  Node synced successfully\n  Normal  NodeReady                165m                 kubelet                Node 10.8.38.66 status is now: NodeReady\n  Normal  RegisteredNode           161m                 node-controller        Node 10.8.38.66 event: Registered Node 10.8.38.66 in Controller\n"
    Feb 20 22:25:14.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7140 describe namespace kubectl-7140'
    Feb 20 22:25:15.121: INFO: stderr: ""
    Feb 20 22:25:15.121: INFO: stdout: "Name:         kubectl-7140\nLabels:       e2e-framework=kubectl\n              e2e-run=24e5f230-9e47-40a6-adcc-763f231ccfc4\n              kubernetes.io/metadata.name=kubectl-7140\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c46,c45\n              openshift.io/sa.scc.supplemental-groups: 1002160000/10000\n              openshift.io/sa.scc.uid-range: 1002160000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:25:15.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7140" for this suite. 02/20/23 22:25:15.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:25:15.172
Feb 20 22:25:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-preemption 02/20/23 22:25:15.176
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:25:15.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:25:15.259
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 20 22:25:15.433: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 20 22:26:15.622: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:26:15.66
Feb 20 22:26:15.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-preemption-path 02/20/23 22:26:15.661
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:15.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:15.743
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Feb 20 22:26:15.801: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 20 22:26:15.811: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Feb 20 22:26:15.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1819" for this suite. 02/20/23 22:26:15.894
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:26:15.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-232" for this suite. 02/20/23 22:26:15.957
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":119,"skipped":2283,"failed":0}
------------------------------
• [SLOW TEST] [60.920 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:25:15.172
    Feb 20 22:25:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-preemption 02/20/23 22:25:15.176
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:25:15.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:25:15.259
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 20 22:25:15.433: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 20 22:26:15.622: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:26:15.66
    Feb 20 22:26:15.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-preemption-path 02/20/23 22:26:15.661
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:15.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:15.743
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Feb 20 22:26:15.801: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 20 22:26:15.811: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Feb 20 22:26:15.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1819" for this suite. 02/20/23 22:26:15.894
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:26:15.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-232" for this suite. 02/20/23 22:26:15.957
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:26:16.095
Feb 20 22:26:16.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 22:26:16.097
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:16.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:16.15
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/20/23 22:26:16.174
Feb 20 22:26:16.174: INFO: Creating simple deployment test-deployment-pcq25
Feb 20 22:26:16.217: INFO: deployment "test-deployment-pcq25" doesn't have the required revision set
STEP: Getting /status 02/20/23 22:26:18.258
Feb 20 22:26:18.268: INFO: Deployment test-deployment-pcq25 has Conditions: [{Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 02/20/23 22:26:18.268
Feb 20 22:26:18.290: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 26, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 26, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 26, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 26, 16, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pcq25-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/20/23 22:26:18.29
Feb 20 22:26:18.298: INFO: Observed &Deployment event: ADDED
Feb 20 22:26:18.298: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
Feb 20 22:26:18.298: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.298: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
Feb 20 22:26:18.298: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 20 22:26:18.299: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.299: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 20 22:26:18.299: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcq25-777898ffcc" is progressing.}
Feb 20 22:26:18.300: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.300: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 20 22:26:18.300: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
Feb 20 22:26:18.300: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.300: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 20 22:26:18.301: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
Feb 20 22:26:18.301: INFO: Found Deployment test-deployment-pcq25 in namespace deployment-3652 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 20 22:26:18.301: INFO: Deployment test-deployment-pcq25 has an updated status
STEP: patching the Statefulset Status 02/20/23 22:26:18.301
Feb 20 22:26:18.301: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 20 22:26:18.313: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/20/23 22:26:18.313
Feb 20 22:26:18.319: INFO: Observed &Deployment event: ADDED
Feb 20 22:26:18.319: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
Feb 20 22:26:18.320: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.320: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
Feb 20 22:26:18.320: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 20 22:26:18.320: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.321: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 20 22:26:18.321: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcq25-777898ffcc" is progressing.}
Feb 20 22:26:18.321: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
Feb 20 22:26:18.322: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 20 22:26:18.323: INFO: Observed &Deployment event: MODIFIED
Feb 20 22:26:18.323: INFO: Found deployment test-deployment-pcq25 in namespace deployment-3652 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 20 22:26:18.323: INFO: Deployment test-deployment-pcq25 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 22:26:18.332: INFO: Deployment "test-deployment-pcq25":
&Deployment{ObjectMeta:{test-deployment-pcq25  deployment-3652  3e6c0b08-2ff0-404c-80bb-477e2aa6b43f 89304 1 2023-02-20 22:26:16 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-20 22:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-20 22:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-20 22:26:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b449ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pcq25-777898ffcc",LastUpdateTime:2023-02-20 22:26:18 +0000 UTC,LastTransitionTime:2023-02-20 22:26:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 22:26:18.346: INFO: New ReplicaSet "test-deployment-pcq25-777898ffcc" of Deployment "test-deployment-pcq25":
&ReplicaSet{ObjectMeta:{test-deployment-pcq25-777898ffcc  deployment-3652  ecb6e812-c65e-4480-8e71-5efe517be60e 89294 1 2023-02-20 22:26:16 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pcq25 3e6c0b08-2ff0-404c-80bb-477e2aa6b43f 0xc0040c22f0 0xc0040c22f1}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e6c0b08-2ff0-404c-80bb-477e2aa6b43f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040c2398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:26:18.358: INFO: Pod "test-deployment-pcq25-777898ffcc-j587l" is available:
&Pod{ObjectMeta:{test-deployment-pcq25-777898ffcc-j587l test-deployment-pcq25-777898ffcc- deployment-3652  c7b7279e-b91d-4fdc-b6bd-264ced09c592 89292 0 2023-02-20 22:26:16 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:fde8ff97345378d40db0fc3b0bd18adc990d83e178573560363293292d64c3a6 cni.projectcalico.org/podIP:172.30.181.208/32 cni.projectcalico.org/podIPs:172.30.181.208/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.208"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.208"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-pcq25-777898ffcc ecb6e812-c65e-4480-8e71-5efe517be60e 0xc0008199d7 0xc0008199d8}] [] [{kube-controller-manager Update v1 2023-02-20 22:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecb6e812-c65e-4480-8e71-5efe517be60e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lgxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lgxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c47,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-57jfw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.208,StartTime:2023-02-20 22:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7c303f85b2a20a5795fe207b16b709d499328dabd5245f7fc879c84de8cd8e87,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 22:26:18.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3652" for this suite. 02/20/23 22:26:18.376
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":120,"skipped":2311,"failed":0}
------------------------------
• [2.306 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:26:16.095
    Feb 20 22:26:16.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 22:26:16.097
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:16.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:16.15
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/20/23 22:26:16.174
    Feb 20 22:26:16.174: INFO: Creating simple deployment test-deployment-pcq25
    Feb 20 22:26:16.217: INFO: deployment "test-deployment-pcq25" doesn't have the required revision set
    STEP: Getting /status 02/20/23 22:26:18.258
    Feb 20 22:26:18.268: INFO: Deployment test-deployment-pcq25 has Conditions: [{Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 02/20/23 22:26:18.268
    Feb 20 22:26:18.290: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 26, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 26, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 26, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 26, 16, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pcq25-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/20/23 22:26:18.29
    Feb 20 22:26:18.298: INFO: Observed &Deployment event: ADDED
    Feb 20 22:26:18.298: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
    Feb 20 22:26:18.298: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.298: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
    Feb 20 22:26:18.298: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 20 22:26:18.299: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.299: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 20 22:26:18.299: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcq25-777898ffcc" is progressing.}
    Feb 20 22:26:18.300: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.300: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 20 22:26:18.300: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
    Feb 20 22:26:18.300: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.300: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 20 22:26:18.301: INFO: Observed Deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
    Feb 20 22:26:18.301: INFO: Found Deployment test-deployment-pcq25 in namespace deployment-3652 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 20 22:26:18.301: INFO: Deployment test-deployment-pcq25 has an updated status
    STEP: patching the Statefulset Status 02/20/23 22:26:18.301
    Feb 20 22:26:18.301: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 20 22:26:18.313: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/20/23 22:26:18.313
    Feb 20 22:26:18.319: INFO: Observed &Deployment event: ADDED
    Feb 20 22:26:18.319: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
    Feb 20 22:26:18.320: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.320: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pcq25-777898ffcc"}
    Feb 20 22:26:18.320: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 20 22:26:18.320: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.321: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 20 22:26:18.321: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:16 +0000 UTC 2023-02-20 22:26:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pcq25-777898ffcc" is progressing.}
    Feb 20 22:26:18.321: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
    Feb 20 22:26:18.322: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-20 22:26:17 +0000 UTC 2023-02-20 22:26:16 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pcq25-777898ffcc" has successfully progressed.}
    Feb 20 22:26:18.322: INFO: Observed deployment test-deployment-pcq25 in namespace deployment-3652 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 20 22:26:18.323: INFO: Observed &Deployment event: MODIFIED
    Feb 20 22:26:18.323: INFO: Found deployment test-deployment-pcq25 in namespace deployment-3652 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 20 22:26:18.323: INFO: Deployment test-deployment-pcq25 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 22:26:18.332: INFO: Deployment "test-deployment-pcq25":
    &Deployment{ObjectMeta:{test-deployment-pcq25  deployment-3652  3e6c0b08-2ff0-404c-80bb-477e2aa6b43f 89304 1 2023-02-20 22:26:16 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-20 22:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-20 22:26:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-20 22:26:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b449ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pcq25-777898ffcc",LastUpdateTime:2023-02-20 22:26:18 +0000 UTC,LastTransitionTime:2023-02-20 22:26:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 20 22:26:18.346: INFO: New ReplicaSet "test-deployment-pcq25-777898ffcc" of Deployment "test-deployment-pcq25":
    &ReplicaSet{ObjectMeta:{test-deployment-pcq25-777898ffcc  deployment-3652  ecb6e812-c65e-4480-8e71-5efe517be60e 89294 1 2023-02-20 22:26:16 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pcq25 3e6c0b08-2ff0-404c-80bb-477e2aa6b43f 0xc0040c22f0 0xc0040c22f1}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e6c0b08-2ff0-404c-80bb-477e2aa6b43f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040c2398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:26:18.358: INFO: Pod "test-deployment-pcq25-777898ffcc-j587l" is available:
    &Pod{ObjectMeta:{test-deployment-pcq25-777898ffcc-j587l test-deployment-pcq25-777898ffcc- deployment-3652  c7b7279e-b91d-4fdc-b6bd-264ced09c592 89292 0 2023-02-20 22:26:16 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:fde8ff97345378d40db0fc3b0bd18adc990d83e178573560363293292d64c3a6 cni.projectcalico.org/podIP:172.30.181.208/32 cni.projectcalico.org/podIPs:172.30.181.208/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.208"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.208"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-pcq25-777898ffcc ecb6e812-c65e-4480-8e71-5efe517be60e 0xc0008199d7 0xc0008199d8}] [] [{kube-controller-manager Update v1 2023-02-20 22:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecb6e812-c65e-4480-8e71-5efe517be60e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-20 22:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lgxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lgxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c47,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-57jfw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:26:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.208,StartTime:2023-02-20 22:26:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:26:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7c303f85b2a20a5795fe207b16b709d499328dabd5245f7fc879c84de8cd8e87,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 22:26:18.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3652" for this suite. 02/20/23 22:26:18.376
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:26:18.403
Feb 20 22:26:18.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:26:18.406
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:18.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:18.476
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:26:18.537
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:26:18.863
STEP: Deploying the webhook pod 02/20/23 22:26:18.892
STEP: Wait for the deployment to be ready 02/20/23 22:26:18.917
Feb 20 22:26:18.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/20/23 22:26:20.974
STEP: Verifying the service has paired with the endpoint 02/20/23 22:26:21.013
Feb 20 22:26:22.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 02/20/23 22:26:22.027
STEP: create a pod 02/20/23 22:26:22.076
Feb 20 22:26:22.150: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2259" to be "running"
Feb 20 22:26:22.167: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.966704ms
Feb 20 22:26:24.186: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035396225s
Feb 20 22:26:24.186: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/20/23 22:26:24.186
Feb 20 22:26:24.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=webhook-2259 attach --namespace=webhook-2259 to-be-attached-pod -i -c=container1'
Feb 20 22:26:24.454: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:26:24.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2259" for this suite. 02/20/23 22:26:24.527
STEP: Destroying namespace "webhook-2259-markers" for this suite. 02/20/23 22:26:24.55
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":121,"skipped":2315,"failed":0}
------------------------------
• [SLOW TEST] [6.280 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:26:18.403
    Feb 20 22:26:18.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:26:18.406
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:18.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:18.476
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:26:18.537
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:26:18.863
    STEP: Deploying the webhook pod 02/20/23 22:26:18.892
    STEP: Wait for the deployment to be ready 02/20/23 22:26:18.917
    Feb 20 22:26:18.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/20/23 22:26:20.974
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:26:21.013
    Feb 20 22:26:22.013: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 02/20/23 22:26:22.027
    STEP: create a pod 02/20/23 22:26:22.076
    Feb 20 22:26:22.150: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2259" to be "running"
    Feb 20 22:26:22.167: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.966704ms
    Feb 20 22:26:24.186: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035396225s
    Feb 20 22:26:24.186: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/20/23 22:26:24.186
    Feb 20 22:26:24.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=webhook-2259 attach --namespace=webhook-2259 to-be-attached-pod -i -c=container1'
    Feb 20 22:26:24.454: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:26:24.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2259" for this suite. 02/20/23 22:26:24.527
    STEP: Destroying namespace "webhook-2259-markers" for this suite. 02/20/23 22:26:24.55
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:26:24.686
Feb 20 22:26:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:26:24.689
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:24.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:24.874
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-6776793e-dc02-47e9-af8f-e04cbeb379d3 02/20/23 22:26:24.901
STEP: Creating a pod to test consume secrets 02/20/23 22:26:24.916
Feb 20 22:26:24.968: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd" in namespace "projected-3704" to be "Succeeded or Failed"
Feb 20 22:26:24.984: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.387155ms
Feb 20 22:26:27.001: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.032670993s
Feb 20 22:26:28.997: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Running", Reason="", readiness=false. Elapsed: 4.028932562s
Feb 20 22:26:30.998: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029912734s
STEP: Saw pod success 02/20/23 22:26:30.998
Feb 20 22:26:30.999: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd" satisfied condition "Succeeded or Failed"
Feb 20 22:26:31.011: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd container projected-secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:26:31.055
Feb 20 22:26:31.088: INFO: Waiting for pod pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd to disappear
Feb 20 22:26:31.099: INFO: Pod pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 22:26:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3704" for this suite. 02/20/23 22:26:31.125
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":122,"skipped":2365,"failed":0}
------------------------------
• [SLOW TEST] [6.465 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:26:24.686
    Feb 20 22:26:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:26:24.689
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:24.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:24.874
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-6776793e-dc02-47e9-af8f-e04cbeb379d3 02/20/23 22:26:24.901
    STEP: Creating a pod to test consume secrets 02/20/23 22:26:24.916
    Feb 20 22:26:24.968: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd" in namespace "projected-3704" to be "Succeeded or Failed"
    Feb 20 22:26:24.984: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.387155ms
    Feb 20 22:26:27.001: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.032670993s
    Feb 20 22:26:28.997: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Running", Reason="", readiness=false. Elapsed: 4.028932562s
    Feb 20 22:26:30.998: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029912734s
    STEP: Saw pod success 02/20/23 22:26:30.998
    Feb 20 22:26:30.999: INFO: Pod "pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd" satisfied condition "Succeeded or Failed"
    Feb 20 22:26:31.011: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:26:31.055
    Feb 20 22:26:31.088: INFO: Waiting for pod pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd to disappear
    Feb 20 22:26:31.099: INFO: Pod pod-projected-secrets-22fab3c3-68c5-4e60-95d4-86b6ecd256bd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 22:26:31.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3704" for this suite. 02/20/23 22:26:31.125
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:26:31.152
Feb 20 22:26:31.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 22:26:31.153
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:31.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:31.206
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-234 02/20/23 22:26:31.215
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-234 02/20/23 22:26:31.232
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-234 02/20/23 22:26:31.246
Feb 20 22:26:31.258: INFO: Found 0 stateful pods, waiting for 1
Feb 20 22:26:41.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/20/23 22:26:41.271
Feb 20 22:26:41.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 22:26:41.657: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 22:26:41.658: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 22:26:41.658: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 22:26:41.669: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 22:26:51.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:26:51.682: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:26:51.723: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Feb 20 22:26:51.723: INFO: ss-0  10.8.38.70  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  }]
Feb 20 22:26:51.723: INFO: 
Feb 20 22:26:51.723: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 20 22:26:52.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986450834s
Feb 20 22:26:53.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971675399s
Feb 20 22:26:54.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952125781s
Feb 20 22:26:55.794: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.934411017s
Feb 20 22:26:56.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.916845261s
Feb 20 22:26:57.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.900100779s
Feb 20 22:26:58.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.883658845s
Feb 20 22:26:59.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.865037531s
Feb 20 22:27:00.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 848.424349ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-234 02/20/23 22:27:01.883
Feb 20 22:27:01.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 22:27:02.213: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 22:27:02.213: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 22:27:02.213: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 22:27:02.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 22:27:02.546: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 20 22:27:02.546: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 22:27:02.546: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 22:27:02.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 22:27:02.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 20 22:27:02.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 22:27:02.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 22:27:02.921: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:27:02.921: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:27:02.921: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/20/23 22:27:02.921
Feb 20 22:27:02.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 22:27:03.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 22:27:03.214: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 22:27:03.214: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 22:27:03.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 22:27:03.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 22:27:03.514: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 22:27:03.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 22:27:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 22:27:03.890: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 22:27:03.890: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 22:27:03.890: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 22:27:03.890: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:27:03.898: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 20 22:27:13.925: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:27:13.925: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:27:13.925: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 22:27:13.959: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Feb 20 22:27:13.959: INFO: ss-0  10.8.38.70  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  }]
Feb 20 22:27:13.959: INFO: ss-1  10.8.38.66  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
Feb 20 22:27:13.959: INFO: ss-2  10.8.38.69  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
Feb 20 22:27:13.959: INFO: 
Feb 20 22:27:13.959: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 22:27:14.976: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
Feb 20 22:27:14.976: INFO: ss-0  10.8.38.70  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  }]
Feb 20 22:27:14.976: INFO: ss-1  10.8.38.66  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
Feb 20 22:27:14.976: INFO: ss-2  10.8.38.69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
Feb 20 22:27:14.976: INFO: 
Feb 20 22:27:14.976: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 22:27:15.988: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.970663148s
Feb 20 22:27:17.002: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.958811427s
Feb 20 22:27:18.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.945009428s
Feb 20 22:27:19.029: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.930662978s
Feb 20 22:27:20.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.918323559s
Feb 20 22:27:21.053: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.905697717s
Feb 20 22:27:22.069: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.893686746s
Feb 20 22:27:23.080: INFO: Verifying statefulset ss doesn't scale past 0 for another 878.576137ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-234 02/20/23 22:27:24.08
Feb 20 22:27:24.092: INFO: Scaling statefulset ss to 0
Feb 20 22:27:24.122: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 22:27:24.130: INFO: Deleting all statefulset in ns statefulset-234
Feb 20 22:27:24.138: INFO: Scaling statefulset ss to 0
Feb 20 22:27:24.168: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:27:24.176: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 22:27:24.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-234" for this suite. 02/20/23 22:27:24.227
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":123,"skipped":2368,"failed":0}
------------------------------
• [SLOW TEST] [53.100 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:26:31.152
    Feb 20 22:26:31.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 22:26:31.153
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:26:31.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:26:31.206
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-234 02/20/23 22:26:31.215
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-234 02/20/23 22:26:31.232
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-234 02/20/23 22:26:31.246
    Feb 20 22:26:31.258: INFO: Found 0 stateful pods, waiting for 1
    Feb 20 22:26:41.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/20/23 22:26:41.271
    Feb 20 22:26:41.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 22:26:41.657: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 22:26:41.658: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 22:26:41.658: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 22:26:41.669: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 20 22:26:51.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 22:26:51.682: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 22:26:51.723: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Feb 20 22:26:51.723: INFO: ss-0  10.8.38.70  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  }]
    Feb 20 22:26:51.723: INFO: 
    Feb 20 22:26:51.723: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb 20 22:26:52.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986450834s
    Feb 20 22:26:53.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971675399s
    Feb 20 22:26:54.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952125781s
    Feb 20 22:26:55.794: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.934411017s
    Feb 20 22:26:56.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.916845261s
    Feb 20 22:26:57.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.900100779s
    Feb 20 22:26:58.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.883658845s
    Feb 20 22:26:59.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.865037531s
    Feb 20 22:27:00.881: INFO: Verifying statefulset ss doesn't scale past 3 for another 848.424349ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-234 02/20/23 22:27:01.883
    Feb 20 22:27:01.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 22:27:02.213: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 22:27:02.213: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 22:27:02.213: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 22:27:02.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 22:27:02.546: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 20 22:27:02.546: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 22:27:02.546: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 22:27:02.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 22:27:02.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 20 22:27:02.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 22:27:02.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 22:27:02.921: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 22:27:02.921: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 22:27:02.921: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/20/23 22:27:02.921
    Feb 20 22:27:02.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 22:27:03.214: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 22:27:03.214: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 22:27:03.214: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 22:27:03.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 22:27:03.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 22:27:03.514: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 22:27:03.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 22:27:03.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-234 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 22:27:03.890: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 22:27:03.890: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 22:27:03.890: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 22:27:03.890: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 22:27:03.898: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Feb 20 22:27:13.925: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 22:27:13.925: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 22:27:13.925: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 22:27:13.959: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Feb 20 22:27:13.959: INFO: ss-0  10.8.38.70  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  }]
    Feb 20 22:27:13.959: INFO: ss-1  10.8.38.66  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
    Feb 20 22:27:13.959: INFO: ss-2  10.8.38.69  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
    Feb 20 22:27:13.959: INFO: 
    Feb 20 22:27:13.959: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 20 22:27:14.976: INFO: POD   NODE        PHASE    GRACE  CONDITIONS
    Feb 20 22:27:14.976: INFO: ss-0  10.8.38.70  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:31 +0000 UTC  }]
    Feb 20 22:27:14.976: INFO: ss-1  10.8.38.66  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
    Feb 20 22:27:14.976: INFO: ss-2  10.8.38.69  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:27:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 22:26:51 +0000 UTC  }]
    Feb 20 22:27:14.976: INFO: 
    Feb 20 22:27:14.976: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 20 22:27:15.988: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.970663148s
    Feb 20 22:27:17.002: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.958811427s
    Feb 20 22:27:18.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.945009428s
    Feb 20 22:27:19.029: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.930662978s
    Feb 20 22:27:20.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.918323559s
    Feb 20 22:27:21.053: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.905697717s
    Feb 20 22:27:22.069: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.893686746s
    Feb 20 22:27:23.080: INFO: Verifying statefulset ss doesn't scale past 0 for another 878.576137ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-234 02/20/23 22:27:24.08
    Feb 20 22:27:24.092: INFO: Scaling statefulset ss to 0
    Feb 20 22:27:24.122: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 22:27:24.130: INFO: Deleting all statefulset in ns statefulset-234
    Feb 20 22:27:24.138: INFO: Scaling statefulset ss to 0
    Feb 20 22:27:24.168: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 22:27:24.176: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 22:27:24.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-234" for this suite. 02/20/23 22:27:24.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:27:24.259
Feb 20 22:27:24.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 22:27:24.261
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:24.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:24.325
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/20/23 22:27:24.352
Feb 20 22:27:24.404: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4899" to be "running and ready"
Feb 20 22:27:24.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.932951ms
Feb 20 22:27:24.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:27:26.429: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024502261s
Feb 20 22:27:26.429: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 20 22:27:26.429: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 02/20/23 22:27:26.44
Feb 20 22:27:26.475: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4899" to be "running and ready"
Feb 20 22:27:26.487: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.347012ms
Feb 20 22:27:26.487: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:27:28.499: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023665494s
Feb 20 22:27:28.499: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 20 22:27:28.499: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/20/23 22:27:28.51
Feb 20 22:27:28.530: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 22:27:28.542: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 22:27:30.544: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 22:27:30.560: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 22:27:32.543: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 22:27:32.555: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/20/23 22:27:32.555
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 20 22:27:32.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4899" for this suite. 02/20/23 22:27:32.618
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":124,"skipped":2463,"failed":0}
------------------------------
• [SLOW TEST] [8.390 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:27:24.259
    Feb 20 22:27:24.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 22:27:24.261
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:24.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:24.325
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/20/23 22:27:24.352
    Feb 20 22:27:24.404: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4899" to be "running and ready"
    Feb 20 22:27:24.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.932951ms
    Feb 20 22:27:24.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:27:26.429: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024502261s
    Feb 20 22:27:26.429: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 20 22:27:26.429: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 02/20/23 22:27:26.44
    Feb 20 22:27:26.475: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4899" to be "running and ready"
    Feb 20 22:27:26.487: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.347012ms
    Feb 20 22:27:26.487: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:27:28.499: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023665494s
    Feb 20 22:27:28.499: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 20 22:27:28.499: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/20/23 22:27:28.51
    Feb 20 22:27:28.530: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 20 22:27:28.542: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 20 22:27:30.544: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 20 22:27:30.560: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 20 22:27:32.543: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 20 22:27:32.555: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/20/23 22:27:32.555
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 20 22:27:32.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4899" for this suite. 02/20/23 22:27:32.618
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:27:32.651
Feb 20 22:27:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename job 02/20/23 22:27:32.653
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:32.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:32.71
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 02/20/23 22:27:32.732
W0220 22:27:32.745229      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Patching the Job 02/20/23 22:27:32.745
W0220 22:27:32.769465      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for Job to be patched 02/20/23 22:27:32.769
Feb 20 22:27:32.776: INFO: Event ADDED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 20 22:27:32.776: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 20 22:27:32.777: INFO: Event MODIFIED found for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/20/23 22:27:32.777
STEP: Watching for Job to be updated 02/20/23 22:27:32.805
Feb 20 22:27:32.813: INFO: Event MODIFIED found for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:32.813: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/20/23 22:27:32.813
Feb 20 22:27:32.822: INFO: Job: e2e-nrctv as labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched]
STEP: Waiting for job to complete 02/20/23 22:27:32.822
STEP: Delete a job collection with a labelselector 02/20/23 22:27:44.832
STEP: Watching for Job to be deleted 02/20/23 22:27:44.847
Feb 20 22:27:44.853: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.853: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.856: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.856: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.856: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 20 22:27:44.857: INFO: Event DELETED found for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/20/23 22:27:44.857
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 20 22:27:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3992" for this suite. 02/20/23 22:27:44.885
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":125,"skipped":2481,"failed":0}
------------------------------
• [SLOW TEST] [12.256 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:27:32.651
    Feb 20 22:27:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename job 02/20/23 22:27:32.653
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:32.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:32.71
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 02/20/23 22:27:32.732
    W0220 22:27:32.745229      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Patching the Job 02/20/23 22:27:32.745
    W0220 22:27:32.769465      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for Job to be patched 02/20/23 22:27:32.769
    Feb 20 22:27:32.776: INFO: Event ADDED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 20 22:27:32.776: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 20 22:27:32.777: INFO: Event MODIFIED found for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/20/23 22:27:32.777
    STEP: Watching for Job to be updated 02/20/23 22:27:32.805
    Feb 20 22:27:32.813: INFO: Event MODIFIED found for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:32.813: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/20/23 22:27:32.813
    Feb 20 22:27:32.822: INFO: Job: e2e-nrctv as labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched]
    STEP: Waiting for job to complete 02/20/23 22:27:32.822
    STEP: Delete a job collection with a labelselector 02/20/23 22:27:44.832
    STEP: Watching for Job to be deleted 02/20/23 22:27:44.847
    Feb 20 22:27:44.853: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.853: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.854: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.856: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.856: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.856: INFO: Event MODIFIED observed for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 20 22:27:44.857: INFO: Event DELETED found for Job e2e-nrctv in namespace job-3992 with labels: map[e2e-job-label:e2e-nrctv e2e-nrctv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/20/23 22:27:44.857
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 20 22:27:44.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3992" for this suite. 02/20/23 22:27:44.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:27:44.919
Feb 20 22:27:44.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:27:44.922
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:44.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:44.979
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 02/20/23 22:27:44.993
Feb 20 22:27:45.043: INFO: Waiting up to 5m0s for pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321" in namespace "downward-api-7117" to be "Succeeded or Failed"
Feb 20 22:27:45.057: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Pending", Reason="", readiness=false. Elapsed: 14.336481ms
Feb 20 22:27:47.071: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028079315s
Feb 20 22:27:49.077: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033893295s
Feb 20 22:27:51.072: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028672134s
STEP: Saw pod success 02/20/23 22:27:51.072
Feb 20 22:27:51.072: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321" satisfied condition "Succeeded or Failed"
Feb 20 22:27:51.085: INFO: Trying to get logs from node 10.8.38.66 pod downward-api-aa8fd779-3292-4417-8dae-f7b629f93321 container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:27:51.111
Feb 20 22:27:51.146: INFO: Waiting for pod downward-api-aa8fd779-3292-4417-8dae-f7b629f93321 to disappear
Feb 20 22:27:51.156: INFO: Pod downward-api-aa8fd779-3292-4417-8dae-f7b629f93321 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 20 22:27:51.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7117" for this suite. 02/20/23 22:27:51.173
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":126,"skipped":2512,"failed":0}
------------------------------
• [SLOW TEST] [6.279 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:27:44.919
    Feb 20 22:27:44.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:27:44.922
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:44.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:44.979
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 02/20/23 22:27:44.993
    Feb 20 22:27:45.043: INFO: Waiting up to 5m0s for pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321" in namespace "downward-api-7117" to be "Succeeded or Failed"
    Feb 20 22:27:45.057: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Pending", Reason="", readiness=false. Elapsed: 14.336481ms
    Feb 20 22:27:47.071: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028079315s
    Feb 20 22:27:49.077: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033893295s
    Feb 20 22:27:51.072: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028672134s
    STEP: Saw pod success 02/20/23 22:27:51.072
    Feb 20 22:27:51.072: INFO: Pod "downward-api-aa8fd779-3292-4417-8dae-f7b629f93321" satisfied condition "Succeeded or Failed"
    Feb 20 22:27:51.085: INFO: Trying to get logs from node 10.8.38.66 pod downward-api-aa8fd779-3292-4417-8dae-f7b629f93321 container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:27:51.111
    Feb 20 22:27:51.146: INFO: Waiting for pod downward-api-aa8fd779-3292-4417-8dae-f7b629f93321 to disappear
    Feb 20 22:27:51.156: INFO: Pod downward-api-aa8fd779-3292-4417-8dae-f7b629f93321 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 20 22:27:51.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7117" for this suite. 02/20/23 22:27:51.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:27:51.209
Feb 20 22:27:51.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:27:51.211
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:51.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:51.28
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6210 02/20/23 22:27:51.292
Feb 20 22:27:51.380: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6210" to be "running and ready"
Feb 20 22:27:51.412: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 32.161153ms
Feb 20 22:27:51.412: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:27:53.427: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.047077274s
Feb 20 22:27:53.427: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 20 22:27:53.427: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 20 22:27:53.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 20 22:27:53.730: INFO: rc: 7
Feb 20 22:27:53.762: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 20 22:27:53.773: INFO: Pod kube-proxy-mode-detector no longer exists
Feb 20 22:27:53.773: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-6210 02/20/23 22:27:53.773
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6210 02/20/23 22:27:53.806
I0220 22:27:53.817556      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6210, replica count: 3
I0220 22:27:56.869064      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:27:56.974: INFO: Creating new exec pod
Feb 20 22:27:57.011: INFO: Waiting up to 5m0s for pod "execpod-affinitydfwxf" in namespace "services-6210" to be "running"
Feb 20 22:27:57.022: INFO: Pod "execpod-affinitydfwxf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.8765ms
Feb 20 22:27:59.037: INFO: Pod "execpod-affinitydfwxf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026451609s
Feb 20 22:28:01.035: INFO: Pod "execpod-affinitydfwxf": Phase="Running", Reason="", readiness=true. Elapsed: 4.024357242s
Feb 20 22:28:01.035: INFO: Pod "execpod-affinitydfwxf" satisfied condition "running"
Feb 20 22:28:02.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 20 22:28:02.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 20 22:28:02.365: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:28:02.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.22.235 80'
Feb 20 22:28:02.667: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.22.235 80\nConnection to 172.21.22.235 80 port [tcp/http] succeeded!\n"
Feb 20 22:28:02.667: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:28:02.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 31052'
Feb 20 22:28:02.974: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 31052\nConnection to 10.8.38.69 31052 port [tcp/*] succeeded!\n"
Feb 20 22:28:02.974: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:28:02.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 31052'
Feb 20 22:28:03.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 31052\nConnection to 10.8.38.66 31052 port [tcp/*] succeeded!\n"
Feb 20 22:28:03.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:28:03.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:31052/ ; done'
Feb 20 22:28:03.627: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n"
Feb 20 22:28:03.627: INFO: stdout: "\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk"
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
Feb 20 22:28:03.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.8.38.66:31052/'
Feb 20 22:28:03.900: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n"
Feb 20 22:28:03.900: INFO: stdout: "affinity-nodeport-timeout-mjggk"
Feb 20 22:28:23.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.8.38.66:31052/'
Feb 20 22:28:24.191: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n"
Feb 20 22:28:24.191: INFO: stdout: "affinity-nodeport-timeout-6ltjp"
Feb 20 22:28:24.192: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6210, will wait for the garbage collector to delete the pods 02/20/23 22:28:24.226
Feb 20 22:28:24.297: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 11.944643ms
Feb 20 22:28:24.398: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.542942ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:28:27.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6210" for this suite. 02/20/23 22:28:27.693
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":127,"skipped":2542,"failed":0}
------------------------------
• [SLOW TEST] [36.519 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:27:51.209
    Feb 20 22:27:51.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:27:51.211
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:27:51.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:27:51.28
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6210 02/20/23 22:27:51.292
    Feb 20 22:27:51.380: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6210" to be "running and ready"
    Feb 20 22:27:51.412: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 32.161153ms
    Feb 20 22:27:51.412: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:27:53.427: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.047077274s
    Feb 20 22:27:53.427: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 20 22:27:53.427: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 20 22:27:53.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 20 22:27:53.730: INFO: rc: 7
    Feb 20 22:27:53.762: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 20 22:27:53.773: INFO: Pod kube-proxy-mode-detector no longer exists
    Feb 20 22:27:53.773: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-nodeport-timeout in namespace services-6210 02/20/23 22:27:53.773
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6210 02/20/23 22:27:53.806
    I0220 22:27:53.817556      20 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6210, replica count: 3
    I0220 22:27:56.869064      20 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:27:56.974: INFO: Creating new exec pod
    Feb 20 22:27:57.011: INFO: Waiting up to 5m0s for pod "execpod-affinitydfwxf" in namespace "services-6210" to be "running"
    Feb 20 22:27:57.022: INFO: Pod "execpod-affinitydfwxf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.8765ms
    Feb 20 22:27:59.037: INFO: Pod "execpod-affinitydfwxf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026451609s
    Feb 20 22:28:01.035: INFO: Pod "execpod-affinitydfwxf": Phase="Running", Reason="", readiness=true. Elapsed: 4.024357242s
    Feb 20 22:28:01.035: INFO: Pod "execpod-affinitydfwxf" satisfied condition "running"
    Feb 20 22:28:02.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Feb 20 22:28:02.365: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Feb 20 22:28:02.365: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:28:02.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.22.235 80'
    Feb 20 22:28:02.667: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.22.235 80\nConnection to 172.21.22.235 80 port [tcp/http] succeeded!\n"
    Feb 20 22:28:02.667: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:28:02.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 31052'
    Feb 20 22:28:02.974: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 31052\nConnection to 10.8.38.69 31052 port [tcp/*] succeeded!\n"
    Feb 20 22:28:02.974: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:28:02.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 31052'
    Feb 20 22:28:03.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 31052\nConnection to 10.8.38.66 31052 port [tcp/*] succeeded!\n"
    Feb 20 22:28:03.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:28:03.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:31052/ ; done'
    Feb 20 22:28:03.627: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n"
    Feb 20 22:28:03.627: INFO: stdout: "\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk\naffinity-nodeport-timeout-mjggk"
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.627: INFO: Received response from host: affinity-nodeport-timeout-mjggk
    Feb 20 22:28:03.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.8.38.66:31052/'
    Feb 20 22:28:03.900: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n"
    Feb 20 22:28:03.900: INFO: stdout: "affinity-nodeport-timeout-mjggk"
    Feb 20 22:28:23.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6210 exec execpod-affinitydfwxf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.8.38.66:31052/'
    Feb 20 22:28:24.191: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.8.38.66:31052/\n"
    Feb 20 22:28:24.191: INFO: stdout: "affinity-nodeport-timeout-6ltjp"
    Feb 20 22:28:24.192: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6210, will wait for the garbage collector to delete the pods 02/20/23 22:28:24.226
    Feb 20 22:28:24.297: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 11.944643ms
    Feb 20 22:28:24.398: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.542942ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:28:27.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6210" for this suite. 02/20/23 22:28:27.693
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:28:27.731
Feb 20 22:28:27.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:28:27.733
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:27.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:27.842
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:28:27.881
Feb 20 22:28:27.928: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7" in namespace "projected-4572" to be "Succeeded or Failed"
Feb 20 22:28:27.948: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.419638ms
Feb 20 22:28:29.986: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058402056s
Feb 20 22:28:31.960: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032090469s
STEP: Saw pod success 02/20/23 22:28:31.96
Feb 20 22:28:31.960: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7" satisfied condition "Succeeded or Failed"
Feb 20 22:28:31.971: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7 container client-container: <nil>
STEP: delete the pod 02/20/23 22:28:31.992
Feb 20 22:28:32.029: INFO: Waiting for pod downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7 to disappear
Feb 20 22:28:32.041: INFO: Pod downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 22:28:32.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4572" for this suite. 02/20/23 22:28:32.058
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":128,"skipped":2566,"failed":0}
------------------------------
• [4.350 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:28:27.731
    Feb 20 22:28:27.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:28:27.733
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:27.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:27.842
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:28:27.881
    Feb 20 22:28:27.928: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7" in namespace "projected-4572" to be "Succeeded or Failed"
    Feb 20 22:28:27.948: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.419638ms
    Feb 20 22:28:29.986: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058402056s
    Feb 20 22:28:31.960: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032090469s
    STEP: Saw pod success 02/20/23 22:28:31.96
    Feb 20 22:28:31.960: INFO: Pod "downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7" satisfied condition "Succeeded or Failed"
    Feb 20 22:28:31.971: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:28:31.992
    Feb 20 22:28:32.029: INFO: Waiting for pod downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7 to disappear
    Feb 20 22:28:32.041: INFO: Pod downwardapi-volume-f0ce427e-977f-44ea-8a79-82469576aaa7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 22:28:32.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4572" for this suite. 02/20/23 22:28:32.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:28:32.087
Feb 20 22:28:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename runtimeclass 02/20/23 22:28:32.088
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:32.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:32.146
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 20 22:28:32.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3975" for this suite. 02/20/23 22:28:32.204
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":129,"skipped":2573,"failed":0}
------------------------------
• [0.155 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:28:32.087
    Feb 20 22:28:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename runtimeclass 02/20/23 22:28:32.088
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:32.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:32.146
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 20 22:28:32.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3975" for this suite. 02/20/23 22:28:32.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:28:32.247
Feb 20 22:28:32.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename security-context-test 02/20/23 22:28:32.249
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:32.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:32.31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Feb 20 22:28:32.367: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64" in namespace "security-context-test-3210" to be "Succeeded or Failed"
Feb 20 22:28:32.380: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Pending", Reason="", readiness=false. Elapsed: 12.952623ms
Feb 20 22:28:34.395: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028149587s
Feb 20 22:28:36.393: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025271781s
Feb 20 22:28:38.393: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025831461s
Feb 20 22:28:38.393: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 20 22:28:38.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3210" for this suite. 02/20/23 22:28:38.411
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":130,"skipped":2595,"failed":0}
------------------------------
• [SLOW TEST] [6.189 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:28:32.247
    Feb 20 22:28:32.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename security-context-test 02/20/23 22:28:32.249
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:32.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:32.31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Feb 20 22:28:32.367: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64" in namespace "security-context-test-3210" to be "Succeeded or Failed"
    Feb 20 22:28:32.380: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Pending", Reason="", readiness=false. Elapsed: 12.952623ms
    Feb 20 22:28:34.395: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028149587s
    Feb 20 22:28:36.393: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025271781s
    Feb 20 22:28:38.393: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025831461s
    Feb 20 22:28:38.393: INFO: Pod "busybox-readonly-false-2f9528b2-f5e7-4a92-ab64-ae43c3c6eb64" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 20 22:28:38.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3210" for this suite. 02/20/23 22:28:38.411
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:28:38.438
Feb 20 22:28:38.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 22:28:38.44
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:38.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:38.487
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 02/20/23 22:28:38.496
W0220 22:28:38.581458      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for pod running 02/20/23 22:28:38.581
Feb 20 22:28:38.581: INFO: Waiting up to 2m0s for pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" in namespace "var-expansion-2644" to be "running"
Feb 20 22:28:38.594: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987": Phase="Pending", Reason="", readiness=false. Elapsed: 12.189787ms
Feb 20 22:28:40.610: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987": Phase="Running", Reason="", readiness=true. Elapsed: 2.028285768s
Feb 20 22:28:40.610: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" satisfied condition "running"
STEP: creating a file in subpath 02/20/23 22:28:40.61
Feb 20 22:28:40.621: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2644 PodName:var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:28:40.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:28:40.623: INFO: ExecWithOptions: Clientset creation
Feb 20 22:28:40.623: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-2644/pods/var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/20/23 22:28:40.768
Feb 20 22:28:40.781: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2644 PodName:var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:28:40.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:28:40.782: INFO: ExecWithOptions: Clientset creation
Feb 20 22:28:40.782: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-2644/pods/var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/20/23 22:28:40.953
Feb 20 22:28:41.501: INFO: Successfully updated pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987"
STEP: waiting for annotated pod running 02/20/23 22:28:41.502
Feb 20 22:28:41.505: INFO: Waiting up to 2m0s for pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" in namespace "var-expansion-2644" to be "running"
Feb 20 22:28:41.516: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987": Phase="Running", Reason="", readiness=true. Elapsed: 10.958725ms
Feb 20 22:28:41.516: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" satisfied condition "running"
STEP: deleting the pod gracefully 02/20/23 22:28:41.516
Feb 20 22:28:41.516: INFO: Deleting pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" in namespace "var-expansion-2644"
Feb 20 22:28:41.535: INFO: Wait up to 5m0s for pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 22:29:15.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2644" for this suite. 02/20/23 22:29:15.581
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":131,"skipped":2597,"failed":0}
------------------------------
• [SLOW TEST] [37.166 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:28:38.438
    Feb 20 22:28:38.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 22:28:38.44
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:28:38.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:28:38.487
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 02/20/23 22:28:38.496
    W0220 22:28:38.581458      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for pod running 02/20/23 22:28:38.581
    Feb 20 22:28:38.581: INFO: Waiting up to 2m0s for pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" in namespace "var-expansion-2644" to be "running"
    Feb 20 22:28:38.594: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987": Phase="Pending", Reason="", readiness=false. Elapsed: 12.189787ms
    Feb 20 22:28:40.610: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987": Phase="Running", Reason="", readiness=true. Elapsed: 2.028285768s
    Feb 20 22:28:40.610: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" satisfied condition "running"
    STEP: creating a file in subpath 02/20/23 22:28:40.61
    Feb 20 22:28:40.621: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2644 PodName:var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:28:40.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:28:40.623: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:28:40.623: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-2644/pods/var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/20/23 22:28:40.768
    Feb 20 22:28:40.781: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2644 PodName:var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:28:40.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:28:40.782: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:28:40.782: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-2644/pods/var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/20/23 22:28:40.953
    Feb 20 22:28:41.501: INFO: Successfully updated pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987"
    STEP: waiting for annotated pod running 02/20/23 22:28:41.502
    Feb 20 22:28:41.505: INFO: Waiting up to 2m0s for pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" in namespace "var-expansion-2644" to be "running"
    Feb 20 22:28:41.516: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987": Phase="Running", Reason="", readiness=true. Elapsed: 10.958725ms
    Feb 20 22:28:41.516: INFO: Pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" satisfied condition "running"
    STEP: deleting the pod gracefully 02/20/23 22:28:41.516
    Feb 20 22:28:41.516: INFO: Deleting pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" in namespace "var-expansion-2644"
    Feb 20 22:28:41.535: INFO: Wait up to 5m0s for pod "var-expansion-94c187fd-8f73-4c99-bc37-3a72d90cf987" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 22:29:15.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2644" for this suite. 02/20/23 22:29:15.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:29:15.607
Feb 20 22:29:15.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename disruption 02/20/23 22:29:15.609
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:15.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:15.703
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 02/20/23 22:29:15.729
STEP: Waiting for all pods to be running 02/20/23 22:29:17.858
Feb 20 22:29:17.870: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 20 22:29:19.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9136" for this suite. 02/20/23 22:29:19.911
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":132,"skipped":2611,"failed":0}
------------------------------
• [4.329 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:29:15.607
    Feb 20 22:29:15.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename disruption 02/20/23 22:29:15.609
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:15.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:15.703
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 02/20/23 22:29:15.729
    STEP: Waiting for all pods to be running 02/20/23 22:29:17.858
    Feb 20 22:29:17.870: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 20 22:29:19.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9136" for this suite. 02/20/23 22:29:19.911
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:29:19.936
Feb 20 22:29:19.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/20/23 22:29:19.937
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:19.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:19.991
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/20/23 22:29:20.014
STEP: Creating hostNetwork=false pod 02/20/23 22:29:20.015
Feb 20 22:29:20.066: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9123" to be "running and ready"
Feb 20 22:29:20.078: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.797517ms
Feb 20 22:29:20.078: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:29:22.091: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025295778s
Feb 20 22:29:22.092: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:29:24.092: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.026107141s
Feb 20 22:29:24.093: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 20 22:29:24.093: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/20/23 22:29:24.104
Feb 20 22:29:24.160: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9123" to be "running and ready"
Feb 20 22:29:24.182: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.348549ms
Feb 20 22:29:24.183: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:29:26.196: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035859952s
Feb 20 22:29:26.196: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:29:28.197: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.036836604s
Feb 20 22:29:28.197: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 20 22:29:28.197: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/20/23 22:29:28.209
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/20/23 22:29:28.209
Feb 20 22:29:28.210: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:28.211: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:28.211: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 20 22:29:28.355: INFO: Exec stderr: ""
Feb 20 22:29:28.355: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:28.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:28.359: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:28.359: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 20 22:29:28.512: INFO: Exec stderr: ""
Feb 20 22:29:28.513: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:28.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:28.515: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:28.515: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 20 22:29:28.714: INFO: Exec stderr: ""
Feb 20 22:29:28.714: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:28.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:28.715: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:28.715: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 20 22:29:28.912: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/20/23 22:29:28.913
Feb 20 22:29:28.913: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:28.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:28.914: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:28.914: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 20 22:29:29.089: INFO: Exec stderr: ""
Feb 20 22:29:29.090: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:29.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:29.091: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:29.091: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 20 22:29:29.342: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/20/23 22:29:29.342
Feb 20 22:29:29.342: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:29.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:29.342: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:29.342: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 20 22:29:29.610: INFO: Exec stderr: ""
Feb 20 22:29:29.610: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:29.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:29.611: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:29.611: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 20 22:29:29.831: INFO: Exec stderr: ""
Feb 20 22:29:29.831: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:29.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:29.832: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:29.832: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 20 22:29:29.971: INFO: Exec stderr: ""
Feb 20 22:29:29.971: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:29:29.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:29:29.972: INFO: ExecWithOptions: Clientset creation
Feb 20 22:29:29.972: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 20 22:29:30.171: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Feb 20 22:29:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9123" for this suite. 02/20/23 22:29:30.189
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":133,"skipped":2611,"failed":0}
------------------------------
• [SLOW TEST] [10.276 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:29:19.936
    Feb 20 22:29:19.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/20/23 22:29:19.937
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:19.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:19.991
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/20/23 22:29:20.014
    STEP: Creating hostNetwork=false pod 02/20/23 22:29:20.015
    Feb 20 22:29:20.066: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9123" to be "running and ready"
    Feb 20 22:29:20.078: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.797517ms
    Feb 20 22:29:20.078: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:29:22.091: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025295778s
    Feb 20 22:29:22.092: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:29:24.092: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.026107141s
    Feb 20 22:29:24.093: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 20 22:29:24.093: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/20/23 22:29:24.104
    Feb 20 22:29:24.160: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9123" to be "running and ready"
    Feb 20 22:29:24.182: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.348549ms
    Feb 20 22:29:24.183: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:29:26.196: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035859952s
    Feb 20 22:29:26.196: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:29:28.197: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.036836604s
    Feb 20 22:29:28.197: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 20 22:29:28.197: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/20/23 22:29:28.209
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/20/23 22:29:28.209
    Feb 20 22:29:28.210: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:28.211: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:28.211: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 20 22:29:28.355: INFO: Exec stderr: ""
    Feb 20 22:29:28.355: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:28.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:28.359: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:28.359: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 20 22:29:28.512: INFO: Exec stderr: ""
    Feb 20 22:29:28.513: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:28.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:28.515: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:28.515: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 20 22:29:28.714: INFO: Exec stderr: ""
    Feb 20 22:29:28.714: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:28.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:28.715: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:28.715: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 20 22:29:28.912: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/20/23 22:29:28.913
    Feb 20 22:29:28.913: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:28.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:28.914: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:28.914: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 20 22:29:29.089: INFO: Exec stderr: ""
    Feb 20 22:29:29.090: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:29.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:29.091: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:29.091: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 20 22:29:29.342: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/20/23 22:29:29.342
    Feb 20 22:29:29.342: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:29.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:29.342: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:29.342: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 20 22:29:29.610: INFO: Exec stderr: ""
    Feb 20 22:29:29.610: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:29.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:29.611: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:29.611: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 20 22:29:29.831: INFO: Exec stderr: ""
    Feb 20 22:29:29.831: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:29.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:29.832: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:29.832: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 20 22:29:29.971: INFO: Exec stderr: ""
    Feb 20 22:29:29.971: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9123 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:29:29.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:29:29.972: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:29:29.972: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9123/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 20 22:29:30.171: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Feb 20 22:29:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9123" for this suite. 02/20/23 22:29:30.189
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:29:30.213
Feb 20 22:29:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replication-controller 02/20/23 22:29:30.216
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:30.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:30.278
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Feb 20 22:29:30.295: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/20/23 22:29:31.331
STEP: Checking rc "condition-test" has the desired failure condition set 02/20/23 22:29:31.346
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/20/23 22:29:32.366
Feb 20 22:29:32.389: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/20/23 22:29:32.389
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 20 22:29:33.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5288" for this suite. 02/20/23 22:29:33.426
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":134,"skipped":2613,"failed":0}
------------------------------
• [3.238 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:29:30.213
    Feb 20 22:29:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replication-controller 02/20/23 22:29:30.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:30.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:30.278
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Feb 20 22:29:30.295: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/20/23 22:29:31.331
    STEP: Checking rc "condition-test" has the desired failure condition set 02/20/23 22:29:31.346
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/20/23 22:29:32.366
    Feb 20 22:29:32.389: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/20/23 22:29:32.389
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 20 22:29:33.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5288" for this suite. 02/20/23 22:29:33.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:29:33.454
Feb 20 22:29:33.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:29:33.458
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:33.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:33.526
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 02/20/23 22:29:33.541
Feb 20 22:29:33.541: INFO: namespace kubectl-9252
Feb 20 22:29:33.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 create -f -'
Feb 20 22:29:34.063: INFO: stderr: ""
Feb 20 22:29:34.063: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/20/23 22:29:34.063
Feb 20 22:29:35.077: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:29:35.077: INFO: Found 0 / 1
Feb 20 22:29:36.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:29:36.075: INFO: Found 0 / 1
Feb 20 22:29:37.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:29:37.075: INFO: Found 1 / 1
Feb 20 22:29:37.075: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 22:29:37.086: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 20 22:29:37.086: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 22:29:37.086: INFO: wait on agnhost-primary startup in kubectl-9252 
Feb 20 22:29:37.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 logs agnhost-primary-cqv4j agnhost-primary'
Feb 20 22:29:37.300: INFO: stderr: ""
Feb 20 22:29:37.300: INFO: stdout: "Paused\n"
STEP: exposing RC 02/20/23 22:29:37.3
Feb 20 22:29:37.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 20 22:29:37.458: INFO: stderr: ""
Feb 20 22:29:37.458: INFO: stdout: "service/rm2 exposed\n"
Feb 20 22:29:37.487: INFO: Service rm2 in namespace kubectl-9252 found.
STEP: exposing service 02/20/23 22:29:39.562
Feb 20 22:29:39.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 20 22:29:39.921: INFO: stderr: ""
Feb 20 22:29:39.921: INFO: stdout: "service/rm3 exposed\n"
Feb 20 22:29:39.942: INFO: Service rm3 in namespace kubectl-9252 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:29:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9252" for this suite. 02/20/23 22:29:41.985
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":135,"skipped":2634,"failed":0}
------------------------------
• [SLOW TEST] [8.554 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:29:33.454
    Feb 20 22:29:33.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:29:33.458
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:33.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:33.526
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 02/20/23 22:29:33.541
    Feb 20 22:29:33.541: INFO: namespace kubectl-9252
    Feb 20 22:29:33.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 create -f -'
    Feb 20 22:29:34.063: INFO: stderr: ""
    Feb 20 22:29:34.063: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/20/23 22:29:34.063
    Feb 20 22:29:35.077: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:29:35.077: INFO: Found 0 / 1
    Feb 20 22:29:36.075: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:29:36.075: INFO: Found 0 / 1
    Feb 20 22:29:37.075: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:29:37.075: INFO: Found 1 / 1
    Feb 20 22:29:37.075: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 20 22:29:37.086: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 20 22:29:37.086: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 20 22:29:37.086: INFO: wait on agnhost-primary startup in kubectl-9252 
    Feb 20 22:29:37.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 logs agnhost-primary-cqv4j agnhost-primary'
    Feb 20 22:29:37.300: INFO: stderr: ""
    Feb 20 22:29:37.300: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/20/23 22:29:37.3
    Feb 20 22:29:37.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 20 22:29:37.458: INFO: stderr: ""
    Feb 20 22:29:37.458: INFO: stdout: "service/rm2 exposed\n"
    Feb 20 22:29:37.487: INFO: Service rm2 in namespace kubectl-9252 found.
    STEP: exposing service 02/20/23 22:29:39.562
    Feb 20 22:29:39.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9252 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 20 22:29:39.921: INFO: stderr: ""
    Feb 20 22:29:39.921: INFO: stdout: "service/rm3 exposed\n"
    Feb 20 22:29:39.942: INFO: Service rm3 in namespace kubectl-9252 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:29:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9252" for this suite. 02/20/23 22:29:41.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:29:42.015
Feb 20 22:29:42.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:29:42.017
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:42.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:42.139
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-e145c7ad-40c5-458c-90be-3adfe3e74b17 02/20/23 22:29:42.152
STEP: Creating a pod to test consume secrets 02/20/23 22:29:42.168
Feb 20 22:29:42.245: INFO: Waiting up to 5m0s for pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6" in namespace "secrets-8708" to be "Succeeded or Failed"
Feb 20 22:29:42.255: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440125ms
Feb 20 22:29:44.271: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025982811s
Feb 20 22:29:46.267: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022033608s
STEP: Saw pod success 02/20/23 22:29:46.267
Feb 20 22:29:46.267: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6" satisfied condition "Succeeded or Failed"
Feb 20 22:29:46.279: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-25786661-3afb-4641-8856-85395aaf06a6 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:29:46.301
Feb 20 22:29:46.327: INFO: Waiting for pod pod-secrets-25786661-3afb-4641-8856-85395aaf06a6 to disappear
Feb 20 22:29:46.338: INFO: Pod pod-secrets-25786661-3afb-4641-8856-85395aaf06a6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:29:46.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8708" for this suite. 02/20/23 22:29:46.355
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":136,"skipped":2660,"failed":0}
------------------------------
• [4.368 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:29:42.015
    Feb 20 22:29:42.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:29:42.017
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:42.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:42.139
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-e145c7ad-40c5-458c-90be-3adfe3e74b17 02/20/23 22:29:42.152
    STEP: Creating a pod to test consume secrets 02/20/23 22:29:42.168
    Feb 20 22:29:42.245: INFO: Waiting up to 5m0s for pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6" in namespace "secrets-8708" to be "Succeeded or Failed"
    Feb 20 22:29:42.255: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440125ms
    Feb 20 22:29:44.271: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025982811s
    Feb 20 22:29:46.267: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022033608s
    STEP: Saw pod success 02/20/23 22:29:46.267
    Feb 20 22:29:46.267: INFO: Pod "pod-secrets-25786661-3afb-4641-8856-85395aaf06a6" satisfied condition "Succeeded or Failed"
    Feb 20 22:29:46.279: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-25786661-3afb-4641-8856-85395aaf06a6 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:29:46.301
    Feb 20 22:29:46.327: INFO: Waiting for pod pod-secrets-25786661-3afb-4641-8856-85395aaf06a6 to disappear
    Feb 20 22:29:46.338: INFO: Pod pod-secrets-25786661-3afb-4641-8856-85395aaf06a6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:29:46.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8708" for this suite. 02/20/23 22:29:46.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:29:46.385
Feb 20 22:29:46.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 22:29:46.388
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:46.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:46.45
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Feb 20 22:29:46.547: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:29:46.559
Feb 20 22:29:46.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:29:46.585: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:29:47.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:29:47.622: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:29:48.625: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 22:29:48.625: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:29:49.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 22:29:49.620: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/20/23 22:29:49.674
STEP: Check that daemon pods images are updated. 02/20/23 22:29:49.708
Feb 20 22:29:49.723: INFO: Wrong image for pod: daemon-set-2ftfn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:49.723: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:49.723: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:50.761: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:50.761: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:51.760: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:51.760: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:52.760: INFO: Pod daemon-set-dw52x is not available
Feb 20 22:29:52.760: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:52.760: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:53.761: INFO: Pod daemon-set-dw52x is not available
Feb 20 22:29:53.761: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:53.761: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:54.759: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:55.768: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:56.761: INFO: Pod daemon-set-7j64z is not available
Feb 20 22:29:56.762: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:57.761: INFO: Pod daemon-set-7j64z is not available
Feb 20 22:29:57.761: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 20 22:29:59.761: INFO: Pod daemon-set-f5x5k is not available
STEP: Check that daemon pods are still running on every node of the cluster. 02/20/23 22:29:59.78
Feb 20 22:29:59.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 22:29:59.819: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
Feb 20 22:30:00.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 22:30:00.848: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
Feb 20 22:30:01.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 22:30:01.854: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/20/23 22:30:02.037
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5350, will wait for the garbage collector to delete the pods 02/20/23 22:30:02.037
Feb 20 22:30:02.114: INFO: Deleting DaemonSet.extensions daemon-set took: 13.103323ms
Feb 20 22:30:02.415: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.061457ms
Feb 20 22:30:05.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:30:05.628: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 20 22:30:05.636: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"92717"},"items":null}

Feb 20 22:30:05.646: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"92717"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:30:05.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5350" for this suite. 02/20/23 22:30:05.714
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":137,"skipped":2668,"failed":0}
------------------------------
• [SLOW TEST] [19.364 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:29:46.385
    Feb 20 22:29:46.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 22:29:46.388
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:29:46.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:29:46.45
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Feb 20 22:29:46.547: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:29:46.559
    Feb 20 22:29:46.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:29:46.585: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:29:47.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:29:47.622: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:29:48.625: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 22:29:48.625: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:29:49.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 22:29:49.620: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/20/23 22:29:49.674
    STEP: Check that daemon pods images are updated. 02/20/23 22:29:49.708
    Feb 20 22:29:49.723: INFO: Wrong image for pod: daemon-set-2ftfn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:49.723: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:49.723: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:50.761: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:50.761: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:51.760: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:51.760: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:52.760: INFO: Pod daemon-set-dw52x is not available
    Feb 20 22:29:52.760: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:52.760: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:53.761: INFO: Pod daemon-set-dw52x is not available
    Feb 20 22:29:53.761: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:53.761: INFO: Wrong image for pod: daemon-set-pc4c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:54.759: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:55.768: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:56.761: INFO: Pod daemon-set-7j64z is not available
    Feb 20 22:29:56.762: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:57.761: INFO: Pod daemon-set-7j64z is not available
    Feb 20 22:29:57.761: INFO: Wrong image for pod: daemon-set-hqgjt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 20 22:29:59.761: INFO: Pod daemon-set-f5x5k is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 02/20/23 22:29:59.78
    Feb 20 22:29:59.819: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 22:29:59.819: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
    Feb 20 22:30:00.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 22:30:00.848: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
    Feb 20 22:30:01.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 22:30:01.854: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/20/23 22:30:02.037
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5350, will wait for the garbage collector to delete the pods 02/20/23 22:30:02.037
    Feb 20 22:30:02.114: INFO: Deleting DaemonSet.extensions daemon-set took: 13.103323ms
    Feb 20 22:30:02.415: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.061457ms
    Feb 20 22:30:05.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:30:05.628: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 20 22:30:05.636: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"92717"},"items":null}

    Feb 20 22:30:05.646: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"92717"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:30:05.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5350" for this suite. 02/20/23 22:30:05.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:30:05.751
Feb 20 22:30:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubelet-test 02/20/23 22:30:05.752
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:05.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:05.85
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 20 22:30:09.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6014" for this suite. 02/20/23 22:30:09.997
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":138,"skipped":2693,"failed":0}
------------------------------
• [4.275 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:30:05.751
    Feb 20 22:30:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubelet-test 02/20/23 22:30:05.752
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:05.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:05.85
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 20 22:30:09.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6014" for this suite. 02/20/23 22:30:09.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:30:10.027
Feb 20 22:30:10.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:30:10.029
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:10.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:10.085
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:30:10.27
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:30:10.821
STEP: Deploying the webhook pod 02/20/23 22:30:10.896
STEP: Wait for the deployment to be ready 02/20/23 22:30:10.924
Feb 20 22:30:10.952: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 22:30:12.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 30, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 30, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 30, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:30:14.993
STEP: Verifying the service has paired with the endpoint 02/20/23 22:30:15.031
Feb 20 22:30:16.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/20/23 22:30:16.043
STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:16.044
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/20/23 22:30:16.094
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/20/23 22:30:17.121
STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:17.121
STEP: Having no error when timeout is longer than webhook latency 02/20/23 22:30:18.198
STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:18.198
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/20/23 22:30:23.291
STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:23.291
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:30:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2433" for this suite. 02/20/23 22:30:28.389
STEP: Destroying namespace "webhook-2433-markers" for this suite. 02/20/23 22:30:28.411
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":139,"skipped":2698,"failed":0}
------------------------------
• [SLOW TEST] [18.510 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:30:10.027
    Feb 20 22:30:10.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:30:10.029
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:10.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:10.085
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:30:10.27
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:30:10.821
    STEP: Deploying the webhook pod 02/20/23 22:30:10.896
    STEP: Wait for the deployment to be ready 02/20/23 22:30:10.924
    Feb 20 22:30:10.952: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 22:30:12.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 30, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 30, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 30, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:30:14.993
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:30:15.031
    Feb 20 22:30:16.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/20/23 22:30:16.043
    STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:16.044
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/20/23 22:30:16.094
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/20/23 22:30:17.121
    STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:17.121
    STEP: Having no error when timeout is longer than webhook latency 02/20/23 22:30:18.198
    STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:18.198
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/20/23 22:30:23.291
    STEP: Registering slow webhook via the AdmissionRegistration API 02/20/23 22:30:23.291
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:30:28.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2433" for this suite. 02/20/23 22:30:28.389
    STEP: Destroying namespace "webhook-2433-markers" for this suite. 02/20/23 22:30:28.411
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:30:28.549
Feb 20 22:30:28.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 22:30:28.55
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:28.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:28.646
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-8342/configmap-test-68b2aa4b-68d5-4fa6-879a-9deb694ff5d3 02/20/23 22:30:28.655
STEP: Creating a pod to test consume configMaps 02/20/23 22:30:28.669
Feb 20 22:30:28.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00" in namespace "configmap-8342" to be "Succeeded or Failed"
Feb 20 22:30:28.752: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Pending", Reason="", readiness=false. Elapsed: 11.181633ms
Feb 20 22:30:30.773: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032649114s
Feb 20 22:30:32.765: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024328287s
Feb 20 22:30:34.768: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027727752s
STEP: Saw pod success 02/20/23 22:30:34.769
Feb 20 22:30:34.769: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00" satisfied condition "Succeeded or Failed"
Feb 20 22:30:34.783: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00 container env-test: <nil>
STEP: delete the pod 02/20/23 22:30:34.805
Feb 20 22:30:34.852: INFO: Waiting for pod pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00 to disappear
Feb 20 22:30:34.864: INFO: Pod pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 22:30:34.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8342" for this suite. 02/20/23 22:30:34.884
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":140,"skipped":2763,"failed":0}
------------------------------
• [SLOW TEST] [6.357 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:30:28.549
    Feb 20 22:30:28.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 22:30:28.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:28.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:28.646
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-8342/configmap-test-68b2aa4b-68d5-4fa6-879a-9deb694ff5d3 02/20/23 22:30:28.655
    STEP: Creating a pod to test consume configMaps 02/20/23 22:30:28.669
    Feb 20 22:30:28.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00" in namespace "configmap-8342" to be "Succeeded or Failed"
    Feb 20 22:30:28.752: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Pending", Reason="", readiness=false. Elapsed: 11.181633ms
    Feb 20 22:30:30.773: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032649114s
    Feb 20 22:30:32.765: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024328287s
    Feb 20 22:30:34.768: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027727752s
    STEP: Saw pod success 02/20/23 22:30:34.769
    Feb 20 22:30:34.769: INFO: Pod "pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00" satisfied condition "Succeeded or Failed"
    Feb 20 22:30:34.783: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00 container env-test: <nil>
    STEP: delete the pod 02/20/23 22:30:34.805
    Feb 20 22:30:34.852: INFO: Waiting for pod pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00 to disappear
    Feb 20 22:30:34.864: INFO: Pod pod-configmaps-944ee36b-54b4-44b4-bcc5-12ad967bcd00 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 22:30:34.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8342" for this suite. 02/20/23 22:30:34.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:30:34.911
Feb 20 22:30:34.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:30:34.914
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:34.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:34.997
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:30:35.008
Feb 20 22:30:35.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6" in namespace "downward-api-7063" to be "Succeeded or Failed"
Feb 20 22:30:35.144: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.882592ms
Feb 20 22:30:37.156: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02359004s
Feb 20 22:30:39.156: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023429145s
STEP: Saw pod success 02/20/23 22:30:39.156
Feb 20 22:30:39.156: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6" satisfied condition "Succeeded or Failed"
Feb 20 22:30:39.167: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6 container client-container: <nil>
STEP: delete the pod 02/20/23 22:30:39.188
Feb 20 22:30:39.219: INFO: Waiting for pod downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6 to disappear
Feb 20 22:30:39.230: INFO: Pod downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:30:39.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7063" for this suite. 02/20/23 22:30:39.248
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":141,"skipped":2780,"failed":0}
------------------------------
• [4.391 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:30:34.911
    Feb 20 22:30:34.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:30:34.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:34.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:34.997
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:30:35.008
    Feb 20 22:30:35.133: INFO: Waiting up to 5m0s for pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6" in namespace "downward-api-7063" to be "Succeeded or Failed"
    Feb 20 22:30:35.144: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.882592ms
    Feb 20 22:30:37.156: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02359004s
    Feb 20 22:30:39.156: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023429145s
    STEP: Saw pod success 02/20/23 22:30:39.156
    Feb 20 22:30:39.156: INFO: Pod "downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6" satisfied condition "Succeeded or Failed"
    Feb 20 22:30:39.167: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:30:39.188
    Feb 20 22:30:39.219: INFO: Waiting for pod downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6 to disappear
    Feb 20 22:30:39.230: INFO: Pod downwardapi-volume-962e208b-c94c-4520-8daf-00571c3421d6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:30:39.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7063" for this suite. 02/20/23 22:30:39.248
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:30:39.307
Feb 20 22:30:39.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:30:39.31
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:39.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:39.38
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/20/23 22:30:39.391
Feb 20 22:30:39.448: INFO: Waiting up to 5m0s for pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857" in namespace "emptydir-8386" to be "Succeeded or Failed"
Feb 20 22:30:39.460: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857": Phase="Pending", Reason="", readiness=false. Elapsed: 11.631615ms
Feb 20 22:30:41.506: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057390475s
Feb 20 22:30:43.473: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025309702s
STEP: Saw pod success 02/20/23 22:30:43.474
Feb 20 22:30:43.474: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857" satisfied condition "Succeeded or Failed"
Feb 20 22:30:43.484: INFO: Trying to get logs from node 10.8.38.66 pod pod-eccd1b7c-bb96-4fdd-b784-2bf423254857 container test-container: <nil>
STEP: delete the pod 02/20/23 22:30:43.506
Feb 20 22:30:43.535: INFO: Waiting for pod pod-eccd1b7c-bb96-4fdd-b784-2bf423254857 to disappear
Feb 20 22:30:43.547: INFO: Pod pod-eccd1b7c-bb96-4fdd-b784-2bf423254857 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:30:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8386" for this suite. 02/20/23 22:30:43.564
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":142,"skipped":2780,"failed":0}
------------------------------
• [4.298 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:30:39.307
    Feb 20 22:30:39.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:30:39.31
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:39.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:39.38
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/20/23 22:30:39.391
    Feb 20 22:30:39.448: INFO: Waiting up to 5m0s for pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857" in namespace "emptydir-8386" to be "Succeeded or Failed"
    Feb 20 22:30:39.460: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857": Phase="Pending", Reason="", readiness=false. Elapsed: 11.631615ms
    Feb 20 22:30:41.506: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057390475s
    Feb 20 22:30:43.473: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025309702s
    STEP: Saw pod success 02/20/23 22:30:43.474
    Feb 20 22:30:43.474: INFO: Pod "pod-eccd1b7c-bb96-4fdd-b784-2bf423254857" satisfied condition "Succeeded or Failed"
    Feb 20 22:30:43.484: INFO: Trying to get logs from node 10.8.38.66 pod pod-eccd1b7c-bb96-4fdd-b784-2bf423254857 container test-container: <nil>
    STEP: delete the pod 02/20/23 22:30:43.506
    Feb 20 22:30:43.535: INFO: Waiting for pod pod-eccd1b7c-bb96-4fdd-b784-2bf423254857 to disappear
    Feb 20 22:30:43.547: INFO: Pod pod-eccd1b7c-bb96-4fdd-b784-2bf423254857 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:30:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8386" for this suite. 02/20/23 22:30:43.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:30:43.608
Feb 20 22:30:43.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:30:43.611
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:43.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:43.696
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 02/20/23 22:30:43.707
Feb 20 22:30:43.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 create -f -'
Feb 20 22:30:44.242: INFO: stderr: ""
Feb 20 22:30:44.242: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:30:44.242
Feb 20 22:30:44.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:30:44.336: INFO: stderr: ""
Feb 20 22:30:44.336: INFO: stdout: "update-demo-nautilus-2b6g8 "
STEP: Replicas for name=update-demo: expected=2 actual=1 02/20/23 22:30:44.336
Feb 20 22:30:49.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:30:49.493: INFO: stderr: ""
Feb 20 22:30:49.493: INFO: stdout: "update-demo-nautilus-2b6g8 update-demo-nautilus-f869z "
Feb 20 22:30:49.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-2b6g8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:30:49.588: INFO: stderr: ""
Feb 20 22:30:49.588: INFO: stdout: "true"
Feb 20 22:30:49.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-2b6g8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:30:49.738: INFO: stderr: ""
Feb 20 22:30:49.738: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:30:49.738: INFO: validating pod update-demo-nautilus-2b6g8
Feb 20 22:30:49.757: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:30:49.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:30:49.757: INFO: update-demo-nautilus-2b6g8 is verified up and running
Feb 20 22:30:49.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:30:49.924: INFO: stderr: ""
Feb 20 22:30:49.924: INFO: stdout: "true"
Feb 20 22:30:49.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:30:50.069: INFO: stderr: ""
Feb 20 22:30:50.069: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:30:50.069: INFO: validating pod update-demo-nautilus-f869z
Feb 20 22:30:50.086: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:30:50.086: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:30:50.087: INFO: update-demo-nautilus-f869z is verified up and running
STEP: scaling down the replication controller 02/20/23 22:30:50.087
Feb 20 22:30:50.097: INFO: scanned /root for discovery docs: <nil>
Feb 20 22:30:50.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 20 22:30:51.280: INFO: stderr: ""
Feb 20 22:30:51.280: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:30:51.28
Feb 20 22:30:51.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:30:51.407: INFO: stderr: ""
Feb 20 22:30:51.407: INFO: stdout: "update-demo-nautilus-2b6g8 update-demo-nautilus-f869z "
STEP: Replicas for name=update-demo: expected=1 actual=2 02/20/23 22:30:51.407
Feb 20 22:30:56.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:30:56.528: INFO: stderr: ""
Feb 20 22:30:56.528: INFO: stdout: "update-demo-nautilus-f869z "
Feb 20 22:30:56.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:30:56.649: INFO: stderr: ""
Feb 20 22:30:56.649: INFO: stdout: "true"
Feb 20 22:30:56.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:30:56.738: INFO: stderr: ""
Feb 20 22:30:56.738: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:30:56.738: INFO: validating pod update-demo-nautilus-f869z
Feb 20 22:30:56.753: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:30:56.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:30:56.754: INFO: update-demo-nautilus-f869z is verified up and running
STEP: scaling up the replication controller 02/20/23 22:30:56.754
Feb 20 22:30:56.770: INFO: scanned /root for discovery docs: <nil>
Feb 20 22:30:56.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 20 22:30:57.965: INFO: stderr: ""
Feb 20 22:30:57.965: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:30:57.965
Feb 20 22:30:57.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:30:58.083: INFO: stderr: ""
Feb 20 22:30:58.083: INFO: stdout: "update-demo-nautilus-f869z update-demo-nautilus-wzqvk "
Feb 20 22:30:58.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:30:58.212: INFO: stderr: ""
Feb 20 22:30:58.212: INFO: stdout: "true"
Feb 20 22:30:58.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:30:58.368: INFO: stderr: ""
Feb 20 22:30:58.368: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:30:58.368: INFO: validating pod update-demo-nautilus-f869z
Feb 20 22:30:58.388: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:30:58.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:30:58.388: INFO: update-demo-nautilus-f869z is verified up and running
Feb 20 22:30:58.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-wzqvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:30:58.501: INFO: stderr: ""
Feb 20 22:30:58.501: INFO: stdout: ""
Feb 20 22:30:58.501: INFO: update-demo-nautilus-wzqvk is created but not running
Feb 20 22:31:03.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 20 22:31:03.644: INFO: stderr: ""
Feb 20 22:31:03.644: INFO: stdout: "update-demo-nautilus-f869z update-demo-nautilus-wzqvk "
Feb 20 22:31:03.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:31:03.761: INFO: stderr: ""
Feb 20 22:31:03.761: INFO: stdout: "true"
Feb 20 22:31:03.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:31:03.880: INFO: stderr: ""
Feb 20 22:31:03.880: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:31:03.880: INFO: validating pod update-demo-nautilus-f869z
Feb 20 22:31:03.894: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:31:03.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:31:03.894: INFO: update-demo-nautilus-f869z is verified up and running
Feb 20 22:31:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-wzqvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 20 22:31:04.007: INFO: stderr: ""
Feb 20 22:31:04.007: INFO: stdout: "true"
Feb 20 22:31:04.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-wzqvk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 20 22:31:04.111: INFO: stderr: ""
Feb 20 22:31:04.111: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 20 22:31:04.111: INFO: validating pod update-demo-nautilus-wzqvk
Feb 20 22:31:04.128: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 22:31:04.128: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 22:31:04.128: INFO: update-demo-nautilus-wzqvk is verified up and running
STEP: using delete to clean up resources 02/20/23 22:31:04.128
Feb 20 22:31:04.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 delete --grace-period=0 --force -f -'
Feb 20 22:31:04.262: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 22:31:04.262: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 22:31:04.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get rc,svc -l name=update-demo --no-headers'
Feb 20 22:31:04.417: INFO: stderr: "No resources found in kubectl-7370 namespace.\n"
Feb 20 22:31:04.417: INFO: stdout: ""
Feb 20 22:31:04.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 22:31:04.567: INFO: stderr: ""
Feb 20 22:31:04.567: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:31:04.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7370" for this suite. 02/20/23 22:31:04.589
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":143,"skipped":2786,"failed":0}
------------------------------
• [SLOW TEST] [21.050 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:30:43.608
    Feb 20 22:30:43.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:30:43.611
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:30:43.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:30:43.696
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 02/20/23 22:30:43.707
    Feb 20 22:30:43.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 create -f -'
    Feb 20 22:30:44.242: INFO: stderr: ""
    Feb 20 22:30:44.242: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:30:44.242
    Feb 20 22:30:44.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:30:44.336: INFO: stderr: ""
    Feb 20 22:30:44.336: INFO: stdout: "update-demo-nautilus-2b6g8 "
    STEP: Replicas for name=update-demo: expected=2 actual=1 02/20/23 22:30:44.336
    Feb 20 22:30:49.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:30:49.493: INFO: stderr: ""
    Feb 20 22:30:49.493: INFO: stdout: "update-demo-nautilus-2b6g8 update-demo-nautilus-f869z "
    Feb 20 22:30:49.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-2b6g8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:30:49.588: INFO: stderr: ""
    Feb 20 22:30:49.588: INFO: stdout: "true"
    Feb 20 22:30:49.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-2b6g8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:30:49.738: INFO: stderr: ""
    Feb 20 22:30:49.738: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:30:49.738: INFO: validating pod update-demo-nautilus-2b6g8
    Feb 20 22:30:49.757: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:30:49.757: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:30:49.757: INFO: update-demo-nautilus-2b6g8 is verified up and running
    Feb 20 22:30:49.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:30:49.924: INFO: stderr: ""
    Feb 20 22:30:49.924: INFO: stdout: "true"
    Feb 20 22:30:49.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:30:50.069: INFO: stderr: ""
    Feb 20 22:30:50.069: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:30:50.069: INFO: validating pod update-demo-nautilus-f869z
    Feb 20 22:30:50.086: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:30:50.086: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:30:50.087: INFO: update-demo-nautilus-f869z is verified up and running
    STEP: scaling down the replication controller 02/20/23 22:30:50.087
    Feb 20 22:30:50.097: INFO: scanned /root for discovery docs: <nil>
    Feb 20 22:30:50.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 20 22:30:51.280: INFO: stderr: ""
    Feb 20 22:30:51.280: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:30:51.28
    Feb 20 22:30:51.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:30:51.407: INFO: stderr: ""
    Feb 20 22:30:51.407: INFO: stdout: "update-demo-nautilus-2b6g8 update-demo-nautilus-f869z "
    STEP: Replicas for name=update-demo: expected=1 actual=2 02/20/23 22:30:51.407
    Feb 20 22:30:56.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:30:56.528: INFO: stderr: ""
    Feb 20 22:30:56.528: INFO: stdout: "update-demo-nautilus-f869z "
    Feb 20 22:30:56.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:30:56.649: INFO: stderr: ""
    Feb 20 22:30:56.649: INFO: stdout: "true"
    Feb 20 22:30:56.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:30:56.738: INFO: stderr: ""
    Feb 20 22:30:56.738: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:30:56.738: INFO: validating pod update-demo-nautilus-f869z
    Feb 20 22:30:56.753: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:30:56.754: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:30:56.754: INFO: update-demo-nautilus-f869z is verified up and running
    STEP: scaling up the replication controller 02/20/23 22:30:56.754
    Feb 20 22:30:56.770: INFO: scanned /root for discovery docs: <nil>
    Feb 20 22:30:56.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 20 22:30:57.965: INFO: stderr: ""
    Feb 20 22:30:57.965: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/20/23 22:30:57.965
    Feb 20 22:30:57.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:30:58.083: INFO: stderr: ""
    Feb 20 22:30:58.083: INFO: stdout: "update-demo-nautilus-f869z update-demo-nautilus-wzqvk "
    Feb 20 22:30:58.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:30:58.212: INFO: stderr: ""
    Feb 20 22:30:58.212: INFO: stdout: "true"
    Feb 20 22:30:58.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:30:58.368: INFO: stderr: ""
    Feb 20 22:30:58.368: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:30:58.368: INFO: validating pod update-demo-nautilus-f869z
    Feb 20 22:30:58.388: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:30:58.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:30:58.388: INFO: update-demo-nautilus-f869z is verified up and running
    Feb 20 22:30:58.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-wzqvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:30:58.501: INFO: stderr: ""
    Feb 20 22:30:58.501: INFO: stdout: ""
    Feb 20 22:30:58.501: INFO: update-demo-nautilus-wzqvk is created but not running
    Feb 20 22:31:03.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 20 22:31:03.644: INFO: stderr: ""
    Feb 20 22:31:03.644: INFO: stdout: "update-demo-nautilus-f869z update-demo-nautilus-wzqvk "
    Feb 20 22:31:03.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:31:03.761: INFO: stderr: ""
    Feb 20 22:31:03.761: INFO: stdout: "true"
    Feb 20 22:31:03.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-f869z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:31:03.880: INFO: stderr: ""
    Feb 20 22:31:03.880: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:31:03.880: INFO: validating pod update-demo-nautilus-f869z
    Feb 20 22:31:03.894: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:31:03.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:31:03.894: INFO: update-demo-nautilus-f869z is verified up and running
    Feb 20 22:31:03.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-wzqvk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 20 22:31:04.007: INFO: stderr: ""
    Feb 20 22:31:04.007: INFO: stdout: "true"
    Feb 20 22:31:04.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods update-demo-nautilus-wzqvk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 20 22:31:04.111: INFO: stderr: ""
    Feb 20 22:31:04.111: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 20 22:31:04.111: INFO: validating pod update-demo-nautilus-wzqvk
    Feb 20 22:31:04.128: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 20 22:31:04.128: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 20 22:31:04.128: INFO: update-demo-nautilus-wzqvk is verified up and running
    STEP: using delete to clean up resources 02/20/23 22:31:04.128
    Feb 20 22:31:04.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 delete --grace-period=0 --force -f -'
    Feb 20 22:31:04.262: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 22:31:04.262: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 20 22:31:04.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get rc,svc -l name=update-demo --no-headers'
    Feb 20 22:31:04.417: INFO: stderr: "No resources found in kubectl-7370 namespace.\n"
    Feb 20 22:31:04.417: INFO: stdout: ""
    Feb 20 22:31:04.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-7370 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 20 22:31:04.567: INFO: stderr: ""
    Feb 20 22:31:04.567: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:31:04.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7370" for this suite. 02/20/23 22:31:04.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:31:04.668
Feb 20 22:31:04.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 22:31:04.67
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:31:04.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:31:04.814
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6868 02/20/23 22:31:04.829
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 02/20/23 22:31:04.857
Feb 20 22:31:04.886: INFO: Found 0 stateful pods, waiting for 3
Feb 20 22:31:14.903: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:31:14.903: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:31:14.903: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:31:14.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 22:31:15.213: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 22:31:15.213: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 22:31:15.213: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/20/23 22:31:25.285
Feb 20 22:31:25.319: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/20/23 22:31:25.319
STEP: Updating Pods in reverse ordinal order 02/20/23 22:31:35.368
Feb 20 22:31:35.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 22:31:35.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 22:31:35.711: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 22:31:35.711: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 22:31:45.785: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
Feb 20 22:31:45.785: INFO: Waiting for Pod statefulset-6868/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 20 22:31:45.785: INFO: Waiting for Pod statefulset-6868/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 20 22:31:55.828: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
Feb 20 22:31:55.828: INFO: Waiting for Pod statefulset-6868/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 20 22:32:05.811: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
STEP: Rolling back to a previous revision 02/20/23 22:32:15.812
Feb 20 22:32:15.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 22:32:16.163: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 22:32:16.163: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 22:32:16.163: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 22:32:26.247: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/20/23 22:32:36.295
Feb 20 22:32:36.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 22:32:36.554: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 22:32:36.554: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 22:32:36.554: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 22:32:46.652: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 22:32:56.677: INFO: Deleting all statefulset in ns statefulset-6868
Feb 20 22:32:56.692: INFO: Scaling statefulset ss2 to 0
Feb 20 22:33:06.738: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 22:33:06.746: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 22:33:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6868" for this suite. 02/20/23 22:33:06.792
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":144,"skipped":2833,"failed":0}
------------------------------
• [SLOW TEST] [122.146 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:31:04.668
    Feb 20 22:31:04.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 22:31:04.67
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:31:04.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:31:04.814
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6868 02/20/23 22:31:04.829
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 02/20/23 22:31:04.857
    Feb 20 22:31:04.886: INFO: Found 0 stateful pods, waiting for 3
    Feb 20 22:31:14.903: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 22:31:14.903: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 22:31:14.903: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 22:31:14.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 22:31:15.213: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 22:31:15.213: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 22:31:15.213: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/20/23 22:31:25.285
    Feb 20 22:31:25.319: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/20/23 22:31:25.319
    STEP: Updating Pods in reverse ordinal order 02/20/23 22:31:35.368
    Feb 20 22:31:35.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 22:31:35.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 22:31:35.711: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 22:31:35.711: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 22:31:45.785: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
    Feb 20 22:31:45.785: INFO: Waiting for Pod statefulset-6868/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 20 22:31:45.785: INFO: Waiting for Pod statefulset-6868/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 20 22:31:55.828: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
    Feb 20 22:31:55.828: INFO: Waiting for Pod statefulset-6868/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 20 22:32:05.811: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
    STEP: Rolling back to a previous revision 02/20/23 22:32:15.812
    Feb 20 22:32:15.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 22:32:16.163: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 22:32:16.163: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 22:32:16.163: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 22:32:26.247: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/20/23 22:32:36.295
    Feb 20 22:32:36.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-6868 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 22:32:36.554: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 22:32:36.554: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 22:32:36.554: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 22:32:46.652: INFO: Waiting for StatefulSet statefulset-6868/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 22:32:56.677: INFO: Deleting all statefulset in ns statefulset-6868
    Feb 20 22:32:56.692: INFO: Scaling statefulset ss2 to 0
    Feb 20 22:33:06.738: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 22:33:06.746: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 22:33:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6868" for this suite. 02/20/23 22:33:06.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:33:06.829
Feb 20 22:33:06.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename aggregator 02/20/23 22:33:06.831
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:06.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:06.891
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 20 22:33:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/20/23 22:33:06.916
Feb 20 22:33:07.629: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 20 22:33:09.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:11.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:13.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:15.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:17.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:19.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:21.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:23.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:25.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:27.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:29.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:31.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:33.943: INFO: Waited 162.747097ms for the sample-apiserver to be ready to handle requests.
I0220 22:33:35.039548      20 request.go:682] Waited for 1.011531212s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/monitoring.coreos.com/v1alpha1
STEP: Read Status for v1alpha1.wardle.example.com 02/20/23 22:33:35.431
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/20/23 22:33:35.451
STEP: List APIServices 02/20/23 22:33:35.501
Feb 20 22:33:35.575: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Feb 20 22:33:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-505" for this suite. 02/20/23 22:33:36.412
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":145,"skipped":2907,"failed":0}
------------------------------
• [SLOW TEST] [29.641 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:33:06.829
    Feb 20 22:33:06.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename aggregator 02/20/23 22:33:06.831
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:06.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:06.891
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 20 22:33:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/20/23 22:33:06.916
    Feb 20 22:33:07.629: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Feb 20 22:33:09.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:11.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:13.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:15.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:17.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:19.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:21.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:23.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:25.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:27.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:29.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:31.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:33.943: INFO: Waited 162.747097ms for the sample-apiserver to be ready to handle requests.
    I0220 22:33:35.039548      20 request.go:682] Waited for 1.011531212s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/monitoring.coreos.com/v1alpha1
    STEP: Read Status for v1alpha1.wardle.example.com 02/20/23 22:33:35.431
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/20/23 22:33:35.451
    STEP: List APIServices 02/20/23 22:33:35.501
    Feb 20 22:33:35.575: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Feb 20 22:33:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-505" for this suite. 02/20/23 22:33:36.412
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:33:36.471
Feb 20 22:33:36.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 22:33:36.475
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:36.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:36.542
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/20/23 22:33:36.602
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5741;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5741;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +notcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_tcp@PTR;sleep 1; done
 02/20/23 22:33:36.667
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5741;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5741;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +notcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_tcp@PTR;sleep 1; done
 02/20/23 22:33:36.667
STEP: creating a pod to probe DNS 02/20/23 22:33:36.667
STEP: submitting the pod to kubernetes 02/20/23 22:33:36.667
Feb 20 22:33:36.717: INFO: Waiting up to 15m0s for pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14" in namespace "dns-5741" to be "running"
Feb 20 22:33:36.747: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 30.439472ms
Feb 20 22:33:38.760: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043716587s
Feb 20 22:33:40.760: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043502954s
Feb 20 22:33:42.761: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044050748s
Feb 20 22:33:44.761: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044403412s
Feb 20 22:33:46.762: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045346998s
Feb 20 22:33:48.762: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Running", Reason="", readiness=true. Elapsed: 12.045256015s
Feb 20 22:33:48.762: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14" satisfied condition "running"
STEP: retrieving the pod 02/20/23 22:33:48.762
STEP: looking for the results for each expected name from probers 02/20/23 22:33:48.775
Feb 20 22:33:48.794: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.832: INFO: Unable to read wheezy_udp@dns-test-service.dns-5741 from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.843: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5741 from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.868: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.881: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.894: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.974: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:48.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-5741 from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:49.025: INFO: Unable to read jessie_tcp@dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:49.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:49.051: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
Feb 20 22:33:49.106: INFO: Lookups using dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5741 wheezy_tcp@dns-test-service.dns-5741 wheezy_udp@dns-test-service.dns-5741.svc wheezy_tcp@dns-test-service.dns-5741.svc wheezy_udp@_http._tcp.dns-test-service.dns-5741.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5741.svc jessie_tcp@dns-test-service jessie_tcp@dns-test-service.dns-5741 jessie_tcp@dns-test-service.dns-5741.svc jessie_udp@_http._tcp.dns-test-service.dns-5741.svc jessie_tcp@_http._tcp.dns-test-service.dns-5741.svc]

Feb 20 22:33:54.428: INFO: DNS probes using dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14 succeeded

STEP: deleting the pod 02/20/23 22:33:54.428
STEP: deleting the test service 02/20/23 22:33:54.467
STEP: deleting the test headless service 02/20/23 22:33:54.515
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 22:33:54.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5741" for this suite. 02/20/23 22:33:54.59
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":146,"skipped":2908,"failed":0}
------------------------------
• [SLOW TEST] [18.147 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:33:36.471
    Feb 20 22:33:36.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 22:33:36.475
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:36.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:36.542
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/20/23 22:33:36.602
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5741;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5741;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +notcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_tcp@PTR;sleep 1; done
     02/20/23 22:33:36.667
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5741;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5741;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5741.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5741.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5741.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5741.svc;check="$$(dig +notcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.55.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.55.67_tcp@PTR;sleep 1; done
     02/20/23 22:33:36.667
    STEP: creating a pod to probe DNS 02/20/23 22:33:36.667
    STEP: submitting the pod to kubernetes 02/20/23 22:33:36.667
    Feb 20 22:33:36.717: INFO: Waiting up to 15m0s for pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14" in namespace "dns-5741" to be "running"
    Feb 20 22:33:36.747: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 30.439472ms
    Feb 20 22:33:38.760: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043716587s
    Feb 20 22:33:40.760: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043502954s
    Feb 20 22:33:42.761: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044050748s
    Feb 20 22:33:44.761: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044403412s
    Feb 20 22:33:46.762: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045346998s
    Feb 20 22:33:48.762: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14": Phase="Running", Reason="", readiness=true. Elapsed: 12.045256015s
    Feb 20 22:33:48.762: INFO: Pod "dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 22:33:48.762
    STEP: looking for the results for each expected name from probers 02/20/23 22:33:48.775
    Feb 20 22:33:48.794: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.815: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.832: INFO: Unable to read wheezy_udp@dns-test-service.dns-5741 from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.843: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5741 from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.868: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.881: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.894: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.974: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:48.999: INFO: Unable to read jessie_tcp@dns-test-service.dns-5741 from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:49.025: INFO: Unable to read jessie_tcp@dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:49.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:49.051: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5741.svc from pod dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14: the server could not find the requested resource (get pods dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14)
    Feb 20 22:33:49.106: INFO: Lookups using dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5741 wheezy_tcp@dns-test-service.dns-5741 wheezy_udp@dns-test-service.dns-5741.svc wheezy_tcp@dns-test-service.dns-5741.svc wheezy_udp@_http._tcp.dns-test-service.dns-5741.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5741.svc jessie_tcp@dns-test-service jessie_tcp@dns-test-service.dns-5741 jessie_tcp@dns-test-service.dns-5741.svc jessie_udp@_http._tcp.dns-test-service.dns-5741.svc jessie_tcp@_http._tcp.dns-test-service.dns-5741.svc]

    Feb 20 22:33:54.428: INFO: DNS probes using dns-5741/dns-test-885840ed-e688-4cc3-af1e-b67d91d83d14 succeeded

    STEP: deleting the pod 02/20/23 22:33:54.428
    STEP: deleting the test service 02/20/23 22:33:54.467
    STEP: deleting the test headless service 02/20/23 22:33:54.515
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 22:33:54.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5741" for this suite. 02/20/23 22:33:54.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:33:54.619
Feb 20 22:33:54.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 22:33:54.62
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:54.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:54.677
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 20 22:33:54.694: INFO: Creating deployment "test-recreate-deployment"
Feb 20 22:33:54.718: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 20 22:33:54.763: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 20 22:33:56.785: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 20 22:33:56.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 22:33:58.803: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 20 22:33:58.828: INFO: Updating deployment test-recreate-deployment
Feb 20 22:33:58.828: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 22:33:59.031: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5239  ebc4a2ee-60ee-4192-a540-8c81d0aa5c48 95366 2 2023-02-20 22:33:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ac0fc68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-20 22:33:58 +0000 UTC,LastTransitionTime:2023-02-20 22:33:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-20 22:33:59 +0000 UTC,LastTransitionTime:2023-02-20 22:33:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 22:33:59.045: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5239  6e01af4f-241d-4b30-aeaf-407a47e7f39b 95365 1 2023-02-20 22:33:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ebc4a2ee-60ee-4192-a540-8c81d0aa5c48 0xc00aea8170 0xc00aea8171}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc4a2ee-60ee-4192-a540-8c81d0aa5c48\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aea8208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:33:59.046: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 20 22:33:59.046: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5239  6d99019e-9de4-415b-abff-6d87bcc6e324 95354 2 2023-02-20 22:33:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ebc4a2ee-60ee-4192-a540-8c81d0aa5c48 0xc00aea8057 0xc00aea8058}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc4a2ee-60ee-4192-a540-8c81d0aa5c48\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aea8108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:33:59.058: INFO: Pod "test-recreate-deployment-9d58999df-5gtcj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-5gtcj test-recreate-deployment-9d58999df- deployment-5239  f85f3e1f-ebfe-4613-a688-cf904ac41f4b 95367 0 2023-02-20 22:33:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 6e01af4f-241d-4b30-aeaf-407a47e7f39b 0xc00aea86c7 0xc00aea86c8}] [] [{kube-controller-manager Update v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e01af4f-241d-4b30-aeaf-407a47e7f39b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rdgh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rdgh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c869m,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:,StartTime:2023-02-20 22:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 22:33:59.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5239" for this suite. 02/20/23 22:33:59.076
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":147,"skipped":2921,"failed":0}
------------------------------
• [4.489 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:33:54.619
    Feb 20 22:33:54.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 22:33:54.62
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:54.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:54.677
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 20 22:33:54.694: INFO: Creating deployment "test-recreate-deployment"
    Feb 20 22:33:54.718: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 20 22:33:54.763: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 20 22:33:56.785: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 20 22:33:56.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 33, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 20 22:33:58.803: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 20 22:33:58.828: INFO: Updating deployment test-recreate-deployment
    Feb 20 22:33:58.828: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 22:33:59.031: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5239  ebc4a2ee-60ee-4192-a540-8c81d0aa5c48 95366 2 2023-02-20 22:33:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00ac0fc68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-20 22:33:58 +0000 UTC,LastTransitionTime:2023-02-20 22:33:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-20 22:33:59 +0000 UTC,LastTransitionTime:2023-02-20 22:33:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 20 22:33:59.045: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5239  6e01af4f-241d-4b30-aeaf-407a47e7f39b 95365 1 2023-02-20 22:33:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ebc4a2ee-60ee-4192-a540-8c81d0aa5c48 0xc00aea8170 0xc00aea8171}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc4a2ee-60ee-4192-a540-8c81d0aa5c48\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aea8208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:33:59.046: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 20 22:33:59.046: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5239  6d99019e-9de4-415b-abff-6d87bcc6e324 95354 2 2023-02-20 22:33:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ebc4a2ee-60ee-4192-a540-8c81d0aa5c48 0xc00aea8057 0xc00aea8058}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebc4a2ee-60ee-4192-a540-8c81d0aa5c48\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00aea8108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:33:59.058: INFO: Pod "test-recreate-deployment-9d58999df-5gtcj" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-5gtcj test-recreate-deployment-9d58999df- deployment-5239  f85f3e1f-ebfe-4613-a688-cf904ac41f4b 95367 0 2023-02-20 22:33:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 6e01af4f-241d-4b30-aeaf-407a47e7f39b 0xc00aea86c7 0xc00aea86c8}] [] [{kube-controller-manager Update v1 2023-02-20 22:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e01af4f-241d-4b30-aeaf-407a47e7f39b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-20 22:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rdgh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rdgh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c869m,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:33:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:,StartTime:2023-02-20 22:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 22:33:59.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5239" for this suite. 02/20/23 22:33:59.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:33:59.116
Feb 20 22:33:59.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pod-network-test 02/20/23 22:33:59.118
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:59.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:59.218
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3559 02/20/23 22:33:59.227
STEP: creating a selector 02/20/23 22:33:59.227
STEP: Creating the service pods in kubernetes 02/20/23 22:33:59.227
Feb 20 22:33:59.227: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 20 22:33:59.408: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3559" to be "running and ready"
Feb 20 22:33:59.425: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.744501ms
Feb 20 22:33:59.425: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:34:01.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030618411s
Feb 20 22:34:01.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:03.442: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.033539357s
Feb 20 22:34:03.442: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:05.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.029730181s
Feb 20 22:34:05.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:07.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.029422588s
Feb 20 22:34:07.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:09.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02948244s
Feb 20 22:34:09.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:11.443: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.034580413s
Feb 20 22:34:11.443: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:13.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030101492s
Feb 20 22:34:13.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:15.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.03111465s
Feb 20 22:34:15.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:17.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.029012629s
Feb 20 22:34:17.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:19.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029778079s
Feb 20 22:34:19.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:21.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.030829846s
Feb 20 22:34:21.439: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 20 22:34:21.439: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 20 22:34:21.450: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3559" to be "running and ready"
Feb 20 22:34:21.461: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.533059ms
Feb 20 22:34:21.461: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 20 22:34:21.461: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 20 22:34:21.471: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3559" to be "running and ready"
Feb 20 22:34:21.482: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.020388ms
Feb 20 22:34:21.483: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 20 22:34:21.483: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/20/23 22:34:21.493
Feb 20 22:34:21.564: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3559" to be "running"
Feb 20 22:34:21.577: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083967ms
Feb 20 22:34:23.591: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026635321s
Feb 20 22:34:23.591: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 20 22:34:23.602: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3559" to be "running"
Feb 20 22:34:23.614: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.047007ms
Feb 20 22:34:23.614: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 20 22:34:23.626: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 20 22:34:23.626: INFO: Going to poll 172.30.181.206 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 20 22:34:23.638: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.181.206 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:34:23.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:34:23.639: INFO: ExecWithOptions: Clientset creation
Feb 20 22:34:23.639: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.181.206+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 22:34:24.850: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 20 22:34:24.850: INFO: Going to poll 172.30.144.249 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 20 22:34:24.862: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.144.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:34:24.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:34:24.864: INFO: ExecWithOptions: Clientset creation
Feb 20 22:34:24.864: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.144.249+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 22:34:26.111: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 20 22:34:26.111: INFO: Going to poll 172.30.31.187 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 20 22:34:26.122: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.31.187 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:34:26.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:34:26.124: INFO: ExecWithOptions: Clientset creation
Feb 20 22:34:26.124: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.31.187+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 22:34:27.309: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 20 22:34:27.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3559" for this suite. 02/20/23 22:34:27.327
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2972,"failed":0}
------------------------------
• [SLOW TEST] [28.235 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:33:59.116
    Feb 20 22:33:59.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pod-network-test 02/20/23 22:33:59.118
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:33:59.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:33:59.218
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3559 02/20/23 22:33:59.227
    STEP: creating a selector 02/20/23 22:33:59.227
    STEP: Creating the service pods in kubernetes 02/20/23 22:33:59.227
    Feb 20 22:33:59.227: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 20 22:33:59.408: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3559" to be "running and ready"
    Feb 20 22:33:59.425: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.744501ms
    Feb 20 22:33:59.425: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:34:01.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030618411s
    Feb 20 22:34:01.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:03.442: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.033539357s
    Feb 20 22:34:03.442: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:05.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.029730181s
    Feb 20 22:34:05.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:07.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.029422588s
    Feb 20 22:34:07.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:09.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02948244s
    Feb 20 22:34:09.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:11.443: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.034580413s
    Feb 20 22:34:11.443: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:13.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.030101492s
    Feb 20 22:34:13.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:15.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.03111465s
    Feb 20 22:34:15.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:17.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.029012629s
    Feb 20 22:34:17.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:19.438: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029778079s
    Feb 20 22:34:19.438: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:21.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.030829846s
    Feb 20 22:34:21.439: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 20 22:34:21.439: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 20 22:34:21.450: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3559" to be "running and ready"
    Feb 20 22:34:21.461: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.533059ms
    Feb 20 22:34:21.461: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 20 22:34:21.461: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 20 22:34:21.471: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3559" to be "running and ready"
    Feb 20 22:34:21.482: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.020388ms
    Feb 20 22:34:21.483: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 20 22:34:21.483: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/20/23 22:34:21.493
    Feb 20 22:34:21.564: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3559" to be "running"
    Feb 20 22:34:21.577: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083967ms
    Feb 20 22:34:23.591: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026635321s
    Feb 20 22:34:23.591: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 20 22:34:23.602: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3559" to be "running"
    Feb 20 22:34:23.614: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.047007ms
    Feb 20 22:34:23.614: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 20 22:34:23.626: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 20 22:34:23.626: INFO: Going to poll 172.30.181.206 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 20 22:34:23.638: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.181.206 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:34:23.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:34:23.639: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:34:23.639: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.181.206+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 22:34:24.850: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 20 22:34:24.850: INFO: Going to poll 172.30.144.249 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 20 22:34:24.862: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.144.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:34:24.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:34:24.864: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:34:24.864: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.144.249+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 22:34:26.111: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 20 22:34:26.111: INFO: Going to poll 172.30.31.187 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 20 22:34:26.122: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.31.187 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:34:26.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:34:26.124: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:34:26.124: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.31.187+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 22:34:27.309: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 20 22:34:27.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3559" for this suite. 02/20/23 22:34:27.327
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:34:27.352
Feb 20 22:34:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pod-network-test 02/20/23 22:34:27.354
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:34:27.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:34:27.418
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-4096 02/20/23 22:34:27.43
STEP: creating a selector 02/20/23 22:34:27.431
STEP: Creating the service pods in kubernetes 02/20/23 22:34:27.431
Feb 20 22:34:27.431: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 20 22:34:27.584: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4096" to be "running and ready"
Feb 20 22:34:27.596: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.920241ms
Feb 20 22:34:27.596: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:34:29.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023256581s
Feb 20 22:34:29.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:31.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.024912845s
Feb 20 22:34:31.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:33.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026431879s
Feb 20 22:34:33.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:35.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025496251s
Feb 20 22:34:35.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:37.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024215512s
Feb 20 22:34:37.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:39.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024747468s
Feb 20 22:34:39.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:41.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.026106059s
Feb 20 22:34:41.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:43.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026635561s
Feb 20 22:34:43.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:45.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026492743s
Feb 20 22:34:45.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:47.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02379579s
Feb 20 22:34:47.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:34:49.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025554129s
Feb 20 22:34:49.610: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 20 22:34:49.610: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 20 22:34:49.621: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4096" to be "running and ready"
Feb 20 22:34:49.633: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.608103ms
Feb 20 22:34:49.633: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 20 22:34:49.634: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 20 22:34:49.645: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4096" to be "running and ready"
Feb 20 22:34:49.656: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.749061ms
Feb 20 22:34:49.656: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 20 22:34:49.656: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/20/23 22:34:49.668
Feb 20 22:34:49.702: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4096" to be "running"
Feb 20 22:34:49.713: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.100556ms
Feb 20 22:34:51.726: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024516598s
Feb 20 22:34:51.727: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 20 22:34:51.739: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 20 22:34:51.739: INFO: Breadth first check of 172.30.181.247 on host 10.8.38.66...
Feb 20 22:34:51.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.181.246:9080/dial?request=hostname&protocol=http&host=172.30.181.247&port=8083&tries=1'] Namespace:pod-network-test-4096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:34:51.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:34:51.753: INFO: ExecWithOptions: Clientset creation
Feb 20 22:34:51.753: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.181.246%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.181.247%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 20 22:34:51.920: INFO: Waiting for responses: map[]
Feb 20 22:34:51.921: INFO: reached 172.30.181.247 after 0/1 tries
Feb 20 22:34:51.921: INFO: Breadth first check of 172.30.144.243 on host 10.8.38.69...
Feb 20 22:34:51.932: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.181.246:9080/dial?request=hostname&protocol=http&host=172.30.144.243&port=8083&tries=1'] Namespace:pod-network-test-4096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:34:51.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:34:51.934: INFO: ExecWithOptions: Clientset creation
Feb 20 22:34:51.934: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.181.246%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.144.243%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 20 22:34:52.109: INFO: Waiting for responses: map[]
Feb 20 22:34:52.109: INFO: reached 172.30.144.243 after 0/1 tries
Feb 20 22:34:52.109: INFO: Breadth first check of 172.30.31.189 on host 10.8.38.70...
Feb 20 22:34:52.120: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.181.246:9080/dial?request=hostname&protocol=http&host=172.30.31.189&port=8083&tries=1'] Namespace:pod-network-test-4096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:34:52.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:34:52.122: INFO: ExecWithOptions: Clientset creation
Feb 20 22:34:52.122: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.181.246%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.31.189%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 20 22:34:52.266: INFO: Waiting for responses: map[]
Feb 20 22:34:52.267: INFO: reached 172.30.31.189 after 0/1 tries
Feb 20 22:34:52.267: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 20 22:34:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4096" for this suite. 02/20/23 22:34:52.285
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":149,"skipped":2974,"failed":0}
------------------------------
• [SLOW TEST] [24.957 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:34:27.352
    Feb 20 22:34:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pod-network-test 02/20/23 22:34:27.354
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:34:27.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:34:27.418
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-4096 02/20/23 22:34:27.43
    STEP: creating a selector 02/20/23 22:34:27.431
    STEP: Creating the service pods in kubernetes 02/20/23 22:34:27.431
    Feb 20 22:34:27.431: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 20 22:34:27.584: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4096" to be "running and ready"
    Feb 20 22:34:27.596: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.920241ms
    Feb 20 22:34:27.596: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:34:29.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023256581s
    Feb 20 22:34:29.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:31.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.024912845s
    Feb 20 22:34:31.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:33.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026431879s
    Feb 20 22:34:33.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:35.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025496251s
    Feb 20 22:34:35.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:37.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024215512s
    Feb 20 22:34:37.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:39.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024747468s
    Feb 20 22:34:39.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:41.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.026106059s
    Feb 20 22:34:41.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:43.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.026635561s
    Feb 20 22:34:43.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:45.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026492743s
    Feb 20 22:34:45.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:47.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02379579s
    Feb 20 22:34:47.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:34:49.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025554129s
    Feb 20 22:34:49.610: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 20 22:34:49.610: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 20 22:34:49.621: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4096" to be "running and ready"
    Feb 20 22:34:49.633: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 11.608103ms
    Feb 20 22:34:49.633: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 20 22:34:49.634: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 20 22:34:49.645: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4096" to be "running and ready"
    Feb 20 22:34:49.656: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.749061ms
    Feb 20 22:34:49.656: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 20 22:34:49.656: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/20/23 22:34:49.668
    Feb 20 22:34:49.702: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4096" to be "running"
    Feb 20 22:34:49.713: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.100556ms
    Feb 20 22:34:51.726: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024516598s
    Feb 20 22:34:51.727: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 20 22:34:51.739: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 20 22:34:51.739: INFO: Breadth first check of 172.30.181.247 on host 10.8.38.66...
    Feb 20 22:34:51.751: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.181.246:9080/dial?request=hostname&protocol=http&host=172.30.181.247&port=8083&tries=1'] Namespace:pod-network-test-4096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:34:51.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:34:51.753: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:34:51.753: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.181.246%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.181.247%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 20 22:34:51.920: INFO: Waiting for responses: map[]
    Feb 20 22:34:51.921: INFO: reached 172.30.181.247 after 0/1 tries
    Feb 20 22:34:51.921: INFO: Breadth first check of 172.30.144.243 on host 10.8.38.69...
    Feb 20 22:34:51.932: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.181.246:9080/dial?request=hostname&protocol=http&host=172.30.144.243&port=8083&tries=1'] Namespace:pod-network-test-4096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:34:51.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:34:51.934: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:34:51.934: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.181.246%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.144.243%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 20 22:34:52.109: INFO: Waiting for responses: map[]
    Feb 20 22:34:52.109: INFO: reached 172.30.144.243 after 0/1 tries
    Feb 20 22:34:52.109: INFO: Breadth first check of 172.30.31.189 on host 10.8.38.70...
    Feb 20 22:34:52.120: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.181.246:9080/dial?request=hostname&protocol=http&host=172.30.31.189&port=8083&tries=1'] Namespace:pod-network-test-4096 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:34:52.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:34:52.122: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:34:52.122: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4096/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.181.246%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.31.189%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 20 22:34:52.266: INFO: Waiting for responses: map[]
    Feb 20 22:34:52.267: INFO: reached 172.30.31.189 after 0/1 tries
    Feb 20 22:34:52.267: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 20 22:34:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4096" for this suite. 02/20/23 22:34:52.285
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:34:52.31
Feb 20 22:34:52.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:34:52.314
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:34:52.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:34:52.372
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:34:52.385
Feb 20 22:34:52.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37" in namespace "projected-4503" to be "Succeeded or Failed"
Feb 20 22:34:52.456: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Pending", Reason="", readiness=false. Elapsed: 11.976844ms
Feb 20 22:34:54.492: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Running", Reason="", readiness=true. Elapsed: 2.047465107s
Feb 20 22:34:56.472: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Running", Reason="", readiness=false. Elapsed: 4.027507201s
Feb 20 22:34:58.468: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024207596s
STEP: Saw pod success 02/20/23 22:34:58.468
Feb 20 22:34:58.469: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37" satisfied condition "Succeeded or Failed"
Feb 20 22:34:58.479: INFO: Trying to get logs from node 10.8.38.70 pod downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37 container client-container: <nil>
STEP: delete the pod 02/20/23 22:34:58.523
Feb 20 22:34:58.562: INFO: Waiting for pod downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37 to disappear
Feb 20 22:34:58.572: INFO: Pod downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 22:34:58.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4503" for this suite. 02/20/23 22:34:58.591
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":150,"skipped":2978,"failed":0}
------------------------------
• [SLOW TEST] [6.305 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:34:52.31
    Feb 20 22:34:52.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:34:52.314
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:34:52.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:34:52.372
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:34:52.385
    Feb 20 22:34:52.444: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37" in namespace "projected-4503" to be "Succeeded or Failed"
    Feb 20 22:34:52.456: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Pending", Reason="", readiness=false. Elapsed: 11.976844ms
    Feb 20 22:34:54.492: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Running", Reason="", readiness=true. Elapsed: 2.047465107s
    Feb 20 22:34:56.472: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Running", Reason="", readiness=false. Elapsed: 4.027507201s
    Feb 20 22:34:58.468: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024207596s
    STEP: Saw pod success 02/20/23 22:34:58.468
    Feb 20 22:34:58.469: INFO: Pod "downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37" satisfied condition "Succeeded or Failed"
    Feb 20 22:34:58.479: INFO: Trying to get logs from node 10.8.38.70 pod downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:34:58.523
    Feb 20 22:34:58.562: INFO: Waiting for pod downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37 to disappear
    Feb 20 22:34:58.572: INFO: Pod downwardapi-volume-97227525-65e8-45c6-9797-d1474851eb37 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 22:34:58.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4503" for this suite. 02/20/23 22:34:58.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:34:58.618
Feb 20 22:34:58.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 22:34:58.62
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:34:58.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:34:58.672
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 20 22:34:58.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:35:02.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2132" for this suite. 02/20/23 22:35:02.044
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":151,"skipped":2995,"failed":0}
------------------------------
• [3.459 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:34:58.618
    Feb 20 22:34:58.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 22:34:58.62
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:34:58.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:34:58.672
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 20 22:34:58.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:35:02.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2132" for this suite. 02/20/23 22:35:02.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:02.082
Feb 20 22:35:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:35:02.083
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:02.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:02.163
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-8043 02/20/23 22:35:02.204
STEP: creating service affinity-clusterip in namespace services-8043 02/20/23 22:35:02.204
STEP: creating replication controller affinity-clusterip in namespace services-8043 02/20/23 22:35:02.249
I0220 22:35:02.285121      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8043, replica count: 3
I0220 22:35:05.336919      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:35:05.373: INFO: Creating new exec pod
Feb 20 22:35:05.412: INFO: Waiting up to 5m0s for pod "execpod-affinityqrvbf" in namespace "services-8043" to be "running"
Feb 20 22:35:05.422: INFO: Pod "execpod-affinityqrvbf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.996318ms
Feb 20 22:35:07.434: INFO: Pod "execpod-affinityqrvbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.022006203s
Feb 20 22:35:07.434: INFO: Pod "execpod-affinityqrvbf" satisfied condition "running"
Feb 20 22:35:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-8043 exec execpod-affinityqrvbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 20 22:35:08.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 20 22:35:08.958: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:35:08.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-8043 exec execpod-affinityqrvbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.61.31 80'
Feb 20 22:35:09.210: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.61.31 80\nConnection to 172.21.61.31 80 port [tcp/http] succeeded!\n"
Feb 20 22:35:09.210: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:35:09.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-8043 exec execpod-affinityqrvbf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.61.31:80/ ; done'
Feb 20 22:35:09.572: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n"
Feb 20 22:35:09.572: INFO: stdout: "\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn"
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
Feb 20 22:35:09.572: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8043, will wait for the garbage collector to delete the pods 02/20/23 22:35:09.605
Feb 20 22:35:09.688: INFO: Deleting ReplicationController affinity-clusterip took: 23.49497ms
Feb 20 22:35:09.788: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.419607ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:35:12.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8043" for this suite. 02/20/23 22:35:12.746
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":152,"skipped":3031,"failed":0}
------------------------------
• [SLOW TEST] [10.687 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:02.082
    Feb 20 22:35:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:35:02.083
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:02.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:02.163
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-8043 02/20/23 22:35:02.204
    STEP: creating service affinity-clusterip in namespace services-8043 02/20/23 22:35:02.204
    STEP: creating replication controller affinity-clusterip in namespace services-8043 02/20/23 22:35:02.249
    I0220 22:35:02.285121      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-8043, replica count: 3
    I0220 22:35:05.336919      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:35:05.373: INFO: Creating new exec pod
    Feb 20 22:35:05.412: INFO: Waiting up to 5m0s for pod "execpod-affinityqrvbf" in namespace "services-8043" to be "running"
    Feb 20 22:35:05.422: INFO: Pod "execpod-affinityqrvbf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.996318ms
    Feb 20 22:35:07.434: INFO: Pod "execpod-affinityqrvbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.022006203s
    Feb 20 22:35:07.434: INFO: Pod "execpod-affinityqrvbf" satisfied condition "running"
    Feb 20 22:35:08.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-8043 exec execpod-affinityqrvbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Feb 20 22:35:08.958: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 20 22:35:08.958: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:35:08.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-8043 exec execpod-affinityqrvbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.61.31 80'
    Feb 20 22:35:09.210: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.61.31 80\nConnection to 172.21.61.31 80 port [tcp/http] succeeded!\n"
    Feb 20 22:35:09.210: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:35:09.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-8043 exec execpod-affinityqrvbf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.61.31:80/ ; done'
    Feb 20 22:35:09.572: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.61.31:80/\n"
    Feb 20 22:35:09.572: INFO: stdout: "\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn\naffinity-clusterip-pxcnn"
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Received response from host: affinity-clusterip-pxcnn
    Feb 20 22:35:09.572: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-8043, will wait for the garbage collector to delete the pods 02/20/23 22:35:09.605
    Feb 20 22:35:09.688: INFO: Deleting ReplicationController affinity-clusterip took: 23.49497ms
    Feb 20 22:35:09.788: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.419607ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:35:12.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8043" for this suite. 02/20/23 22:35:12.746
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:12.77
Feb 20 22:35:12.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename endpointslice 02/20/23 22:35:12.773
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:12.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:12.839
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 20 22:35:14.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6679" for this suite. 02/20/23 22:35:15.007
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":153,"skipped":3034,"failed":0}
------------------------------
• [2.258 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:12.77
    Feb 20 22:35:12.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename endpointslice 02/20/23 22:35:12.773
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:12.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:12.839
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 20 22:35:14.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6679" for this suite. 02/20/23 22:35:15.007
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:15.029
Feb 20 22:35:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:35:15.03
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:15.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:15.088
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 02/20/23 22:35:15.099
STEP: Counting existing ResourceQuota 02/20/23 22:35:21.126
STEP: Creating a ResourceQuota 02/20/23 22:35:26.135
STEP: Ensuring resource quota status is calculated 02/20/23 22:35:26.145
STEP: Creating a Secret 02/20/23 22:35:28.155
STEP: Ensuring resource quota status captures secret creation 02/20/23 22:35:28.177
STEP: Deleting a secret 02/20/23 22:35:30.192
STEP: Ensuring resource quota status released usage 02/20/23 22:35:30.21
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:35:32.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5659" for this suite. 02/20/23 22:35:32.237
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":154,"skipped":3035,"failed":0}
------------------------------
• [SLOW TEST] [17.245 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:15.029
    Feb 20 22:35:15.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:35:15.03
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:15.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:15.088
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 02/20/23 22:35:15.099
    STEP: Counting existing ResourceQuota 02/20/23 22:35:21.126
    STEP: Creating a ResourceQuota 02/20/23 22:35:26.135
    STEP: Ensuring resource quota status is calculated 02/20/23 22:35:26.145
    STEP: Creating a Secret 02/20/23 22:35:28.155
    STEP: Ensuring resource quota status captures secret creation 02/20/23 22:35:28.177
    STEP: Deleting a secret 02/20/23 22:35:30.192
    STEP: Ensuring resource quota status released usage 02/20/23 22:35:30.21
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:35:32.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5659" for this suite. 02/20/23 22:35:32.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:32.276
Feb 20 22:35:32.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename podtemplate 02/20/23 22:35:32.278
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:32.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:32.341
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/20/23 22:35:32.353
STEP: Replace a pod template 02/20/23 22:35:32.365
Feb 20 22:35:32.382: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 20 22:35:32.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2842" for this suite. 02/20/23 22:35:32.398
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":155,"skipped":3042,"failed":0}
------------------------------
• [0.147 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:32.276
    Feb 20 22:35:32.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename podtemplate 02/20/23 22:35:32.278
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:32.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:32.341
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/20/23 22:35:32.353
    STEP: Replace a pod template 02/20/23 22:35:32.365
    Feb 20 22:35:32.382: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 20 22:35:32.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2842" for this suite. 02/20/23 22:35:32.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:32.425
Feb 20 22:35:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:35:32.427
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:32.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:32.493
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 02/20/23 22:35:32.506
STEP: Creating a ResourceQuota 02/20/23 22:35:37.518
STEP: Ensuring resource quota status is calculated 02/20/23 22:35:37.53
STEP: Creating a ReplicaSet 02/20/23 22:35:39.542
STEP: Ensuring resource quota status captures replicaset creation 02/20/23 22:35:39.611
STEP: Deleting a ReplicaSet 02/20/23 22:35:41.624
STEP: Ensuring resource quota status released usage 02/20/23 22:35:41.655
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:35:43.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2395" for this suite. 02/20/23 22:35:43.682
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":156,"skipped":3057,"failed":0}
------------------------------
• [SLOW TEST] [11.279 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:32.425
    Feb 20 22:35:32.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:35:32.427
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:32.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:32.493
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 02/20/23 22:35:32.506
    STEP: Creating a ResourceQuota 02/20/23 22:35:37.518
    STEP: Ensuring resource quota status is calculated 02/20/23 22:35:37.53
    STEP: Creating a ReplicaSet 02/20/23 22:35:39.542
    STEP: Ensuring resource quota status captures replicaset creation 02/20/23 22:35:39.611
    STEP: Deleting a ReplicaSet 02/20/23 22:35:41.624
    STEP: Ensuring resource quota status released usage 02/20/23 22:35:41.655
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:35:43.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2395" for this suite. 02/20/23 22:35:43.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:43.705
Feb 20 22:35:43.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:35:43.708
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:43.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:43.768
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 02/20/23 22:35:43.779
Feb 20 22:35:43.923: INFO: Waiting up to 5m0s for pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de" in namespace "downward-api-85" to be "Succeeded or Failed"
Feb 20 22:35:43.935: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de": Phase="Pending", Reason="", readiness=false. Elapsed: 11.831356ms
Feb 20 22:35:45.948: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024915944s
Feb 20 22:35:47.948: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024334045s
STEP: Saw pod success 02/20/23 22:35:47.948
Feb 20 22:35:47.948: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de" satisfied condition "Succeeded or Failed"
Feb 20 22:35:47.959: INFO: Trying to get logs from node 10.8.38.66 pod downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:35:48.003
Feb 20 22:35:48.035: INFO: Waiting for pod downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de to disappear
Feb 20 22:35:48.045: INFO: Pod downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 20 22:35:48.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-85" for this suite. 02/20/23 22:35:48.062
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":157,"skipped":3063,"failed":0}
------------------------------
• [4.386 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:43.705
    Feb 20 22:35:43.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:35:43.708
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:43.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:43.768
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 02/20/23 22:35:43.779
    Feb 20 22:35:43.923: INFO: Waiting up to 5m0s for pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de" in namespace "downward-api-85" to be "Succeeded or Failed"
    Feb 20 22:35:43.935: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de": Phase="Pending", Reason="", readiness=false. Elapsed: 11.831356ms
    Feb 20 22:35:45.948: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024915944s
    Feb 20 22:35:47.948: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024334045s
    STEP: Saw pod success 02/20/23 22:35:47.948
    Feb 20 22:35:47.948: INFO: Pod "downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de" satisfied condition "Succeeded or Failed"
    Feb 20 22:35:47.959: INFO: Trying to get logs from node 10.8.38.66 pod downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:35:48.003
    Feb 20 22:35:48.035: INFO: Waiting for pod downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de to disappear
    Feb 20 22:35:48.045: INFO: Pod downward-api-d33b307c-39e7-4d17-bc2c-c7ad75d314de no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 20 22:35:48.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-85" for this suite. 02/20/23 22:35:48.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:48.095
Feb 20 22:35:48.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:35:48.098
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:48.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:48.158
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-b57a7e1b-0171-4ac1-8995-878df0c4c43d 02/20/23 22:35:48.169
STEP: Creating a pod to test consume secrets 02/20/23 22:35:48.183
W0220 22:35:48.246097      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:35:48.246: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a" in namespace "projected-2649" to be "Succeeded or Failed"
Feb 20 22:35:48.260: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.633717ms
Feb 20 22:35:50.273: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026595155s
Feb 20 22:35:52.272: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025797622s
STEP: Saw pod success 02/20/23 22:35:52.272
Feb 20 22:35:52.272: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a" satisfied condition "Succeeded or Failed"
Feb 20 22:35:52.283: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a container projected-secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:35:52.305
Feb 20 22:35:52.337: INFO: Waiting for pod pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a to disappear
Feb 20 22:35:52.348: INFO: Pod pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 22:35:52.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2649" for this suite. 02/20/23 22:35:52.365
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":158,"skipped":3093,"failed":0}
------------------------------
• [4.291 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:48.095
    Feb 20 22:35:48.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:35:48.098
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:48.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:48.158
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-b57a7e1b-0171-4ac1-8995-878df0c4c43d 02/20/23 22:35:48.169
    STEP: Creating a pod to test consume secrets 02/20/23 22:35:48.183
    W0220 22:35:48.246097      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:35:48.246: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a" in namespace "projected-2649" to be "Succeeded or Failed"
    Feb 20 22:35:48.260: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.633717ms
    Feb 20 22:35:50.273: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026595155s
    Feb 20 22:35:52.272: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025797622s
    STEP: Saw pod success 02/20/23 22:35:52.272
    Feb 20 22:35:52.272: INFO: Pod "pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a" satisfied condition "Succeeded or Failed"
    Feb 20 22:35:52.283: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:35:52.305
    Feb 20 22:35:52.337: INFO: Waiting for pod pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a to disappear
    Feb 20 22:35:52.348: INFO: Pod pod-projected-secrets-543966bd-a8bb-41c2-b6ef-0f2632ea861a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 22:35:52.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2649" for this suite. 02/20/23 22:35:52.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:52.395
Feb 20 22:35:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename controllerrevisions 02/20/23 22:35:52.398
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:52.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:52.459
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-t8lxw-daemon-set" 02/20/23 22:35:52.543
STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:35:52.554
Feb 20 22:35:52.578: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 0
Feb 20 22:35:52.578: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:35:53.618: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 0
Feb 20 22:35:53.622: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:35:54.609: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 1
Feb 20 22:35:54.609: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:35:55.623: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 3
Feb 20 22:35:55.623: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-t8lxw-daemon-set
STEP: Confirm DaemonSet "e2e-t8lxw-daemon-set" successfully created with "daemonset-name=e2e-t8lxw-daemon-set" label 02/20/23 22:35:55.631
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-t8lxw-daemon-set" 02/20/23 22:35:55.655
Feb 20 22:35:55.699: INFO: Located ControllerRevision: "e2e-t8lxw-daemon-set-75696b94b"
STEP: Patching ControllerRevision "e2e-t8lxw-daemon-set-75696b94b" 02/20/23 22:35:55.714
Feb 20 22:35:55.736: INFO: e2e-t8lxw-daemon-set-75696b94b has been patched
STEP: Create a new ControllerRevision 02/20/23 22:35:55.736
Feb 20 22:35:55.751: INFO: Created ControllerRevision: e2e-t8lxw-daemon-set-5c5fd7bc48
STEP: Confirm that there are two ControllerRevisions 02/20/23 22:35:55.751
Feb 20 22:35:55.752: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 20 22:35:55.763: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-t8lxw-daemon-set-75696b94b" 02/20/23 22:35:55.763
STEP: Confirm that there is only one ControllerRevision 02/20/23 22:35:55.783
Feb 20 22:35:55.783: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 20 22:35:55.796: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-t8lxw-daemon-set-5c5fd7bc48" 02/20/23 22:35:55.808
Feb 20 22:35:55.838: INFO: e2e-t8lxw-daemon-set-5c5fd7bc48 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/20/23 22:35:55.838
W0220 22:35:55.850284      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/20/23 22:35:55.85
Feb 20 22:35:55.851: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 20 22:35:56.864: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 20 22:35:56.879: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-t8lxw-daemon-set-5c5fd7bc48=updated" 02/20/23 22:35:56.879
STEP: Confirm that there is only one ControllerRevision 02/20/23 22:35:56.931
Feb 20 22:35:56.932: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 20 22:35:56.944: INFO: Found 1 ControllerRevisions
Feb 20 22:35:56.955: INFO: ControllerRevision "e2e-t8lxw-daemon-set-594b44ff65" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-t8lxw-daemon-set" 02/20/23 22:35:56.974
STEP: deleting DaemonSet.extensions e2e-t8lxw-daemon-set in namespace controllerrevisions-4321, will wait for the garbage collector to delete the pods 02/20/23 22:35:56.974
Feb 20 22:35:57.046: INFO: Deleting DaemonSet.extensions e2e-t8lxw-daemon-set took: 12.922534ms
Feb 20 22:35:57.148: INFO: Terminating DaemonSet.extensions e2e-t8lxw-daemon-set pods took: 101.494935ms
Feb 20 22:35:59.159: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 0
Feb 20 22:35:59.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-t8lxw-daemon-set
Feb 20 22:35:59.166: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"97284"},"items":null}

Feb 20 22:35:59.201: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"97284"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:35:59.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-4321" for this suite. 02/20/23 22:35:59.27
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":159,"skipped":3121,"failed":0}
------------------------------
• [SLOW TEST] [6.914 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:52.395
    Feb 20 22:35:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename controllerrevisions 02/20/23 22:35:52.398
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:52.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:52.459
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-t8lxw-daemon-set" 02/20/23 22:35:52.543
    STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:35:52.554
    Feb 20 22:35:52.578: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 0
    Feb 20 22:35:52.578: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:35:53.618: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 0
    Feb 20 22:35:53.622: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:35:54.609: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 1
    Feb 20 22:35:54.609: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:35:55.623: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 3
    Feb 20 22:35:55.623: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-t8lxw-daemon-set
    STEP: Confirm DaemonSet "e2e-t8lxw-daemon-set" successfully created with "daemonset-name=e2e-t8lxw-daemon-set" label 02/20/23 22:35:55.631
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-t8lxw-daemon-set" 02/20/23 22:35:55.655
    Feb 20 22:35:55.699: INFO: Located ControllerRevision: "e2e-t8lxw-daemon-set-75696b94b"
    STEP: Patching ControllerRevision "e2e-t8lxw-daemon-set-75696b94b" 02/20/23 22:35:55.714
    Feb 20 22:35:55.736: INFO: e2e-t8lxw-daemon-set-75696b94b has been patched
    STEP: Create a new ControllerRevision 02/20/23 22:35:55.736
    Feb 20 22:35:55.751: INFO: Created ControllerRevision: e2e-t8lxw-daemon-set-5c5fd7bc48
    STEP: Confirm that there are two ControllerRevisions 02/20/23 22:35:55.751
    Feb 20 22:35:55.752: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 20 22:35:55.763: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-t8lxw-daemon-set-75696b94b" 02/20/23 22:35:55.763
    STEP: Confirm that there is only one ControllerRevision 02/20/23 22:35:55.783
    Feb 20 22:35:55.783: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 20 22:35:55.796: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-t8lxw-daemon-set-5c5fd7bc48" 02/20/23 22:35:55.808
    Feb 20 22:35:55.838: INFO: e2e-t8lxw-daemon-set-5c5fd7bc48 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/20/23 22:35:55.838
    W0220 22:35:55.850284      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/20/23 22:35:55.85
    Feb 20 22:35:55.851: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 20 22:35:56.864: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 20 22:35:56.879: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-t8lxw-daemon-set-5c5fd7bc48=updated" 02/20/23 22:35:56.879
    STEP: Confirm that there is only one ControllerRevision 02/20/23 22:35:56.931
    Feb 20 22:35:56.932: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 20 22:35:56.944: INFO: Found 1 ControllerRevisions
    Feb 20 22:35:56.955: INFO: ControllerRevision "e2e-t8lxw-daemon-set-594b44ff65" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-t8lxw-daemon-set" 02/20/23 22:35:56.974
    STEP: deleting DaemonSet.extensions e2e-t8lxw-daemon-set in namespace controllerrevisions-4321, will wait for the garbage collector to delete the pods 02/20/23 22:35:56.974
    Feb 20 22:35:57.046: INFO: Deleting DaemonSet.extensions e2e-t8lxw-daemon-set took: 12.922534ms
    Feb 20 22:35:57.148: INFO: Terminating DaemonSet.extensions e2e-t8lxw-daemon-set pods took: 101.494935ms
    Feb 20 22:35:59.159: INFO: Number of nodes with available pods controlled by daemonset e2e-t8lxw-daemon-set: 0
    Feb 20 22:35:59.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-t8lxw-daemon-set
    Feb 20 22:35:59.166: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"97284"},"items":null}

    Feb 20 22:35:59.201: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"97284"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:35:59.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-4321" for this suite. 02/20/23 22:35:59.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:35:59.312
Feb 20 22:35:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:35:59.314
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:59.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:59.396
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:35:59.477
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:36:00.239
STEP: Deploying the webhook pod 02/20/23 22:36:00.264
STEP: Wait for the deployment to be ready 02/20/23 22:36:00.29
Feb 20 22:36:00.317: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 20 22:36:02.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:36:04.362
STEP: Verifying the service has paired with the endpoint 02/20/23 22:36:04.416
Feb 20 22:36:05.418: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/20/23 22:36:05.429
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/20/23 22:36:05.465
STEP: Creating a dummy validating-webhook-configuration object 02/20/23 22:36:05.496
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/20/23 22:36:05.52
STEP: Creating a dummy mutating-webhook-configuration object 02/20/23 22:36:05.539
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/20/23 22:36:05.562
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:36:05.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6576" for this suite. 02/20/23 22:36:05.633
STEP: Destroying namespace "webhook-6576-markers" for this suite. 02/20/23 22:36:05.657
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":160,"skipped":3126,"failed":0}
------------------------------
• [SLOW TEST] [6.472 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:35:59.312
    Feb 20 22:35:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:35:59.314
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:35:59.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:35:59.396
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:35:59.477
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:36:00.239
    STEP: Deploying the webhook pod 02/20/23 22:36:00.264
    STEP: Wait for the deployment to be ready 02/20/23 22:36:00.29
    Feb 20 22:36:00.317: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Feb 20 22:36:02.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 36, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:36:04.362
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:36:04.416
    Feb 20 22:36:05.418: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/20/23 22:36:05.429
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/20/23 22:36:05.465
    STEP: Creating a dummy validating-webhook-configuration object 02/20/23 22:36:05.496
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/20/23 22:36:05.52
    STEP: Creating a dummy mutating-webhook-configuration object 02/20/23 22:36:05.539
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/20/23 22:36:05.562
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:36:05.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6576" for this suite. 02/20/23 22:36:05.633
    STEP: Destroying namespace "webhook-6576-markers" for this suite. 02/20/23 22:36:05.657
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:36:05.784
Feb 20 22:36:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:36:05.785
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:05.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:05.842
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 02/20/23 22:36:05.857
STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:36:05.866
STEP: Creating a ResourceQuota with not best effort scope 02/20/23 22:36:07.876
STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:36:07.887
STEP: Creating a best-effort pod 02/20/23 22:36:09.897
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/20/23 22:36:09.97
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/20/23 22:36:11.979
STEP: Deleting the pod 02/20/23 22:36:13.989
STEP: Ensuring resource quota status released the pod usage 02/20/23 22:36:14.017
STEP: Creating a not best-effort pod 02/20/23 22:36:16.027
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/20/23 22:36:16.065
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/20/23 22:36:18.075
STEP: Deleting the pod 02/20/23 22:36:20.086
STEP: Ensuring resource quota status released the pod usage 02/20/23 22:36:20.112
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:36:22.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6310" for this suite. 02/20/23 22:36:22.138
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":161,"skipped":3126,"failed":0}
------------------------------
• [SLOW TEST] [16.376 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:36:05.784
    Feb 20 22:36:05.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:36:05.785
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:05.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:05.842
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 02/20/23 22:36:05.857
    STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:36:05.866
    STEP: Creating a ResourceQuota with not best effort scope 02/20/23 22:36:07.876
    STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:36:07.887
    STEP: Creating a best-effort pod 02/20/23 22:36:09.897
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/20/23 22:36:09.97
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/20/23 22:36:11.979
    STEP: Deleting the pod 02/20/23 22:36:13.989
    STEP: Ensuring resource quota status released the pod usage 02/20/23 22:36:14.017
    STEP: Creating a not best-effort pod 02/20/23 22:36:16.027
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/20/23 22:36:16.065
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/20/23 22:36:18.075
    STEP: Deleting the pod 02/20/23 22:36:20.086
    STEP: Ensuring resource quota status released the pod usage 02/20/23 22:36:20.112
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:36:22.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6310" for this suite. 02/20/23 22:36:22.138
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:36:22.17
Feb 20 22:36:22.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 22:36:22.172
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:22.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:22.239
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/20/23 22:36:22.25
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/20/23 22:36:22.274
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/20/23 22:36:22.274
STEP: creating a pod to probe DNS 02/20/23 22:36:22.274
STEP: submitting the pod to kubernetes 02/20/23 22:36:22.274
W0220 22:36:22.342275      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:36:22.342: INFO: Waiting up to 15m0s for pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1" in namespace "dns-9936" to be "running"
Feb 20 22:36:22.355: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.148376ms
Feb 20 22:36:24.368: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025893778s
Feb 20 22:36:26.368: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1": Phase="Running", Reason="", readiness=true. Elapsed: 4.025402621s
Feb 20 22:36:26.368: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1" satisfied condition "running"
STEP: retrieving the pod 02/20/23 22:36:26.368
STEP: looking for the results for each expected name from probers 02/20/23 22:36:26.379
Feb 20 22:36:26.437: INFO: DNS probes using dns-9936/dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1 succeeded

STEP: deleting the pod 02/20/23 22:36:26.437
STEP: deleting the test headless service 02/20/23 22:36:26.477
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 22:36:26.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9936" for this suite. 02/20/23 22:36:26.58
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":162,"skipped":3129,"failed":0}
------------------------------
• [4.435 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:36:22.17
    Feb 20 22:36:22.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 22:36:22.172
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:22.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:22.239
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/20/23 22:36:22.25
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/20/23 22:36:22.274
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9936.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/20/23 22:36:22.274
    STEP: creating a pod to probe DNS 02/20/23 22:36:22.274
    STEP: submitting the pod to kubernetes 02/20/23 22:36:22.274
    W0220 22:36:22.342275      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:36:22.342: INFO: Waiting up to 15m0s for pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1" in namespace "dns-9936" to be "running"
    Feb 20 22:36:22.355: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.148376ms
    Feb 20 22:36:24.368: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025893778s
    Feb 20 22:36:26.368: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1": Phase="Running", Reason="", readiness=true. Elapsed: 4.025402621s
    Feb 20 22:36:26.368: INFO: Pod "dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 22:36:26.368
    STEP: looking for the results for each expected name from probers 02/20/23 22:36:26.379
    Feb 20 22:36:26.437: INFO: DNS probes using dns-9936/dns-test-c1ed089a-92bf-48e8-ae57-79e3abcf2af1 succeeded

    STEP: deleting the pod 02/20/23 22:36:26.437
    STEP: deleting the test headless service 02/20/23 22:36:26.477
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 22:36:26.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9936" for this suite. 02/20/23 22:36:26.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:36:26.612
Feb 20 22:36:26.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:36:26.615
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:26.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:26.694
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 02/20/23 22:36:26.703
Feb 20 22:36:26.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 20 22:36:26.896: INFO: stderr: ""
Feb 20 22:36:26.896: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 02/20/23 22:36:26.896
Feb 20 22:36:26.896: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 20 22:36:26.897: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6259" to be "running and ready, or succeeded"
Feb 20 22:36:26.914: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.427039ms
Feb 20 22:36:26.914: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Feb 20 22:36:28.926: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029479099s
Feb 20 22:36:28.926: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 20 22:36:28.926: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/20/23 22:36:28.926
Feb 20 22:36:28.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator'
Feb 20 22:36:29.067: INFO: stderr: ""
Feb 20 22:36:29.067: INFO: stdout: "I0220 22:36:28.208315       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/wqg4 474\nI0220 22:36:28.408732       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/kd58 416\nI0220 22:36:28.611280       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/68l 460\nI0220 22:36:28.808349       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5mnf 349\nI0220 22:36:29.008628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/c9bs 595\n"
Feb 20 22:36:31.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator'
Feb 20 22:36:31.235: INFO: stderr: ""
Feb 20 22:36:31.235: INFO: stdout: "I0220 22:36:28.208315       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/wqg4 474\nI0220 22:36:28.408732       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/kd58 416\nI0220 22:36:28.611280       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/68l 460\nI0220 22:36:28.808349       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5mnf 349\nI0220 22:36:29.008628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/c9bs 595\nI0220 22:36:29.208611       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/djv 210\nI0220 22:36:29.409049       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/89d4 233\nI0220 22:36:29.608340       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zk2c 433\nI0220 22:36:29.808724       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/f4n 340\nI0220 22:36:30.008147       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/7m7 208\nI0220 22:36:30.209356       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/59zn 390\nI0220 22:36:30.408774       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/d4f 386\nI0220 22:36:30.609176       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/5jz 443\nI0220 22:36:30.808433       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/g8t 591\nI0220 22:36:31.008827       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/rcgp 383\nI0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\n"
STEP: limiting log lines 02/20/23 22:36:31.235
Feb 20 22:36:31.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --tail=1'
Feb 20 22:36:31.370: INFO: stderr: ""
Feb 20 22:36:31.370: INFO: stdout: "I0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\n"
Feb 20 22:36:31.370: INFO: got output "I0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\n"
STEP: limiting log bytes 02/20/23 22:36:31.37
Feb 20 22:36:31.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --limit-bytes=1'
Feb 20 22:36:31.521: INFO: stderr: ""
Feb 20 22:36:31.521: INFO: stdout: "I"
Feb 20 22:36:31.521: INFO: got output "I"
STEP: exposing timestamps 02/20/23 22:36:31.521
Feb 20 22:36:31.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 20 22:36:31.731: INFO: stderr: ""
Feb 20 22:36:31.731: INFO: stdout: "2023-02-20T16:36:31.608822214-06:00 I0220 22:36:31.608768       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/pb2d 219\n"
Feb 20 22:36:31.731: INFO: got output "2023-02-20T16:36:31.608822214-06:00 I0220 22:36:31.608768       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/pb2d 219\n"
STEP: restricting to a time range 02/20/23 22:36:31.731
Feb 20 22:36:34.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --since=1s'
Feb 20 22:36:34.393: INFO: stderr: ""
Feb 20 22:36:34.393: INFO: stdout: "I0220 22:36:33.409176       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/gw6w 240\nI0220 22:36:33.608664       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/4rhw 379\nI0220 22:36:33.809012       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/j5s 416\nI0220 22:36:34.009225       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/z69 408\nI0220 22:36:34.208350       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/t8sp 536\n"
Feb 20 22:36:34.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --since=24h'
Feb 20 22:36:34.532: INFO: stderr: ""
Feb 20 22:36:34.532: INFO: stdout: "I0220 22:36:28.208315       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/wqg4 474\nI0220 22:36:28.408732       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/kd58 416\nI0220 22:36:28.611280       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/68l 460\nI0220 22:36:28.808349       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5mnf 349\nI0220 22:36:29.008628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/c9bs 595\nI0220 22:36:29.208611       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/djv 210\nI0220 22:36:29.409049       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/89d4 233\nI0220 22:36:29.608340       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zk2c 433\nI0220 22:36:29.808724       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/f4n 340\nI0220 22:36:30.008147       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/7m7 208\nI0220 22:36:30.209356       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/59zn 390\nI0220 22:36:30.408774       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/d4f 386\nI0220 22:36:30.609176       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/5jz 443\nI0220 22:36:30.808433       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/g8t 591\nI0220 22:36:31.008827       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/rcgp 383\nI0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\nI0220 22:36:31.408395       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/68s5 455\nI0220 22:36:31.608768       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/pb2d 219\nI0220 22:36:31.810601       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/7q7x 317\nI0220 22:36:32.009007       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/sxn 358\nI0220 22:36:32.208298       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/6v5 398\nI0220 22:36:32.408717       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/gg8 575\nI0220 22:36:32.609097       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/bm9t 434\nI0220 22:36:32.808488       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/djq 401\nI0220 22:36:33.008884       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/ztm2 475\nI0220 22:36:33.208700       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/d6m 453\nI0220 22:36:33.409176       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/gw6w 240\nI0220 22:36:33.608664       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/4rhw 379\nI0220 22:36:33.809012       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/j5s 416\nI0220 22:36:34.009225       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/z69 408\nI0220 22:36:34.208350       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/t8sp 536\nI0220 22:36:34.408362       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/8rg 381\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Feb 20 22:36:34.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 delete pod logs-generator'
Feb 20 22:36:35.972: INFO: stderr: ""
Feb 20 22:36:35.972: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:36:35.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6259" for this suite. 02/20/23 22:36:35.99
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":163,"skipped":3138,"failed":0}
------------------------------
• [SLOW TEST] [9.403 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:36:26.612
    Feb 20 22:36:26.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:36:26.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:26.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:26.694
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 02/20/23 22:36:26.703
    Feb 20 22:36:26.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 20 22:36:26.896: INFO: stderr: ""
    Feb 20 22:36:26.896: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 02/20/23 22:36:26.896
    Feb 20 22:36:26.896: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 20 22:36:26.897: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6259" to be "running and ready, or succeeded"
    Feb 20 22:36:26.914: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.427039ms
    Feb 20 22:36:26.914: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Feb 20 22:36:28.926: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029479099s
    Feb 20 22:36:28.926: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 20 22:36:28.926: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/20/23 22:36:28.926
    Feb 20 22:36:28.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator'
    Feb 20 22:36:29.067: INFO: stderr: ""
    Feb 20 22:36:29.067: INFO: stdout: "I0220 22:36:28.208315       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/wqg4 474\nI0220 22:36:28.408732       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/kd58 416\nI0220 22:36:28.611280       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/68l 460\nI0220 22:36:28.808349       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5mnf 349\nI0220 22:36:29.008628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/c9bs 595\n"
    Feb 20 22:36:31.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator'
    Feb 20 22:36:31.235: INFO: stderr: ""
    Feb 20 22:36:31.235: INFO: stdout: "I0220 22:36:28.208315       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/wqg4 474\nI0220 22:36:28.408732       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/kd58 416\nI0220 22:36:28.611280       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/68l 460\nI0220 22:36:28.808349       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5mnf 349\nI0220 22:36:29.008628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/c9bs 595\nI0220 22:36:29.208611       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/djv 210\nI0220 22:36:29.409049       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/89d4 233\nI0220 22:36:29.608340       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zk2c 433\nI0220 22:36:29.808724       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/f4n 340\nI0220 22:36:30.008147       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/7m7 208\nI0220 22:36:30.209356       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/59zn 390\nI0220 22:36:30.408774       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/d4f 386\nI0220 22:36:30.609176       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/5jz 443\nI0220 22:36:30.808433       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/g8t 591\nI0220 22:36:31.008827       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/rcgp 383\nI0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\n"
    STEP: limiting log lines 02/20/23 22:36:31.235
    Feb 20 22:36:31.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --tail=1'
    Feb 20 22:36:31.370: INFO: stderr: ""
    Feb 20 22:36:31.370: INFO: stdout: "I0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\n"
    Feb 20 22:36:31.370: INFO: got output "I0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\n"
    STEP: limiting log bytes 02/20/23 22:36:31.37
    Feb 20 22:36:31.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --limit-bytes=1'
    Feb 20 22:36:31.521: INFO: stderr: ""
    Feb 20 22:36:31.521: INFO: stdout: "I"
    Feb 20 22:36:31.521: INFO: got output "I"
    STEP: exposing timestamps 02/20/23 22:36:31.521
    Feb 20 22:36:31.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 20 22:36:31.731: INFO: stderr: ""
    Feb 20 22:36:31.731: INFO: stdout: "2023-02-20T16:36:31.608822214-06:00 I0220 22:36:31.608768       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/pb2d 219\n"
    Feb 20 22:36:31.731: INFO: got output "2023-02-20T16:36:31.608822214-06:00 I0220 22:36:31.608768       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/pb2d 219\n"
    STEP: restricting to a time range 02/20/23 22:36:31.731
    Feb 20 22:36:34.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --since=1s'
    Feb 20 22:36:34.393: INFO: stderr: ""
    Feb 20 22:36:34.393: INFO: stdout: "I0220 22:36:33.409176       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/gw6w 240\nI0220 22:36:33.608664       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/4rhw 379\nI0220 22:36:33.809012       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/j5s 416\nI0220 22:36:34.009225       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/z69 408\nI0220 22:36:34.208350       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/t8sp 536\n"
    Feb 20 22:36:34.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 logs logs-generator logs-generator --since=24h'
    Feb 20 22:36:34.532: INFO: stderr: ""
    Feb 20 22:36:34.532: INFO: stdout: "I0220 22:36:28.208315       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/wqg4 474\nI0220 22:36:28.408732       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/kd58 416\nI0220 22:36:28.611280       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/68l 460\nI0220 22:36:28.808349       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/5mnf 349\nI0220 22:36:29.008628       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/c9bs 595\nI0220 22:36:29.208611       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/djv 210\nI0220 22:36:29.409049       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/89d4 233\nI0220 22:36:29.608340       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zk2c 433\nI0220 22:36:29.808724       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/f4n 340\nI0220 22:36:30.008147       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/7m7 208\nI0220 22:36:30.209356       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/59zn 390\nI0220 22:36:30.408774       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/d4f 386\nI0220 22:36:30.609176       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/5jz 443\nI0220 22:36:30.808433       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/g8t 591\nI0220 22:36:31.008827       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/rcgp 383\nI0220 22:36:31.208134       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/4j5d 262\nI0220 22:36:31.408395       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/68s5 455\nI0220 22:36:31.608768       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/pb2d 219\nI0220 22:36:31.810601       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/7q7x 317\nI0220 22:36:32.009007       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/sxn 358\nI0220 22:36:32.208298       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/6v5 398\nI0220 22:36:32.408717       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/gg8 575\nI0220 22:36:32.609097       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/bm9t 434\nI0220 22:36:32.808488       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/djq 401\nI0220 22:36:33.008884       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/ztm2 475\nI0220 22:36:33.208700       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/d6m 453\nI0220 22:36:33.409176       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/gw6w 240\nI0220 22:36:33.608664       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/4rhw 379\nI0220 22:36:33.809012       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/j5s 416\nI0220 22:36:34.009225       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/ns/pods/z69 408\nI0220 22:36:34.208350       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/t8sp 536\nI0220 22:36:34.408362       1 logs_generator.go:76] 31 POST /api/v1/namespaces/default/pods/8rg 381\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Feb 20 22:36:34.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-6259 delete pod logs-generator'
    Feb 20 22:36:35.972: INFO: stderr: ""
    Feb 20 22:36:35.972: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:36:35.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6259" for this suite. 02/20/23 22:36:35.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:36:36.019
Feb 20 22:36:36.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 22:36:36.021
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:36.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:36.119
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/20/23 22:36:36.129
W0220 22:36:36.139393      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 02/20/23 22:36:36.139
STEP: delete the deployment 02/20/23 22:36:36.199
STEP: wait for all rs to be garbage collected 02/20/23 22:36:36.211
STEP: Gathering metrics 02/20/23 22:36:36.273
W0220 22:36:36.303730      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 20 22:36:36.304: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 22:36:36.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5494" for this suite. 02/20/23 22:36:36.318
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":164,"skipped":3167,"failed":0}
------------------------------
• [0.388 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:36:36.019
    Feb 20 22:36:36.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 22:36:36.021
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:36.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:36.119
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/20/23 22:36:36.129
    W0220 22:36:36.139393      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 02/20/23 22:36:36.139
    STEP: delete the deployment 02/20/23 22:36:36.199
    STEP: wait for all rs to be garbage collected 02/20/23 22:36:36.211
    STEP: Gathering metrics 02/20/23 22:36:36.273
    W0220 22:36:36.303730      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 20 22:36:36.304: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 22:36:36.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5494" for this suite. 02/20/23 22:36:36.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:36:36.417
Feb 20 22:36:36.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename watch 02/20/23 22:36:36.418
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:36.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:36.52
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/20/23 22:36:36.531
STEP: creating a watch on configmaps with label B 02/20/23 22:36:36.537
STEP: creating a watch on configmaps with label A or B 02/20/23 22:36:36.548
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/20/23 22:36:36.558
Feb 20 22:36:36.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97963 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:36:36.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97963 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/20/23 22:36:36.573
Feb 20 22:36:36.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97966 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:36:36.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97966 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/20/23 22:36:36.598
Feb 20 22:36:36.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97970 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:36:36.632: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97970 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/20/23 22:36:36.632
Feb 20 22:36:36.650: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97975 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:36:36.650: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97975 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/20/23 22:36:36.651
Feb 20 22:36:36.664: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 97976 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:36:36.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 97976 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/20/23 22:36:46.665
Feb 20 22:36:46.684: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 98116 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:36:46.684: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 98116 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 20 22:36:56.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8278" for this suite. 02/20/23 22:36:56.708
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":165,"skipped":3212,"failed":0}
------------------------------
• [SLOW TEST] [20.314 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:36:36.417
    Feb 20 22:36:36.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename watch 02/20/23 22:36:36.418
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:36.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:36.52
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/20/23 22:36:36.531
    STEP: creating a watch on configmaps with label B 02/20/23 22:36:36.537
    STEP: creating a watch on configmaps with label A or B 02/20/23 22:36:36.548
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/20/23 22:36:36.558
    Feb 20 22:36:36.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97963 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:36:36.573: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97963 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/20/23 22:36:36.573
    Feb 20 22:36:36.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97966 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:36:36.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97966 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/20/23 22:36:36.598
    Feb 20 22:36:36.631: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97970 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:36:36.632: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97970 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/20/23 22:36:36.632
    Feb 20 22:36:36.650: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97975 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:36:36.650: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8278  830afe4c-b48b-47c0-adc9-fd5a0f86cb56 97975 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/20/23 22:36:36.651
    Feb 20 22:36:36.664: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 97976 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:36:36.665: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 97976 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/20/23 22:36:46.665
    Feb 20 22:36:46.684: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 98116 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:36:46.684: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8278  96e45cd2-2482-4cad-b3e6-f701c3e14a20 98116 0 2023-02-20 22:36:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-20 22:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 20 22:36:56.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8278" for this suite. 02/20/23 22:36:56.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:36:56.739
Feb 20 22:36:56.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename cronjob 02/20/23 22:36:56.742
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:56.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:56.807
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/20/23 22:36:56.817
STEP: Ensuring a job is scheduled 02/20/23 22:36:56.878
STEP: Ensuring exactly one is scheduled 02/20/23 22:37:00.89
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/20/23 22:37:00.901
STEP: Ensuring the job is replaced with a new one 02/20/23 22:37:00.909
STEP: Removing cronjob 02/20/23 22:38:00.925
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 20 22:38:00.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3435" for this suite. 02/20/23 22:38:00.961
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":166,"skipped":3256,"failed":0}
------------------------------
• [SLOW TEST] [64.247 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:36:56.739
    Feb 20 22:36:56.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename cronjob 02/20/23 22:36:56.742
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:36:56.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:36:56.807
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/20/23 22:36:56.817
    STEP: Ensuring a job is scheduled 02/20/23 22:36:56.878
    STEP: Ensuring exactly one is scheduled 02/20/23 22:37:00.89
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/20/23 22:37:00.901
    STEP: Ensuring the job is replaced with a new one 02/20/23 22:37:00.909
    STEP: Removing cronjob 02/20/23 22:38:00.925
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 20 22:38:00.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3435" for this suite. 02/20/23 22:38:00.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:38:00.987
Feb 20 22:38:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename runtimeclass 02/20/23 22:38:00.99
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:38:01.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:38:01.049
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 20 22:38:01.152: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3499 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 20 22:38:01.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3499" for this suite. 02/20/23 22:38:01.193
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":167,"skipped":3261,"failed":0}
------------------------------
• [0.230 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:38:00.987
    Feb 20 22:38:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename runtimeclass 02/20/23 22:38:00.99
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:38:01.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:38:01.049
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 20 22:38:01.152: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3499 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 20 22:38:01.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3499" for this suite. 02/20/23 22:38:01.193
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:38:01.219
Feb 20 22:38:01.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:38:01.221
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:38:01.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:38:01.288
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Feb 20 22:38:01.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/20/23 22:38:10.739
Feb 20 22:38:10.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 create -f -'
Feb 20 22:38:12.670: INFO: stderr: ""
Feb 20 22:38:12.670: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 20 22:38:12.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 delete e2e-test-crd-publish-openapi-1395-crds test-cr'
Feb 20 22:38:12.807: INFO: stderr: ""
Feb 20 22:38:12.807: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 20 22:38:12.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 apply -f -'
Feb 20 22:38:13.417: INFO: stderr: ""
Feb 20 22:38:13.417: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 20 22:38:13.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 delete e2e-test-crd-publish-openapi-1395-crds test-cr'
Feb 20 22:38:13.540: INFO: stderr: ""
Feb 20 22:38:13.540: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/20/23 22:38:13.541
Feb 20 22:38:13.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 explain e2e-test-crd-publish-openapi-1395-crds'
Feb 20 22:38:15.065: INFO: stderr: ""
Feb 20 22:38:15.065: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1395-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:38:23.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1320" for this suite. 02/20/23 22:38:23.922
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":168,"skipped":3265,"failed":0}
------------------------------
• [SLOW TEST] [22.727 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:38:01.219
    Feb 20 22:38:01.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:38:01.221
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:38:01.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:38:01.288
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Feb 20 22:38:01.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/20/23 22:38:10.739
    Feb 20 22:38:10.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 create -f -'
    Feb 20 22:38:12.670: INFO: stderr: ""
    Feb 20 22:38:12.670: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 20 22:38:12.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 delete e2e-test-crd-publish-openapi-1395-crds test-cr'
    Feb 20 22:38:12.807: INFO: stderr: ""
    Feb 20 22:38:12.807: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 20 22:38:12.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 apply -f -'
    Feb 20 22:38:13.417: INFO: stderr: ""
    Feb 20 22:38:13.417: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 20 22:38:13.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 --namespace=crd-publish-openapi-1320 delete e2e-test-crd-publish-openapi-1395-crds test-cr'
    Feb 20 22:38:13.540: INFO: stderr: ""
    Feb 20 22:38:13.540: INFO: stdout: "e2e-test-crd-publish-openapi-1395-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/20/23 22:38:13.541
    Feb 20 22:38:13.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1320 explain e2e-test-crd-publish-openapi-1395-crds'
    Feb 20 22:38:15.065: INFO: stderr: ""
    Feb 20 22:38:15.065: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1395-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:38:23.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1320" for this suite. 02/20/23 22:38:23.922
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:38:23.946
Feb 20 22:38:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:38:23.954
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:38:24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:38:24.019
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 02/20/23 22:38:24.039
Feb 20 22:38:24.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: mark a version not serverd 02/20/23 22:38:43.008
STEP: check the unserved version gets removed 02/20/23 22:38:43.071
STEP: check the other version is not changed 02/20/23 22:38:51.746
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:39:07.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7483" for this suite. 02/20/23 22:39:07.706
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":169,"skipped":3265,"failed":0}
------------------------------
• [SLOW TEST] [43.783 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:38:23.946
    Feb 20 22:38:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:38:23.954
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:38:24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:38:24.019
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 02/20/23 22:38:24.039
    Feb 20 22:38:24.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: mark a version not serverd 02/20/23 22:38:43.008
    STEP: check the unserved version gets removed 02/20/23 22:38:43.071
    STEP: check the other version is not changed 02/20/23 22:38:51.746
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:39:07.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7483" for this suite. 02/20/23 22:39:07.706
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:07.732
Feb 20 22:39:07.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename events 02/20/23 22:39:07.733
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:07.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:07.791
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/20/23 22:39:07.805
STEP: get a list of Events with a label in the current namespace 02/20/23 22:39:07.844
STEP: delete a list of events 02/20/23 22:39:07.852
Feb 20 22:39:07.852: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/20/23 22:39:07.891
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 20 22:39:07.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8600" for this suite. 02/20/23 22:39:07.919
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":170,"skipped":3268,"failed":0}
------------------------------
• [0.235 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:07.732
    Feb 20 22:39:07.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename events 02/20/23 22:39:07.733
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:07.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:07.791
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/20/23 22:39:07.805
    STEP: get a list of Events with a label in the current namespace 02/20/23 22:39:07.844
    STEP: delete a list of events 02/20/23 22:39:07.852
    Feb 20 22:39:07.852: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/20/23 22:39:07.891
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 20 22:39:07.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8600" for this suite. 02/20/23 22:39:07.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:07.97
Feb 20 22:39:07.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:39:07.972
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:08.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:08.081
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-100085a0-1e44-4bba-97a9-eb2f0e0d8d85 02/20/23 22:39:08.102
STEP: Creating a pod to test consume secrets 02/20/23 22:39:08.119
Feb 20 22:39:08.222: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45" in namespace "projected-1022" to be "Succeeded or Failed"
Feb 20 22:39:08.233: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Pending", Reason="", readiness=false. Elapsed: 11.164659ms
Feb 20 22:39:10.245: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023548184s
Feb 20 22:39:12.245: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022883973s
Feb 20 22:39:14.247: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025100255s
STEP: Saw pod success 02/20/23 22:39:14.247
Feb 20 22:39:14.247: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45" satisfied condition "Succeeded or Failed"
Feb 20 22:39:14.262: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:39:14.302
Feb 20 22:39:14.340: INFO: Waiting for pod pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45 to disappear
Feb 20 22:39:14.353: INFO: Pod pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 22:39:14.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1022" for this suite. 02/20/23 22:39:14.374
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":171,"skipped":3276,"failed":0}
------------------------------
• [SLOW TEST] [6.430 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:07.97
    Feb 20 22:39:07.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:39:07.972
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:08.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:08.081
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-100085a0-1e44-4bba-97a9-eb2f0e0d8d85 02/20/23 22:39:08.102
    STEP: Creating a pod to test consume secrets 02/20/23 22:39:08.119
    Feb 20 22:39:08.222: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45" in namespace "projected-1022" to be "Succeeded or Failed"
    Feb 20 22:39:08.233: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Pending", Reason="", readiness=false. Elapsed: 11.164659ms
    Feb 20 22:39:10.245: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023548184s
    Feb 20 22:39:12.245: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022883973s
    Feb 20 22:39:14.247: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025100255s
    STEP: Saw pod success 02/20/23 22:39:14.247
    Feb 20 22:39:14.247: INFO: Pod "pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45" satisfied condition "Succeeded or Failed"
    Feb 20 22:39:14.262: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:39:14.302
    Feb 20 22:39:14.340: INFO: Waiting for pod pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45 to disappear
    Feb 20 22:39:14.353: INFO: Pod pod-projected-secrets-bf2bd5a8-dada-4885-962d-f1b53050ae45 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 22:39:14.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1022" for this suite. 02/20/23 22:39:14.374
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:14.402
Feb 20 22:39:14.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubelet-test 02/20/23 22:39:14.404
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:14.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:14.466
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 20 22:39:14.546: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4" in namespace "kubelet-test-6526" to be "running and ready"
Feb 20 22:39:14.562: INFO: Pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.860253ms
Feb 20 22:39:14.563: INFO: The phase of Pod busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:39:16.575: INFO: Pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4": Phase="Running", Reason="", readiness=true. Elapsed: 2.027964694s
Feb 20 22:39:16.575: INFO: The phase of Pod busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4 is Running (Ready = true)
Feb 20 22:39:16.575: INFO: Pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 20 22:39:16.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6526" for this suite. 02/20/23 22:39:16.629
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":172,"skipped":3279,"failed":0}
------------------------------
• [2.251 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:14.402
    Feb 20 22:39:14.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubelet-test 02/20/23 22:39:14.404
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:14.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:14.466
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 20 22:39:14.546: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4" in namespace "kubelet-test-6526" to be "running and ready"
    Feb 20 22:39:14.562: INFO: Pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.860253ms
    Feb 20 22:39:14.563: INFO: The phase of Pod busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:39:16.575: INFO: Pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4": Phase="Running", Reason="", readiness=true. Elapsed: 2.027964694s
    Feb 20 22:39:16.575: INFO: The phase of Pod busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4 is Running (Ready = true)
    Feb 20 22:39:16.575: INFO: Pod "busybox-readonly-fsa11af8f5-a7ee-48f1-9006-20718f8afad4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 20 22:39:16.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6526" for this suite. 02/20/23 22:39:16.629
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:16.658
Feb 20 22:39:16.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:39:16.66
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:16.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:16.735
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-c76a9502-7ad8-4670-ba54-87782a7031b5 02/20/23 22:39:16.746
STEP: Creating a pod to test consume secrets 02/20/23 22:39:16.76
Feb 20 22:39:16.816: INFO: Waiting up to 5m0s for pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc" in namespace "secrets-6731" to be "Succeeded or Failed"
Feb 20 22:39:16.828: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.796067ms
Feb 20 22:39:18.844: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02770071s
Feb 20 22:39:20.841: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025031566s
Feb 20 22:39:22.841: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025103072s
STEP: Saw pod success 02/20/23 22:39:22.841
Feb 20 22:39:22.842: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc" satisfied condition "Succeeded or Failed"
Feb 20 22:39:22.854: INFO: Trying to get logs from node 10.8.38.70 pod pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:39:22.887
Feb 20 22:39:22.919: INFO: Waiting for pod pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc to disappear
Feb 20 22:39:22.929: INFO: Pod pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:39:22.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6731" for this suite. 02/20/23 22:39:22.949
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":173,"skipped":3280,"failed":0}
------------------------------
• [SLOW TEST] [6.319 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:16.658
    Feb 20 22:39:16.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:39:16.66
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:16.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:16.735
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-c76a9502-7ad8-4670-ba54-87782a7031b5 02/20/23 22:39:16.746
    STEP: Creating a pod to test consume secrets 02/20/23 22:39:16.76
    Feb 20 22:39:16.816: INFO: Waiting up to 5m0s for pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc" in namespace "secrets-6731" to be "Succeeded or Failed"
    Feb 20 22:39:16.828: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.796067ms
    Feb 20 22:39:18.844: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02770071s
    Feb 20 22:39:20.841: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025031566s
    Feb 20 22:39:22.841: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025103072s
    STEP: Saw pod success 02/20/23 22:39:22.841
    Feb 20 22:39:22.842: INFO: Pod "pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc" satisfied condition "Succeeded or Failed"
    Feb 20 22:39:22.854: INFO: Trying to get logs from node 10.8.38.70 pod pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:39:22.887
    Feb 20 22:39:22.919: INFO: Waiting for pod pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc to disappear
    Feb 20 22:39:22.929: INFO: Pod pod-secrets-4591b031-992f-4fd8-9fc1-393908c17dcc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:39:22.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6731" for this suite. 02/20/23 22:39:22.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:22.984
Feb 20 22:39:22.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:39:22.986
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:23.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:23.062
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 02/20/23 22:39:23.072
Feb 20 22:39:23.135: INFO: Waiting up to 5m0s for pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f" in namespace "downward-api-293" to be "Succeeded or Failed"
Feb 20 22:39:23.146: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.565855ms
Feb 20 22:39:25.158: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023496736s
Feb 20 22:39:27.158: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023396227s
Feb 20 22:39:29.159: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024381829s
STEP: Saw pod success 02/20/23 22:39:29.159
Feb 20 22:39:29.160: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f" satisfied condition "Succeeded or Failed"
Feb 20 22:39:29.170: INFO: Trying to get logs from node 10.8.38.70 pod downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:39:29.194
Feb 20 22:39:29.225: INFO: Waiting for pod downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f to disappear
Feb 20 22:39:29.245: INFO: Pod downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 20 22:39:29.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-293" for this suite. 02/20/23 22:39:29.27
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":174,"skipped":3288,"failed":0}
------------------------------
• [SLOW TEST] [6.315 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:22.984
    Feb 20 22:39:22.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:39:22.986
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:23.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:23.062
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 02/20/23 22:39:23.072
    Feb 20 22:39:23.135: INFO: Waiting up to 5m0s for pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f" in namespace "downward-api-293" to be "Succeeded or Failed"
    Feb 20 22:39:23.146: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.565855ms
    Feb 20 22:39:25.158: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023496736s
    Feb 20 22:39:27.158: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023396227s
    Feb 20 22:39:29.159: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024381829s
    STEP: Saw pod success 02/20/23 22:39:29.159
    Feb 20 22:39:29.160: INFO: Pod "downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f" satisfied condition "Succeeded or Failed"
    Feb 20 22:39:29.170: INFO: Trying to get logs from node 10.8.38.70 pod downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:39:29.194
    Feb 20 22:39:29.225: INFO: Waiting for pod downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f to disappear
    Feb 20 22:39:29.245: INFO: Pod downward-api-a18c95ab-a9c3-4fc0-94f6-a4704840244f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 20 22:39:29.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-293" for this suite. 02/20/23 22:39:29.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:29.303
Feb 20 22:39:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 22:39:29.305
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:29.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:29.402
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 02/20/23 22:39:29.416
Feb 20 22:39:29.459: INFO: Waiting up to 5m0s for pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a" in namespace "pods-8515" to be "running and ready"
Feb 20 22:39:29.470: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.745587ms
Feb 20 22:39:29.470: INFO: The phase of Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:39:31.481: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021977078s
Feb 20 22:39:31.481: INFO: The phase of Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:39:33.483: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a": Phase="Running", Reason="", readiness=true. Elapsed: 4.023838012s
Feb 20 22:39:33.483: INFO: The phase of Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a is Running (Ready = true)
Feb 20 22:39:33.484: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a" satisfied condition "running and ready"
Feb 20 22:39:33.507: INFO: Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a has hostIP: 10.8.38.70
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 22:39:33.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8515" for this suite. 02/20/23 22:39:33.528
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":175,"skipped":3312,"failed":0}
------------------------------
• [4.250 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:29.303
    Feb 20 22:39:29.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 22:39:29.305
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:29.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:29.402
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 02/20/23 22:39:29.416
    Feb 20 22:39:29.459: INFO: Waiting up to 5m0s for pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a" in namespace "pods-8515" to be "running and ready"
    Feb 20 22:39:29.470: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.745587ms
    Feb 20 22:39:29.470: INFO: The phase of Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:39:31.481: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021977078s
    Feb 20 22:39:31.481: INFO: The phase of Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:39:33.483: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a": Phase="Running", Reason="", readiness=true. Elapsed: 4.023838012s
    Feb 20 22:39:33.483: INFO: The phase of Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a is Running (Ready = true)
    Feb 20 22:39:33.484: INFO: Pod "pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a" satisfied condition "running and ready"
    Feb 20 22:39:33.507: INFO: Pod pod-hostip-5cde7ff2-213f-49e6-a9ad-d6e73331e35a has hostIP: 10.8.38.70
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 22:39:33.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8515" for this suite. 02/20/23 22:39:33.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:33.554
Feb 20 22:39:33.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 22:39:33.557
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:33.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:33.627
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/20/23 22:39:33.646
STEP: delete the rc 02/20/23 22:39:38.671
STEP: wait for all pods to be garbage collected 02/20/23 22:39:38.683
STEP: Gathering metrics 02/20/23 22:39:43.704
W0220 22:39:43.729189      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 20 22:39:43.729: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 22:39:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9401" for this suite. 02/20/23 22:39:43.746
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":176,"skipped":3322,"failed":0}
------------------------------
• [SLOW TEST] [10.215 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:33.554
    Feb 20 22:39:33.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 22:39:33.557
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:33.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:33.627
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/20/23 22:39:33.646
    STEP: delete the rc 02/20/23 22:39:38.671
    STEP: wait for all pods to be garbage collected 02/20/23 22:39:38.683
    STEP: Gathering metrics 02/20/23 22:39:43.704
    W0220 22:39:43.729189      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 20 22:39:43.729: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 22:39:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9401" for this suite. 02/20/23 22:39:43.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:43.771
Feb 20 22:39:43.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:39:43.773
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:43.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:43.836
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:39:43.847
Feb 20 22:39:43.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c" in namespace "downward-api-7004" to be "Succeeded or Failed"
Feb 20 22:39:43.924: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.828469ms
Feb 20 22:39:45.937: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025017326s
Feb 20 22:39:47.936: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024125052s
Feb 20 22:39:49.937: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024685516s
STEP: Saw pod success 02/20/23 22:39:49.937
Feb 20 22:39:49.937: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c" satisfied condition "Succeeded or Failed"
Feb 20 22:39:49.951: INFO: Trying to get logs from node 10.8.38.70 pod downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c container client-container: <nil>
STEP: delete the pod 02/20/23 22:39:49.971
Feb 20 22:39:50.006: INFO: Waiting for pod downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c to disappear
Feb 20 22:39:50.019: INFO: Pod downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:39:50.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7004" for this suite. 02/20/23 22:39:50.037
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":177,"skipped":3329,"failed":0}
------------------------------
• [SLOW TEST] [6.292 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:43.771
    Feb 20 22:39:43.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:39:43.773
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:43.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:43.836
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:39:43.847
    Feb 20 22:39:43.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c" in namespace "downward-api-7004" to be "Succeeded or Failed"
    Feb 20 22:39:43.924: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.828469ms
    Feb 20 22:39:45.937: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025017326s
    Feb 20 22:39:47.936: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024125052s
    Feb 20 22:39:49.937: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024685516s
    STEP: Saw pod success 02/20/23 22:39:49.937
    Feb 20 22:39:49.937: INFO: Pod "downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c" satisfied condition "Succeeded or Failed"
    Feb 20 22:39:49.951: INFO: Trying to get logs from node 10.8.38.70 pod downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c container client-container: <nil>
    STEP: delete the pod 02/20/23 22:39:49.971
    Feb 20 22:39:50.006: INFO: Waiting for pod downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c to disappear
    Feb 20 22:39:50.019: INFO: Pod downwardapi-volume-be88ba28-d5ea-443c-8f74-b96c161d077c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:39:50.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7004" for this suite. 02/20/23 22:39:50.037
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:50.066
Feb 20 22:39:50.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:39:50.069
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:50.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:50.126
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/20/23 22:39:50.139
Feb 20 22:39:50.195: INFO: Waiting up to 5m0s for pod "pod-da464814-7052-47c7-b28c-656ef5506ed8" in namespace "emptydir-5531" to be "Succeeded or Failed"
Feb 20 22:39:50.209: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.405977ms
Feb 20 22:39:52.221: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025072636s
Feb 20 22:39:54.224: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02769663s
STEP: Saw pod success 02/20/23 22:39:54.226
Feb 20 22:39:54.226: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8" satisfied condition "Succeeded or Failed"
Feb 20 22:39:54.237: INFO: Trying to get logs from node 10.8.38.70 pod pod-da464814-7052-47c7-b28c-656ef5506ed8 container test-container: <nil>
STEP: delete the pod 02/20/23 22:39:54.258
Feb 20 22:39:54.291: INFO: Waiting for pod pod-da464814-7052-47c7-b28c-656ef5506ed8 to disappear
Feb 20 22:39:54.302: INFO: Pod pod-da464814-7052-47c7-b28c-656ef5506ed8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:39:54.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5531" for this suite. 02/20/23 22:39:54.322
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":3330,"failed":0}
------------------------------
• [4.297 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:50.066
    Feb 20 22:39:50.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:39:50.069
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:50.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:50.126
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/20/23 22:39:50.139
    Feb 20 22:39:50.195: INFO: Waiting up to 5m0s for pod "pod-da464814-7052-47c7-b28c-656ef5506ed8" in namespace "emptydir-5531" to be "Succeeded or Failed"
    Feb 20 22:39:50.209: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.405977ms
    Feb 20 22:39:52.221: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025072636s
    Feb 20 22:39:54.224: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02769663s
    STEP: Saw pod success 02/20/23 22:39:54.226
    Feb 20 22:39:54.226: INFO: Pod "pod-da464814-7052-47c7-b28c-656ef5506ed8" satisfied condition "Succeeded or Failed"
    Feb 20 22:39:54.237: INFO: Trying to get logs from node 10.8.38.70 pod pod-da464814-7052-47c7-b28c-656ef5506ed8 container test-container: <nil>
    STEP: delete the pod 02/20/23 22:39:54.258
    Feb 20 22:39:54.291: INFO: Waiting for pod pod-da464814-7052-47c7-b28c-656ef5506ed8 to disappear
    Feb 20 22:39:54.302: INFO: Pod pod-da464814-7052-47c7-b28c-656ef5506ed8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:39:54.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5531" for this suite. 02/20/23 22:39:54.322
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:39:54.364
Feb 20 22:39:54.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:39:54.368
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:54.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:54.453
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 02/20/23 22:39:54.47
STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:39:54.482
STEP: Creating a ResourceQuota with not terminating scope 02/20/23 22:39:56.509
STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:39:56.527
STEP: Creating a long running pod 02/20/23 22:39:58.537
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/20/23 22:39:58.608
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/20/23 22:40:00.619
STEP: Deleting the pod 02/20/23 22:40:02.638
STEP: Ensuring resource quota status released the pod usage 02/20/23 22:40:02.686
STEP: Creating a terminating pod 02/20/23 22:40:04.697
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/20/23 22:40:04.74
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/20/23 22:40:06.752
STEP: Deleting the pod 02/20/23 22:40:08.762
STEP: Ensuring resource quota status released the pod usage 02/20/23 22:40:08.794
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:40:10.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2854" for this suite. 02/20/23 22:40:10.823
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":179,"skipped":3334,"failed":0}
------------------------------
• [SLOW TEST] [16.484 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:39:54.364
    Feb 20 22:39:54.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:39:54.368
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:39:54.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:39:54.453
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 02/20/23 22:39:54.47
    STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:39:54.482
    STEP: Creating a ResourceQuota with not terminating scope 02/20/23 22:39:56.509
    STEP: Ensuring ResourceQuota status is calculated 02/20/23 22:39:56.527
    STEP: Creating a long running pod 02/20/23 22:39:58.537
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/20/23 22:39:58.608
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/20/23 22:40:00.619
    STEP: Deleting the pod 02/20/23 22:40:02.638
    STEP: Ensuring resource quota status released the pod usage 02/20/23 22:40:02.686
    STEP: Creating a terminating pod 02/20/23 22:40:04.697
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/20/23 22:40:04.74
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/20/23 22:40:06.752
    STEP: Deleting the pod 02/20/23 22:40:08.762
    STEP: Ensuring resource quota status released the pod usage 02/20/23 22:40:08.794
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:40:10.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2854" for this suite. 02/20/23 22:40:10.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:40:10.85
Feb 20 22:40:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 22:40:10.852
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:40:10.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:40:10.907
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-571 02/20/23 22:40:10.918
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
W0220 22:40:10.954468      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:40:10.966: INFO: Found 0 stateful pods, waiting for 1
Feb 20 22:40:20.979: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/20/23 22:40:21.001
W0220 22:40:21.015503      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 20 22:40:21.035: INFO: Found 1 stateful pods, waiting for 2
Feb 20 22:40:31.048: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 22:40:31.049: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/20/23 22:40:31.069
STEP: Delete all of the StatefulSets 02/20/23 22:40:31.078
STEP: Verify that StatefulSets have been deleted 02/20/23 22:40:31.093
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 22:40:31.102: INFO: Deleting all statefulset in ns statefulset-571
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 22:40:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-571" for this suite. 02/20/23 22:40:31.144
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":180,"skipped":3365,"failed":0}
------------------------------
• [SLOW TEST] [20.330 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:40:10.85
    Feb 20 22:40:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 22:40:10.852
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:40:10.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:40:10.907
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-571 02/20/23 22:40:10.918
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    W0220 22:40:10.954468      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:40:10.966: INFO: Found 0 stateful pods, waiting for 1
    Feb 20 22:40:20.979: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/20/23 22:40:21.001
    W0220 22:40:21.015503      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 20 22:40:21.035: INFO: Found 1 stateful pods, waiting for 2
    Feb 20 22:40:31.048: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 22:40:31.049: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/20/23 22:40:31.069
    STEP: Delete all of the StatefulSets 02/20/23 22:40:31.078
    STEP: Verify that StatefulSets have been deleted 02/20/23 22:40:31.093
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 22:40:31.102: INFO: Deleting all statefulset in ns statefulset-571
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 22:40:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-571" for this suite. 02/20/23 22:40:31.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:40:31.181
Feb 20 22:40:31.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:40:31.182
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:40:31.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:40:31.236
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
Feb 20 22:40:31.265: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-b3f4cd57-cb2e-4f3d-bc9f-c0c6ca68da37 02/20/23 22:40:31.265
STEP: Creating configMap with name cm-test-opt-upd-92573e43-114a-4460-a182-44ff7d8b92b3 02/20/23 22:40:31.28
STEP: Creating the pod 02/20/23 22:40:31.298
Feb 20 22:40:31.343: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7" in namespace "projected-5077" to be "running and ready"
Feb 20 22:40:31.355: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.853923ms
Feb 20 22:40:31.355: INFO: The phase of Pod pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:40:33.367: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024129417s
Feb 20 22:40:33.367: INFO: The phase of Pod pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:40:35.368: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7": Phase="Running", Reason="", readiness=true. Elapsed: 4.025173462s
Feb 20 22:40:35.368: INFO: The phase of Pod pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7 is Running (Ready = true)
Feb 20 22:40:35.369: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b3f4cd57-cb2e-4f3d-bc9f-c0c6ca68da37 02/20/23 22:40:35.478
STEP: Updating configmap cm-test-opt-upd-92573e43-114a-4460-a182-44ff7d8b92b3 02/20/23 22:40:35.496
STEP: Creating configMap with name cm-test-opt-create-af40feca-feb7-4532-8b18-a0ef4df21ee9 02/20/23 22:40:35.51
STEP: waiting to observe update in volume 02/20/23 22:40:35.524
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 22:41:44.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5077" for this suite. 02/20/23 22:41:44.762
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":181,"skipped":3374,"failed":0}
------------------------------
• [SLOW TEST] [73.605 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:40:31.181
    Feb 20 22:40:31.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:40:31.182
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:40:31.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:40:31.236
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    Feb 20 22:40:31.265: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-b3f4cd57-cb2e-4f3d-bc9f-c0c6ca68da37 02/20/23 22:40:31.265
    STEP: Creating configMap with name cm-test-opt-upd-92573e43-114a-4460-a182-44ff7d8b92b3 02/20/23 22:40:31.28
    STEP: Creating the pod 02/20/23 22:40:31.298
    Feb 20 22:40:31.343: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7" in namespace "projected-5077" to be "running and ready"
    Feb 20 22:40:31.355: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.853923ms
    Feb 20 22:40:31.355: INFO: The phase of Pod pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:40:33.367: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024129417s
    Feb 20 22:40:33.367: INFO: The phase of Pod pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:40:35.368: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7": Phase="Running", Reason="", readiness=true. Elapsed: 4.025173462s
    Feb 20 22:40:35.368: INFO: The phase of Pod pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7 is Running (Ready = true)
    Feb 20 22:40:35.369: INFO: Pod "pod-projected-configmaps-731b1ce7-75a6-45f9-8ad2-4c77cd5efcc7" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b3f4cd57-cb2e-4f3d-bc9f-c0c6ca68da37 02/20/23 22:40:35.478
    STEP: Updating configmap cm-test-opt-upd-92573e43-114a-4460-a182-44ff7d8b92b3 02/20/23 22:40:35.496
    STEP: Creating configMap with name cm-test-opt-create-af40feca-feb7-4532-8b18-a0ef4df21ee9 02/20/23 22:40:35.51
    STEP: waiting to observe update in volume 02/20/23 22:40:35.524
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 22:41:44.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5077" for this suite. 02/20/23 22:41:44.762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:41:44.787
Feb 20 22:41:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 22:41:44.789
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:41:44.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:41:44.893
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/20/23 22:41:44.925
STEP: delete the rc 02/20/23 22:41:49.952
STEP: wait for the rc to be deleted 02/20/23 22:41:49.964
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/20/23 22:41:54.98
STEP: Gathering metrics 02/20/23 22:42:25.016
W0220 22:42:25.039856      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 20 22:42:25.040: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 20 22:42:25.040: INFO: Deleting pod "simpletest.rc-25kq9" in namespace "gc-9467"
Feb 20 22:42:25.083: INFO: Deleting pod "simpletest.rc-2knvf" in namespace "gc-9467"
Feb 20 22:42:25.108: INFO: Deleting pod "simpletest.rc-45bzk" in namespace "gc-9467"
Feb 20 22:42:25.142: INFO: Deleting pod "simpletest.rc-4m5tm" in namespace "gc-9467"
Feb 20 22:42:25.168: INFO: Deleting pod "simpletest.rc-4zvst" in namespace "gc-9467"
Feb 20 22:42:25.202: INFO: Deleting pod "simpletest.rc-54l72" in namespace "gc-9467"
Feb 20 22:42:25.242: INFO: Deleting pod "simpletest.rc-5qv9n" in namespace "gc-9467"
Feb 20 22:42:25.275: INFO: Deleting pod "simpletest.rc-5tmrj" in namespace "gc-9467"
Feb 20 22:42:25.306: INFO: Deleting pod "simpletest.rc-629zf" in namespace "gc-9467"
Feb 20 22:42:25.342: INFO: Deleting pod "simpletest.rc-64n6b" in namespace "gc-9467"
Feb 20 22:42:25.377: INFO: Deleting pod "simpletest.rc-66xt7" in namespace "gc-9467"
Feb 20 22:42:25.424: INFO: Deleting pod "simpletest.rc-6q56h" in namespace "gc-9467"
Feb 20 22:42:25.454: INFO: Deleting pod "simpletest.rc-789p8" in namespace "gc-9467"
Feb 20 22:42:25.495: INFO: Deleting pod "simpletest.rc-82l4r" in namespace "gc-9467"
Feb 20 22:42:25.529: INFO: Deleting pod "simpletest.rc-86wpz" in namespace "gc-9467"
Feb 20 22:42:25.580: INFO: Deleting pod "simpletest.rc-8hkj9" in namespace "gc-9467"
Feb 20 22:42:25.640: INFO: Deleting pod "simpletest.rc-8lrnj" in namespace "gc-9467"
Feb 20 22:42:25.667: INFO: Deleting pod "simpletest.rc-8sgn7" in namespace "gc-9467"
Feb 20 22:42:25.755: INFO: Deleting pod "simpletest.rc-95mgg" in namespace "gc-9467"
Feb 20 22:42:25.791: INFO: Deleting pod "simpletest.rc-96rvb" in namespace "gc-9467"
Feb 20 22:42:25.820: INFO: Deleting pod "simpletest.rc-96rwb" in namespace "gc-9467"
Feb 20 22:42:25.857: INFO: Deleting pod "simpletest.rc-9c7kr" in namespace "gc-9467"
Feb 20 22:42:25.896: INFO: Deleting pod "simpletest.rc-9n2zx" in namespace "gc-9467"
Feb 20 22:42:25.926: INFO: Deleting pod "simpletest.rc-bftth" in namespace "gc-9467"
Feb 20 22:42:25.953: INFO: Deleting pod "simpletest.rc-br72v" in namespace "gc-9467"
Feb 20 22:42:25.983: INFO: Deleting pod "simpletest.rc-bsr6f" in namespace "gc-9467"
Feb 20 22:42:26.022: INFO: Deleting pod "simpletest.rc-c9r8g" in namespace "gc-9467"
Feb 20 22:42:26.050: INFO: Deleting pod "simpletest.rc-ct4cb" in namespace "gc-9467"
Feb 20 22:42:26.098: INFO: Deleting pod "simpletest.rc-cv5sv" in namespace "gc-9467"
Feb 20 22:42:26.145: INFO: Deleting pod "simpletest.rc-cwrqk" in namespace "gc-9467"
Feb 20 22:42:26.181: INFO: Deleting pod "simpletest.rc-cxn9d" in namespace "gc-9467"
Feb 20 22:42:26.243: INFO: Deleting pod "simpletest.rc-czbmr" in namespace "gc-9467"
Feb 20 22:42:26.291: INFO: Deleting pod "simpletest.rc-d8spj" in namespace "gc-9467"
Feb 20 22:42:26.315: INFO: Deleting pod "simpletest.rc-drlsw" in namespace "gc-9467"
Feb 20 22:42:26.360: INFO: Deleting pod "simpletest.rc-dt2sr" in namespace "gc-9467"
Feb 20 22:42:26.403: INFO: Deleting pod "simpletest.rc-dvnsf" in namespace "gc-9467"
Feb 20 22:42:26.455: INFO: Deleting pod "simpletest.rc-dzw8f" in namespace "gc-9467"
Feb 20 22:42:26.497: INFO: Deleting pod "simpletest.rc-f4lbh" in namespace "gc-9467"
Feb 20 22:42:26.530: INFO: Deleting pod "simpletest.rc-flv58" in namespace "gc-9467"
Feb 20 22:42:26.561: INFO: Deleting pod "simpletest.rc-gkv22" in namespace "gc-9467"
Feb 20 22:42:26.593: INFO: Deleting pod "simpletest.rc-gt2n8" in namespace "gc-9467"
Feb 20 22:42:26.643: INFO: Deleting pod "simpletest.rc-gt62v" in namespace "gc-9467"
Feb 20 22:42:26.692: INFO: Deleting pod "simpletest.rc-hcpjw" in namespace "gc-9467"
Feb 20 22:42:26.790: INFO: Deleting pod "simpletest.rc-hm7fd" in namespace "gc-9467"
Feb 20 22:42:26.827: INFO: Deleting pod "simpletest.rc-hrvs4" in namespace "gc-9467"
Feb 20 22:42:26.886: INFO: Deleting pod "simpletest.rc-hrvst" in namespace "gc-9467"
Feb 20 22:42:26.912: INFO: Deleting pod "simpletest.rc-jbcjr" in namespace "gc-9467"
Feb 20 22:42:26.945: INFO: Deleting pod "simpletest.rc-jdkkf" in namespace "gc-9467"
Feb 20 22:42:26.978: INFO: Deleting pod "simpletest.rc-jgbmw" in namespace "gc-9467"
Feb 20 22:42:27.013: INFO: Deleting pod "simpletest.rc-jgkcd" in namespace "gc-9467"
Feb 20 22:42:27.051: INFO: Deleting pod "simpletest.rc-jnr9p" in namespace "gc-9467"
Feb 20 22:42:27.101: INFO: Deleting pod "simpletest.rc-jpflk" in namespace "gc-9467"
Feb 20 22:42:27.145: INFO: Deleting pod "simpletest.rc-k5dj9" in namespace "gc-9467"
Feb 20 22:42:27.204: INFO: Deleting pod "simpletest.rc-kfpzq" in namespace "gc-9467"
Feb 20 22:42:27.240: INFO: Deleting pod "simpletest.rc-kpl2b" in namespace "gc-9467"
Feb 20 22:42:27.290: INFO: Deleting pod "simpletest.rc-kwv94" in namespace "gc-9467"
Feb 20 22:42:27.317: INFO: Deleting pod "simpletest.rc-l2khc" in namespace "gc-9467"
Feb 20 22:42:27.351: INFO: Deleting pod "simpletest.rc-l5cqt" in namespace "gc-9467"
Feb 20 22:42:27.380: INFO: Deleting pod "simpletest.rc-l8vlh" in namespace "gc-9467"
Feb 20 22:42:27.407: INFO: Deleting pod "simpletest.rc-ljhjc" in namespace "gc-9467"
Feb 20 22:42:27.442: INFO: Deleting pod "simpletest.rc-lq5sg" in namespace "gc-9467"
Feb 20 22:42:27.480: INFO: Deleting pod "simpletest.rc-lt7cv" in namespace "gc-9467"
Feb 20 22:42:27.512: INFO: Deleting pod "simpletest.rc-m26m4" in namespace "gc-9467"
Feb 20 22:42:27.596: INFO: Deleting pod "simpletest.rc-m8rvc" in namespace "gc-9467"
Feb 20 22:42:27.631: INFO: Deleting pod "simpletest.rc-mg9qm" in namespace "gc-9467"
Feb 20 22:42:27.660: INFO: Deleting pod "simpletest.rc-mk574" in namespace "gc-9467"
Feb 20 22:42:27.702: INFO: Deleting pod "simpletest.rc-nbfhs" in namespace "gc-9467"
Feb 20 22:42:27.736: INFO: Deleting pod "simpletest.rc-ngx87" in namespace "gc-9467"
Feb 20 22:42:27.773: INFO: Deleting pod "simpletest.rc-nnpjc" in namespace "gc-9467"
Feb 20 22:42:27.806: INFO: Deleting pod "simpletest.rc-pgjgn" in namespace "gc-9467"
Feb 20 22:42:27.837: INFO: Deleting pod "simpletest.rc-pj9k4" in namespace "gc-9467"
Feb 20 22:42:27.861: INFO: Deleting pod "simpletest.rc-plmqq" in namespace "gc-9467"
Feb 20 22:42:27.887: INFO: Deleting pod "simpletest.rc-plxhp" in namespace "gc-9467"
Feb 20 22:42:27.921: INFO: Deleting pod "simpletest.rc-pm9tp" in namespace "gc-9467"
Feb 20 22:42:27.951: INFO: Deleting pod "simpletest.rc-ppn7h" in namespace "gc-9467"
Feb 20 22:42:28.017: INFO: Deleting pod "simpletest.rc-ppxqc" in namespace "gc-9467"
Feb 20 22:42:28.045: INFO: Deleting pod "simpletest.rc-pth6h" in namespace "gc-9467"
Feb 20 22:42:28.077: INFO: Deleting pod "simpletest.rc-qbxw5" in namespace "gc-9467"
Feb 20 22:42:28.112: INFO: Deleting pod "simpletest.rc-qm6bt" in namespace "gc-9467"
Feb 20 22:42:28.154: INFO: Deleting pod "simpletest.rc-qm6d2" in namespace "gc-9467"
Feb 20 22:42:28.186: INFO: Deleting pod "simpletest.rc-rd8k2" in namespace "gc-9467"
Feb 20 22:42:28.216: INFO: Deleting pod "simpletest.rc-rrwp4" in namespace "gc-9467"
Feb 20 22:42:28.249: INFO: Deleting pod "simpletest.rc-s6m9n" in namespace "gc-9467"
Feb 20 22:42:28.275: INFO: Deleting pod "simpletest.rc-sgdx6" in namespace "gc-9467"
Feb 20 22:42:28.312: INFO: Deleting pod "simpletest.rc-sr86n" in namespace "gc-9467"
Feb 20 22:42:28.344: INFO: Deleting pod "simpletest.rc-swfgg" in namespace "gc-9467"
Feb 20 22:42:28.374: INFO: Deleting pod "simpletest.rc-tnhdg" in namespace "gc-9467"
Feb 20 22:42:28.399: INFO: Deleting pod "simpletest.rc-tpglz" in namespace "gc-9467"
Feb 20 22:42:28.432: INFO: Deleting pod "simpletest.rc-v7zm6" in namespace "gc-9467"
Feb 20 22:42:28.467: INFO: Deleting pod "simpletest.rc-vbt8p" in namespace "gc-9467"
Feb 20 22:42:28.548: INFO: Deleting pod "simpletest.rc-vdbcs" in namespace "gc-9467"
Feb 20 22:42:28.576: INFO: Deleting pod "simpletest.rc-vgc7j" in namespace "gc-9467"
Feb 20 22:42:28.610: INFO: Deleting pod "simpletest.rc-vptcd" in namespace "gc-9467"
Feb 20 22:42:28.654: INFO: Deleting pod "simpletest.rc-vvlnq" in namespace "gc-9467"
Feb 20 22:42:28.712: INFO: Deleting pod "simpletest.rc-whqlg" in namespace "gc-9467"
Feb 20 22:42:28.768: INFO: Deleting pod "simpletest.rc-wpxnd" in namespace "gc-9467"
Feb 20 22:42:28.815: INFO: Deleting pod "simpletest.rc-x25wn" in namespace "gc-9467"
Feb 20 22:42:28.856: INFO: Deleting pod "simpletest.rc-x495z" in namespace "gc-9467"
Feb 20 22:42:28.892: INFO: Deleting pod "simpletest.rc-xtplq" in namespace "gc-9467"
Feb 20 22:42:28.948: INFO: Deleting pod "simpletest.rc-zx8jt" in namespace "gc-9467"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 22:42:28.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9467" for this suite. 02/20/23 22:42:29.014
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":182,"skipped":3377,"failed":0}
------------------------------
• [SLOW TEST] [44.252 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:41:44.787
    Feb 20 22:41:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 22:41:44.789
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:41:44.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:41:44.893
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/20/23 22:41:44.925
    STEP: delete the rc 02/20/23 22:41:49.952
    STEP: wait for the rc to be deleted 02/20/23 22:41:49.964
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/20/23 22:41:54.98
    STEP: Gathering metrics 02/20/23 22:42:25.016
    W0220 22:42:25.039856      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 20 22:42:25.040: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 20 22:42:25.040: INFO: Deleting pod "simpletest.rc-25kq9" in namespace "gc-9467"
    Feb 20 22:42:25.083: INFO: Deleting pod "simpletest.rc-2knvf" in namespace "gc-9467"
    Feb 20 22:42:25.108: INFO: Deleting pod "simpletest.rc-45bzk" in namespace "gc-9467"
    Feb 20 22:42:25.142: INFO: Deleting pod "simpletest.rc-4m5tm" in namespace "gc-9467"
    Feb 20 22:42:25.168: INFO: Deleting pod "simpletest.rc-4zvst" in namespace "gc-9467"
    Feb 20 22:42:25.202: INFO: Deleting pod "simpletest.rc-54l72" in namespace "gc-9467"
    Feb 20 22:42:25.242: INFO: Deleting pod "simpletest.rc-5qv9n" in namespace "gc-9467"
    Feb 20 22:42:25.275: INFO: Deleting pod "simpletest.rc-5tmrj" in namespace "gc-9467"
    Feb 20 22:42:25.306: INFO: Deleting pod "simpletest.rc-629zf" in namespace "gc-9467"
    Feb 20 22:42:25.342: INFO: Deleting pod "simpletest.rc-64n6b" in namespace "gc-9467"
    Feb 20 22:42:25.377: INFO: Deleting pod "simpletest.rc-66xt7" in namespace "gc-9467"
    Feb 20 22:42:25.424: INFO: Deleting pod "simpletest.rc-6q56h" in namespace "gc-9467"
    Feb 20 22:42:25.454: INFO: Deleting pod "simpletest.rc-789p8" in namespace "gc-9467"
    Feb 20 22:42:25.495: INFO: Deleting pod "simpletest.rc-82l4r" in namespace "gc-9467"
    Feb 20 22:42:25.529: INFO: Deleting pod "simpletest.rc-86wpz" in namespace "gc-9467"
    Feb 20 22:42:25.580: INFO: Deleting pod "simpletest.rc-8hkj9" in namespace "gc-9467"
    Feb 20 22:42:25.640: INFO: Deleting pod "simpletest.rc-8lrnj" in namespace "gc-9467"
    Feb 20 22:42:25.667: INFO: Deleting pod "simpletest.rc-8sgn7" in namespace "gc-9467"
    Feb 20 22:42:25.755: INFO: Deleting pod "simpletest.rc-95mgg" in namespace "gc-9467"
    Feb 20 22:42:25.791: INFO: Deleting pod "simpletest.rc-96rvb" in namespace "gc-9467"
    Feb 20 22:42:25.820: INFO: Deleting pod "simpletest.rc-96rwb" in namespace "gc-9467"
    Feb 20 22:42:25.857: INFO: Deleting pod "simpletest.rc-9c7kr" in namespace "gc-9467"
    Feb 20 22:42:25.896: INFO: Deleting pod "simpletest.rc-9n2zx" in namespace "gc-9467"
    Feb 20 22:42:25.926: INFO: Deleting pod "simpletest.rc-bftth" in namespace "gc-9467"
    Feb 20 22:42:25.953: INFO: Deleting pod "simpletest.rc-br72v" in namespace "gc-9467"
    Feb 20 22:42:25.983: INFO: Deleting pod "simpletest.rc-bsr6f" in namespace "gc-9467"
    Feb 20 22:42:26.022: INFO: Deleting pod "simpletest.rc-c9r8g" in namespace "gc-9467"
    Feb 20 22:42:26.050: INFO: Deleting pod "simpletest.rc-ct4cb" in namespace "gc-9467"
    Feb 20 22:42:26.098: INFO: Deleting pod "simpletest.rc-cv5sv" in namespace "gc-9467"
    Feb 20 22:42:26.145: INFO: Deleting pod "simpletest.rc-cwrqk" in namespace "gc-9467"
    Feb 20 22:42:26.181: INFO: Deleting pod "simpletest.rc-cxn9d" in namespace "gc-9467"
    Feb 20 22:42:26.243: INFO: Deleting pod "simpletest.rc-czbmr" in namespace "gc-9467"
    Feb 20 22:42:26.291: INFO: Deleting pod "simpletest.rc-d8spj" in namespace "gc-9467"
    Feb 20 22:42:26.315: INFO: Deleting pod "simpletest.rc-drlsw" in namespace "gc-9467"
    Feb 20 22:42:26.360: INFO: Deleting pod "simpletest.rc-dt2sr" in namespace "gc-9467"
    Feb 20 22:42:26.403: INFO: Deleting pod "simpletest.rc-dvnsf" in namespace "gc-9467"
    Feb 20 22:42:26.455: INFO: Deleting pod "simpletest.rc-dzw8f" in namespace "gc-9467"
    Feb 20 22:42:26.497: INFO: Deleting pod "simpletest.rc-f4lbh" in namespace "gc-9467"
    Feb 20 22:42:26.530: INFO: Deleting pod "simpletest.rc-flv58" in namespace "gc-9467"
    Feb 20 22:42:26.561: INFO: Deleting pod "simpletest.rc-gkv22" in namespace "gc-9467"
    Feb 20 22:42:26.593: INFO: Deleting pod "simpletest.rc-gt2n8" in namespace "gc-9467"
    Feb 20 22:42:26.643: INFO: Deleting pod "simpletest.rc-gt62v" in namespace "gc-9467"
    Feb 20 22:42:26.692: INFO: Deleting pod "simpletest.rc-hcpjw" in namespace "gc-9467"
    Feb 20 22:42:26.790: INFO: Deleting pod "simpletest.rc-hm7fd" in namespace "gc-9467"
    Feb 20 22:42:26.827: INFO: Deleting pod "simpletest.rc-hrvs4" in namespace "gc-9467"
    Feb 20 22:42:26.886: INFO: Deleting pod "simpletest.rc-hrvst" in namespace "gc-9467"
    Feb 20 22:42:26.912: INFO: Deleting pod "simpletest.rc-jbcjr" in namespace "gc-9467"
    Feb 20 22:42:26.945: INFO: Deleting pod "simpletest.rc-jdkkf" in namespace "gc-9467"
    Feb 20 22:42:26.978: INFO: Deleting pod "simpletest.rc-jgbmw" in namespace "gc-9467"
    Feb 20 22:42:27.013: INFO: Deleting pod "simpletest.rc-jgkcd" in namespace "gc-9467"
    Feb 20 22:42:27.051: INFO: Deleting pod "simpletest.rc-jnr9p" in namespace "gc-9467"
    Feb 20 22:42:27.101: INFO: Deleting pod "simpletest.rc-jpflk" in namespace "gc-9467"
    Feb 20 22:42:27.145: INFO: Deleting pod "simpletest.rc-k5dj9" in namespace "gc-9467"
    Feb 20 22:42:27.204: INFO: Deleting pod "simpletest.rc-kfpzq" in namespace "gc-9467"
    Feb 20 22:42:27.240: INFO: Deleting pod "simpletest.rc-kpl2b" in namespace "gc-9467"
    Feb 20 22:42:27.290: INFO: Deleting pod "simpletest.rc-kwv94" in namespace "gc-9467"
    Feb 20 22:42:27.317: INFO: Deleting pod "simpletest.rc-l2khc" in namespace "gc-9467"
    Feb 20 22:42:27.351: INFO: Deleting pod "simpletest.rc-l5cqt" in namespace "gc-9467"
    Feb 20 22:42:27.380: INFO: Deleting pod "simpletest.rc-l8vlh" in namespace "gc-9467"
    Feb 20 22:42:27.407: INFO: Deleting pod "simpletest.rc-ljhjc" in namespace "gc-9467"
    Feb 20 22:42:27.442: INFO: Deleting pod "simpletest.rc-lq5sg" in namespace "gc-9467"
    Feb 20 22:42:27.480: INFO: Deleting pod "simpletest.rc-lt7cv" in namespace "gc-9467"
    Feb 20 22:42:27.512: INFO: Deleting pod "simpletest.rc-m26m4" in namespace "gc-9467"
    Feb 20 22:42:27.596: INFO: Deleting pod "simpletest.rc-m8rvc" in namespace "gc-9467"
    Feb 20 22:42:27.631: INFO: Deleting pod "simpletest.rc-mg9qm" in namespace "gc-9467"
    Feb 20 22:42:27.660: INFO: Deleting pod "simpletest.rc-mk574" in namespace "gc-9467"
    Feb 20 22:42:27.702: INFO: Deleting pod "simpletest.rc-nbfhs" in namespace "gc-9467"
    Feb 20 22:42:27.736: INFO: Deleting pod "simpletest.rc-ngx87" in namespace "gc-9467"
    Feb 20 22:42:27.773: INFO: Deleting pod "simpletest.rc-nnpjc" in namespace "gc-9467"
    Feb 20 22:42:27.806: INFO: Deleting pod "simpletest.rc-pgjgn" in namespace "gc-9467"
    Feb 20 22:42:27.837: INFO: Deleting pod "simpletest.rc-pj9k4" in namespace "gc-9467"
    Feb 20 22:42:27.861: INFO: Deleting pod "simpletest.rc-plmqq" in namespace "gc-9467"
    Feb 20 22:42:27.887: INFO: Deleting pod "simpletest.rc-plxhp" in namespace "gc-9467"
    Feb 20 22:42:27.921: INFO: Deleting pod "simpletest.rc-pm9tp" in namespace "gc-9467"
    Feb 20 22:42:27.951: INFO: Deleting pod "simpletest.rc-ppn7h" in namespace "gc-9467"
    Feb 20 22:42:28.017: INFO: Deleting pod "simpletest.rc-ppxqc" in namespace "gc-9467"
    Feb 20 22:42:28.045: INFO: Deleting pod "simpletest.rc-pth6h" in namespace "gc-9467"
    Feb 20 22:42:28.077: INFO: Deleting pod "simpletest.rc-qbxw5" in namespace "gc-9467"
    Feb 20 22:42:28.112: INFO: Deleting pod "simpletest.rc-qm6bt" in namespace "gc-9467"
    Feb 20 22:42:28.154: INFO: Deleting pod "simpletest.rc-qm6d2" in namespace "gc-9467"
    Feb 20 22:42:28.186: INFO: Deleting pod "simpletest.rc-rd8k2" in namespace "gc-9467"
    Feb 20 22:42:28.216: INFO: Deleting pod "simpletest.rc-rrwp4" in namespace "gc-9467"
    Feb 20 22:42:28.249: INFO: Deleting pod "simpletest.rc-s6m9n" in namespace "gc-9467"
    Feb 20 22:42:28.275: INFO: Deleting pod "simpletest.rc-sgdx6" in namespace "gc-9467"
    Feb 20 22:42:28.312: INFO: Deleting pod "simpletest.rc-sr86n" in namespace "gc-9467"
    Feb 20 22:42:28.344: INFO: Deleting pod "simpletest.rc-swfgg" in namespace "gc-9467"
    Feb 20 22:42:28.374: INFO: Deleting pod "simpletest.rc-tnhdg" in namespace "gc-9467"
    Feb 20 22:42:28.399: INFO: Deleting pod "simpletest.rc-tpglz" in namespace "gc-9467"
    Feb 20 22:42:28.432: INFO: Deleting pod "simpletest.rc-v7zm6" in namespace "gc-9467"
    Feb 20 22:42:28.467: INFO: Deleting pod "simpletest.rc-vbt8p" in namespace "gc-9467"
    Feb 20 22:42:28.548: INFO: Deleting pod "simpletest.rc-vdbcs" in namespace "gc-9467"
    Feb 20 22:42:28.576: INFO: Deleting pod "simpletest.rc-vgc7j" in namespace "gc-9467"
    Feb 20 22:42:28.610: INFO: Deleting pod "simpletest.rc-vptcd" in namespace "gc-9467"
    Feb 20 22:42:28.654: INFO: Deleting pod "simpletest.rc-vvlnq" in namespace "gc-9467"
    Feb 20 22:42:28.712: INFO: Deleting pod "simpletest.rc-whqlg" in namespace "gc-9467"
    Feb 20 22:42:28.768: INFO: Deleting pod "simpletest.rc-wpxnd" in namespace "gc-9467"
    Feb 20 22:42:28.815: INFO: Deleting pod "simpletest.rc-x25wn" in namespace "gc-9467"
    Feb 20 22:42:28.856: INFO: Deleting pod "simpletest.rc-x495z" in namespace "gc-9467"
    Feb 20 22:42:28.892: INFO: Deleting pod "simpletest.rc-xtplq" in namespace "gc-9467"
    Feb 20 22:42:28.948: INFO: Deleting pod "simpletest.rc-zx8jt" in namespace "gc-9467"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 22:42:28.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9467" for this suite. 02/20/23 22:42:29.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:42:29.041
Feb 20 22:42:29.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename cronjob 02/20/23 22:42:29.043
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:42:29.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:42:29.135
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/20/23 22:42:29.148
W0220 22:42:29.163332      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 02/20/23 22:42:29.163
STEP: Ensuring exactly one is scheduled 02/20/23 22:43:01.177
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/20/23 22:43:01.188
STEP: Ensuring no more jobs are scheduled 02/20/23 22:43:01.196
STEP: Removing cronjob 02/20/23 22:48:01.221
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 20 22:48:01.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-874" for this suite. 02/20/23 22:48:01.258
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":183,"skipped":3416,"failed":0}
------------------------------
• [SLOW TEST] [332.243 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:42:29.041
    Feb 20 22:42:29.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename cronjob 02/20/23 22:42:29.043
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:42:29.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:42:29.135
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/20/23 22:42:29.148
    W0220 22:42:29.163332      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 02/20/23 22:42:29.163
    STEP: Ensuring exactly one is scheduled 02/20/23 22:43:01.177
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/20/23 22:43:01.188
    STEP: Ensuring no more jobs are scheduled 02/20/23 22:43:01.196
    STEP: Removing cronjob 02/20/23 22:48:01.221
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 20 22:48:01.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-874" for this suite. 02/20/23 22:48:01.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:01.289
Feb 20 22:48:01.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:48:01.291
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:01.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:01.359
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 02/20/23 22:48:01.441
STEP: waiting for available Endpoint 02/20/23 22:48:01.521
STEP: listing all Endpoints 02/20/23 22:48:01.53
STEP: updating the Endpoint 02/20/23 22:48:01.597
STEP: fetching the Endpoint 02/20/23 22:48:01.616
STEP: patching the Endpoint 02/20/23 22:48:01.626
STEP: fetching the Endpoint 02/20/23 22:48:01.647
STEP: deleting the Endpoint by Collection 02/20/23 22:48:01.659
STEP: waiting for Endpoint deletion 02/20/23 22:48:01.684
STEP: fetching the Endpoint 02/20/23 22:48:01.69
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:48:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6751" for this suite. 02/20/23 22:48:01.738
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":184,"skipped":3421,"failed":0}
------------------------------
• [0.475 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:01.289
    Feb 20 22:48:01.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:48:01.291
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:01.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:01.359
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 02/20/23 22:48:01.441
    STEP: waiting for available Endpoint 02/20/23 22:48:01.521
    STEP: listing all Endpoints 02/20/23 22:48:01.53
    STEP: updating the Endpoint 02/20/23 22:48:01.597
    STEP: fetching the Endpoint 02/20/23 22:48:01.616
    STEP: patching the Endpoint 02/20/23 22:48:01.626
    STEP: fetching the Endpoint 02/20/23 22:48:01.647
    STEP: deleting the Endpoint by Collection 02/20/23 22:48:01.659
    STEP: waiting for Endpoint deletion 02/20/23 22:48:01.684
    STEP: fetching the Endpoint 02/20/23 22:48:01.69
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:48:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6751" for this suite. 02/20/23 22:48:01.738
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:01.773
Feb 20 22:48:01.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename job 02/20/23 22:48:01.776
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:01.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:01.914
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 02/20/23 22:48:01.924
STEP: Ensuring active pods == parallelism 02/20/23 22:48:01.937
STEP: Orphaning one of the Job's Pods 02/20/23 22:48:05.952
Feb 20 22:48:06.503: INFO: Successfully updated pod "adopt-release-2hm5v"
STEP: Checking that the Job readopts the Pod 02/20/23 22:48:06.503
Feb 20 22:48:06.504: INFO: Waiting up to 15m0s for pod "adopt-release-2hm5v" in namespace "job-1934" to be "adopted"
Feb 20 22:48:06.518: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 13.858534ms
Feb 20 22:48:08.533: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 2.029017305s
Feb 20 22:48:08.533: INFO: Pod "adopt-release-2hm5v" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/20/23 22:48:08.533
Feb 20 22:48:09.117: INFO: Successfully updated pod "adopt-release-2hm5v"
STEP: Checking that the Job releases the Pod 02/20/23 22:48:09.117
Feb 20 22:48:09.117: INFO: Waiting up to 15m0s for pod "adopt-release-2hm5v" in namespace "job-1934" to be "released"
Feb 20 22:48:09.135: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 17.705321ms
Feb 20 22:48:11.150: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 2.03202097s
Feb 20 22:48:11.150: INFO: Pod "adopt-release-2hm5v" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 20 22:48:11.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1934" for this suite. 02/20/23 22:48:11.168
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":185,"skipped":3482,"failed":0}
------------------------------
• [SLOW TEST] [9.422 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:01.773
    Feb 20 22:48:01.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename job 02/20/23 22:48:01.776
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:01.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:01.914
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 02/20/23 22:48:01.924
    STEP: Ensuring active pods == parallelism 02/20/23 22:48:01.937
    STEP: Orphaning one of the Job's Pods 02/20/23 22:48:05.952
    Feb 20 22:48:06.503: INFO: Successfully updated pod "adopt-release-2hm5v"
    STEP: Checking that the Job readopts the Pod 02/20/23 22:48:06.503
    Feb 20 22:48:06.504: INFO: Waiting up to 15m0s for pod "adopt-release-2hm5v" in namespace "job-1934" to be "adopted"
    Feb 20 22:48:06.518: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 13.858534ms
    Feb 20 22:48:08.533: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 2.029017305s
    Feb 20 22:48:08.533: INFO: Pod "adopt-release-2hm5v" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/20/23 22:48:08.533
    Feb 20 22:48:09.117: INFO: Successfully updated pod "adopt-release-2hm5v"
    STEP: Checking that the Job releases the Pod 02/20/23 22:48:09.117
    Feb 20 22:48:09.117: INFO: Waiting up to 15m0s for pod "adopt-release-2hm5v" in namespace "job-1934" to be "released"
    Feb 20 22:48:09.135: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 17.705321ms
    Feb 20 22:48:11.150: INFO: Pod "adopt-release-2hm5v": Phase="Running", Reason="", readiness=true. Elapsed: 2.03202097s
    Feb 20 22:48:11.150: INFO: Pod "adopt-release-2hm5v" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 20 22:48:11.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1934" for this suite. 02/20/23 22:48:11.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:11.199
Feb 20 22:48:11.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:48:11.2
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:11.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:11.261
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-6961 02/20/23 22:48:11.274
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[] 02/20/23 22:48:11.308
Feb 20 22:48:11.347: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6961 02/20/23 22:48:11.347
Feb 20 22:48:11.406: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6961" to be "running and ready"
Feb 20 22:48:11.417: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.075497ms
Feb 20 22:48:11.417: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:48:13.431: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024770649s
Feb 20 22:48:13.431: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:48:15.433: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.02652562s
Feb 20 22:48:15.433: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 20 22:48:15.433: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[pod1:[80]] 02/20/23 22:48:15.444
Feb 20 22:48:15.477: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/20/23 22:48:15.477
Feb 20 22:48:15.477: INFO: Creating new exec pod
Feb 20 22:48:15.514: INFO: Waiting up to 5m0s for pod "execpodt2q7k" in namespace "services-6961" to be "running"
Feb 20 22:48:15.525: INFO: Pod "execpodt2q7k": Phase="Pending", Reason="", readiness=false. Elapsed: 10.836704ms
Feb 20 22:48:17.537: INFO: Pod "execpodt2q7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.022984656s
Feb 20 22:48:17.537: INFO: Pod "execpodt2q7k" satisfied condition "running"
Feb 20 22:48:18.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 20 22:48:18.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:18.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:48:18.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.82 80'
Feb 20 22:48:19.196: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.82 80\nConnection to 172.21.46.82 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:19.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6961 02/20/23 22:48:19.196
Feb 20 22:48:19.230: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6961" to be "running and ready"
Feb 20 22:48:19.249: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.975345ms
Feb 20 22:48:19.249: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:48:21.264: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034494428s
Feb 20 22:48:21.264: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:48:23.264: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034203013s
Feb 20 22:48:23.264: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 20 22:48:23.264: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[pod1:[80] pod2:[80]] 02/20/23 22:48:23.286
Feb 20 22:48:23.334: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/20/23 22:48:23.335
Feb 20 22:48:24.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 20 22:48:24.591: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:24.591: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:48:24.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.82 80'
Feb 20 22:48:24.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.82 80\nConnection to 172.21.46.82 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:24.854: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6961 02/20/23 22:48:24.854
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[pod2:[80]] 02/20/23 22:48:24.891
Feb 20 22:48:25.957: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/20/23 22:48:25.957
Feb 20 22:48:26.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 20 22:48:27.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:27.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:48:27.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.82 80'
Feb 20 22:48:27.522: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.82 80\nConnection to 172.21.46.82 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:27.522: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6961 02/20/23 22:48:27.522
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[] 02/20/23 22:48:27.561
Feb 20 22:48:28.627: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:48:28.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6961" for this suite. 02/20/23 22:48:28.684
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":186,"skipped":3494,"failed":0}
------------------------------
• [SLOW TEST] [17.513 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:11.199
    Feb 20 22:48:11.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:48:11.2
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:11.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:11.261
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-6961 02/20/23 22:48:11.274
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[] 02/20/23 22:48:11.308
    Feb 20 22:48:11.347: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6961 02/20/23 22:48:11.347
    Feb 20 22:48:11.406: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6961" to be "running and ready"
    Feb 20 22:48:11.417: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.075497ms
    Feb 20 22:48:11.417: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:48:13.431: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024770649s
    Feb 20 22:48:13.431: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:48:15.433: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.02652562s
    Feb 20 22:48:15.433: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 20 22:48:15.433: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[pod1:[80]] 02/20/23 22:48:15.444
    Feb 20 22:48:15.477: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/20/23 22:48:15.477
    Feb 20 22:48:15.477: INFO: Creating new exec pod
    Feb 20 22:48:15.514: INFO: Waiting up to 5m0s for pod "execpodt2q7k" in namespace "services-6961" to be "running"
    Feb 20 22:48:15.525: INFO: Pod "execpodt2q7k": Phase="Pending", Reason="", readiness=false. Elapsed: 10.836704ms
    Feb 20 22:48:17.537: INFO: Pod "execpodt2q7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.022984656s
    Feb 20 22:48:17.537: INFO: Pod "execpodt2q7k" satisfied condition "running"
    Feb 20 22:48:18.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 20 22:48:18.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:18.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:48:18.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.82 80'
    Feb 20 22:48:19.196: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.82 80\nConnection to 172.21.46.82 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:19.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-6961 02/20/23 22:48:19.196
    Feb 20 22:48:19.230: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6961" to be "running and ready"
    Feb 20 22:48:19.249: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.975345ms
    Feb 20 22:48:19.249: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:48:21.264: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034494428s
    Feb 20 22:48:21.264: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:48:23.264: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034203013s
    Feb 20 22:48:23.264: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 20 22:48:23.264: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[pod1:[80] pod2:[80]] 02/20/23 22:48:23.286
    Feb 20 22:48:23.334: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/20/23 22:48:23.335
    Feb 20 22:48:24.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 20 22:48:24.591: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:24.591: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:48:24.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.82 80'
    Feb 20 22:48:24.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.82 80\nConnection to 172.21.46.82 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:24.854: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-6961 02/20/23 22:48:24.854
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[pod2:[80]] 02/20/23 22:48:24.891
    Feb 20 22:48:25.957: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/20/23 22:48:25.957
    Feb 20 22:48:26.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 20 22:48:27.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:27.231: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:48:27.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-6961 exec execpodt2q7k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.82 80'
    Feb 20 22:48:27.522: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.82 80\nConnection to 172.21.46.82 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:27.522: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-6961 02/20/23 22:48:27.522
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6961 to expose endpoints map[] 02/20/23 22:48:27.561
    Feb 20 22:48:28.627: INFO: successfully validated that service endpoint-test2 in namespace services-6961 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:48:28.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6961" for this suite. 02/20/23 22:48:28.684
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:28.712
Feb 20 22:48:28.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename ingress 02/20/23 22:48:28.714
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:28.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:28.777
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/20/23 22:48:28.788
STEP: getting /apis/networking.k8s.io 02/20/23 22:48:28.798
STEP: getting /apis/networking.k8s.iov1 02/20/23 22:48:28.806
STEP: creating 02/20/23 22:48:28.81
STEP: getting 02/20/23 22:48:28.859
STEP: listing 02/20/23 22:48:28.871
STEP: watching 02/20/23 22:48:28.882
Feb 20 22:48:28.883: INFO: starting watch
STEP: cluster-wide listing 02/20/23 22:48:28.888
STEP: cluster-wide watching 02/20/23 22:48:28.899
Feb 20 22:48:28.899: INFO: starting watch
STEP: patching 02/20/23 22:48:28.904
STEP: updating 02/20/23 22:48:28.919
Feb 20 22:48:28.942: INFO: waiting for watch events with expected annotations
Feb 20 22:48:28.943: INFO: saw patched and updated annotations
STEP: patching /status 02/20/23 22:48:28.943
STEP: updating /status 02/20/23 22:48:28.957
STEP: get /status 02/20/23 22:48:28.981
STEP: deleting 02/20/23 22:48:28.994
STEP: deleting a collection 02/20/23 22:48:29.034
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Feb 20 22:48:29.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5106" for this suite. 02/20/23 22:48:29.124
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":187,"skipped":3501,"failed":0}
------------------------------
• [0.435 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:28.712
    Feb 20 22:48:28.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename ingress 02/20/23 22:48:28.714
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:28.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:28.777
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/20/23 22:48:28.788
    STEP: getting /apis/networking.k8s.io 02/20/23 22:48:28.798
    STEP: getting /apis/networking.k8s.iov1 02/20/23 22:48:28.806
    STEP: creating 02/20/23 22:48:28.81
    STEP: getting 02/20/23 22:48:28.859
    STEP: listing 02/20/23 22:48:28.871
    STEP: watching 02/20/23 22:48:28.882
    Feb 20 22:48:28.883: INFO: starting watch
    STEP: cluster-wide listing 02/20/23 22:48:28.888
    STEP: cluster-wide watching 02/20/23 22:48:28.899
    Feb 20 22:48:28.899: INFO: starting watch
    STEP: patching 02/20/23 22:48:28.904
    STEP: updating 02/20/23 22:48:28.919
    Feb 20 22:48:28.942: INFO: waiting for watch events with expected annotations
    Feb 20 22:48:28.943: INFO: saw patched and updated annotations
    STEP: patching /status 02/20/23 22:48:28.943
    STEP: updating /status 02/20/23 22:48:28.957
    STEP: get /status 02/20/23 22:48:28.981
    STEP: deleting 02/20/23 22:48:28.994
    STEP: deleting a collection 02/20/23 22:48:29.034
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Feb 20 22:48:29.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-5106" for this suite. 02/20/23 22:48:29.124
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:29.149
Feb 20 22:48:29.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:48:29.152
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:29.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:29.249
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-7478 02/20/23 22:48:29.264
STEP: creating service affinity-clusterip-transition in namespace services-7478 02/20/23 22:48:29.264
STEP: creating replication controller affinity-clusterip-transition in namespace services-7478 02/20/23 22:48:29.326
I0220 22:48:29.340052      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7478, replica count: 3
I0220 22:48:32.390416      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:48:32.415: INFO: Creating new exec pod
Feb 20 22:48:32.477: INFO: Waiting up to 5m0s for pod "execpod-affinityd2n2q" in namespace "services-7478" to be "running"
Feb 20 22:48:32.500: INFO: Pod "execpod-affinityd2n2q": Phase="Pending", Reason="", readiness=false. Elapsed: 22.95462ms
Feb 20 22:48:34.517: INFO: Pod "execpod-affinityd2n2q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039817337s
Feb 20 22:48:36.512: INFO: Pod "execpod-affinityd2n2q": Phase="Running", Reason="", readiness=true. Elapsed: 4.035225352s
Feb 20 22:48:36.512: INFO: Pod "execpod-affinityd2n2q" satisfied condition "running"
Feb 20 22:48:37.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 20 22:48:37.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:37.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:48:37.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.173.88 80'
Feb 20 22:48:38.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.173.88 80\nConnection to 172.21.173.88 80 port [tcp/http] succeeded!\n"
Feb 20 22:48:38.049: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:48:38.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.88:80/ ; done'
Feb 20 22:48:38.494: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n"
Feb 20 22:48:38.494: INFO: stdout: "\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz"
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
Feb 20 22:48:38.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.88:80/ ; done'
Feb 20 22:48:38.944: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n"
Feb 20 22:48:38.944: INFO: stdout: "\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j"
Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
Feb 20 22:48:38.945: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7478, will wait for the garbage collector to delete the pods 02/20/23 22:48:38.971
Feb 20 22:48:39.045: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.308084ms
Feb 20 22:48:39.148: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.312395ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:48:42.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7478" for this suite. 02/20/23 22:48:42.318
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":188,"skipped":3504,"failed":0}
------------------------------
• [SLOW TEST] [13.215 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:29.149
    Feb 20 22:48:29.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:48:29.152
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:29.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:29.249
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-7478 02/20/23 22:48:29.264
    STEP: creating service affinity-clusterip-transition in namespace services-7478 02/20/23 22:48:29.264
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7478 02/20/23 22:48:29.326
    I0220 22:48:29.340052      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7478, replica count: 3
    I0220 22:48:32.390416      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:48:32.415: INFO: Creating new exec pod
    Feb 20 22:48:32.477: INFO: Waiting up to 5m0s for pod "execpod-affinityd2n2q" in namespace "services-7478" to be "running"
    Feb 20 22:48:32.500: INFO: Pod "execpod-affinityd2n2q": Phase="Pending", Reason="", readiness=false. Elapsed: 22.95462ms
    Feb 20 22:48:34.517: INFO: Pod "execpod-affinityd2n2q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039817337s
    Feb 20 22:48:36.512: INFO: Pod "execpod-affinityd2n2q": Phase="Running", Reason="", readiness=true. Elapsed: 4.035225352s
    Feb 20 22:48:36.512: INFO: Pod "execpod-affinityd2n2q" satisfied condition "running"
    Feb 20 22:48:37.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Feb 20 22:48:37.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:37.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:48:37.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.173.88 80'
    Feb 20 22:48:38.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.173.88 80\nConnection to 172.21.173.88 80 port [tcp/http] succeeded!\n"
    Feb 20 22:48:38.049: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:48:38.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.88:80/ ; done'
    Feb 20 22:48:38.494: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n"
    Feb 20 22:48:38.494: INFO: stdout: "\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-trxnw\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-8mkpz\naffinity-clusterip-transition-8mkpz"
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-trxnw
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.494: INFO: Received response from host: affinity-clusterip-transition-8mkpz
    Feb 20 22:48:38.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7478 exec execpod-affinityd2n2q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.88:80/ ; done'
    Feb 20 22:48:38.944: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.88:80/\n"
    Feb 20 22:48:38.944: INFO: stdout: "\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j\naffinity-clusterip-transition-lrr9j"
    Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.944: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Received response from host: affinity-clusterip-transition-lrr9j
    Feb 20 22:48:38.945: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7478, will wait for the garbage collector to delete the pods 02/20/23 22:48:38.971
    Feb 20 22:48:39.045: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.308084ms
    Feb 20 22:48:39.148: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.312395ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:48:42.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7478" for this suite. 02/20/23 22:48:42.318
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:42.367
Feb 20 22:48:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename disruption 02/20/23 22:48:42.368
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:42.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:42.456
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:42.47
Feb 20 22:48:42.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename disruption-2 02/20/23 22:48:42.471
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:42.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:42.534
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 02/20/23 22:48:42.561
STEP: Waiting for the pdb to be processed 02/20/23 22:48:44.597
STEP: Waiting for the pdb to be processed 02/20/23 22:48:46.634
STEP: listing a collection of PDBs across all namespaces 02/20/23 22:48:48.657
STEP: listing a collection of PDBs in namespace disruption-4783 02/20/23 22:48:48.673
STEP: deleting a collection of PDBs 02/20/23 22:48:48.685
STEP: Waiting for the PDB collection to be deleted 02/20/23 22:48:48.72
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Feb 20 22:48:48.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-350" for this suite. 02/20/23 22:48:48.753
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 20 22:48:48.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4783" for this suite. 02/20/23 22:48:48.811
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":189,"skipped":3529,"failed":0}
------------------------------
• [SLOW TEST] [6.470 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:42.367
    Feb 20 22:48:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename disruption 02/20/23 22:48:42.368
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:42.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:42.456
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:42.47
    Feb 20 22:48:42.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename disruption-2 02/20/23 22:48:42.471
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:42.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:42.534
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 02/20/23 22:48:42.561
    STEP: Waiting for the pdb to be processed 02/20/23 22:48:44.597
    STEP: Waiting for the pdb to be processed 02/20/23 22:48:46.634
    STEP: listing a collection of PDBs across all namespaces 02/20/23 22:48:48.657
    STEP: listing a collection of PDBs in namespace disruption-4783 02/20/23 22:48:48.673
    STEP: deleting a collection of PDBs 02/20/23 22:48:48.685
    STEP: Waiting for the PDB collection to be deleted 02/20/23 22:48:48.72
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Feb 20 22:48:48.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-350" for this suite. 02/20/23 22:48:48.753
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 20 22:48:48.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4783" for this suite. 02/20/23 22:48:48.811
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:48.845
Feb 20 22:48:48.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 22:48:48.846
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:48.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:48.936
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 02/20/23 22:48:48.945
Feb 20 22:48:49.058: INFO: Waiting up to 5m0s for pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc" in namespace "var-expansion-8740" to be "Succeeded or Failed"
Feb 20 22:48:49.070: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91935ms
Feb 20 22:48:51.084: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025604647s
Feb 20 22:48:53.084: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02533858s
STEP: Saw pod success 02/20/23 22:48:53.084
Feb 20 22:48:53.084: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc" satisfied condition "Succeeded or Failed"
Feb 20 22:48:53.096: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-62a031ef-80ba-40de-955a-04db293810bc container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:48:53.14
Feb 20 22:48:53.181: INFO: Waiting for pod var-expansion-62a031ef-80ba-40de-955a-04db293810bc to disappear
Feb 20 22:48:53.194: INFO: Pod var-expansion-62a031ef-80ba-40de-955a-04db293810bc no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 22:48:53.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8740" for this suite. 02/20/23 22:48:53.216
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":190,"skipped":3529,"failed":0}
------------------------------
• [4.398 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:48.845
    Feb 20 22:48:48.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 22:48:48.846
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:48.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:48.936
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 02/20/23 22:48:48.945
    Feb 20 22:48:49.058: INFO: Waiting up to 5m0s for pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc" in namespace "var-expansion-8740" to be "Succeeded or Failed"
    Feb 20 22:48:49.070: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91935ms
    Feb 20 22:48:51.084: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025604647s
    Feb 20 22:48:53.084: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02533858s
    STEP: Saw pod success 02/20/23 22:48:53.084
    Feb 20 22:48:53.084: INFO: Pod "var-expansion-62a031ef-80ba-40de-955a-04db293810bc" satisfied condition "Succeeded or Failed"
    Feb 20 22:48:53.096: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-62a031ef-80ba-40de-955a-04db293810bc container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:48:53.14
    Feb 20 22:48:53.181: INFO: Waiting for pod var-expansion-62a031ef-80ba-40de-955a-04db293810bc to disappear
    Feb 20 22:48:53.194: INFO: Pod var-expansion-62a031ef-80ba-40de-955a-04db293810bc no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 22:48:53.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8740" for this suite. 02/20/23 22:48:53.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:53.246
Feb 20 22:48:53.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 22:48:53.249
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:53.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:53.31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Feb 20 22:48:53.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: creating the pod 02/20/23 22:48:53.327
STEP: submitting the pod to kubernetes 02/20/23 22:48:53.327
Feb 20 22:48:53.388: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58" in namespace "pods-9331" to be "running and ready"
Feb 20 22:48:53.402: INFO: Pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58": Phase="Pending", Reason="", readiness=false. Elapsed: 13.767587ms
Feb 20 22:48:53.402: INFO: The phase of Pod pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:48:55.414: INFO: Pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58": Phase="Running", Reason="", readiness=true. Elapsed: 2.026412094s
Feb 20 22:48:55.414: INFO: The phase of Pod pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58 is Running (Ready = true)
Feb 20 22:48:55.414: INFO: Pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 22:48:55.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9331" for this suite. 02/20/23 22:48:55.48
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":191,"skipped":3551,"failed":0}
------------------------------
• [2.262 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:53.246
    Feb 20 22:48:53.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 22:48:53.249
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:53.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:53.31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Feb 20 22:48:53.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: creating the pod 02/20/23 22:48:53.327
    STEP: submitting the pod to kubernetes 02/20/23 22:48:53.327
    Feb 20 22:48:53.388: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58" in namespace "pods-9331" to be "running and ready"
    Feb 20 22:48:53.402: INFO: Pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58": Phase="Pending", Reason="", readiness=false. Elapsed: 13.767587ms
    Feb 20 22:48:53.402: INFO: The phase of Pod pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:48:55.414: INFO: Pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58": Phase="Running", Reason="", readiness=true. Elapsed: 2.026412094s
    Feb 20 22:48:55.414: INFO: The phase of Pod pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58 is Running (Ready = true)
    Feb 20 22:48:55.414: INFO: Pod "pod-logs-websocket-2b7c81c3-1c62-47cb-9f19-c2a023c7ca58" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 22:48:55.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9331" for this suite. 02/20/23 22:48:55.48
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:48:55.509
Feb 20 22:48:55.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:48:55.514
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:55.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:55.589
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-9496478d-f716-4a65-a2e0-76c87ef647ec 02/20/23 22:48:55.603
STEP: Creating a pod to test consume configMaps 02/20/23 22:48:55.62
Feb 20 22:48:55.701: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23" in namespace "projected-9617" to be "Succeeded or Failed"
Feb 20 22:48:55.715: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Pending", Reason="", readiness=false. Elapsed: 13.120346ms
Feb 20 22:48:57.728: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026311828s
Feb 20 22:48:59.726: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024612324s
Feb 20 22:49:01.736: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034427731s
STEP: Saw pod success 02/20/23 22:49:01.736
Feb 20 22:49:01.736: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23" satisfied condition "Succeeded or Failed"
Feb 20 22:49:01.751: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:49:01.808
Feb 20 22:49:01.844: INFO: Waiting for pod pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23 to disappear
Feb 20 22:49:01.864: INFO: Pod pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 22:49:01.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9617" for this suite. 02/20/23 22:49:01.907
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":192,"skipped":3552,"failed":0}
------------------------------
• [SLOW TEST] [6.463 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:48:55.509
    Feb 20 22:48:55.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:48:55.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:48:55.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:48:55.589
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-9496478d-f716-4a65-a2e0-76c87ef647ec 02/20/23 22:48:55.603
    STEP: Creating a pod to test consume configMaps 02/20/23 22:48:55.62
    Feb 20 22:48:55.701: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23" in namespace "projected-9617" to be "Succeeded or Failed"
    Feb 20 22:48:55.715: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Pending", Reason="", readiness=false. Elapsed: 13.120346ms
    Feb 20 22:48:57.728: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026311828s
    Feb 20 22:48:59.726: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024612324s
    Feb 20 22:49:01.736: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034427731s
    STEP: Saw pod success 02/20/23 22:49:01.736
    Feb 20 22:49:01.736: INFO: Pod "pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23" satisfied condition "Succeeded or Failed"
    Feb 20 22:49:01.751: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:49:01.808
    Feb 20 22:49:01.844: INFO: Waiting for pod pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23 to disappear
    Feb 20 22:49:01.864: INFO: Pod pod-projected-configmaps-f38ac93d-74fa-4ec9-8358-99f4c2168e23 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 22:49:01.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9617" for this suite. 02/20/23 22:49:01.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:49:01.972
Feb 20 22:49:01.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:49:01.973
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:49:02.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:49:02.121
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Feb 20 22:49:02.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/20/23 22:49:11.636
Feb 20 22:49:11.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 create -f -'
Feb 20 22:49:13.600: INFO: stderr: ""
Feb 20 22:49:13.600: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 20 22:49:13.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 delete e2e-test-crd-publish-openapi-4900-crds test-cr'
Feb 20 22:49:13.759: INFO: stderr: ""
Feb 20 22:49:13.760: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 20 22:49:13.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 apply -f -'
Feb 20 22:49:14.341: INFO: stderr: ""
Feb 20 22:49:14.341: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 20 22:49:14.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 delete e2e-test-crd-publish-openapi-4900-crds test-cr'
Feb 20 22:49:14.499: INFO: stderr: ""
Feb 20 22:49:14.499: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/20/23 22:49:14.499
Feb 20 22:49:14.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 explain e2e-test-crd-publish-openapi-4900-crds'
Feb 20 22:49:15.002: INFO: stderr: ""
Feb 20 22:49:15.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4900-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:49:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2204" for this suite. 02/20/23 22:49:24.138
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":193,"skipped":3571,"failed":0}
------------------------------
• [SLOW TEST] [22.181 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:49:01.972
    Feb 20 22:49:01.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 22:49:01.973
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:49:02.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:49:02.121
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Feb 20 22:49:02.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/20/23 22:49:11.636
    Feb 20 22:49:11.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 create -f -'
    Feb 20 22:49:13.600: INFO: stderr: ""
    Feb 20 22:49:13.600: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 20 22:49:13.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 delete e2e-test-crd-publish-openapi-4900-crds test-cr'
    Feb 20 22:49:13.759: INFO: stderr: ""
    Feb 20 22:49:13.760: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 20 22:49:13.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 apply -f -'
    Feb 20 22:49:14.341: INFO: stderr: ""
    Feb 20 22:49:14.341: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 20 22:49:14.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 --namespace=crd-publish-openapi-2204 delete e2e-test-crd-publish-openapi-4900-crds test-cr'
    Feb 20 22:49:14.499: INFO: stderr: ""
    Feb 20 22:49:14.499: INFO: stdout: "e2e-test-crd-publish-openapi-4900-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/20/23 22:49:14.499
    Feb 20 22:49:14.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-2204 explain e2e-test-crd-publish-openapi-4900-crds'
    Feb 20 22:49:15.002: INFO: stderr: ""
    Feb 20 22:49:15.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4900-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:49:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2204" for this suite. 02/20/23 22:49:24.138
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:49:24.154
Feb 20 22:49:24.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:49:24.16
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:49:24.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:49:24.216
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 02/20/23 22:49:24.234
Feb 20 22:49:24.303: INFO: Waiting up to 5m0s for pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21" in namespace "downward-api-7005" to be "running and ready"
Feb 20 22:49:24.323: INFO: Pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21": Phase="Pending", Reason="", readiness=false. Elapsed: 19.65663ms
Feb 20 22:49:24.323: INFO: The phase of Pod labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:49:26.332: INFO: Pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21": Phase="Running", Reason="", readiness=true. Elapsed: 2.028809641s
Feb 20 22:49:26.332: INFO: The phase of Pod labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21 is Running (Ready = true)
Feb 20 22:49:26.332: INFO: Pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21" satisfied condition "running and ready"
Feb 20 22:49:26.916: INFO: Successfully updated pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:49:28.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7005" for this suite. 02/20/23 22:49:28.969
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":194,"skipped":3575,"failed":0}
------------------------------
• [4.830 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:49:24.154
    Feb 20 22:49:24.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:49:24.16
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:49:24.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:49:24.216
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 02/20/23 22:49:24.234
    Feb 20 22:49:24.303: INFO: Waiting up to 5m0s for pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21" in namespace "downward-api-7005" to be "running and ready"
    Feb 20 22:49:24.323: INFO: Pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21": Phase="Pending", Reason="", readiness=false. Elapsed: 19.65663ms
    Feb 20 22:49:24.323: INFO: The phase of Pod labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:49:26.332: INFO: Pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21": Phase="Running", Reason="", readiness=true. Elapsed: 2.028809641s
    Feb 20 22:49:26.332: INFO: The phase of Pod labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21 is Running (Ready = true)
    Feb 20 22:49:26.332: INFO: Pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21" satisfied condition "running and ready"
    Feb 20 22:49:26.916: INFO: Successfully updated pod "labelsupdate70e04a4b-81aa-4f32-9574-f5cba6bb1f21"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:49:28.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7005" for this suite. 02/20/23 22:49:28.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:49:28.99
Feb 20 22:49:28.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-watch 02/20/23 22:49:28.991
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:49:29.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:49:29.079
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 20 22:49:29.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Creating first CR  02/20/23 22:49:31.727
Feb 20 22:49:31.740: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:31Z]] name:name1 resourceVersion:107430 uid:25f82536-fbc3-40e0-9588-b006fd30a7bd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/20/23 22:49:41.742
Feb 20 22:49:41.757: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:41Z]] name:name2 resourceVersion:107534 uid:01c841be-2955-4ef5-bc9b-d65178205fce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/20/23 22:49:51.757
Feb 20 22:49:51.774: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:51Z]] name:name1 resourceVersion:107588 uid:25f82536-fbc3-40e0-9588-b006fd30a7bd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/20/23 22:50:01.775
Feb 20 22:50:01.821: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:50:01Z]] name:name2 resourceVersion:107637 uid:01c841be-2955-4ef5-bc9b-d65178205fce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/20/23 22:50:11.821
Feb 20 22:50:11.849: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:51Z]] name:name1 resourceVersion:107679 uid:25f82536-fbc3-40e0-9588-b006fd30a7bd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/20/23 22:50:21.849
Feb 20 22:50:21.870: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:50:01Z]] name:name2 resourceVersion:107712 uid:01c841be-2955-4ef5-bc9b-d65178205fce] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:50:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6245" for this suite. 02/20/23 22:50:32.409
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":195,"skipped":3621,"failed":0}
------------------------------
• [SLOW TEST] [63.433 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:49:28.99
    Feb 20 22:49:28.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-watch 02/20/23 22:49:28.991
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:49:29.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:49:29.079
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 20 22:49:29.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Creating first CR  02/20/23 22:49:31.727
    Feb 20 22:49:31.740: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:31Z]] name:name1 resourceVersion:107430 uid:25f82536-fbc3-40e0-9588-b006fd30a7bd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/20/23 22:49:41.742
    Feb 20 22:49:41.757: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:41Z]] name:name2 resourceVersion:107534 uid:01c841be-2955-4ef5-bc9b-d65178205fce] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/20/23 22:49:51.757
    Feb 20 22:49:51.774: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:51Z]] name:name1 resourceVersion:107588 uid:25f82536-fbc3-40e0-9588-b006fd30a7bd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/20/23 22:50:01.775
    Feb 20 22:50:01.821: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:50:01Z]] name:name2 resourceVersion:107637 uid:01c841be-2955-4ef5-bc9b-d65178205fce] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/20/23 22:50:11.821
    Feb 20 22:50:11.849: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:49:51Z]] name:name1 resourceVersion:107679 uid:25f82536-fbc3-40e0-9588-b006fd30a7bd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/20/23 22:50:21.849
    Feb 20 22:50:21.870: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-20T22:49:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-20T22:50:01Z]] name:name2 resourceVersion:107712 uid:01c841be-2955-4ef5-bc9b-d65178205fce] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:50:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6245" for this suite. 02/20/23 22:50:32.409
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:50:32.425
Feb 20 22:50:32.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:50:32.43
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:32.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:32.481
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:50:32.552
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:50:32.934
STEP: Deploying the webhook pod 02/20/23 22:50:32.958
STEP: Wait for the deployment to be ready 02/20/23 22:50:32.981
Feb 20 22:50:33.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 22:50:35.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:50:37.082
STEP: Verifying the service has paired with the endpoint 02/20/23 22:50:37.113
Feb 20 22:50:38.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/20/23 22:50:38.123
STEP: create a configmap that should be updated by the webhook 02/20/23 22:50:38.165
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:50:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9514" for this suite. 02/20/23 22:50:38.249
STEP: Destroying namespace "webhook-9514-markers" for this suite. 02/20/23 22:50:38.267
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":196,"skipped":3624,"failed":0}
------------------------------
• [SLOW TEST] [5.950 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:50:32.425
    Feb 20 22:50:32.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:50:32.43
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:32.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:32.481
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:50:32.552
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:50:32.934
    STEP: Deploying the webhook pod 02/20/23 22:50:32.958
    STEP: Wait for the deployment to be ready 02/20/23 22:50:32.981
    Feb 20 22:50:33.031: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 22:50:35.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:50:37.082
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:50:37.113
    Feb 20 22:50:38.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/20/23 22:50:38.123
    STEP: create a configmap that should be updated by the webhook 02/20/23 22:50:38.165
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:50:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9514" for this suite. 02/20/23 22:50:38.249
    STEP: Destroying namespace "webhook-9514-markers" for this suite. 02/20/23 22:50:38.267
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:50:38.385
Feb 20 22:50:38.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:50:38.387
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:38.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:38.476
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 02/20/23 22:50:38.484
Feb 20 22:50:38.533: INFO: Waiting up to 5m0s for pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f" in namespace "downward-api-9895" to be "running and ready"
Feb 20 22:50:38.545: INFO: Pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134821ms
Feb 20 22:50:38.545: INFO: The phase of Pod annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:50:40.556: INFO: Pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022667036s
Feb 20 22:50:40.556: INFO: The phase of Pod annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f is Running (Ready = true)
Feb 20 22:50:40.556: INFO: Pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f" satisfied condition "running and ready"
Feb 20 22:50:41.141: INFO: Successfully updated pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:50:43.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9895" for this suite. 02/20/23 22:50:43.19
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":197,"skipped":3648,"failed":0}
------------------------------
• [4.822 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:50:38.385
    Feb 20 22:50:38.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:50:38.387
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:38.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:38.476
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 02/20/23 22:50:38.484
    Feb 20 22:50:38.533: INFO: Waiting up to 5m0s for pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f" in namespace "downward-api-9895" to be "running and ready"
    Feb 20 22:50:38.545: INFO: Pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134821ms
    Feb 20 22:50:38.545: INFO: The phase of Pod annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:50:40.556: INFO: Pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022667036s
    Feb 20 22:50:40.556: INFO: The phase of Pod annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f is Running (Ready = true)
    Feb 20 22:50:40.556: INFO: Pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f" satisfied condition "running and ready"
    Feb 20 22:50:41.141: INFO: Successfully updated pod "annotationupdatef41f5dae-07f9-489d-b694-e1d743cbd24f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:50:43.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9895" for this suite. 02/20/23 22:50:43.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:50:43.207
Feb 20 22:50:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:50:43.209
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:43.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:43.264
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 02/20/23 22:50:43.272
Feb 20 22:50:43.302: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd" in namespace "emptydir-3237" to be "running"
Feb 20 22:50:43.313: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.140971ms
Feb 20 22:50:45.326: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024264937s
Feb 20 22:50:47.323: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd": Phase="Running", Reason="", readiness=false. Elapsed: 4.020658355s
Feb 20 22:50:47.323: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/20/23 22:50:47.323
Feb 20 22:50:47.323: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3237 PodName:pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:50:47.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:50:47.325: INFO: ExecWithOptions: Clientset creation
Feb 20 22:50:47.325: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-3237/pods/pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 20 22:50:47.465: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:50:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3237" for this suite. 02/20/23 22:50:47.478
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":198,"skipped":3656,"failed":0}
------------------------------
• [4.287 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:50:43.207
    Feb 20 22:50:43.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:50:43.209
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:43.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:43.264
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 02/20/23 22:50:43.272
    Feb 20 22:50:43.302: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd" in namespace "emptydir-3237" to be "running"
    Feb 20 22:50:43.313: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.140971ms
    Feb 20 22:50:45.326: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024264937s
    Feb 20 22:50:47.323: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd": Phase="Running", Reason="", readiness=false. Elapsed: 4.020658355s
    Feb 20 22:50:47.323: INFO: Pod "pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/20/23 22:50:47.323
    Feb 20 22:50:47.323: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3237 PodName:pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:50:47.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:50:47.325: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:50:47.325: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-3237/pods/pod-sharedvolume-df64874a-08cd-4df4-b833-76902a51fabd/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 20 22:50:47.465: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:50:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3237" for this suite. 02/20/23 22:50:47.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:50:47.498
Feb 20 22:50:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:50:47.502
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:47.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:47.568
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:50:47.606
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:50:48.268
STEP: Deploying the webhook pod 02/20/23 22:50:48.29
STEP: Wait for the deployment to be ready 02/20/23 22:50:48.312
Feb 20 22:50:48.328: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 20 22:50:50.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:50:52.364
STEP: Verifying the service has paired with the endpoint 02/20/23 22:50:52.385
Feb 20 22:50:53.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 02/20/23 22:50:53.402
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/20/23 22:50:53.406
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/20/23 22:50:53.407
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/20/23 22:50:53.407
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/20/23 22:50:53.416
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/20/23 22:50:53.416
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/20/23 22:50:53.43
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:50:53.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1440" for this suite. 02/20/23 22:50:53.443
STEP: Destroying namespace "webhook-1440-markers" for this suite. 02/20/23 22:50:53.457
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":199,"skipped":3668,"failed":0}
------------------------------
• [SLOW TEST] [6.070 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:50:47.498
    Feb 20 22:50:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:50:47.502
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:47.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:47.568
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:50:47.606
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:50:48.268
    STEP: Deploying the webhook pod 02/20/23 22:50:48.29
    STEP: Wait for the deployment to be ready 02/20/23 22:50:48.312
    Feb 20 22:50:48.328: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Feb 20 22:50:50.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 50, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:50:52.364
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:50:52.385
    Feb 20 22:50:53.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 02/20/23 22:50:53.402
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/20/23 22:50:53.406
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/20/23 22:50:53.407
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/20/23 22:50:53.407
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/20/23 22:50:53.416
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/20/23 22:50:53.416
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/20/23 22:50:53.43
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:50:53.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1440" for this suite. 02/20/23 22:50:53.443
    STEP: Destroying namespace "webhook-1440-markers" for this suite. 02/20/23 22:50:53.457
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:50:53.575
Feb 20 22:50:53.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:50:53.577
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:53.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:53.666
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 22:50:53.681
Feb 20 22:50:53.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 20 22:50:53.872: INFO: stderr: ""
Feb 20 22:50:53.873: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/20/23 22:50:53.873
STEP: verifying the pod e2e-test-httpd-pod was created 02/20/23 22:50:58.924
Feb 20 22:50:58.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 get pod e2e-test-httpd-pod -o json'
Feb 20 22:50:59.042: INFO: stderr: ""
Feb 20 22:50:59.042: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"b08fb4605e65fc639da1cb35fb65855113bbc3ee32ebbe631d99d91c2e21248b\",\n            \"cni.projectcalico.org/podIP\": \"172.30.181.245/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.181.245/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.181.245\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.181.245\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-02-20T22:50:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-905\",\n        \"resourceVersion\": \"108360\",\n        \"uid\": \"e3a18c99-2b90-44b5-baab-854ab4df5856\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-rglc8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-j52wn\"\n            }\n        ],\n        \"nodeName\": \"10.8.38.66\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c55,c40\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-rglc8\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9ff869f62e87d0c7b31949b34105e8a6bdc15445b6d035a8c1fdffff71ef237c\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-20T22:50:55Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.8.38.66\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.181.245\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.181.245\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-20T22:50:53Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/20/23 22:50:59.043
Feb 20 22:50:59.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 replace -f -'
Feb 20 22:51:01.455: INFO: stderr: ""
Feb 20 22:51:01.455: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/20/23 22:51:01.456
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Feb 20 22:51:01.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 delete pods e2e-test-httpd-pod'
Feb 20 22:51:03.442: INFO: stderr: ""
Feb 20 22:51:03.442: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:51:03.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-905" for this suite. 02/20/23 22:51:03.472
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":200,"skipped":3674,"failed":0}
------------------------------
• [SLOW TEST] [9.916 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:50:53.575
    Feb 20 22:50:53.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:50:53.577
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:50:53.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:50:53.666
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/20/23 22:50:53.681
    Feb 20 22:50:53.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 20 22:50:53.872: INFO: stderr: ""
    Feb 20 22:50:53.873: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/20/23 22:50:53.873
    STEP: verifying the pod e2e-test-httpd-pod was created 02/20/23 22:50:58.924
    Feb 20 22:50:58.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 get pod e2e-test-httpd-pod -o json'
    Feb 20 22:50:59.042: INFO: stderr: ""
    Feb 20 22:50:59.042: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"b08fb4605e65fc639da1cb35fb65855113bbc3ee32ebbe631d99d91c2e21248b\",\n            \"cni.projectcalico.org/podIP\": \"172.30.181.245/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.181.245/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.181.245\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.181.245\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-02-20T22:50:53Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-905\",\n        \"resourceVersion\": \"108360\",\n        \"uid\": \"e3a18c99-2b90-44b5-baab-854ab4df5856\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-rglc8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-j52wn\"\n            }\n        ],\n        \"nodeName\": \"10.8.38.66\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c55,c40\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-rglc8\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:53Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-20T22:50:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://9ff869f62e87d0c7b31949b34105e8a6bdc15445b6d035a8c1fdffff71ef237c\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-20T22:50:55Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.8.38.66\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.181.245\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.181.245\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-20T22:50:53Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/20/23 22:50:59.043
    Feb 20 22:50:59.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 replace -f -'
    Feb 20 22:51:01.455: INFO: stderr: ""
    Feb 20 22:51:01.455: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/20/23 22:51:01.456
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Feb 20 22:51:01.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-905 delete pods e2e-test-httpd-pod'
    Feb 20 22:51:03.442: INFO: stderr: ""
    Feb 20 22:51:03.442: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:51:03.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-905" for this suite. 02/20/23 22:51:03.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:03.492
Feb 20 22:51:03.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename job 02/20/23 22:51:03.494
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:03.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:03.56
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 02/20/23 22:51:03.571
W0220 22:51:03.582466      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to paralellism count is attached to the job 02/20/23 22:51:03.582
STEP: patching /status 02/20/23 22:51:07.595
STEP: updating /status 02/20/23 22:51:07.608
STEP: get /status 02/20/23 22:51:07.627
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 20 22:51:07.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4012" for this suite. 02/20/23 22:51:07.648
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":201,"skipped":3682,"failed":0}
------------------------------
• [4.170 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:03.492
    Feb 20 22:51:03.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename job 02/20/23 22:51:03.494
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:03.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:03.56
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 02/20/23 22:51:03.571
    W0220 22:51:03.582466      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to paralellism count is attached to the job 02/20/23 22:51:03.582
    STEP: patching /status 02/20/23 22:51:07.595
    STEP: updating /status 02/20/23 22:51:07.608
    STEP: get /status 02/20/23 22:51:07.627
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 20 22:51:07.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4012" for this suite. 02/20/23 22:51:07.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:07.673
Feb 20 22:51:07.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:51:07.675
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:07.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:07.747
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-1032 02/20/23 22:51:07.755
STEP: creating service affinity-nodeport in namespace services-1032 02/20/23 22:51:07.756
STEP: creating replication controller affinity-nodeport in namespace services-1032 02/20/23 22:51:07.789
I0220 22:51:07.808264      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1032, replica count: 3
I0220 22:51:10.861465      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:51:10.894: INFO: Creating new exec pod
Feb 20 22:51:10.920: INFO: Waiting up to 5m0s for pod "execpod-affinitywktw5" in namespace "services-1032" to be "running"
Feb 20 22:51:10.930: INFO: Pod "execpod-affinitywktw5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.335995ms
Feb 20 22:51:12.939: INFO: Pod "execpod-affinitywktw5": Phase="Running", Reason="", readiness=true. Elapsed: 2.018296714s
Feb 20 22:51:12.939: INFO: Pod "execpod-affinitywktw5" satisfied condition "running"
Feb 20 22:51:13.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 20 22:51:14.273: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 20 22:51:14.273: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:51:14.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.28.1 80'
Feb 20 22:51:14.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.28.1 80\nConnection to 172.21.28.1 80 port [tcp/http] succeeded!\n"
Feb 20 22:51:14.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:51:14.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 30673'
Feb 20 22:51:15.016: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 30673\nConnection to 10.8.38.66 30673 port [tcp/*] succeeded!\n"
Feb 20 22:51:15.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:51:15.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 30673'
Feb 20 22:51:15.563: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 30673\nConnection to 10.8.38.70 30673 port [tcp/*] succeeded!\n"
Feb 20 22:51:15.563: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 22:51:15.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:30673/ ; done'
Feb 20 22:51:15.996: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n"
Feb 20 22:51:15.996: INFO: stdout: "\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g"
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
Feb 20 22:51:15.996: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1032, will wait for the garbage collector to delete the pods 02/20/23 22:51:16.016
Feb 20 22:51:16.114: INFO: Deleting ReplicationController affinity-nodeport took: 12.375579ms
Feb 20 22:51:16.217: INFO: Terminating ReplicationController affinity-nodeport pods took: 103.230512ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:51:18.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1032" for this suite. 02/20/23 22:51:18.987
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":202,"skipped":3732,"failed":0}
------------------------------
• [SLOW TEST] [11.327 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:07.673
    Feb 20 22:51:07.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:51:07.675
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:07.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:07.747
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-1032 02/20/23 22:51:07.755
    STEP: creating service affinity-nodeport in namespace services-1032 02/20/23 22:51:07.756
    STEP: creating replication controller affinity-nodeport in namespace services-1032 02/20/23 22:51:07.789
    I0220 22:51:07.808264      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1032, replica count: 3
    I0220 22:51:10.861465      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:51:10.894: INFO: Creating new exec pod
    Feb 20 22:51:10.920: INFO: Waiting up to 5m0s for pod "execpod-affinitywktw5" in namespace "services-1032" to be "running"
    Feb 20 22:51:10.930: INFO: Pod "execpod-affinitywktw5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.335995ms
    Feb 20 22:51:12.939: INFO: Pod "execpod-affinitywktw5": Phase="Running", Reason="", readiness=true. Elapsed: 2.018296714s
    Feb 20 22:51:12.939: INFO: Pod "execpod-affinitywktw5" satisfied condition "running"
    Feb 20 22:51:13.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Feb 20 22:51:14.273: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 20 22:51:14.273: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:51:14.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.28.1 80'
    Feb 20 22:51:14.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.28.1 80\nConnection to 172.21.28.1 80 port [tcp/http] succeeded!\n"
    Feb 20 22:51:14.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:51:14.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.66 30673'
    Feb 20 22:51:15.016: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.66 30673\nConnection to 10.8.38.66 30673 port [tcp/*] succeeded!\n"
    Feb 20 22:51:15.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:51:15.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 30673'
    Feb 20 22:51:15.563: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 30673\nConnection to 10.8.38.70 30673 port [tcp/*] succeeded!\n"
    Feb 20 22:51:15.563: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 22:51:15.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-1032 exec execpod-affinitywktw5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:30673/ ; done'
    Feb 20 22:51:15.996: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:30673/\n"
    Feb 20 22:51:15.996: INFO: stdout: "\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g\naffinity-nodeport-ccm7g"
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Received response from host: affinity-nodeport-ccm7g
    Feb 20 22:51:15.996: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1032, will wait for the garbage collector to delete the pods 02/20/23 22:51:16.016
    Feb 20 22:51:16.114: INFO: Deleting ReplicationController affinity-nodeport took: 12.375579ms
    Feb 20 22:51:16.217: INFO: Terminating ReplicationController affinity-nodeport pods took: 103.230512ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:51:18.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1032" for this suite. 02/20/23 22:51:18.987
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:19.005
Feb 20 22:51:19.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename prestop 02/20/23 22:51:19.006
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:19.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:19.064
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9066 02/20/23 22:51:19.096
STEP: Waiting for pods to come up. 02/20/23 22:51:19.127
Feb 20 22:51:19.128: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9066" to be "running"
Feb 20 22:51:19.137: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.609441ms
Feb 20 22:51:21.147: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018908877s
Feb 20 22:51:23.170: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.042407892s
Feb 20 22:51:23.170: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9066 02/20/23 22:51:23.179
Feb 20 22:51:23.201: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9066" to be "running"
Feb 20 22:51:23.209: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.815561ms
Feb 20 22:51:25.221: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019274281s
Feb 20 22:51:27.224: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.022316977s
Feb 20 22:51:27.224: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/20/23 22:51:27.224
Feb 20 22:51:32.274: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/20/23 22:51:32.274
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Feb 20 22:51:32.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9066" for this suite. 02/20/23 22:51:32.308
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":203,"skipped":3735,"failed":0}
------------------------------
• [SLOW TEST] [13.324 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:19.005
    Feb 20 22:51:19.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename prestop 02/20/23 22:51:19.006
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:19.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:19.064
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9066 02/20/23 22:51:19.096
    STEP: Waiting for pods to come up. 02/20/23 22:51:19.127
    Feb 20 22:51:19.128: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9066" to be "running"
    Feb 20 22:51:19.137: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.609441ms
    Feb 20 22:51:21.147: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018908877s
    Feb 20 22:51:23.170: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.042407892s
    Feb 20 22:51:23.170: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9066 02/20/23 22:51:23.179
    Feb 20 22:51:23.201: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9066" to be "running"
    Feb 20 22:51:23.209: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.815561ms
    Feb 20 22:51:25.221: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019274281s
    Feb 20 22:51:27.224: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.022316977s
    Feb 20 22:51:27.224: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/20/23 22:51:27.224
    Feb 20 22:51:32.274: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/20/23 22:51:32.274
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Feb 20 22:51:32.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9066" for this suite. 02/20/23 22:51:32.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:32.354
Feb 20 22:51:32.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replication-controller 02/20/23 22:51:32.355
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:32.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:32.424
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 02/20/23 22:51:32.444
STEP: When the matched label of one of its pods change 02/20/23 22:51:32.479
Feb 20 22:51:32.499: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 20 22:51:37.509: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/20/23 22:51:37.544
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 20 22:51:38.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9152" for this suite. 02/20/23 22:51:38.576
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":204,"skipped":3771,"failed":0}
------------------------------
• [SLOW TEST] [6.235 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:32.354
    Feb 20 22:51:32.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replication-controller 02/20/23 22:51:32.355
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:32.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:32.424
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 02/20/23 22:51:32.444
    STEP: When the matched label of one of its pods change 02/20/23 22:51:32.479
    Feb 20 22:51:32.499: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb 20 22:51:37.509: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/20/23 22:51:37.544
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 20 22:51:38.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9152" for this suite. 02/20/23 22:51:38.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:38.59
Feb 20 22:51:38.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:51:38.591
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:38.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:38.639
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-efcbfa25-b220-4c76-9669-1657c84021a8 02/20/23 22:51:38.647
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:51:38.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9619" for this suite. 02/20/23 22:51:38.676
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":205,"skipped":3786,"failed":0}
------------------------------
• [0.101 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:38.59
    Feb 20 22:51:38.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:51:38.591
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:38.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:38.639
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-efcbfa25-b220-4c76-9669-1657c84021a8 02/20/23 22:51:38.647
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:51:38.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9619" for this suite. 02/20/23 22:51:38.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:38.697
Feb 20 22:51:38.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:51:38.698
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:38.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:38.796
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-7348281f-e491-40ed-b1e6-7a6b39f44d1a 02/20/23 22:51:38.803
STEP: Creating a pod to test consume secrets 02/20/23 22:51:38.814
Feb 20 22:51:38.845: INFO: Waiting up to 5m0s for pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c" in namespace "secrets-604" to be "Succeeded or Failed"
Feb 20 22:51:38.855: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048711ms
Feb 20 22:51:40.872: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027002147s
Feb 20 22:51:42.864: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018732587s
STEP: Saw pod success 02/20/23 22:51:42.864
Feb 20 22:51:42.864: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c" satisfied condition "Succeeded or Failed"
Feb 20 22:51:42.872: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c container secret-env-test: <nil>
STEP: delete the pod 02/20/23 22:51:42.89
Feb 20 22:51:42.916: INFO: Waiting for pod pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c to disappear
Feb 20 22:51:42.924: INFO: Pod pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:51:42.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-604" for this suite. 02/20/23 22:51:42.938
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":206,"skipped":3798,"failed":0}
------------------------------
• [4.256 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:38.697
    Feb 20 22:51:38.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:51:38.698
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:38.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:38.796
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-7348281f-e491-40ed-b1e6-7a6b39f44d1a 02/20/23 22:51:38.803
    STEP: Creating a pod to test consume secrets 02/20/23 22:51:38.814
    Feb 20 22:51:38.845: INFO: Waiting up to 5m0s for pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c" in namespace "secrets-604" to be "Succeeded or Failed"
    Feb 20 22:51:38.855: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048711ms
    Feb 20 22:51:40.872: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027002147s
    Feb 20 22:51:42.864: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018732587s
    STEP: Saw pod success 02/20/23 22:51:42.864
    Feb 20 22:51:42.864: INFO: Pod "pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c" satisfied condition "Succeeded or Failed"
    Feb 20 22:51:42.872: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c container secret-env-test: <nil>
    STEP: delete the pod 02/20/23 22:51:42.89
    Feb 20 22:51:42.916: INFO: Waiting for pod pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c to disappear
    Feb 20 22:51:42.924: INFO: Pod pod-secrets-577e4992-9f56-440e-a9b6-e1a967f8314c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:51:42.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-604" for this suite. 02/20/23 22:51:42.938
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:42.957
Feb 20 22:51:42.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:51:42.961
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:42.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:43.01
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-9eaa32ad-78fb-4c72-be6e-657baf98f6e0 02/20/23 22:51:43.018
STEP: Creating a pod to test consume configMaps 02/20/23 22:51:43.037
Feb 20 22:51:43.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5" in namespace "projected-8858" to be "Succeeded or Failed"
Feb 20 22:51:43.076: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.52076ms
Feb 20 22:51:45.085: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017350593s
Feb 20 22:51:47.085: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017313667s
Feb 20 22:51:49.086: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018721829s
STEP: Saw pod success 02/20/23 22:51:49.086
Feb 20 22:51:49.087: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5" satisfied condition "Succeeded or Failed"
Feb 20 22:51:49.095: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:51:49.113
Feb 20 22:51:49.138: INFO: Waiting for pod pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5 to disappear
Feb 20 22:51:49.146: INFO: Pod pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 22:51:49.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8858" for this suite. 02/20/23 22:51:49.159
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":207,"skipped":3802,"failed":0}
------------------------------
• [SLOW TEST] [6.216 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:42.957
    Feb 20 22:51:42.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:51:42.961
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:42.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:43.01
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-9eaa32ad-78fb-4c72-be6e-657baf98f6e0 02/20/23 22:51:43.018
    STEP: Creating a pod to test consume configMaps 02/20/23 22:51:43.037
    Feb 20 22:51:43.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5" in namespace "projected-8858" to be "Succeeded or Failed"
    Feb 20 22:51:43.076: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.52076ms
    Feb 20 22:51:45.085: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017350593s
    Feb 20 22:51:47.085: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017313667s
    Feb 20 22:51:49.086: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018721829s
    STEP: Saw pod success 02/20/23 22:51:49.086
    Feb 20 22:51:49.087: INFO: Pod "pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5" satisfied condition "Succeeded or Failed"
    Feb 20 22:51:49.095: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:51:49.113
    Feb 20 22:51:49.138: INFO: Waiting for pod pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5 to disappear
    Feb 20 22:51:49.146: INFO: Pod pod-projected-configmaps-1c7202fb-eadc-415e-bf66-8ca02bbc41b5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 22:51:49.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8858" for this suite. 02/20/23 22:51:49.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:51:49.178
Feb 20 22:51:49.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 22:51:49.18
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:49.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:49.272
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-7460 02/20/23 22:51:49.282
STEP: creating replication controller nodeport-test in namespace services-7460 02/20/23 22:51:49.315
I0220 22:51:49.328363      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7460, replica count: 2
I0220 22:51:52.380237      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:51:52.380: INFO: Creating new exec pod
Feb 20 22:51:52.409: INFO: Waiting up to 5m0s for pod "execpod429d8" in namespace "services-7460" to be "running"
Feb 20 22:51:52.417: INFO: Pod "execpod429d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.394497ms
Feb 20 22:51:54.428: INFO: Pod "execpod429d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018657678s
Feb 20 22:51:54.428: INFO: Pod "execpod429d8" satisfied condition "running"
Feb 20 22:51:55.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 20 22:51:55.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 20 22:51:55.719: INFO: stdout: "nodeport-test-f269r"
Feb 20 22:51:55.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.74.101 80'
Feb 20 22:51:55.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.74.101 80\nConnection to 172.21.74.101 80 port [tcp/http] succeeded!\n"
Feb 20 22:51:55.962: INFO: stdout: "nodeport-test-f269r"
Feb 20 22:51:55.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
Feb 20 22:51:56.302: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
Feb 20 22:51:56.303: INFO: stdout: ""
Feb 20 22:51:57.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
Feb 20 22:51:57.801: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
Feb 20 22:51:57.801: INFO: stdout: ""
Feb 20 22:51:58.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
Feb 20 22:51:58.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
Feb 20 22:51:58.612: INFO: stdout: ""
Feb 20 22:51:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
Feb 20 22:51:59.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
Feb 20 22:51:59.713: INFO: stdout: "nodeport-test-6k8tm"
Feb 20 22:51:59.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 32421'
Feb 20 22:52:00.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 32421\nConnection to 10.8.38.69 32421 port [tcp/*] succeeded!\n"
Feb 20 22:52:00.087: INFO: stdout: ""
Feb 20 22:52:01.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 32421'
Feb 20 22:52:01.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 32421\nConnection to 10.8.38.69 32421 port [tcp/*] succeeded!\n"
Feb 20 22:52:01.461: INFO: stdout: ""
Feb 20 22:52:02.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 32421'
Feb 20 22:52:02.527: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 32421\nConnection to 10.8.38.69 32421 port [tcp/*] succeeded!\n"
Feb 20 22:52:02.527: INFO: stdout: "nodeport-test-f269r"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 22:52:02.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7460" for this suite. 02/20/23 22:52:02.568
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":208,"skipped":3836,"failed":0}
------------------------------
• [SLOW TEST] [13.404 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:51:49.178
    Feb 20 22:51:49.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 22:51:49.18
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:51:49.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:51:49.272
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-7460 02/20/23 22:51:49.282
    STEP: creating replication controller nodeport-test in namespace services-7460 02/20/23 22:51:49.315
    I0220 22:51:49.328363      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7460, replica count: 2
    I0220 22:51:52.380237      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:51:52.380: INFO: Creating new exec pod
    Feb 20 22:51:52.409: INFO: Waiting up to 5m0s for pod "execpod429d8" in namespace "services-7460" to be "running"
    Feb 20 22:51:52.417: INFO: Pod "execpod429d8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.394497ms
    Feb 20 22:51:54.428: INFO: Pod "execpod429d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018657678s
    Feb 20 22:51:54.428: INFO: Pod "execpod429d8" satisfied condition "running"
    Feb 20 22:51:55.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 20 22:51:55.719: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 20 22:51:55.719: INFO: stdout: "nodeport-test-f269r"
    Feb 20 22:51:55.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.74.101 80'
    Feb 20 22:51:55.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.74.101 80\nConnection to 172.21.74.101 80 port [tcp/http] succeeded!\n"
    Feb 20 22:51:55.962: INFO: stdout: "nodeport-test-f269r"
    Feb 20 22:51:55.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
    Feb 20 22:51:56.302: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:51:56.303: INFO: stdout: ""
    Feb 20 22:51:57.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
    Feb 20 22:51:57.801: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:51:57.801: INFO: stdout: ""
    Feb 20 22:51:58.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
    Feb 20 22:51:58.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:51:58.612: INFO: stdout: ""
    Feb 20 22:51:59.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 32421'
    Feb 20 22:51:59.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 32421\nConnection to 10.8.38.70 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:51:59.713: INFO: stdout: "nodeport-test-6k8tm"
    Feb 20 22:51:59.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 32421'
    Feb 20 22:52:00.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 32421\nConnection to 10.8.38.69 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:52:00.087: INFO: stdout: ""
    Feb 20 22:52:01.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 32421'
    Feb 20 22:52:01.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 32421\nConnection to 10.8.38.69 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:52:01.461: INFO: stdout: ""
    Feb 20 22:52:02.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7460 exec execpod429d8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 32421'
    Feb 20 22:52:02.527: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 32421\nConnection to 10.8.38.69 32421 port [tcp/*] succeeded!\n"
    Feb 20 22:52:02.527: INFO: stdout: "nodeport-test-f269r"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 22:52:02.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7460" for this suite. 02/20/23 22:52:02.568
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:02.583
Feb 20 22:52:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:52:02.586
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:02.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:02.659
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 02/20/23 22:52:02.699
Feb 20 22:52:02.733: INFO: Waiting up to 5m0s for pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0" in namespace "downward-api-9260" to be "Succeeded or Failed"
Feb 20 22:52:02.742: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.042124ms
Feb 20 22:52:04.754: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021590428s
Feb 20 22:52:06.758: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025417957s
Feb 20 22:52:08.753: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019924825s
STEP: Saw pod success 02/20/23 22:52:08.753
Feb 20 22:52:08.753: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0" satisfied condition "Succeeded or Failed"
Feb 20 22:52:08.761: INFO: Trying to get logs from node 10.8.38.69 pod downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0 container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:52:08.823
Feb 20 22:52:08.846: INFO: Waiting for pod downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0 to disappear
Feb 20 22:52:08.858: INFO: Pod downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 20 22:52:08.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9260" for this suite. 02/20/23 22:52:08.874
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":209,"skipped":3838,"failed":0}
------------------------------
• [SLOW TEST] [6.306 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:02.583
    Feb 20 22:52:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:52:02.586
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:02.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:02.659
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 02/20/23 22:52:02.699
    Feb 20 22:52:02.733: INFO: Waiting up to 5m0s for pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0" in namespace "downward-api-9260" to be "Succeeded or Failed"
    Feb 20 22:52:02.742: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.042124ms
    Feb 20 22:52:04.754: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021590428s
    Feb 20 22:52:06.758: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025417957s
    Feb 20 22:52:08.753: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019924825s
    STEP: Saw pod success 02/20/23 22:52:08.753
    Feb 20 22:52:08.753: INFO: Pod "downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0" satisfied condition "Succeeded or Failed"
    Feb 20 22:52:08.761: INFO: Trying to get logs from node 10.8.38.69 pod downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0 container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:52:08.823
    Feb 20 22:52:08.846: INFO: Waiting for pod downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0 to disappear
    Feb 20 22:52:08.858: INFO: Pod downward-api-1834abdb-ee95-4430-87ae-20947a07a4b0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 20 22:52:08.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9260" for this suite. 02/20/23 22:52:08.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:08.895
Feb 20 22:52:08.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-runtime 02/20/23 22:52:08.898
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:08.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:08.954
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 02/20/23 22:52:08.968
STEP: wait for the container to reach Succeeded 02/20/23 22:52:08.999
STEP: get the container status 02/20/23 22:52:14.062
STEP: the container should be terminated 02/20/23 22:52:14.071
STEP: the termination message should be set 02/20/23 22:52:14.071
Feb 20 22:52:14.072: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/20/23 22:52:14.072
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 20 22:52:14.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8205" for this suite. 02/20/23 22:52:14.119
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":210,"skipped":3853,"failed":0}
------------------------------
• [SLOW TEST] [5.237 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:08.895
    Feb 20 22:52:08.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-runtime 02/20/23 22:52:08.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:08.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:08.954
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 02/20/23 22:52:08.968
    STEP: wait for the container to reach Succeeded 02/20/23 22:52:08.999
    STEP: get the container status 02/20/23 22:52:14.062
    STEP: the container should be terminated 02/20/23 22:52:14.071
    STEP: the termination message should be set 02/20/23 22:52:14.071
    Feb 20 22:52:14.072: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/20/23 22:52:14.072
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 20 22:52:14.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8205" for this suite. 02/20/23 22:52:14.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:14.141
Feb 20 22:52:14.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:52:14.143
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:14.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:14.249
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-88a069cb-c865-43e8-af89-89d76026ce61 02/20/23 22:52:14.278
STEP: Creating a pod to test consume configMaps 02/20/23 22:52:14.319
Feb 20 22:52:14.355: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408" in namespace "projected-3641" to be "Succeeded or Failed"
Feb 20 22:52:14.366: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408": Phase="Pending", Reason="", readiness=false. Elapsed: 11.234886ms
Feb 20 22:52:16.377: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021776663s
Feb 20 22:52:18.377: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021834048s
STEP: Saw pod success 02/20/23 22:52:18.377
Feb 20 22:52:18.377: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408" satisfied condition "Succeeded or Failed"
Feb 20 22:52:18.385: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:52:18.404
Feb 20 22:52:18.429: INFO: Waiting for pod pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408 to disappear
Feb 20 22:52:18.436: INFO: Pod pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 22:52:18.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3641" for this suite. 02/20/23 22:52:18.447
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":211,"skipped":3894,"failed":0}
------------------------------
• [4.320 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:14.141
    Feb 20 22:52:14.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:52:14.143
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:14.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:14.249
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-88a069cb-c865-43e8-af89-89d76026ce61 02/20/23 22:52:14.278
    STEP: Creating a pod to test consume configMaps 02/20/23 22:52:14.319
    Feb 20 22:52:14.355: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408" in namespace "projected-3641" to be "Succeeded or Failed"
    Feb 20 22:52:14.366: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408": Phase="Pending", Reason="", readiness=false. Elapsed: 11.234886ms
    Feb 20 22:52:16.377: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021776663s
    Feb 20 22:52:18.377: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021834048s
    STEP: Saw pod success 02/20/23 22:52:18.377
    Feb 20 22:52:18.377: INFO: Pod "pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408" satisfied condition "Succeeded or Failed"
    Feb 20 22:52:18.385: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:52:18.404
    Feb 20 22:52:18.429: INFO: Waiting for pod pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408 to disappear
    Feb 20 22:52:18.436: INFO: Pod pod-projected-configmaps-ddba97e4-5093-45f1-b6fa-062d1831c408 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 22:52:18.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3641" for this suite. 02/20/23 22:52:18.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:18.467
Feb 20 22:52:18.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 22:52:18.469
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:18.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:18.516
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-4611c6c4-9c5c-4c97-b0a9-738fbf6d044e 02/20/23 22:52:18.528
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 22:52:18.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4510" for this suite. 02/20/23 22:52:18.579
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":212,"skipped":3927,"failed":0}
------------------------------
• [0.127 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:18.467
    Feb 20 22:52:18.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 22:52:18.469
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:18.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:18.516
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-4611c6c4-9c5c-4c97-b0a9-738fbf6d044e 02/20/23 22:52:18.528
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 22:52:18.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4510" for this suite. 02/20/23 22:52:18.579
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:18.596
Feb 20 22:52:18.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replication-controller 02/20/23 22:52:18.598
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:18.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:18.652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 02/20/23 22:52:18.688
W0220 22:52:18.722159      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:52:18.722: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-208" to be "running and ready"
Feb 20 22:52:18.733: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.893754ms
Feb 20 22:52:18.733: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:52:20.743: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020766589s
Feb 20 22:52:20.743: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:52:22.744: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.021973827s
Feb 20 22:52:22.744: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 20 22:52:22.744: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/20/23 22:52:22.754
STEP: Then the orphan pod is adopted 02/20/23 22:52:22.766
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 20 22:52:23.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-208" for this suite. 02/20/23 22:52:23.798
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":213,"skipped":3931,"failed":0}
------------------------------
• [SLOW TEST] [5.216 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:18.596
    Feb 20 22:52:18.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replication-controller 02/20/23 22:52:18.598
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:18.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:18.652
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/20/23 22:52:18.688
    W0220 22:52:18.722159      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:52:18.722: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-208" to be "running and ready"
    Feb 20 22:52:18.733: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 10.893754ms
    Feb 20 22:52:18.733: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:52:20.743: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020766589s
    Feb 20 22:52:20.743: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:52:22.744: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.021973827s
    Feb 20 22:52:22.744: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 20 22:52:22.744: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/20/23 22:52:22.754
    STEP: Then the orphan pod is adopted 02/20/23 22:52:22.766
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 20 22:52:23.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-208" for this suite. 02/20/23 22:52:23.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:23.822
Feb 20 22:52:23.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename watch 02/20/23 22:52:23.824
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:23.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:23.919
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/20/23 22:52:23.956
STEP: modifying the configmap once 02/20/23 22:52:23.986
STEP: modifying the configmap a second time 02/20/23 22:52:24.017
STEP: deleting the configmap 02/20/23 22:52:24.08
STEP: creating a watch on configmaps from the resource version returned by the first update 02/20/23 22:52:24.103
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/20/23 22:52:24.108
Feb 20 22:52:24.108: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2159  372a0bbb-00c5-4234-805e-ec3cf72d0596 110089 0 2023-02-20 22:52:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-20 22:52:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 20 22:52:24.109: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2159  372a0bbb-00c5-4234-805e-ec3cf72d0596 110090 0 2023-02-20 22:52:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-20 22:52:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 20 22:52:24.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2159" for this suite. 02/20/23 22:52:24.121
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":214,"skipped":3945,"failed":0}
------------------------------
• [0.313 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:23.822
    Feb 20 22:52:23.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename watch 02/20/23 22:52:23.824
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:23.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:23.919
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/20/23 22:52:23.956
    STEP: modifying the configmap once 02/20/23 22:52:23.986
    STEP: modifying the configmap a second time 02/20/23 22:52:24.017
    STEP: deleting the configmap 02/20/23 22:52:24.08
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/20/23 22:52:24.103
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/20/23 22:52:24.108
    Feb 20 22:52:24.108: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2159  372a0bbb-00c5-4234-805e-ec3cf72d0596 110089 0 2023-02-20 22:52:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-20 22:52:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 20 22:52:24.109: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2159  372a0bbb-00c5-4234-805e-ec3cf72d0596 110090 0 2023-02-20 22:52:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-20 22:52:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 20 22:52:24.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2159" for this suite. 02/20/23 22:52:24.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:24.143
Feb 20 22:52:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 22:52:24.145
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:24.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:24.261
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 02/20/23 22:52:41.305
STEP: Creating a ResourceQuota 02/20/23 22:52:46.315
STEP: Ensuring resource quota status is calculated 02/20/23 22:52:46.326
STEP: Creating a ConfigMap 02/20/23 22:52:48.336
STEP: Ensuring resource quota status captures configMap creation 02/20/23 22:52:48.359
STEP: Deleting a ConfigMap 02/20/23 22:52:50.368
STEP: Ensuring resource quota status released usage 02/20/23 22:52:50.389
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 22:52:52.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4288" for this suite. 02/20/23 22:52:52.41
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":215,"skipped":3998,"failed":0}
------------------------------
• [SLOW TEST] [28.282 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:24.143
    Feb 20 22:52:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 22:52:24.145
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:24.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:24.261
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 02/20/23 22:52:41.305
    STEP: Creating a ResourceQuota 02/20/23 22:52:46.315
    STEP: Ensuring resource quota status is calculated 02/20/23 22:52:46.326
    STEP: Creating a ConfigMap 02/20/23 22:52:48.336
    STEP: Ensuring resource quota status captures configMap creation 02/20/23 22:52:48.359
    STEP: Deleting a ConfigMap 02/20/23 22:52:50.368
    STEP: Ensuring resource quota status released usage 02/20/23 22:52:50.389
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 22:52:52.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4288" for this suite. 02/20/23 22:52:52.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:52.432
Feb 20 22:52:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 22:52:52.434
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:52.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:52.484
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 20 22:52:52.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:52:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-966" for this suite. 02/20/23 22:52:53.112
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":216,"skipped":4011,"failed":0}
------------------------------
• [0.704 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:52.432
    Feb 20 22:52:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 22:52:52.434
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:52.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:52.484
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 20 22:52:52.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:52:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-966" for this suite. 02/20/23 22:52:53.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:53.138
Feb 20 22:52:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:52:53.14
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:53.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:53.184
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  02/20/23 22:52:53.193
Feb 20 22:52:53.226: INFO: Waiting up to 5m0s for pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e" in namespace "svcaccounts-5415" to be "Succeeded or Failed"
Feb 20 22:52:53.236: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.000539ms
Feb 20 22:52:55.246: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019658427s
Feb 20 22:52:57.247: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020717716s
STEP: Saw pod success 02/20/23 22:52:57.247
Feb 20 22:52:57.248: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e" satisfied condition "Succeeded or Failed"
Feb 20 22:52:57.256: INFO: Trying to get logs from node 10.8.38.66 pod test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:52:57.273
Feb 20 22:52:57.292: INFO: Waiting for pod test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e to disappear
Feb 20 22:52:57.302: INFO: Pod test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 20 22:52:57.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5415" for this suite. 02/20/23 22:52:57.316
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":217,"skipped":4029,"failed":0}
------------------------------
• [4.194 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:53.138
    Feb 20 22:52:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:52:53.14
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:53.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:53.184
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  02/20/23 22:52:53.193
    Feb 20 22:52:53.226: INFO: Waiting up to 5m0s for pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e" in namespace "svcaccounts-5415" to be "Succeeded or Failed"
    Feb 20 22:52:53.236: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.000539ms
    Feb 20 22:52:55.246: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019658427s
    Feb 20 22:52:57.247: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020717716s
    STEP: Saw pod success 02/20/23 22:52:57.247
    Feb 20 22:52:57.248: INFO: Pod "test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e" satisfied condition "Succeeded or Failed"
    Feb 20 22:52:57.256: INFO: Trying to get logs from node 10.8.38.66 pod test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:52:57.273
    Feb 20 22:52:57.292: INFO: Waiting for pod test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e to disappear
    Feb 20 22:52:57.302: INFO: Pod test-pod-4fdc5c11-366b-4401-aec9-89da6fd0f94e no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 20 22:52:57.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5415" for this suite. 02/20/23 22:52:57.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:52:57.34
Feb 20 22:52:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:52:57.344
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:57.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:57.394
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-bc4be2a3-aa04-4628-b884-9343d2f90f6b 02/20/23 22:52:57.418
STEP: Creating a pod to test consume secrets 02/20/23 22:52:57.428
W0220 22:52:57.464381      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:52:57.464: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970" in namespace "projected-519" to be "Succeeded or Failed"
Feb 20 22:52:57.473: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970": Phase="Pending", Reason="", readiness=false. Elapsed: 8.656987ms
Feb 20 22:52:59.481: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017166204s
Feb 20 22:53:01.482: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017594874s
STEP: Saw pod success 02/20/23 22:53:01.482
Feb 20 22:53:01.482: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970" satisfied condition "Succeeded or Failed"
Feb 20 22:53:01.490: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:53:01.508
Feb 20 22:53:01.535: INFO: Waiting for pod pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970 to disappear
Feb 20 22:53:01.544: INFO: Pod pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 22:53:01.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-519" for this suite. 02/20/23 22:53:01.574
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":218,"skipped":4044,"failed":0}
------------------------------
• [4.250 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:52:57.34
    Feb 20 22:52:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:52:57.344
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:52:57.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:52:57.394
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-bc4be2a3-aa04-4628-b884-9343d2f90f6b 02/20/23 22:52:57.418
    STEP: Creating a pod to test consume secrets 02/20/23 22:52:57.428
    W0220 22:52:57.464381      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:52:57.464: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970" in namespace "projected-519" to be "Succeeded or Failed"
    Feb 20 22:52:57.473: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970": Phase="Pending", Reason="", readiness=false. Elapsed: 8.656987ms
    Feb 20 22:52:59.481: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017166204s
    Feb 20 22:53:01.482: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017594874s
    STEP: Saw pod success 02/20/23 22:53:01.482
    Feb 20 22:53:01.482: INFO: Pod "pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970" satisfied condition "Succeeded or Failed"
    Feb 20 22:53:01.490: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:53:01.508
    Feb 20 22:53:01.535: INFO: Waiting for pod pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970 to disappear
    Feb 20 22:53:01.544: INFO: Pod pod-projected-secrets-6f5b18f4-582a-4e8d-b31f-2bfe5f554970 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 22:53:01.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-519" for this suite. 02/20/23 22:53:01.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:01.604
Feb 20 22:53:01.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename disruption 02/20/23 22:53:01.606
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:01.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:01.663
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 02/20/23 22:53:01.676
STEP: Waiting for the pdb to be processed 02/20/23 22:53:01.687
STEP: updating the pdb 02/20/23 22:53:03.706
STEP: Waiting for the pdb to be processed 02/20/23 22:53:03.728
STEP: patching the pdb 02/20/23 22:53:05.746
STEP: Waiting for the pdb to be processed 02/20/23 22:53:05.768
STEP: Waiting for the pdb to be deleted 02/20/23 22:53:07.799
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 20 22:53:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3366" for this suite. 02/20/23 22:53:07.82
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":219,"skipped":4073,"failed":0}
------------------------------
• [SLOW TEST] [6.230 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:01.604
    Feb 20 22:53:01.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename disruption 02/20/23 22:53:01.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:01.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:01.663
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 02/20/23 22:53:01.676
    STEP: Waiting for the pdb to be processed 02/20/23 22:53:01.687
    STEP: updating the pdb 02/20/23 22:53:03.706
    STEP: Waiting for the pdb to be processed 02/20/23 22:53:03.728
    STEP: patching the pdb 02/20/23 22:53:05.746
    STEP: Waiting for the pdb to be processed 02/20/23 22:53:05.768
    STEP: Waiting for the pdb to be deleted 02/20/23 22:53:07.799
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 20 22:53:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3366" for this suite. 02/20/23 22:53:07.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:07.84
Feb 20 22:53:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 22:53:07.842
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:07.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:07.89
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 02/20/23 22:53:07.903
Feb 20 22:53:07.937: INFO: Waiting up to 5m0s for pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a" in namespace "var-expansion-6212" to be "Succeeded or Failed"
Feb 20 22:53:07.945: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122936ms
Feb 20 22:53:09.954: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017057556s
Feb 20 22:53:11.954: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017404867s
Feb 20 22:53:13.955: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018023321s
STEP: Saw pod success 02/20/23 22:53:13.955
Feb 20 22:53:13.956: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a" satisfied condition "Succeeded or Failed"
Feb 20 22:53:13.964: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:53:13.981
Feb 20 22:53:14.008: INFO: Waiting for pod var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a to disappear
Feb 20 22:53:14.017: INFO: Pod var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 22:53:14.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6212" for this suite. 02/20/23 22:53:14.03
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":220,"skipped":4086,"failed":0}
------------------------------
• [SLOW TEST] [6.208 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:07.84
    Feb 20 22:53:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 22:53:07.842
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:07.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:07.89
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 02/20/23 22:53:07.903
    Feb 20 22:53:07.937: INFO: Waiting up to 5m0s for pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a" in namespace "var-expansion-6212" to be "Succeeded or Failed"
    Feb 20 22:53:07.945: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122936ms
    Feb 20 22:53:09.954: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017057556s
    Feb 20 22:53:11.954: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017404867s
    Feb 20 22:53:13.955: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018023321s
    STEP: Saw pod success 02/20/23 22:53:13.955
    Feb 20 22:53:13.956: INFO: Pod "var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a" satisfied condition "Succeeded or Failed"
    Feb 20 22:53:13.964: INFO: Trying to get logs from node 10.8.38.66 pod var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:53:13.981
    Feb 20 22:53:14.008: INFO: Waiting for pod var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a to disappear
    Feb 20 22:53:14.017: INFO: Pod var-expansion-a9ad5b8a-27c1-4af9-869e-df78b7f6c56a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 22:53:14.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6212" for this suite. 02/20/23 22:53:14.03
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:14.05
Feb 20 22:53:14.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:53:14.053
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:14.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:14.136
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-0c33f942-4aad-4786-98f6-b3795e9bff63 02/20/23 22:53:14.145
STEP: Creating a pod to test consume secrets 02/20/23 22:53:14.192
Feb 20 22:53:14.293: INFO: Waiting up to 5m0s for pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16" in namespace "secrets-7922" to be "Succeeded or Failed"
Feb 20 22:53:14.312: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16": Phase="Pending", Reason="", readiness=false. Elapsed: 18.7258ms
Feb 20 22:53:16.322: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028836592s
Feb 20 22:53:18.324: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030762782s
STEP: Saw pod success 02/20/23 22:53:18.324
Feb 20 22:53:18.324: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16" satisfied condition "Succeeded or Failed"
Feb 20 22:53:18.333: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:53:18.387
Feb 20 22:53:18.410: INFO: Waiting for pod pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16 to disappear
Feb 20 22:53:18.427: INFO: Pod pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:53:18.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7922" for this suite. 02/20/23 22:53:18.446
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":221,"skipped":4089,"failed":0}
------------------------------
• [4.411 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:14.05
    Feb 20 22:53:14.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:53:14.053
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:14.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:14.136
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-0c33f942-4aad-4786-98f6-b3795e9bff63 02/20/23 22:53:14.145
    STEP: Creating a pod to test consume secrets 02/20/23 22:53:14.192
    Feb 20 22:53:14.293: INFO: Waiting up to 5m0s for pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16" in namespace "secrets-7922" to be "Succeeded or Failed"
    Feb 20 22:53:14.312: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16": Phase="Pending", Reason="", readiness=false. Elapsed: 18.7258ms
    Feb 20 22:53:16.322: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028836592s
    Feb 20 22:53:18.324: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030762782s
    STEP: Saw pod success 02/20/23 22:53:18.324
    Feb 20 22:53:18.324: INFO: Pod "pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16" satisfied condition "Succeeded or Failed"
    Feb 20 22:53:18.333: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:53:18.387
    Feb 20 22:53:18.410: INFO: Waiting for pod pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16 to disappear
    Feb 20 22:53:18.427: INFO: Pod pod-secrets-db41d270-3651-4ec8-ab6a-d2291cceff16 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:53:18.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7922" for this suite. 02/20/23 22:53:18.446
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:18.468
Feb 20 22:53:18.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:53:18.47
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:18.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:18.517
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 02/20/23 22:53:18.567
STEP: watching for the ServiceAccount to be added 02/20/23 22:53:18.629
STEP: patching the ServiceAccount 02/20/23 22:53:18.633
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/20/23 22:53:18.643
STEP: deleting the ServiceAccount 02/20/23 22:53:18.665
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 20 22:53:18.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8026" for this suite. 02/20/23 22:53:18.749
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":222,"skipped":4093,"failed":0}
------------------------------
• [0.341 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:18.468
    Feb 20 22:53:18.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:53:18.47
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:18.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:18.517
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 02/20/23 22:53:18.567
    STEP: watching for the ServiceAccount to be added 02/20/23 22:53:18.629
    STEP: patching the ServiceAccount 02/20/23 22:53:18.633
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/20/23 22:53:18.643
    STEP: deleting the ServiceAccount 02/20/23 22:53:18.665
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 20 22:53:18.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8026" for this suite. 02/20/23 22:53:18.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:18.81
Feb 20 22:53:18.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 22:53:18.817
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:18.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:18.87
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-cdc2c388-2eb2-491c-bb5c-168b56d3b601 02/20/23 22:53:18.885
STEP: Creating a pod to test consume secrets 02/20/23 22:53:18.906
Feb 20 22:53:18.936: INFO: Waiting up to 5m0s for pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592" in namespace "secrets-5273" to be "Succeeded or Failed"
Feb 20 22:53:18.944: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041583ms
Feb 20 22:53:20.953: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01718788s
Feb 20 22:53:22.954: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018386275s
Feb 20 22:53:24.979: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043417416s
STEP: Saw pod success 02/20/23 22:53:24.979
Feb 20 22:53:24.980: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592" satisfied condition "Succeeded or Failed"
Feb 20 22:53:25.011: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 22:53:25.063
Feb 20 22:53:25.087: INFO: Waiting for pod pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592 to disappear
Feb 20 22:53:25.096: INFO: Pod pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 22:53:25.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5273" for this suite. 02/20/23 22:53:25.11
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":223,"skipped":4125,"failed":0}
------------------------------
• [SLOW TEST] [6.320 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:18.81
    Feb 20 22:53:18.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 22:53:18.817
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:18.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:18.87
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-cdc2c388-2eb2-491c-bb5c-168b56d3b601 02/20/23 22:53:18.885
    STEP: Creating a pod to test consume secrets 02/20/23 22:53:18.906
    Feb 20 22:53:18.936: INFO: Waiting up to 5m0s for pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592" in namespace "secrets-5273" to be "Succeeded or Failed"
    Feb 20 22:53:18.944: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041583ms
    Feb 20 22:53:20.953: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01718788s
    Feb 20 22:53:22.954: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018386275s
    Feb 20 22:53:24.979: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043417416s
    STEP: Saw pod success 02/20/23 22:53:24.979
    Feb 20 22:53:24.980: INFO: Pod "pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592" satisfied condition "Succeeded or Failed"
    Feb 20 22:53:25.011: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 22:53:25.063
    Feb 20 22:53:25.087: INFO: Waiting for pod pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592 to disappear
    Feb 20 22:53:25.096: INFO: Pod pod-secrets-0ec235e5-7615-4b18-8b88-b6e9f7ae7592 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 22:53:25.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5273" for this suite. 02/20/23 22:53:25.11
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:25.13
Feb 20 22:53:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 22:53:25.133
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:25.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:25.178
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/20/23 22:53:25.212
Feb 20 22:53:25.245: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8504" to be "running and ready"
Feb 20 22:53:25.258: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.70462ms
Feb 20 22:53:25.263: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:53:27.274: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.028379989s
Feb 20 22:53:27.274: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 20 22:53:27.274: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 02/20/23 22:53:27.299
Feb 20 22:53:27.321: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-8504" to be "running and ready"
Feb 20 22:53:27.329: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.195712ms
Feb 20 22:53:27.329: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:53:29.338: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.0171965s
Feb 20 22:53:29.338: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 20 22:53:29.338: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/20/23 22:53:29.347
STEP: delete the pod with lifecycle hook 02/20/23 22:53:29.366
Feb 20 22:53:29.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 22:53:29.388: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 22:53:31.393: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 22:53:31.402: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 22:53:33.388: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 22:53:33.397: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 20 22:53:33.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8504" for this suite. 02/20/23 22:53:33.411
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":224,"skipped":4126,"failed":0}
------------------------------
• [SLOW TEST] [8.295 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:25.13
    Feb 20 22:53:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 22:53:25.133
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:25.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:25.178
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/20/23 22:53:25.212
    Feb 20 22:53:25.245: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8504" to be "running and ready"
    Feb 20 22:53:25.258: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.70462ms
    Feb 20 22:53:25.263: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:53:27.274: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.028379989s
    Feb 20 22:53:27.274: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 20 22:53:27.274: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 02/20/23 22:53:27.299
    Feb 20 22:53:27.321: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-8504" to be "running and ready"
    Feb 20 22:53:27.329: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.195712ms
    Feb 20 22:53:27.329: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:53:29.338: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.0171965s
    Feb 20 22:53:29.338: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 20 22:53:29.338: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/20/23 22:53:29.347
    STEP: delete the pod with lifecycle hook 02/20/23 22:53:29.366
    Feb 20 22:53:29.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 20 22:53:29.388: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 20 22:53:31.393: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 20 22:53:31.402: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 20 22:53:33.388: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 20 22:53:33.397: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 20 22:53:33.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8504" for this suite. 02/20/23 22:53:33.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:33.428
Feb 20 22:53:33.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:53:33.431
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:33.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:33.482
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 02/20/23 22:53:33.491
Feb 20 22:53:33.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9780 cluster-info'
Feb 20 22:53:33.614: INFO: stderr: ""
Feb 20 22:53:33.615: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:53:33.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9780" for this suite. 02/20/23 22:53:33.626
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":225,"skipped":4131,"failed":0}
------------------------------
• [0.214 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:33.428
    Feb 20 22:53:33.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:53:33.431
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:33.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:33.482
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 02/20/23 22:53:33.491
    Feb 20 22:53:33.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9780 cluster-info'
    Feb 20 22:53:33.614: INFO: stderr: ""
    Feb 20 22:53:33.615: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:53:33.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9780" for this suite. 02/20/23 22:53:33.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:33.644
Feb 20 22:53:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replicaset 02/20/23 22:53:33.649
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:33.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:33.703
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 20 22:53:33.711: INFO: Creating ReplicaSet my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e
Feb 20 22:53:33.729: INFO: Pod name my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e: Found 0 pods out of 1
Feb 20 22:53:38.740: INFO: Pod name my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e: Found 1 pods out of 1
Feb 20 22:53:38.740: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e" is running
Feb 20 22:53:38.740: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf" in namespace "replicaset-3103" to be "running"
Feb 20 22:53:38.748: INFO: Pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.078819ms
Feb 20 22:53:38.748: INFO: Pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf" satisfied condition "running"
Feb 20 22:53:38.748: INFO: Pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:33 +0000 UTC Reason: Message:}])
Feb 20 22:53:38.748: INFO: Trying to dial the pod
Feb 20 22:53:43.783: INFO: Controller my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e: Got expected result from replica 1 [my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf]: "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 20 22:53:43.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3103" for this suite. 02/20/23 22:53:43.795
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":226,"skipped":4137,"failed":0}
------------------------------
• [SLOW TEST] [10.166 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:33.644
    Feb 20 22:53:33.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replicaset 02/20/23 22:53:33.649
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:33.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:33.703
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 20 22:53:33.711: INFO: Creating ReplicaSet my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e
    Feb 20 22:53:33.729: INFO: Pod name my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e: Found 0 pods out of 1
    Feb 20 22:53:38.740: INFO: Pod name my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e: Found 1 pods out of 1
    Feb 20 22:53:38.740: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e" is running
    Feb 20 22:53:38.740: INFO: Waiting up to 5m0s for pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf" in namespace "replicaset-3103" to be "running"
    Feb 20 22:53:38.748: INFO: Pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.078819ms
    Feb 20 22:53:38.748: INFO: Pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf" satisfied condition "running"
    Feb 20 22:53:38.748: INFO: Pod "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-20 22:53:33 +0000 UTC Reason: Message:}])
    Feb 20 22:53:38.748: INFO: Trying to dial the pod
    Feb 20 22:53:43.783: INFO: Controller my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e: Got expected result from replica 1 [my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf]: "my-hostname-basic-6eb74cef-4a41-4653-b3d2-03a018cdbd2e-pktzf", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 20 22:53:43.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3103" for this suite. 02/20/23 22:53:43.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:53:43.819
Feb 20 22:53:43.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 22:53:43.821
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:43.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:43.872
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 02/20/23 22:53:43.881
Feb 20 22:53:43.911: INFO: Waiting up to 2m0s for pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" in namespace "var-expansion-543" to be "running"
Feb 20 22:53:43.922: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 10.586983ms
Feb 20 22:53:45.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020097476s
Feb 20 22:53:47.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02010941s
Feb 20 22:53:49.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020267645s
Feb 20 22:53:51.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019809655s
Feb 20 22:53:53.937: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025449833s
Feb 20 22:53:55.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020855663s
Feb 20 22:53:57.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020811345s
Feb 20 22:53:59.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020858203s
Feb 20 22:54:01.936: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024618993s
Feb 20 22:54:03.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019620832s
Feb 20 22:54:05.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022100654s
Feb 20 22:54:07.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019743849s
Feb 20 22:54:09.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021559285s
Feb 20 22:54:11.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01998725s
Feb 20 22:54:13.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020347952s
Feb 20 22:54:15.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 32.020797853s
Feb 20 22:54:17.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021155571s
Feb 20 22:54:19.935: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023537798s
Feb 20 22:54:21.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019903001s
Feb 20 22:54:23.937: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025481423s
Feb 20 22:54:25.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02045757s
Feb 20 22:54:27.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020349359s
Feb 20 22:54:29.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 46.019477564s
Feb 20 22:54:31.935: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023553705s
Feb 20 22:54:33.941: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 50.03014502s
Feb 20 22:54:35.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021249215s
Feb 20 22:54:37.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019984399s
Feb 20 22:54:39.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021651726s
Feb 20 22:54:41.939: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 58.027865754s
Feb 20 22:54:43.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.021888106s
Feb 20 22:54:45.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021753822s
Feb 20 22:54:47.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.020578635s
Feb 20 22:54:49.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.020934365s
Feb 20 22:54:51.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01980367s
Feb 20 22:54:53.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019882264s
Feb 20 22:54:55.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020994956s
Feb 20 22:54:57.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020864959s
Feb 20 22:54:59.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.019106524s
Feb 20 22:55:01.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019996155s
Feb 20 22:55:03.937: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.025997159s
Feb 20 22:55:05.941: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029552441s
Feb 20 22:55:07.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.021354466s
Feb 20 22:55:09.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.021360935s
Feb 20 22:55:11.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02069714s
Feb 20 22:55:13.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.020844043s
Feb 20 22:55:15.939: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027916362s
Feb 20 22:55:17.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019267361s
Feb 20 22:55:19.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019332357s
Feb 20 22:55:21.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018514151s
Feb 20 22:55:23.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019744855s
Feb 20 22:55:25.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.020328549s
Feb 20 22:55:27.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.019685456s
Feb 20 22:55:29.938: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026779814s
Feb 20 22:55:31.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018870461s
Feb 20 22:55:33.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.020960597s
Feb 20 22:55:35.945: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.033693338s
Feb 20 22:55:37.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022336726s
Feb 20 22:55:39.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.02135539s
Feb 20 22:55:41.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019641951s
Feb 20 22:55:43.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021272466s
Feb 20 22:55:43.940: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028918745s
STEP: updating the pod 02/20/23 22:55:43.94
Feb 20 22:55:44.473: INFO: Successfully updated pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145"
STEP: waiting for pod running 02/20/23 22:55:44.474
Feb 20 22:55:44.475: INFO: Waiting up to 2m0s for pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" in namespace "var-expansion-543" to be "running"
Feb 20 22:55:44.483: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.271247ms
Feb 20 22:55:46.493: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Running", Reason="", readiness=true. Elapsed: 2.018431074s
Feb 20 22:55:46.493: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" satisfied condition "running"
STEP: deleting the pod gracefully 02/20/23 22:55:46.493
Feb 20 22:55:46.494: INFO: Deleting pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" in namespace "var-expansion-543"
Feb 20 22:55:46.509: INFO: Wait up to 5m0s for pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 22:56:18.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-543" for this suite. 02/20/23 22:56:18.541
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":227,"skipped":4210,"failed":0}
------------------------------
• [SLOW TEST] [154.738 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:53:43.819
    Feb 20 22:53:43.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 22:53:43.821
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:53:43.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:53:43.872
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 02/20/23 22:53:43.881
    Feb 20 22:53:43.911: INFO: Waiting up to 2m0s for pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" in namespace "var-expansion-543" to be "running"
    Feb 20 22:53:43.922: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 10.586983ms
    Feb 20 22:53:45.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020097476s
    Feb 20 22:53:47.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02010941s
    Feb 20 22:53:49.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020267645s
    Feb 20 22:53:51.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019809655s
    Feb 20 22:53:53.937: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025449833s
    Feb 20 22:53:55.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 12.020855663s
    Feb 20 22:53:57.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020811345s
    Feb 20 22:53:59.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020858203s
    Feb 20 22:54:01.936: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024618993s
    Feb 20 22:54:03.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019620832s
    Feb 20 22:54:05.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022100654s
    Feb 20 22:54:07.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019743849s
    Feb 20 22:54:09.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021559285s
    Feb 20 22:54:11.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01998725s
    Feb 20 22:54:13.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020347952s
    Feb 20 22:54:15.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 32.020797853s
    Feb 20 22:54:17.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 34.021155571s
    Feb 20 22:54:19.935: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023537798s
    Feb 20 22:54:21.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 38.019903001s
    Feb 20 22:54:23.937: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025481423s
    Feb 20 22:54:25.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02045757s
    Feb 20 22:54:27.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020349359s
    Feb 20 22:54:29.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 46.019477564s
    Feb 20 22:54:31.935: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 48.023553705s
    Feb 20 22:54:33.941: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 50.03014502s
    Feb 20 22:54:35.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021249215s
    Feb 20 22:54:37.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019984399s
    Feb 20 22:54:39.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021651726s
    Feb 20 22:54:41.939: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 58.027865754s
    Feb 20 22:54:43.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.021888106s
    Feb 20 22:54:45.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.021753822s
    Feb 20 22:54:47.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.020578635s
    Feb 20 22:54:49.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.020934365s
    Feb 20 22:54:51.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.01980367s
    Feb 20 22:54:53.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019882264s
    Feb 20 22:54:55.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.020994956s
    Feb 20 22:54:57.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020864959s
    Feb 20 22:54:59.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.019106524s
    Feb 20 22:55:01.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019996155s
    Feb 20 22:55:03.937: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.025997159s
    Feb 20 22:55:05.941: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029552441s
    Feb 20 22:55:07.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.021354466s
    Feb 20 22:55:09.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.021360935s
    Feb 20 22:55:11.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02069714s
    Feb 20 22:55:13.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.020844043s
    Feb 20 22:55:15.939: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027916362s
    Feb 20 22:55:17.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.019267361s
    Feb 20 22:55:19.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019332357s
    Feb 20 22:55:21.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018514151s
    Feb 20 22:55:23.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.019744855s
    Feb 20 22:55:25.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.020328549s
    Feb 20 22:55:27.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.019685456s
    Feb 20 22:55:29.938: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026779814s
    Feb 20 22:55:31.930: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018870461s
    Feb 20 22:55:33.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.020960597s
    Feb 20 22:55:35.945: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.033693338s
    Feb 20 22:55:37.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.022336726s
    Feb 20 22:55:39.933: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.02135539s
    Feb 20 22:55:41.931: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019641951s
    Feb 20 22:55:43.932: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.021272466s
    Feb 20 22:55:43.940: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028918745s
    STEP: updating the pod 02/20/23 22:55:43.94
    Feb 20 22:55:44.473: INFO: Successfully updated pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145"
    STEP: waiting for pod running 02/20/23 22:55:44.474
    Feb 20 22:55:44.475: INFO: Waiting up to 2m0s for pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" in namespace "var-expansion-543" to be "running"
    Feb 20 22:55:44.483: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.271247ms
    Feb 20 22:55:46.493: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145": Phase="Running", Reason="", readiness=true. Elapsed: 2.018431074s
    Feb 20 22:55:46.493: INFO: Pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" satisfied condition "running"
    STEP: deleting the pod gracefully 02/20/23 22:55:46.493
    Feb 20 22:55:46.494: INFO: Deleting pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" in namespace "var-expansion-543"
    Feb 20 22:55:46.509: INFO: Wait up to 5m0s for pod "var-expansion-56bda6a9-b76c-4e7f-b32f-99588657e145" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 22:56:18.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-543" for this suite. 02/20/23 22:56:18.541
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:18.558
Feb 20 22:56:18.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 22:56:18.564
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:18.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:18.61
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 22:56:18.67
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:56:18.927
STEP: Deploying the webhook pod 02/20/23 22:56:18.952
STEP: Wait for the deployment to be ready 02/20/23 22:56:18.974
Feb 20 22:56:18.991: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 20 22:56:21.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 56, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 56, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 56, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 56, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 22:56:23.028
STEP: Verifying the service has paired with the endpoint 02/20/23 22:56:23.049
Feb 20 22:56:24.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 02/20/23 22:56:24.075
STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:56:24.118
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/20/23 22:56:24.14
STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:56:24.195
STEP: Patching a validating webhook configuration's rules to include the create operation 02/20/23 22:56:24.245
STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:56:24.265
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:56:24.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8325" for this suite. 02/20/23 22:56:24.332
STEP: Destroying namespace "webhook-8325-markers" for this suite. 02/20/23 22:56:24.347
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":228,"skipped":4211,"failed":0}
------------------------------
• [SLOW TEST] [5.983 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:18.558
    Feb 20 22:56:18.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 22:56:18.564
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:18.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:18.61
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 22:56:18.67
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 22:56:18.927
    STEP: Deploying the webhook pod 02/20/23 22:56:18.952
    STEP: Wait for the deployment to be ready 02/20/23 22:56:18.974
    Feb 20 22:56:18.991: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Feb 20 22:56:21.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 22, 56, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 56, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 22, 56, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 22, 56, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 22:56:23.028
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:56:23.049
    Feb 20 22:56:24.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 02/20/23 22:56:24.075
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:56:24.118
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/20/23 22:56:24.14
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:56:24.195
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/20/23 22:56:24.245
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/20/23 22:56:24.265
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:56:24.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8325" for this suite. 02/20/23 22:56:24.332
    STEP: Destroying namespace "webhook-8325-markers" for this suite. 02/20/23 22:56:24.347
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:24.548
Feb 20 22:56:24.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 22:56:24.552
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:24.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:24.606
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/20/23 22:56:24.628
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-780.svc.cluster.local;sleep 1; done
 02/20/23 22:56:24.706
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-780.svc.cluster.local;sleep 1; done
 02/20/23 22:56:24.706
STEP: creating a pod to probe DNS 02/20/23 22:56:24.706
STEP: submitting the pod to kubernetes 02/20/23 22:56:24.706
Feb 20 22:56:24.774: INFO: Waiting up to 15m0s for pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd" in namespace "dns-780" to be "running"
Feb 20 22:56:24.783: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.819467ms
Feb 20 22:56:26.792: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018184409s
Feb 20 22:56:28.793: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd": Phase="Running", Reason="", readiness=true. Elapsed: 4.018716695s
Feb 20 22:56:28.793: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd" satisfied condition "running"
STEP: retrieving the pod 02/20/23 22:56:28.793
STEP: looking for the results for each expected name from probers 02/20/23 22:56:28.802
Feb 20 22:56:28.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
Feb 20 22:56:28.829: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
Feb 20 22:56:28.840: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
Feb 20 22:56:28.864: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
Feb 20 22:56:28.876: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
Feb 20 22:56:28.887: INFO: Unable to read jessie_udp@dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
Feb 20 22:56:28.899: INFO: Lookups using dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local wheezy_udp@dns-test-service-2.dns-780.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local jessie_udp@dns-test-service-2.dns-780.svc.cluster.local]

Feb 20 22:56:33.997: INFO: DNS probes using dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd succeeded

STEP: deleting the pod 02/20/23 22:56:33.997
STEP: deleting the test headless service 02/20/23 22:56:34.028
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 22:56:34.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-780" for this suite. 02/20/23 22:56:34.064
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":229,"skipped":4231,"failed":0}
------------------------------
• [SLOW TEST] [9.529 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:24.548
    Feb 20 22:56:24.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 22:56:24.552
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:24.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:24.606
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/20/23 22:56:24.628
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-780.svc.cluster.local;sleep 1; done
     02/20/23 22:56:24.706
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-780.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-780.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-780.svc.cluster.local;sleep 1; done
     02/20/23 22:56:24.706
    STEP: creating a pod to probe DNS 02/20/23 22:56:24.706
    STEP: submitting the pod to kubernetes 02/20/23 22:56:24.706
    Feb 20 22:56:24.774: INFO: Waiting up to 15m0s for pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd" in namespace "dns-780" to be "running"
    Feb 20 22:56:24.783: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.819467ms
    Feb 20 22:56:26.792: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018184409s
    Feb 20 22:56:28.793: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd": Phase="Running", Reason="", readiness=true. Elapsed: 4.018716695s
    Feb 20 22:56:28.793: INFO: Pod "dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 22:56:28.793
    STEP: looking for the results for each expected name from probers 02/20/23 22:56:28.802
    Feb 20 22:56:28.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
    Feb 20 22:56:28.829: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
    Feb 20 22:56:28.840: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
    Feb 20 22:56:28.864: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
    Feb 20 22:56:28.876: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
    Feb 20 22:56:28.887: INFO: Unable to read jessie_udp@dns-test-service-2.dns-780.svc.cluster.local from pod dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd: the server could not find the requested resource (get pods dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd)
    Feb 20 22:56:28.899: INFO: Lookups using dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local wheezy_udp@dns-test-service-2.dns-780.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-780.svc.cluster.local jessie_udp@dns-test-service-2.dns-780.svc.cluster.local]

    Feb 20 22:56:33.997: INFO: DNS probes using dns-780/dns-test-0c7619bc-4b62-43fd-a397-2eeb02cb79cd succeeded

    STEP: deleting the pod 02/20/23 22:56:33.997
    STEP: deleting the test headless service 02/20/23 22:56:34.028
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 22:56:34.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-780" for this suite. 02/20/23 22:56:34.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:34.084
Feb 20 22:56:34.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir-wrapper 02/20/23 22:56:34.087
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:34.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:34.142
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 20 22:56:34.221: INFO: Waiting up to 5m0s for pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644" in namespace "emptydir-wrapper-9833" to be "running and ready"
Feb 20 22:56:34.229: INFO: Pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650781ms
Feb 20 22:56:34.229: INFO: The phase of Pod pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:56:36.241: INFO: Pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644": Phase="Running", Reason="", readiness=true. Elapsed: 2.019920942s
Feb 20 22:56:36.241: INFO: The phase of Pod pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644 is Running (Ready = true)
Feb 20 22:56:36.241: INFO: Pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/20/23 22:56:36.249
STEP: Cleaning up the configmap 02/20/23 22:56:36.262
STEP: Cleaning up the pod 02/20/23 22:56:36.283
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 20 22:56:36.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9833" for this suite. 02/20/23 22:56:36.322
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":230,"skipped":4239,"failed":0}
------------------------------
• [2.258 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:34.084
    Feb 20 22:56:34.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir-wrapper 02/20/23 22:56:34.087
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:34.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:34.142
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 20 22:56:34.221: INFO: Waiting up to 5m0s for pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644" in namespace "emptydir-wrapper-9833" to be "running and ready"
    Feb 20 22:56:34.229: INFO: Pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650781ms
    Feb 20 22:56:34.229: INFO: The phase of Pod pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:56:36.241: INFO: Pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644": Phase="Running", Reason="", readiness=true. Elapsed: 2.019920942s
    Feb 20 22:56:36.241: INFO: The phase of Pod pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644 is Running (Ready = true)
    Feb 20 22:56:36.241: INFO: Pod "pod-secrets-8e2a247c-8a46-47a4-811d-572614b31644" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/20/23 22:56:36.249
    STEP: Cleaning up the configmap 02/20/23 22:56:36.262
    STEP: Cleaning up the pod 02/20/23 22:56:36.283
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:56:36.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-9833" for this suite. 02/20/23 22:56:36.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:36.347
Feb 20 22:56:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 22:56:36.352
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:36.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:36.406
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/20/23 22:56:36.418
Feb 20 22:56:36.452: INFO: Waiting up to 5m0s for pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf" in namespace "emptydir-9946" to be "Succeeded or Failed"
Feb 20 22:56:36.465: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.517455ms
Feb 20 22:56:38.475: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02306399s
Feb 20 22:56:40.476: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023738984s
STEP: Saw pod success 02/20/23 22:56:40.476
Feb 20 22:56:40.476: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf" satisfied condition "Succeeded or Failed"
Feb 20 22:56:40.486: INFO: Trying to get logs from node 10.8.38.66 pod pod-5048128d-e11b-4dc0-8209-86ca89250faf container test-container: <nil>
STEP: delete the pod 02/20/23 22:56:40.528
Feb 20 22:56:40.547: INFO: Waiting for pod pod-5048128d-e11b-4dc0-8209-86ca89250faf to disappear
Feb 20 22:56:40.557: INFO: Pod pod-5048128d-e11b-4dc0-8209-86ca89250faf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 22:56:40.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9946" for this suite. 02/20/23 22:56:40.571
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":231,"skipped":4270,"failed":0}
------------------------------
• [4.238 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:36.347
    Feb 20 22:56:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 22:56:36.352
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:36.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:36.406
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/20/23 22:56:36.418
    Feb 20 22:56:36.452: INFO: Waiting up to 5m0s for pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf" in namespace "emptydir-9946" to be "Succeeded or Failed"
    Feb 20 22:56:36.465: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.517455ms
    Feb 20 22:56:38.475: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02306399s
    Feb 20 22:56:40.476: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023738984s
    STEP: Saw pod success 02/20/23 22:56:40.476
    Feb 20 22:56:40.476: INFO: Pod "pod-5048128d-e11b-4dc0-8209-86ca89250faf" satisfied condition "Succeeded or Failed"
    Feb 20 22:56:40.486: INFO: Trying to get logs from node 10.8.38.66 pod pod-5048128d-e11b-4dc0-8209-86ca89250faf container test-container: <nil>
    STEP: delete the pod 02/20/23 22:56:40.528
    Feb 20 22:56:40.547: INFO: Waiting for pod pod-5048128d-e11b-4dc0-8209-86ca89250faf to disappear
    Feb 20 22:56:40.557: INFO: Pod pod-5048128d-e11b-4dc0-8209-86ca89250faf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 22:56:40.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9946" for this suite. 02/20/23 22:56:40.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:40.586
Feb 20 22:56:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-webhook 02/20/23 22:56:40.591
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:40.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:40.667
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/20/23 22:56:40.677
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/20/23 22:56:41.294
STEP: Deploying the custom resource conversion webhook pod 02/20/23 22:56:41.313
STEP: Wait for the deployment to be ready 02/20/23 22:56:41.336
Feb 20 22:56:41.353: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/20/23 22:56:43.38
STEP: Verifying the service has paired with the endpoint 02/20/23 22:56:43.403
Feb 20 22:56:44.404: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 20 22:56:44.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Creating a v1 custom resource 02/20/23 22:56:47.126
STEP: Create a v2 custom resource 02/20/23 22:56:47.196
STEP: List CRs in v1 02/20/23 22:56:47.28
STEP: List CRs in v2 02/20/23 22:56:47.293
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 22:56:47.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2833" for this suite. 02/20/23 22:56:47.853
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":232,"skipped":4275,"failed":0}
------------------------------
• [SLOW TEST] [7.364 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:40.586
    Feb 20 22:56:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-webhook 02/20/23 22:56:40.591
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:40.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:40.667
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/20/23 22:56:40.677
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/20/23 22:56:41.294
    STEP: Deploying the custom resource conversion webhook pod 02/20/23 22:56:41.313
    STEP: Wait for the deployment to be ready 02/20/23 22:56:41.336
    Feb 20 22:56:41.353: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/20/23 22:56:43.38
    STEP: Verifying the service has paired with the endpoint 02/20/23 22:56:43.403
    Feb 20 22:56:44.404: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 20 22:56:44.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Creating a v1 custom resource 02/20/23 22:56:47.126
    STEP: Create a v2 custom resource 02/20/23 22:56:47.196
    STEP: List CRs in v1 02/20/23 22:56:47.28
    STEP: List CRs in v2 02/20/23 22:56:47.293
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 22:56:47.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-2833" for this suite. 02/20/23 22:56:47.853
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:47.953
Feb 20 22:56:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 22:56:47.955
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:48.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:48.051
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 02/20/23 22:56:48.061
STEP: submitting the pod to kubernetes 02/20/23 22:56:48.061
Feb 20 22:56:48.092: INFO: Waiting up to 5m0s for pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" in namespace "pods-5982" to be "running and ready"
Feb 20 22:56:48.129: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.81373ms
Feb 20 22:56:48.129: INFO: The phase of Pod pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:56:50.140: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04824622s
Feb 20 22:56:50.140: INFO: The phase of Pod pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:56:52.138: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Running", Reason="", readiness=true. Elapsed: 4.046135817s
Feb 20 22:56:52.138: INFO: The phase of Pod pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4 is Running (Ready = true)
Feb 20 22:56:52.138: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/20/23 22:56:52.146
STEP: updating the pod 02/20/23 22:56:52.156
Feb 20 22:56:52.692: INFO: Successfully updated pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4"
Feb 20 22:56:52.692: INFO: Waiting up to 5m0s for pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" in namespace "pods-5982" to be "running"
Feb 20 22:56:52.704: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Running", Reason="", readiness=true. Elapsed: 11.700421ms
Feb 20 22:56:52.704: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/20/23 22:56:52.704
Feb 20 22:56:52.713: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 22:56:52.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5982" for this suite. 02/20/23 22:56:52.725
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":233,"skipped":4292,"failed":0}
------------------------------
• [4.788 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:47.953
    Feb 20 22:56:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 22:56:47.955
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:48.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:48.051
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 02/20/23 22:56:48.061
    STEP: submitting the pod to kubernetes 02/20/23 22:56:48.061
    Feb 20 22:56:48.092: INFO: Waiting up to 5m0s for pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" in namespace "pods-5982" to be "running and ready"
    Feb 20 22:56:48.129: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.81373ms
    Feb 20 22:56:48.129: INFO: The phase of Pod pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:56:50.140: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04824622s
    Feb 20 22:56:50.140: INFO: The phase of Pod pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:56:52.138: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Running", Reason="", readiness=true. Elapsed: 4.046135817s
    Feb 20 22:56:52.138: INFO: The phase of Pod pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4 is Running (Ready = true)
    Feb 20 22:56:52.138: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/20/23 22:56:52.146
    STEP: updating the pod 02/20/23 22:56:52.156
    Feb 20 22:56:52.692: INFO: Successfully updated pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4"
    Feb 20 22:56:52.692: INFO: Waiting up to 5m0s for pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" in namespace "pods-5982" to be "running"
    Feb 20 22:56:52.704: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4": Phase="Running", Reason="", readiness=true. Elapsed: 11.700421ms
    Feb 20 22:56:52.704: INFO: Pod "pod-update-fc770a36-fb53-4388-8e05-e8de5a4219b4" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/20/23 22:56:52.704
    Feb 20 22:56:52.713: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 22:56:52.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5982" for this suite. 02/20/23 22:56:52.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:56:52.747
Feb 20 22:56:52.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:56:52.749
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:52.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:52.813
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Feb 20 22:56:52.869: INFO: created pod
Feb 20 22:56:52.869: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9893" to be "Succeeded or Failed"
Feb 20 22:56:52.878: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.417325ms
Feb 20 22:56:54.894: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024693066s
Feb 20 22:56:56.887: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018066268s
Feb 20 22:56:58.887: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017920592s
STEP: Saw pod success 02/20/23 22:56:58.887
Feb 20 22:56:58.887: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 20 22:57:28.889: INFO: polling logs
Feb 20 22:57:28.930: INFO: Pod logs: 
I0220 22:56:54.268877       1 log.go:195] OK: Got token
I0220 22:56:54.268943       1 log.go:195] validating with in-cluster discovery
I0220 22:56:54.269559       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0220 22:56:54.269608       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9893:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676934413, NotBefore:1676933813, IssuedAt:1676933813, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9893", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"7017ba79-f9db-4241-bcfb-b51b4f6469e6"}}}
I0220 22:56:54.304438       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0220 22:56:54.338524       1 log.go:195] OK: Validated signature on JWT
I0220 22:56:54.338730       1 log.go:195] OK: Got valid claims from token!
I0220 22:56:54.338761       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9893:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676934413, NotBefore:1676933813, IssuedAt:1676933813, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9893", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"7017ba79-f9db-4241-bcfb-b51b4f6469e6"}}}

Feb 20 22:57:28.930: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 20 22:57:28.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9893" for this suite. 02/20/23 22:57:28.963
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":234,"skipped":4321,"failed":0}
------------------------------
• [SLOW TEST] [36.230 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:56:52.747
    Feb 20 22:56:52.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:56:52.749
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:56:52.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:56:52.813
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Feb 20 22:56:52.869: INFO: created pod
    Feb 20 22:56:52.869: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9893" to be "Succeeded or Failed"
    Feb 20 22:56:52.878: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.417325ms
    Feb 20 22:56:54.894: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024693066s
    Feb 20 22:56:56.887: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018066268s
    Feb 20 22:56:58.887: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017920592s
    STEP: Saw pod success 02/20/23 22:56:58.887
    Feb 20 22:56:58.887: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 20 22:57:28.889: INFO: polling logs
    Feb 20 22:57:28.930: INFO: Pod logs: 
    I0220 22:56:54.268877       1 log.go:195] OK: Got token
    I0220 22:56:54.268943       1 log.go:195] validating with in-cluster discovery
    I0220 22:56:54.269559       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0220 22:56:54.269608       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9893:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676934413, NotBefore:1676933813, IssuedAt:1676933813, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9893", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"7017ba79-f9db-4241-bcfb-b51b4f6469e6"}}}
    I0220 22:56:54.304438       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0220 22:56:54.338524       1 log.go:195] OK: Validated signature on JWT
    I0220 22:56:54.338730       1 log.go:195] OK: Got valid claims from token!
    I0220 22:56:54.338761       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-9893:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676934413, NotBefore:1676933813, IssuedAt:1676933813, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9893", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"7017ba79-f9db-4241-bcfb-b51b4f6469e6"}}}

    Feb 20 22:57:28.930: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 20 22:57:28.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9893" for this suite. 02/20/23 22:57:28.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:57:28.985
Feb 20 22:57:28.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename init-container 02/20/23 22:57:28.987
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:29.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:29.039
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 02/20/23 22:57:29.045
Feb 20 22:57:29.045: INFO: PodSpec: initContainers in spec.initContainers
W0220 22:57:29.075047      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 22:57:35.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6064" for this suite. 02/20/23 22:57:35.359
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":235,"skipped":4353,"failed":0}
------------------------------
• [SLOW TEST] [6.389 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:57:28.985
    Feb 20 22:57:28.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename init-container 02/20/23 22:57:28.987
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:29.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:29.039
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 02/20/23 22:57:29.045
    Feb 20 22:57:29.045: INFO: PodSpec: initContainers in spec.initContainers
    W0220 22:57:29.075047      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 22:57:35.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6064" for this suite. 02/20/23 22:57:35.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:57:35.376
Feb 20 22:57:35.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 22:57:35.379
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:35.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:35.428
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 20 22:57:35.438: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 20 22:57:35.459: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 20 22:57:40.469: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/20/23 22:57:40.469
Feb 20 22:57:40.469: INFO: Creating deployment "test-rolling-update-deployment"
Feb 20 22:57:40.482: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 20 22:57:40.498: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 20 22:57:42.523: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 20 22:57:42.530: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 22:57:42.552: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3035  b9bfeebe-d5f2-4dab-8157-5d30156a181a 113611 1 2023-02-20 22:57:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-20 22:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d22188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-20 22:57:40 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-20 22:57:42 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 22:57:42.566: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3035  144882bd-5e09-435f-8120-5009db219395 113601 1 2023-02-20 22:57:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b9bfeebe-d5f2-4dab-8157-5d30156a181a 0xc003d226a7 0xc003d226a8}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9bfeebe-d5f2-4dab-8157-5d30156a181a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d22758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:57:42.566: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 20 22:57:42.566: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3035  e1d48104-2ece-4aa2-a9cd-39bfee907cb8 113610 2 2023-02-20 22:57:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b9bfeebe-d5f2-4dab-8157-5d30156a181a 0xc003d22577 0xc003d22578}] [] [{e2e.test Update apps/v1 2023-02-20 22:57:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9bfeebe-d5f2-4dab-8157-5d30156a181a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d22638 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 22:57:42.575: INFO: Pod "test-rolling-update-deployment-78f575d8ff-pdjfn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-pdjfn test-rolling-update-deployment-78f575d8ff- deployment-3035  0062e1d1-17e2-4e68-9a07-0b242cdaff5e 113600 0 2023-02-20 22:57:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:2db6f06ba135001bd3541f9988f5c77a6ccf74e248a015ce720ed5b01276dd97 cni.projectcalico.org/podIP:172.30.31.191/32 cni.projectcalico.org/podIPs:172.30.31.191/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.31.191"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 144882bd-5e09-435f-8120-5009db219395 0xc0063ffa87 0xc0063ffa88}] [] [{kube-controller-manager Update v1 2023-02-20 22:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"144882bd-5e09-435f-8120-5009db219395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9h9nl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9h9nl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ddvb2,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:172.30.31.191,StartTime:2023-02-20 22:57:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:57:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://eaaa3c00b01177bbb429cd79a73ceda5647c8f7d72e5b35e3f035cdcca2ad5d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 22:57:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3035" for this suite. 02/20/23 22:57:42.587
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":236,"skipped":4367,"failed":0}
------------------------------
• [SLOW TEST] [7.224 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:57:35.376
    Feb 20 22:57:35.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 22:57:35.379
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:35.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:35.428
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 20 22:57:35.438: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 20 22:57:35.459: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 20 22:57:40.469: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/20/23 22:57:40.469
    Feb 20 22:57:40.469: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 20 22:57:40.482: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 20 22:57:40.498: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb 20 22:57:42.523: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 20 22:57:42.530: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 22:57:42.552: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3035  b9bfeebe-d5f2-4dab-8157-5d30156a181a 113611 1 2023-02-20 22:57:40 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-20 22:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d22188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-20 22:57:40 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-20 22:57:42 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 20 22:57:42.566: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3035  144882bd-5e09-435f-8120-5009db219395 113601 1 2023-02-20 22:57:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b9bfeebe-d5f2-4dab-8157-5d30156a181a 0xc003d226a7 0xc003d226a8}] [] [{kube-controller-manager Update apps/v1 2023-02-20 22:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9bfeebe-d5f2-4dab-8157-5d30156a181a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d22758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:57:42.566: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 20 22:57:42.566: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3035  e1d48104-2ece-4aa2-a9cd-39bfee907cb8 113610 2 2023-02-20 22:57:35 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b9bfeebe-d5f2-4dab-8157-5d30156a181a 0xc003d22577 0xc003d22578}] [] [{e2e.test Update apps/v1 2023-02-20 22:57:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9bfeebe-d5f2-4dab-8157-5d30156a181a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d22638 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 22:57:42.575: INFO: Pod "test-rolling-update-deployment-78f575d8ff-pdjfn" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-pdjfn test-rolling-update-deployment-78f575d8ff- deployment-3035  0062e1d1-17e2-4e68-9a07-0b242cdaff5e 113600 0 2023-02-20 22:57:40 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:2db6f06ba135001bd3541f9988f5c77a6ccf74e248a015ce720ed5b01276dd97 cni.projectcalico.org/podIP:172.30.31.191/32 cni.projectcalico.org/podIPs:172.30.31.191/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.191"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.31.191"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 144882bd-5e09-435f-8120-5009db219395 0xc0063ffa87 0xc0063ffa88}] [] [{kube-controller-manager Update v1 2023-02-20 22:57:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"144882bd-5e09-435f-8120-5009db219395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-20 22:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-02-20 22:57:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 22:57:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.31.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9h9nl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9h9nl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.70,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ddvb2,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 22:57:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.70,PodIP:172.30.31.191,StartTime:2023-02-20 22:57:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 22:57:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://eaaa3c00b01177bbb429cd79a73ceda5647c8f7d72e5b35e3f035cdcca2ad5d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.31.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 22:57:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3035" for this suite. 02/20/23 22:57:42.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:57:42.601
Feb 20 22:57:42.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:57:42.602
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:42.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:42.647
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Feb 20 22:57:42.708: INFO: created pod pod-service-account-defaultsa
Feb 20 22:57:42.708: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 20 22:57:42.740: INFO: created pod pod-service-account-mountsa
Feb 20 22:57:42.740: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 20 22:57:42.769: INFO: created pod pod-service-account-nomountsa
Feb 20 22:57:42.769: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 20 22:57:42.795: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 20 22:57:42.795: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 20 22:57:42.820: INFO: created pod pod-service-account-mountsa-mountspec
Feb 20 22:57:42.820: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 20 22:57:42.841: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 20 22:57:42.841: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 20 22:57:42.873: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 20 22:57:42.873: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 20 22:57:42.898: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 20 22:57:42.898: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 20 22:57:42.930: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 20 22:57:42.930: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 20 22:57:42.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6653" for this suite. 02/20/23 22:57:42.944
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":237,"skipped":4376,"failed":0}
------------------------------
• [0.371 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:57:42.601
    Feb 20 22:57:42.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:57:42.602
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:42.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:42.647
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Feb 20 22:57:42.708: INFO: created pod pod-service-account-defaultsa
    Feb 20 22:57:42.708: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 20 22:57:42.740: INFO: created pod pod-service-account-mountsa
    Feb 20 22:57:42.740: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 20 22:57:42.769: INFO: created pod pod-service-account-nomountsa
    Feb 20 22:57:42.769: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 20 22:57:42.795: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 20 22:57:42.795: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 20 22:57:42.820: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 20 22:57:42.820: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 20 22:57:42.841: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 20 22:57:42.841: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 20 22:57:42.873: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 20 22:57:42.873: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 20 22:57:42.898: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 20 22:57:42.898: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 20 22:57:42.930: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 20 22:57:42.930: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 20 22:57:42.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6653" for this suite. 02/20/23 22:57:42.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:57:42.975
Feb 20 22:57:42.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 22:57:42.976
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:43.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:43.055
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/20/23 22:57:43.073
STEP: create the rc2 02/20/23 22:57:43.099
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/20/23 22:57:48.12
STEP: delete the rc simpletest-rc-to-be-deleted 02/20/23 22:57:49.046
STEP: wait for the rc to be deleted 02/20/23 22:57:49.058
STEP: Gathering metrics 02/20/23 22:57:54.117
W0220 22:57:54.131468      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Feb 20 22:57:54.131: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 20 22:57:54.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-2blbc" in namespace "gc-8041"
Feb 20 22:57:54.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d2vq" in namespace "gc-8041"
Feb 20 22:57:54.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jznb" in namespace "gc-8041"
Feb 20 22:57:54.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kxnx" in namespace "gc-8041"
Feb 20 22:57:54.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-2trvr" in namespace "gc-8041"
Feb 20 22:57:54.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v6hv" in namespace "gc-8041"
Feb 20 22:57:54.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z8bf" in namespace "gc-8041"
Feb 20 22:57:54.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-4266b" in namespace "gc-8041"
Feb 20 22:57:54.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-48687" in namespace "gc-8041"
Feb 20 22:57:54.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qndv" in namespace "gc-8041"
Feb 20 22:57:54.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w4br" in namespace "gc-8041"
Feb 20 22:57:54.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fsmp" in namespace "gc-8041"
Feb 20 22:57:54.452: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vz5b" in namespace "gc-8041"
Feb 20 22:57:54.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-6q4x6" in namespace "gc-8041"
Feb 20 22:57:54.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-6t9mp" in namespace "gc-8041"
Feb 20 22:57:54.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r2xf" in namespace "gc-8041"
Feb 20 22:57:54.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s744" in namespace "gc-8041"
Feb 20 22:57:54.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tbwr" in namespace "gc-8041"
Feb 20 22:57:54.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wq45" in namespace "gc-8041"
Feb 20 22:57:54.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-88fmq" in namespace "gc-8041"
Feb 20 22:57:54.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f6q9" in namespace "gc-8041"
Feb 20 22:57:54.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jw42" in namespace "gc-8041"
Feb 20 22:57:54.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qw6x" in namespace "gc-8041"
Feb 20 22:57:54.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-96b5j" in namespace "gc-8041"
Feb 20 22:57:54.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cs2h" in namespace "gc-8041"
Feb 20 22:57:54.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jlj5" in namespace "gc-8041"
Feb 20 22:57:54.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q2hd" in namespace "gc-8041"
Feb 20 22:57:54.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vgfl" in namespace "gc-8041"
Feb 20 22:57:54.951: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zb7x" in namespace "gc-8041"
Feb 20 22:57:54.979: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ztkg" in namespace "gc-8041"
Feb 20 22:57:55.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-blr9k" in namespace "gc-8041"
Feb 20 22:57:55.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxsmt" in namespace "gc-8041"
Feb 20 22:57:55.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4m5j" in namespace "gc-8041"
Feb 20 22:57:55.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-c65pl" in namespace "gc-8041"
Feb 20 22:57:55.127: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg4g6" in namespace "gc-8041"
Feb 20 22:57:55.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7cq7" in namespace "gc-8041"
Feb 20 22:57:55.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg6xv" in namespace "gc-8041"
Feb 20 22:57:55.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgxw2" in namespace "gc-8041"
Feb 20 22:57:55.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl9tp" in namespace "gc-8041"
Feb 20 22:57:55.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-f79g6" in namespace "gc-8041"
Feb 20 22:57:55.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc6n9" in namespace "gc-8041"
Feb 20 22:57:55.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpgww" in namespace "gc-8041"
Feb 20 22:57:55.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq6nt" in namespace "gc-8041"
Feb 20 22:57:55.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-frklx" in namespace "gc-8041"
Feb 20 22:57:55.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvkms" in namespace "gc-8041"
Feb 20 22:57:55.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvlkm" in namespace "gc-8041"
Feb 20 22:57:55.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2cr8" in namespace "gc-8041"
Feb 20 22:57:55.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbxd5" in namespace "gc-8041"
Feb 20 22:57:55.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcm7n" in namespace "gc-8041"
Feb 20 22:57:55.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxhxq" in namespace "gc-8041"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 22:57:55.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8041" for this suite. 02/20/23 22:57:55.74
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":238,"skipped":4382,"failed":0}
------------------------------
• [SLOW TEST] [12.780 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:57:42.975
    Feb 20 22:57:42.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 22:57:42.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:43.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:43.055
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/20/23 22:57:43.073
    STEP: create the rc2 02/20/23 22:57:43.099
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/20/23 22:57:48.12
    STEP: delete the rc simpletest-rc-to-be-deleted 02/20/23 22:57:49.046
    STEP: wait for the rc to be deleted 02/20/23 22:57:49.058
    STEP: Gathering metrics 02/20/23 22:57:54.117
    W0220 22:57:54.131468      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Feb 20 22:57:54.131: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 20 22:57:54.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-2blbc" in namespace "gc-8041"
    Feb 20 22:57:54.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d2vq" in namespace "gc-8041"
    Feb 20 22:57:54.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jznb" in namespace "gc-8041"
    Feb 20 22:57:54.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-2kxnx" in namespace "gc-8041"
    Feb 20 22:57:54.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-2trvr" in namespace "gc-8041"
    Feb 20 22:57:54.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v6hv" in namespace "gc-8041"
    Feb 20 22:57:54.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-2z8bf" in namespace "gc-8041"
    Feb 20 22:57:54.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-4266b" in namespace "gc-8041"
    Feb 20 22:57:54.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-48687" in namespace "gc-8041"
    Feb 20 22:57:54.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qndv" in namespace "gc-8041"
    Feb 20 22:57:54.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w4br" in namespace "gc-8041"
    Feb 20 22:57:54.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fsmp" in namespace "gc-8041"
    Feb 20 22:57:54.452: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vz5b" in namespace "gc-8041"
    Feb 20 22:57:54.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-6q4x6" in namespace "gc-8041"
    Feb 20 22:57:54.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-6t9mp" in namespace "gc-8041"
    Feb 20 22:57:54.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-7r2xf" in namespace "gc-8041"
    Feb 20 22:57:54.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-7s744" in namespace "gc-8041"
    Feb 20 22:57:54.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-7tbwr" in namespace "gc-8041"
    Feb 20 22:57:54.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wq45" in namespace "gc-8041"
    Feb 20 22:57:54.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-88fmq" in namespace "gc-8041"
    Feb 20 22:57:54.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f6q9" in namespace "gc-8041"
    Feb 20 22:57:54.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jw42" in namespace "gc-8041"
    Feb 20 22:57:54.779: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qw6x" in namespace "gc-8041"
    Feb 20 22:57:54.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-96b5j" in namespace "gc-8041"
    Feb 20 22:57:54.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cs2h" in namespace "gc-8041"
    Feb 20 22:57:54.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jlj5" in namespace "gc-8041"
    Feb 20 22:57:54.905: INFO: Deleting pod "simpletest-rc-to-be-deleted-9q2hd" in namespace "gc-8041"
    Feb 20 22:57:54.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vgfl" in namespace "gc-8041"
    Feb 20 22:57:54.951: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zb7x" in namespace "gc-8041"
    Feb 20 22:57:54.979: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ztkg" in namespace "gc-8041"
    Feb 20 22:57:55.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-blr9k" in namespace "gc-8041"
    Feb 20 22:57:55.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxsmt" in namespace "gc-8041"
    Feb 20 22:57:55.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4m5j" in namespace "gc-8041"
    Feb 20 22:57:55.086: INFO: Deleting pod "simpletest-rc-to-be-deleted-c65pl" in namespace "gc-8041"
    Feb 20 22:57:55.127: INFO: Deleting pod "simpletest-rc-to-be-deleted-cg4g6" in namespace "gc-8041"
    Feb 20 22:57:55.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7cq7" in namespace "gc-8041"
    Feb 20 22:57:55.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-dg6xv" in namespace "gc-8041"
    Feb 20 22:57:55.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgxw2" in namespace "gc-8041"
    Feb 20 22:57:55.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl9tp" in namespace "gc-8041"
    Feb 20 22:57:55.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-f79g6" in namespace "gc-8041"
    Feb 20 22:57:55.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc6n9" in namespace "gc-8041"
    Feb 20 22:57:55.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpgww" in namespace "gc-8041"
    Feb 20 22:57:55.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq6nt" in namespace "gc-8041"
    Feb 20 22:57:55.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-frklx" in namespace "gc-8041"
    Feb 20 22:57:55.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvkms" in namespace "gc-8041"
    Feb 20 22:57:55.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvlkm" in namespace "gc-8041"
    Feb 20 22:57:55.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2cr8" in namespace "gc-8041"
    Feb 20 22:57:55.645: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbxd5" in namespace "gc-8041"
    Feb 20 22:57:55.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcm7n" in namespace "gc-8041"
    Feb 20 22:57:55.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxhxq" in namespace "gc-8041"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 22:57:55.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8041" for this suite. 02/20/23 22:57:55.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:57:55.757
Feb 20 22:57:55.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 22:57:55.759
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:55.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:55.891
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Feb 20 22:57:55.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9984 version'
Feb 20 22:57:56.270: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 20 22:57:56.270: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+a34b9e9\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2023-01-10T15:55:28Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 22:57:56.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9984" for this suite. 02/20/23 22:57:56.29
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":239,"skipped":4407,"failed":0}
------------------------------
• [0.586 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:57:55.757
    Feb 20 22:57:55.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 22:57:55.759
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:55.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:55.891
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Feb 20 22:57:55.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-9984 version'
    Feb 20 22:57:56.270: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 20 22:57:56.270: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+a34b9e9\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2023-01-10T15:55:28Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 22:57:56.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9984" for this suite. 02/20/23 22:57:56.29
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:57:56.344
Feb 20 22:57:56.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename containers 02/20/23 22:57:56.345
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:56.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:56.464
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Feb 20 22:57:56.514: INFO: Waiting up to 5m0s for pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665" in namespace "containers-1245" to be "running"
Feb 20 22:57:56.532: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 17.941766ms
Feb 20 22:57:58.542: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027576933s
Feb 20 22:58:00.542: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027425245s
Feb 20 22:58:02.543: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028444771s
Feb 20 22:58:04.543: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Running", Reason="", readiness=true. Elapsed: 8.028656732s
Feb 20 22:58:04.543: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 20 22:58:04.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1245" for this suite. 02/20/23 22:58:04.576
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":240,"skipped":4407,"failed":0}
------------------------------
• [SLOW TEST] [8.255 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:57:56.344
    Feb 20 22:57:56.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename containers 02/20/23 22:57:56.345
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:57:56.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:57:56.464
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Feb 20 22:57:56.514: INFO: Waiting up to 5m0s for pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665" in namespace "containers-1245" to be "running"
    Feb 20 22:57:56.532: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 17.941766ms
    Feb 20 22:57:58.542: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027576933s
    Feb 20 22:58:00.542: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027425245s
    Feb 20 22:58:02.543: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028444771s
    Feb 20 22:58:04.543: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665": Phase="Running", Reason="", readiness=true. Elapsed: 8.028656732s
    Feb 20 22:58:04.543: INFO: Pod "client-containers-f7e9c37f-2fd8-4274-bfe8-69604d206665" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 20 22:58:04.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1245" for this suite. 02/20/23 22:58:04.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:04.601
Feb 20 22:58:04.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 22:58:04.602
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:04.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:04.694
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-3e3d54ed-f9d2-48e2-8a42-fadfded5b94e 02/20/23 22:58:04.703
STEP: Creating a pod to test consume configMaps 02/20/23 22:58:04.718
Feb 20 22:58:04.753: INFO: Waiting up to 5m0s for pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2" in namespace "configmap-3521" to be "Succeeded or Failed"
Feb 20 22:58:04.760: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.260152ms
Feb 20 22:58:06.771: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017676725s
Feb 20 22:58:08.779: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025735038s
Feb 20 22:58:10.771: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017376124s
STEP: Saw pod success 02/20/23 22:58:10.771
Feb 20 22:58:10.771: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2" satisfied condition "Succeeded or Failed"
Feb 20 22:58:10.780: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 22:58:10.811
Feb 20 22:58:10.833: INFO: Waiting for pod pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2 to disappear
Feb 20 22:58:10.841: INFO: Pod pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 22:58:10.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3521" for this suite. 02/20/23 22:58:10.853
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":241,"skipped":4444,"failed":0}
------------------------------
• [SLOW TEST] [6.265 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:04.601
    Feb 20 22:58:04.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 22:58:04.602
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:04.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:04.694
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-3e3d54ed-f9d2-48e2-8a42-fadfded5b94e 02/20/23 22:58:04.703
    STEP: Creating a pod to test consume configMaps 02/20/23 22:58:04.718
    Feb 20 22:58:04.753: INFO: Waiting up to 5m0s for pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2" in namespace "configmap-3521" to be "Succeeded or Failed"
    Feb 20 22:58:04.760: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.260152ms
    Feb 20 22:58:06.771: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017676725s
    Feb 20 22:58:08.779: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025735038s
    Feb 20 22:58:10.771: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017376124s
    STEP: Saw pod success 02/20/23 22:58:10.771
    Feb 20 22:58:10.771: INFO: Pod "pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2" satisfied condition "Succeeded or Failed"
    Feb 20 22:58:10.780: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 22:58:10.811
    Feb 20 22:58:10.833: INFO: Waiting for pod pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2 to disappear
    Feb 20 22:58:10.841: INFO: Pod pod-configmaps-b21d8392-4af8-4524-bc50-5abb08cdceb2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 22:58:10.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3521" for this suite. 02/20/23 22:58:10.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:10.871
Feb 20 22:58:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:58:10.873
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:10.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:10.994
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:58:11.001
Feb 20 22:58:11.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1" in namespace "downward-api-5248" to be "Succeeded or Failed"
Feb 20 22:58:11.062: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.532879ms
Feb 20 22:58:13.075: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024512762s
Feb 20 22:58:15.071: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020924043s
Feb 20 22:58:17.071: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020938447s
STEP: Saw pod success 02/20/23 22:58:17.071
Feb 20 22:58:17.072: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1" satisfied condition "Succeeded or Failed"
Feb 20 22:58:17.080: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1 container client-container: <nil>
STEP: delete the pod 02/20/23 22:58:17.108
Feb 20 22:58:17.136: INFO: Waiting for pod downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1 to disappear
Feb 20 22:58:17.144: INFO: Pod downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 22:58:17.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5248" for this suite. 02/20/23 22:58:17.157
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":242,"skipped":4458,"failed":0}
------------------------------
• [SLOW TEST] [6.301 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:10.871
    Feb 20 22:58:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:58:10.873
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:10.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:10.994
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:58:11.001
    Feb 20 22:58:11.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1" in namespace "downward-api-5248" to be "Succeeded or Failed"
    Feb 20 22:58:11.062: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.532879ms
    Feb 20 22:58:13.075: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024512762s
    Feb 20 22:58:15.071: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020924043s
    Feb 20 22:58:17.071: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020938447s
    STEP: Saw pod success 02/20/23 22:58:17.071
    Feb 20 22:58:17.072: INFO: Pod "downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1" satisfied condition "Succeeded or Failed"
    Feb 20 22:58:17.080: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:58:17.108
    Feb 20 22:58:17.136: INFO: Waiting for pod downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1 to disappear
    Feb 20 22:58:17.144: INFO: Pod downwardapi-volume-7d791598-374d-44f6-b840-19cfbfe4eef1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 22:58:17.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5248" for this suite. 02/20/23 22:58:17.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:17.172
Feb 20 22:58:17.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 22:58:17.176
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:17.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:17.228
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 02/20/23 22:58:17.239
W0220 22:58:17.286504      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:58:17.287: INFO: Waiting up to 5m0s for pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b" in namespace "downward-api-7850" to be "Succeeded or Failed"
Feb 20 22:58:17.298: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.573671ms
Feb 20 22:58:19.307: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019873492s
Feb 20 22:58:21.308: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020829296s
STEP: Saw pod success 02/20/23 22:58:21.308
Feb 20 22:58:21.309: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b" satisfied condition "Succeeded or Failed"
Feb 20 22:58:21.319: INFO: Trying to get logs from node 10.8.38.70 pod downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b container dapi-container: <nil>
STEP: delete the pod 02/20/23 22:58:21.347
Feb 20 22:58:21.372: INFO: Waiting for pod downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b to disappear
Feb 20 22:58:21.380: INFO: Pod downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 20 22:58:21.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7850" for this suite. 02/20/23 22:58:21.393
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":243,"skipped":4464,"failed":0}
------------------------------
• [4.235 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:17.172
    Feb 20 22:58:17.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 22:58:17.176
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:17.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:17.228
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 02/20/23 22:58:17.239
    W0220 22:58:17.286504      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:58:17.287: INFO: Waiting up to 5m0s for pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b" in namespace "downward-api-7850" to be "Succeeded or Failed"
    Feb 20 22:58:17.298: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.573671ms
    Feb 20 22:58:19.307: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019873492s
    Feb 20 22:58:21.308: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020829296s
    STEP: Saw pod success 02/20/23 22:58:21.308
    Feb 20 22:58:21.309: INFO: Pod "downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b" satisfied condition "Succeeded or Failed"
    Feb 20 22:58:21.319: INFO: Trying to get logs from node 10.8.38.70 pod downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b container dapi-container: <nil>
    STEP: delete the pod 02/20/23 22:58:21.347
    Feb 20 22:58:21.372: INFO: Waiting for pod downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b to disappear
    Feb 20 22:58:21.380: INFO: Pod downward-api-6f05e382-5de4-4b0f-bce3-4b9e800a5c3b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 20 22:58:21.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7850" for this suite. 02/20/23 22:58:21.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:21.411
Feb 20 22:58:21.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pod-network-test 02/20/23 22:58:21.412
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:21.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:21.46
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4121 02/20/23 22:58:21.468
STEP: creating a selector 02/20/23 22:58:21.468
STEP: Creating the service pods in kubernetes 02/20/23 22:58:21.469
Feb 20 22:58:21.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 20 22:58:21.578: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4121" to be "running and ready"
Feb 20 22:58:21.592: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.65019ms
Feb 20 22:58:21.592: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:58:23.602: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023418795s
Feb 20 22:58:23.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:25.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.022702611s
Feb 20 22:58:25.601: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:27.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039132902s
Feb 20 22:58:27.618: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:29.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02310589s
Feb 20 22:58:29.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:31.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02305564s
Feb 20 22:58:31.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:33.602: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024079175s
Feb 20 22:58:33.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:35.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024599743s
Feb 20 22:58:35.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:37.602: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.023398086s
Feb 20 22:58:37.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:39.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.024228235s
Feb 20 22:58:39.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:41.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024297035s
Feb 20 22:58:41.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 22:58:43.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024858224s
Feb 20 22:58:43.603: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 20 22:58:43.603: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 20 22:58:43.611: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4121" to be "running and ready"
Feb 20 22:58:43.621: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.993535ms
Feb 20 22:58:43.621: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 20 22:58:43.621: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 20 22:58:43.629: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4121" to be "running and ready"
Feb 20 22:58:43.637: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.397693ms
Feb 20 22:58:43.638: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 20 22:58:43.638: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/20/23 22:58:43.648
Feb 20 22:58:43.679: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4121" to be "running"
Feb 20 22:58:43.686: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543711ms
Feb 20 22:58:45.696: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017797309s
Feb 20 22:58:45.697: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 20 22:58:45.705: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 20 22:58:45.705: INFO: Breadth first check of 172.30.181.220 on host 10.8.38.66...
Feb 20 22:58:45.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.156:9080/dial?request=hostname&protocol=udp&host=172.30.181.220&port=8081&tries=1'] Namespace:pod-network-test-4121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:58:45.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:58:45.714: INFO: ExecWithOptions: Clientset creation
Feb 20 22:58:45.714: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4121/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.31.156%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.181.220%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 20 22:58:45.913: INFO: Waiting for responses: map[]
Feb 20 22:58:45.913: INFO: reached 172.30.181.220 after 0/1 tries
Feb 20 22:58:45.913: INFO: Breadth first check of 172.30.144.250 on host 10.8.38.69...
Feb 20 22:58:45.922: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.156:9080/dial?request=hostname&protocol=udp&host=172.30.144.250&port=8081&tries=1'] Namespace:pod-network-test-4121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:58:45.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:58:45.923: INFO: ExecWithOptions: Clientset creation
Feb 20 22:58:45.923: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4121/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.31.156%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.144.250%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 20 22:58:46.076: INFO: Waiting for responses: map[]
Feb 20 22:58:46.076: INFO: reached 172.30.144.250 after 0/1 tries
Feb 20 22:58:46.077: INFO: Breadth first check of 172.30.31.157 on host 10.8.38.70...
Feb 20 22:58:46.085: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.156:9080/dial?request=hostname&protocol=udp&host=172.30.31.157&port=8081&tries=1'] Namespace:pod-network-test-4121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:58:46.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:58:46.087: INFO: ExecWithOptions: Clientset creation
Feb 20 22:58:46.087: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4121/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.31.156%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.31.157%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 20 22:58:46.232: INFO: Waiting for responses: map[]
Feb 20 22:58:46.232: INFO: reached 172.30.31.157 after 0/1 tries
Feb 20 22:58:46.232: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 20 22:58:46.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4121" for this suite. 02/20/23 22:58:46.244
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":244,"skipped":4488,"failed":0}
------------------------------
• [SLOW TEST] [24.849 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:21.411
    Feb 20 22:58:21.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pod-network-test 02/20/23 22:58:21.412
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:21.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:21.46
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4121 02/20/23 22:58:21.468
    STEP: creating a selector 02/20/23 22:58:21.468
    STEP: Creating the service pods in kubernetes 02/20/23 22:58:21.469
    Feb 20 22:58:21.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 20 22:58:21.578: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4121" to be "running and ready"
    Feb 20 22:58:21.592: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.65019ms
    Feb 20 22:58:21.592: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:58:23.602: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023418795s
    Feb 20 22:58:23.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:25.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.022702611s
    Feb 20 22:58:25.601: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:27.617: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039132902s
    Feb 20 22:58:27.618: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:29.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02310589s
    Feb 20 22:58:29.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:31.601: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02305564s
    Feb 20 22:58:31.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:33.602: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024079175s
    Feb 20 22:58:33.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:35.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.024599743s
    Feb 20 22:58:35.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:37.602: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.023398086s
    Feb 20 22:58:37.602: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:39.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.024228235s
    Feb 20 22:58:39.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:41.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024297035s
    Feb 20 22:58:41.603: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 22:58:43.603: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024858224s
    Feb 20 22:58:43.603: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 20 22:58:43.603: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 20 22:58:43.611: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4121" to be "running and ready"
    Feb 20 22:58:43.621: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.993535ms
    Feb 20 22:58:43.621: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 20 22:58:43.621: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 20 22:58:43.629: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4121" to be "running and ready"
    Feb 20 22:58:43.637: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.397693ms
    Feb 20 22:58:43.638: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 20 22:58:43.638: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/20/23 22:58:43.648
    Feb 20 22:58:43.679: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4121" to be "running"
    Feb 20 22:58:43.686: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543711ms
    Feb 20 22:58:45.696: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017797309s
    Feb 20 22:58:45.697: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 20 22:58:45.705: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 20 22:58:45.705: INFO: Breadth first check of 172.30.181.220 on host 10.8.38.66...
    Feb 20 22:58:45.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.156:9080/dial?request=hostname&protocol=udp&host=172.30.181.220&port=8081&tries=1'] Namespace:pod-network-test-4121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:58:45.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:58:45.714: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:58:45.714: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4121/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.31.156%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.181.220%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 20 22:58:45.913: INFO: Waiting for responses: map[]
    Feb 20 22:58:45.913: INFO: reached 172.30.181.220 after 0/1 tries
    Feb 20 22:58:45.913: INFO: Breadth first check of 172.30.144.250 on host 10.8.38.69...
    Feb 20 22:58:45.922: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.156:9080/dial?request=hostname&protocol=udp&host=172.30.144.250&port=8081&tries=1'] Namespace:pod-network-test-4121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:58:45.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:58:45.923: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:58:45.923: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4121/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.31.156%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.144.250%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 20 22:58:46.076: INFO: Waiting for responses: map[]
    Feb 20 22:58:46.076: INFO: reached 172.30.144.250 after 0/1 tries
    Feb 20 22:58:46.077: INFO: Breadth first check of 172.30.31.157 on host 10.8.38.70...
    Feb 20 22:58:46.085: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.31.156:9080/dial?request=hostname&protocol=udp&host=172.30.31.157&port=8081&tries=1'] Namespace:pod-network-test-4121 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:58:46.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:58:46.087: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:58:46.087: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4121/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.31.156%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.31.157%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 20 22:58:46.232: INFO: Waiting for responses: map[]
    Feb 20 22:58:46.232: INFO: reached 172.30.31.157 after 0/1 tries
    Feb 20 22:58:46.232: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 20 22:58:46.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4121" for this suite. 02/20/23 22:58:46.244
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:46.261
Feb 20 22:58:46.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svc-latency 02/20/23 22:58:46.264
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:46.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:46.343
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 20 22:58:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7324 02/20/23 22:58:46.352
W0220 22:58:46.376957      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0220 22:58:46.377384      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7324, replica count: 1
I0220 22:58:47.428281      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 22:58:48.430006      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 22:58:48.552: INFO: Created: latency-svc-6ztwk
Feb 20 22:58:48.570: INFO: Got endpoints: latency-svc-6ztwk [39.626155ms]
Feb 20 22:58:48.597: INFO: Created: latency-svc-chk58
Feb 20 22:58:48.606: INFO: Created: latency-svc-jqtwf
Feb 20 22:58:48.608: INFO: Got endpoints: latency-svc-chk58 [36.187155ms]
Feb 20 22:58:48.616: INFO: Created: latency-svc-jcvsm
Feb 20 22:58:48.635: INFO: Got endpoints: latency-svc-jqtwf [64.571234ms]
Feb 20 22:58:48.643: INFO: Got endpoints: latency-svc-jcvsm [71.117245ms]
Feb 20 22:58:48.644: INFO: Created: latency-svc-9sz2n
Feb 20 22:58:48.647: INFO: Created: latency-svc-w2zhm
Feb 20 22:58:48.655: INFO: Got endpoints: latency-svc-9sz2n [82.072631ms]
Feb 20 22:58:48.655: INFO: Created: latency-svc-xgb8g
Feb 20 22:58:48.664: INFO: Got endpoints: latency-svc-w2zhm [91.76446ms]
Feb 20 22:58:48.665: INFO: Created: latency-svc-p7mfc
Feb 20 22:58:48.674: INFO: Created: latency-svc-l25f6
Feb 20 22:58:48.684: INFO: Created: latency-svc-m5hqc
Feb 20 22:58:48.698: INFO: Got endpoints: latency-svc-p7mfc [127.032729ms]
Feb 20 22:58:48.698: INFO: Created: latency-svc-lhldt
Feb 20 22:58:48.703: INFO: Created: latency-svc-jq92j
Feb 20 22:58:48.712: INFO: Created: latency-svc-trhtn
Feb 20 22:58:48.722: INFO: Created: latency-svc-bngwz
Feb 20 22:58:48.726: INFO: Got endpoints: latency-svc-m5hqc [154.51843ms]
Feb 20 22:58:48.729: INFO: Got endpoints: latency-svc-l25f6 [156.107387ms]
Feb 20 22:58:48.732: INFO: Created: latency-svc-h4hx6
Feb 20 22:58:48.741: INFO: Created: latency-svc-krzm5
Feb 20 22:58:48.748: INFO: Got endpoints: latency-svc-lhldt [175.793901ms]
Feb 20 22:58:48.749: INFO: Got endpoints: latency-svc-xgb8g [177.362444ms]
Feb 20 22:58:48.751: INFO: Created: latency-svc-xtdp2
Feb 20 22:58:48.760: INFO: Created: latency-svc-pw97q
Feb 20 22:58:48.772: INFO: Created: latency-svc-kk7cn
Feb 20 22:58:48.781: INFO: Created: latency-svc-7m26b
Feb 20 22:58:48.787: INFO: Got endpoints: latency-svc-jq92j [215.507332ms]
Feb 20 22:58:48.791: INFO: Created: latency-svc-rg9t6
Feb 20 22:58:48.800: INFO: Created: latency-svc-pz2dl
Feb 20 22:58:48.805: INFO: Got endpoints: latency-svc-trhtn [231.906821ms]
Feb 20 22:58:48.810: INFO: Created: latency-svc-g8fkh
Feb 20 22:58:48.811: INFO: Got endpoints: latency-svc-krzm5 [238.617355ms]
Feb 20 22:58:48.819: INFO: Created: latency-svc-zvw2s
Feb 20 22:58:48.823: INFO: Got endpoints: latency-svc-h4hx6 [250.480475ms]
Feb 20 22:58:48.824: INFO: Got endpoints: latency-svc-bngwz [252.651765ms]
Feb 20 22:58:48.842: INFO: Created: latency-svc-stg4t
Feb 20 22:58:48.842: INFO: Created: latency-svc-b8qpl
Feb 20 22:58:48.846: INFO: Created: latency-svc-s4crq
Feb 20 22:58:48.852: INFO: Got endpoints: latency-svc-xtdp2 [243.289758ms]
Feb 20 22:58:48.861: INFO: Created: latency-svc-7h8pc
Feb 20 22:58:48.872: INFO: Got endpoints: latency-svc-kk7cn [229.508407ms]
Feb 20 22:58:48.872: INFO: Created: latency-svc-hln5p
Feb 20 22:58:48.889: INFO: Created: latency-svc-slxw9
Feb 20 22:58:48.889: INFO: Got endpoints: latency-svc-pw97q [254.246554ms]
Feb 20 22:58:48.890: INFO: Got endpoints: latency-svc-7m26b [234.835712ms]
Feb 20 22:58:48.891: INFO: Got endpoints: latency-svc-pz2dl [192.641472ms]
Feb 20 22:58:48.891: INFO: Created: latency-svc-cncnr
Feb 20 22:58:48.901: INFO: Created: latency-svc-pcmcj
Feb 20 22:58:48.907: INFO: Got endpoints: latency-svc-rg9t6 [243.368388ms]
Feb 20 22:58:48.911: INFO: Created: latency-svc-5wrdm
Feb 20 22:58:48.923: INFO: Created: latency-svc-4xb68
Feb 20 22:58:48.923: INFO: Got endpoints: latency-svc-g8fkh [196.915622ms]
Feb 20 22:58:48.932: INFO: Created: latency-svc-jw2z5
Feb 20 22:58:48.941: INFO: Created: latency-svc-7b54h
Feb 20 22:58:48.942: INFO: Got endpoints: latency-svc-zvw2s [213.458961ms]
Feb 20 22:58:48.944: INFO: Got endpoints: latency-svc-stg4t [195.358297ms]
Feb 20 22:58:48.951: INFO: Created: latency-svc-gl2rn
Feb 20 22:58:48.952: INFO: Got endpoints: latency-svc-b8qpl [203.923939ms]
Feb 20 22:58:48.960: INFO: Created: latency-svc-mrw75
Feb 20 22:58:48.973: INFO: Created: latency-svc-6ccp6
Feb 20 22:58:48.983: INFO: Created: latency-svc-k6n46
Feb 20 22:58:48.984: INFO: Got endpoints: latency-svc-s4crq [194.893564ms]
Feb 20 22:58:48.985: INFO: Got endpoints: latency-svc-7h8pc [180.216532ms]
Feb 20 22:58:48.991: INFO: Created: latency-svc-gcd7x
Feb 20 22:58:48.991: INFO: Got endpoints: latency-svc-cncnr [166.617829ms]
Feb 20 22:58:48.991: INFO: Got endpoints: latency-svc-slxw9 [168.164696ms]
Feb 20 22:58:48.991: INFO: Got endpoints: latency-svc-hln5p [180.368723ms]
Feb 20 22:58:48.993: INFO: Got endpoints: latency-svc-pcmcj [141.060065ms]
Feb 20 22:58:48.994: INFO: Got endpoints: latency-svc-5wrdm [121.522116ms]
Feb 20 22:58:49.006: INFO: Created: latency-svc-cn98p
Feb 20 22:58:49.016: INFO: Created: latency-svc-lmss6
Feb 20 22:58:49.020: INFO: Got endpoints: latency-svc-jw2z5 [129.718027ms]
Feb 20 22:58:49.023: INFO: Got endpoints: latency-svc-7b54h [133.047664ms]
Feb 20 22:58:49.023: INFO: Got endpoints: latency-svc-mrw75 [99.433878ms]
Feb 20 22:58:49.024: INFO: Got endpoints: latency-svc-gl2rn [117.053028ms]
Feb 20 22:58:49.025: INFO: Got endpoints: latency-svc-4xb68 [135.273448ms]
Feb 20 22:58:49.031: INFO: Created: latency-svc-m9p9n
Feb 20 22:58:49.034: INFO: Got endpoints: latency-svc-6ccp6 [92.316208ms]
Feb 20 22:58:49.037: INFO: Got endpoints: latency-svc-gcd7x [85.614379ms]
Feb 20 22:58:49.038: INFO: Got endpoints: latency-svc-cn98p [53.907425ms]
Feb 20 22:58:49.040: INFO: Got endpoints: latency-svc-k6n46 [95.761225ms]
Feb 20 22:58:49.044: INFO: Got endpoints: latency-svc-lmss6 [59.09058ms]
Feb 20 22:58:49.055: INFO: Got endpoints: latency-svc-m9p9n [64.141755ms]
Feb 20 22:58:49.188: INFO: Created: latency-svc-7pjrj
Feb 20 22:58:49.194: INFO: Created: latency-svc-p4tpc
Feb 20 22:58:49.195: INFO: Created: latency-svc-27fh7
Feb 20 22:58:49.195: INFO: Created: latency-svc-9bcrb
Feb 20 22:58:49.199: INFO: Created: latency-svc-hrz5f
Feb 20 22:58:49.199: INFO: Created: latency-svc-7bdjs
Feb 20 22:58:49.199: INFO: Created: latency-svc-94b9v
Feb 20 22:58:49.200: INFO: Created: latency-svc-sf2kd
Feb 20 22:58:49.201: INFO: Created: latency-svc-k2p5m
Feb 20 22:58:49.201: INFO: Created: latency-svc-twlff
Feb 20 22:58:49.202: INFO: Created: latency-svc-pzljs
Feb 20 22:58:49.202: INFO: Created: latency-svc-cn786
Feb 20 22:58:49.202: INFO: Got endpoints: latency-svc-p4tpc [181.041274ms]
Feb 20 22:58:49.202: INFO: Created: latency-svc-7f2jc
Feb 20 22:58:49.202: INFO: Created: latency-svc-t4ln8
Feb 20 22:58:49.202: INFO: Got endpoints: latency-svc-7pjrj [177.3975ms]
Feb 20 22:58:49.204: INFO: Got endpoints: latency-svc-94b9v [148.626312ms]
Feb 20 22:58:49.206: INFO: Created: latency-svc-wj2qw
Feb 20 22:58:49.210: INFO: Got endpoints: latency-svc-t4ln8 [171.450456ms]
Feb 20 22:58:49.210: INFO: Got endpoints: latency-svc-27fh7 [218.83ms]
Feb 20 22:58:49.210: INFO: Got endpoints: latency-svc-wj2qw [187.575775ms]
Feb 20 22:58:49.226: INFO: Created: latency-svc-6lb5w
Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-sf2kd [201.104784ms]
Feb 20 22:58:49.236: INFO: Created: latency-svc-jzd7s
Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-cn786 [244.330604ms]
Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-hrz5f [213.273006ms]
Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-7bdjs [242.332038ms]
Feb 20 22:58:49.238: INFO: Got endpoints: latency-svc-9bcrb [200.272049ms]
Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-7f2jc [222.847168ms]
Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-6lb5w [61.193315ms]
Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-twlff [238.406434ms]
Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-k2p5m [270.551509ms]
Feb 20 22:58:49.263: INFO: Created: latency-svc-cscl6
Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-pzljs [219.300662ms]
Feb 20 22:58:49.318: INFO: Got endpoints: latency-svc-cscl6 [113.537524ms]
Feb 20 22:58:49.318: INFO: Created: latency-svc-vxzzx
Feb 20 22:58:49.318: INFO: Got endpoints: latency-svc-jzd7s [116.2374ms]
Feb 20 22:58:49.320: INFO: Created: latency-svc-2vvcw
Feb 20 22:58:49.320: INFO: Created: latency-svc-wqrwn
Feb 20 22:58:49.324: INFO: Got endpoints: latency-svc-2vvcw [113.534251ms]
Feb 20 22:58:49.324: INFO: Got endpoints: latency-svc-wqrwn [113.39582ms]
Feb 20 22:58:49.324: INFO: Got endpoints: latency-svc-vxzzx [113.947546ms]
Feb 20 22:58:49.371: INFO: Created: latency-svc-7zftg
Feb 20 22:58:49.423: INFO: Got endpoints: latency-svc-7zftg [187.710517ms]
Feb 20 22:58:49.423: INFO: Created: latency-svc-th8md
Feb 20 22:58:49.461: INFO: Got endpoints: latency-svc-th8md [225.574584ms]
Feb 20 22:58:49.462: INFO: Created: latency-svc-nflth
Feb 20 22:58:49.465: INFO: Got endpoints: latency-svc-nflth [228.880294ms]
Feb 20 22:58:49.465: INFO: Created: latency-svc-zchrq
Feb 20 22:58:49.495: INFO: Created: latency-svc-s2hjr
Feb 20 22:58:49.495: INFO: Got endpoints: latency-svc-s2hjr [257.464134ms]
Feb 20 22:58:49.495: INFO: Created: latency-svc-j8pkn
Feb 20 22:58:49.495: INFO: Got endpoints: latency-svc-zchrq [259.336464ms]
Feb 20 22:58:49.498: INFO: Created: latency-svc-fkbmj
Feb 20 22:58:49.509: INFO: Got endpoints: latency-svc-j8pkn [245.871399ms]
Feb 20 22:58:49.509: INFO: Got endpoints: latency-svc-fkbmj [246.273627ms]
Feb 20 22:58:49.510: INFO: Created: latency-svc-bql8m
Feb 20 22:58:49.534: INFO: Created: latency-svc-gn8j2
Feb 20 22:58:49.534: INFO: Got endpoints: latency-svc-gn8j2 [271.134795ms]
Feb 20 22:58:49.535: INFO: Got endpoints: latency-svc-bql8m [271.849884ms]
Feb 20 22:58:49.746: INFO: Created: latency-svc-vmhqh
Feb 20 22:58:49.746: INFO: Created: latency-svc-6jc9d
Feb 20 22:58:49.746: INFO: Created: latency-svc-zjbdp
Feb 20 22:58:49.746: INFO: Created: latency-svc-x7628
Feb 20 22:58:49.746: INFO: Created: latency-svc-c26cp
Feb 20 22:58:49.747: INFO: Created: latency-svc-rnjzp
Feb 20 22:58:49.747: INFO: Created: latency-svc-klfh5
Feb 20 22:58:49.747: INFO: Created: latency-svc-qxxfn
Feb 20 22:58:49.747: INFO: Created: latency-svc-gb9zf
Feb 20 22:58:49.747: INFO: Created: latency-svc-mlg4z
Feb 20 22:58:49.747: INFO: Created: latency-svc-gr4kf
Feb 20 22:58:49.747: INFO: Created: latency-svc-h2h2z
Feb 20 22:58:49.747: INFO: Created: latency-svc-l5gml
Feb 20 22:58:49.748: INFO: Created: latency-svc-9wht9
Feb 20 22:58:49.748: INFO: Created: latency-svc-qfqt7
Feb 20 22:58:49.762: INFO: Got endpoints: latency-svc-vmhqh [432.460659ms]
Feb 20 22:58:49.770: INFO: Got endpoints: latency-svc-6jc9d [440.124472ms]
Feb 20 22:58:49.770: INFO: Got endpoints: latency-svc-qfqt7 [273.561389ms]
Feb 20 22:58:49.770: INFO: Got endpoints: latency-svc-zjbdp [235.706612ms]
Feb 20 22:58:49.771: INFO: Got endpoints: latency-svc-klfh5 [305.744966ms]
Feb 20 22:58:49.777: INFO: Got endpoints: latency-svc-gb9zf [513.89303ms]
Feb 20 22:58:49.784: INFO: Got endpoints: latency-svc-c26cp [466.433829ms]
Feb 20 22:58:49.788: INFO: Created: latency-svc-bdxkk
Feb 20 22:58:49.788: INFO: Got endpoints: latency-svc-gr4kf [326.812489ms]
Feb 20 22:58:49.789: INFO: Got endpoints: latency-svc-rnjzp [253.955974ms]
Feb 20 22:58:49.789: INFO: Got endpoints: latency-svc-x7628 [280.74089ms]
Feb 20 22:58:49.790: INFO: Got endpoints: latency-svc-l5gml [366.118617ms]
Feb 20 22:58:49.797: INFO: Created: latency-svc-4pz6k
Feb 20 22:58:49.809: INFO: Got endpoints: latency-svc-mlg4z [313.293856ms]
Feb 20 22:58:49.809: INFO: Got endpoints: latency-svc-h2h2z [299.712889ms]
Feb 20 22:58:49.809: INFO: Got endpoints: latency-svc-9wht9 [479.782167ms]
Feb 20 22:58:49.826: INFO: Created: latency-svc-ckfl5
Feb 20 22:58:49.826: INFO: Created: latency-svc-ngp8p
Feb 20 22:58:49.827: INFO: Created: latency-svc-gw5k5
Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-gw5k5 [58.293243ms]
Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-qxxfn [511.186144ms]
Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-bdxkk [66.764034ms]
Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-4pz6k [59.317432ms]
Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-ckfl5 [59.058886ms]
Feb 20 22:58:49.836: INFO: Created: latency-svc-6rbp5
Feb 20 22:58:49.836: INFO: Got endpoints: latency-svc-ngp8p [65.60505ms]
Feb 20 22:58:49.854: INFO: Got endpoints: latency-svc-6rbp5 [76.44788ms]
Feb 20 22:58:49.854: INFO: Created: latency-svc-f95g7
Feb 20 22:58:49.855: INFO: Created: latency-svc-rv57k
Feb 20 22:58:49.859: INFO: Got endpoints: latency-svc-rv57k [74.211987ms]
Feb 20 22:58:49.864: INFO: Created: latency-svc-xrlxx
Feb 20 22:58:49.881: INFO: Created: latency-svc-4hlz9
Feb 20 22:58:49.881: INFO: Got endpoints: latency-svc-xrlxx [92.772508ms]
Feb 20 22:58:49.881: INFO: Got endpoints: latency-svc-f95g7 [92.372053ms]
Feb 20 22:58:49.900: INFO: Created: latency-svc-jt6mg
Feb 20 22:58:49.901: INFO: Created: latency-svc-6xlxp
Feb 20 22:58:49.904: INFO: Got endpoints: latency-svc-jt6mg [114.497497ms]
Feb 20 22:58:49.905: INFO: Got endpoints: latency-svc-4hlz9 [115.471947ms]
Feb 20 22:58:49.906: INFO: Created: latency-svc-pzvn5
Feb 20 22:58:49.915: INFO: Got endpoints: latency-svc-6xlxp [105.331304ms]
Feb 20 22:58:49.915: INFO: Created: latency-svc-7lsdw
Feb 20 22:58:49.918: INFO: Got endpoints: latency-svc-pzvn5 [108.974402ms]
Feb 20 22:58:49.926: INFO: Got endpoints: latency-svc-7lsdw [116.931208ms]
Feb 20 22:58:49.927: INFO: Created: latency-svc-pfn2j
Feb 20 22:58:49.934: INFO: Created: latency-svc-b6mrn
Feb 20 22:58:49.938: INFO: Got endpoints: latency-svc-pfn2j [108.888158ms]
Feb 20 22:58:49.943: INFO: Created: latency-svc-xgx9h
Feb 20 22:58:49.949: INFO: Got endpoints: latency-svc-b6mrn [120.15751ms]
Feb 20 22:58:49.964: INFO: Got endpoints: latency-svc-xgx9h [134.537979ms]
Feb 20 22:58:49.964: INFO: Created: latency-svc-jtcls
Feb 20 22:58:49.964: INFO: Created: latency-svc-7wctq
Feb 20 22:58:49.964: INFO: Got endpoints: latency-svc-7wctq [135.8644ms]
Feb 20 22:58:49.975: INFO: Created: latency-svc-kbmgp
Feb 20 22:58:49.978: INFO: Got endpoints: latency-svc-jtcls [149.414566ms]
Feb 20 22:58:49.983: INFO: Got endpoints: latency-svc-kbmgp [146.468027ms]
Feb 20 22:58:49.987: INFO: Created: latency-svc-fgngc
Feb 20 22:58:49.996: INFO: Got endpoints: latency-svc-fgngc [141.996368ms]
Feb 20 22:58:49.996: INFO: Created: latency-svc-hmn22
Feb 20 22:58:50.009: INFO: Got endpoints: latency-svc-hmn22 [150.375012ms]
Feb 20 22:58:50.009: INFO: Created: latency-svc-l95sm
Feb 20 22:58:50.022: INFO: Got endpoints: latency-svc-l95sm [140.844592ms]
Feb 20 22:58:50.025: INFO: Created: latency-svc-8dhtx
Feb 20 22:58:50.031: INFO: Created: latency-svc-w22wv
Feb 20 22:58:50.036: INFO: Got endpoints: latency-svc-8dhtx [154.698821ms]
Feb 20 22:58:50.041: INFO: Created: latency-svc-bq6v8
Feb 20 22:58:50.045: INFO: Got endpoints: latency-svc-w22wv [141.188798ms]
Feb 20 22:58:50.050: INFO: Created: latency-svc-8nj6v
Feb 20 22:58:50.060: INFO: Created: latency-svc-pdj5g
Feb 20 22:58:50.065: INFO: Got endpoints: latency-svc-bq6v8 [159.795797ms]
Feb 20 22:58:50.068: INFO: Got endpoints: latency-svc-8nj6v [152.957357ms]
Feb 20 22:58:50.075: INFO: Got endpoints: latency-svc-pdj5g [157.360732ms]
Feb 20 22:58:50.075: INFO: Created: latency-svc-nhrmj
Feb 20 22:58:50.086: INFO: Got endpoints: latency-svc-nhrmj [159.392684ms]
Feb 20 22:58:50.088: INFO: Created: latency-svc-xkb4w
Feb 20 22:58:50.100: INFO: Created: latency-svc-7v9j6
Feb 20 22:58:50.106: INFO: Got endpoints: latency-svc-xkb4w [167.935863ms]
Feb 20 22:58:50.109: INFO: Created: latency-svc-jlc5m
Feb 20 22:58:50.113: INFO: Got endpoints: latency-svc-7v9j6 [163.688225ms]
Feb 20 22:58:50.120: INFO: Created: latency-svc-kqr7g
Feb 20 22:58:50.121: INFO: Got endpoints: latency-svc-jlc5m [157.225496ms]
Feb 20 22:58:50.129: INFO: Created: latency-svc-dbrjs
Feb 20 22:58:50.139: INFO: Created: latency-svc-nthpv
Feb 20 22:58:50.140: INFO: Got endpoints: latency-svc-kqr7g [175.141502ms]
Feb 20 22:58:50.142: INFO: Got endpoints: latency-svc-dbrjs [164.046127ms]
Feb 20 22:58:50.150: INFO: Created: latency-svc-g7pnw
Feb 20 22:58:50.154: INFO: Created: latency-svc-lv7j5
Feb 20 22:58:50.165: INFO: Created: latency-svc-z9nmc
Feb 20 22:58:50.171: INFO: Created: latency-svc-ndnqw
Feb 20 22:58:50.177: INFO: Got endpoints: latency-svc-nthpv [194.742334ms]
Feb 20 22:58:50.180: INFO: Got endpoints: latency-svc-z9nmc [157.802738ms]
Feb 20 22:58:50.181: INFO: Got endpoints: latency-svc-g7pnw [185.390016ms]
Feb 20 22:58:50.182: INFO: Got endpoints: latency-svc-lv7j5 [172.49725ms]
Feb 20 22:58:50.184: INFO: Created: latency-svc-7jsxx
Feb 20 22:58:50.185: INFO: Got endpoints: latency-svc-ndnqw [149.382189ms]
Feb 20 22:58:50.191: INFO: Created: latency-svc-sbhcg
Feb 20 22:58:50.194: INFO: Got endpoints: latency-svc-7jsxx [148.742127ms]
Feb 20 22:58:50.199: INFO: Got endpoints: latency-svc-sbhcg [134.353683ms]
Feb 20 22:58:50.201: INFO: Created: latency-svc-j7plp
Feb 20 22:58:50.213: INFO: Created: latency-svc-c7pz4
Feb 20 22:58:50.213: INFO: Got endpoints: latency-svc-j7plp [145.331804ms]
Feb 20 22:58:50.221: INFO: Got endpoints: latency-svc-c7pz4 [145.976117ms]
Feb 20 22:58:50.353: INFO: Created: latency-svc-czmgl
Feb 20 22:58:50.359: INFO: Created: latency-svc-x7klv
Feb 20 22:58:50.359: INFO: Created: latency-svc-s54xk
Feb 20 22:58:50.360: INFO: Created: latency-svc-v5b4p
Feb 20 22:58:50.360: INFO: Created: latency-svc-rvjkm
Feb 20 22:58:50.360: INFO: Created: latency-svc-shklr
Feb 20 22:58:50.363: INFO: Created: latency-svc-6xgcl
Feb 20 22:58:50.363: INFO: Created: latency-svc-xbknv
Feb 20 22:58:50.360: INFO: Created: latency-svc-lxlnw
Feb 20 22:58:50.360: INFO: Created: latency-svc-t9cp7
Feb 20 22:58:50.362: INFO: Created: latency-svc-8fdtt
Feb 20 22:58:50.362: INFO: Created: latency-svc-xjv52
Feb 20 22:58:50.363: INFO: Created: latency-svc-48pps
Feb 20 22:58:50.363: INFO: Created: latency-svc-w2zsc
Feb 20 22:58:50.364: INFO: Created: latency-svc-v88q5
Feb 20 22:58:50.365: INFO: Got endpoints: latency-svc-czmgl [187.894883ms]
Feb 20 22:58:50.366: INFO: Got endpoints: latency-svc-x7klv [223.391235ms]
Feb 20 22:58:50.366: INFO: Got endpoints: latency-svc-v5b4p [180.848866ms]
Feb 20 22:58:50.374: INFO: Got endpoints: latency-svc-v88q5 [174.854329ms]
Feb 20 22:58:50.374: INFO: Got endpoints: latency-svc-s54xk [288.320607ms]
Feb 20 22:58:50.376: INFO: Got endpoints: latency-svc-shklr [154.901189ms]
Feb 20 22:58:50.376: INFO: Got endpoints: latency-svc-8fdtt [196.362126ms]
Feb 20 22:58:50.378: INFO: Got endpoints: latency-svc-t9cp7 [196.124193ms]
Feb 20 22:58:50.378: INFO: Got endpoints: latency-svc-6xgcl [271.859738ms]
Feb 20 22:58:50.381: INFO: Got endpoints: latency-svc-rvjkm [241.404097ms]
Feb 20 22:58:50.386: INFO: Got endpoints: latency-svc-48pps [204.233097ms]
Feb 20 22:58:50.387: INFO: Got endpoints: latency-svc-xbknv [265.609466ms]
Feb 20 22:58:50.388: INFO: Got endpoints: latency-svc-xjv52 [194.028656ms]
Feb 20 22:58:50.390: INFO: Got endpoints: latency-svc-lxlnw [176.722003ms]
Feb 20 22:58:50.396: INFO: Created: latency-svc-67lhv
Feb 20 22:58:50.398: INFO: Got endpoints: latency-svc-w2zsc [285.450682ms]
Feb 20 22:58:50.405: INFO: Got endpoints: latency-svc-67lhv [39.329569ms]
Feb 20 22:58:50.532: INFO: Created: latency-svc-27q4n
Feb 20 22:58:50.533: INFO: Created: latency-svc-2f2rx
Feb 20 22:58:50.535: INFO: Created: latency-svc-vgtt2
Feb 20 22:58:50.536: INFO: Created: latency-svc-292tn
Feb 20 22:58:50.539: INFO: Created: latency-svc-bd9rq
Feb 20 22:58:50.540: INFO: Created: latency-svc-cgrl5
Feb 20 22:58:50.540: INFO: Created: latency-svc-m442v
Feb 20 22:58:50.541: INFO: Created: latency-svc-4l87c
Feb 20 22:58:50.541: INFO: Created: latency-svc-779b8
Feb 20 22:58:50.542: INFO: Created: latency-svc-nzn2h
Feb 20 22:58:50.543: INFO: Created: latency-svc-lzbg8
Feb 20 22:58:50.543: INFO: Created: latency-svc-k7x4n
Feb 20 22:58:50.543: INFO: Created: latency-svc-wk2z9
Feb 20 22:58:50.543: INFO: Created: latency-svc-gd9dc
Feb 20 22:58:50.543: INFO: Created: latency-svc-825p9
Feb 20 22:58:50.551: INFO: Got endpoints: latency-svc-27q4n [176.064884ms]
Feb 20 22:58:50.551: INFO: Got endpoints: latency-svc-2f2rx [174.956052ms]
Feb 20 22:58:50.551: INFO: Got endpoints: latency-svc-vgtt2 [161.296474ms]
Feb 20 22:58:50.552: INFO: Got endpoints: latency-svc-292tn [173.99711ms]
Feb 20 22:58:50.552: INFO: Got endpoints: latency-svc-bd9rq [166.209826ms]
Feb 20 22:58:50.553: INFO: Got endpoints: latency-svc-k7x4n [171.645124ms]
Feb 20 22:58:50.553: INFO: Got endpoints: latency-svc-cgrl5 [164.500688ms]
Feb 20 22:58:50.554: INFO: Got endpoints: latency-svc-825p9 [176.302408ms]
Feb 20 22:58:50.567: INFO: Got endpoints: latency-svc-lzbg8 [201.344715ms]
Feb 20 22:58:50.576: INFO: Created: latency-svc-q6qxt
Feb 20 22:58:50.577: INFO: Got endpoints: latency-svc-wk2z9 [189.864223ms]
Feb 20 22:58:50.577: INFO: Got endpoints: latency-svc-4l87c [179.207873ms]
Feb 20 22:58:50.578: INFO: Got endpoints: latency-svc-m442v [201.583503ms]
Feb 20 22:58:50.578: INFO: Got endpoints: latency-svc-gd9dc [212.738716ms]
Feb 20 22:58:50.582: INFO: Got endpoints: latency-svc-nzn2h [177.177177ms]
Feb 20 22:58:50.587: INFO: Created: latency-svc-bl94d
Feb 20 22:58:50.588: INFO: Got endpoints: latency-svc-779b8 [213.695441ms]
Feb 20 22:58:50.588: INFO: Got endpoints: latency-svc-q6qxt [37.312157ms]
Feb 20 22:58:50.603: INFO: Created: latency-svc-xzmrl
Feb 20 22:58:50.604: INFO: Got endpoints: latency-svc-bl94d [52.959771ms]
Feb 20 22:58:50.613: INFO: Created: latency-svc-mzqjr
Feb 20 22:58:50.618: INFO: Got endpoints: latency-svc-xzmrl [66.702833ms]
Feb 20 22:58:50.624: INFO: Created: latency-svc-lh6ws
Feb 20 22:58:50.628: INFO: Got endpoints: latency-svc-mzqjr [76.133648ms]
Feb 20 22:58:50.634: INFO: Created: latency-svc-8nrmx
Feb 20 22:58:50.635: INFO: Got endpoints: latency-svc-lh6ws [82.083578ms]
Feb 20 22:58:50.644: INFO: Created: latency-svc-gt5tm
Feb 20 22:58:50.654: INFO: Created: latency-svc-8twt2
Feb 20 22:58:50.661: INFO: Created: latency-svc-d5hnk
Feb 20 22:58:50.666: INFO: Got endpoints: latency-svc-gt5tm [112.788005ms]
Feb 20 22:58:50.666: INFO: Got endpoints: latency-svc-8nrmx [114.161061ms]
Feb 20 22:58:50.670: INFO: Got endpoints: latency-svc-8twt2 [115.053392ms]
Feb 20 22:58:50.671: INFO: Created: latency-svc-nd8xj
Feb 20 22:58:50.675: INFO: Got endpoints: latency-svc-d5hnk [107.375788ms]
Feb 20 22:58:50.683: INFO: Created: latency-svc-lbqn7
Feb 20 22:58:50.687: INFO: Got endpoints: latency-svc-nd8xj [110.002115ms]
Feb 20 22:58:50.695: INFO: Created: latency-svc-wcg6n
Feb 20 22:58:50.697: INFO: Got endpoints: latency-svc-lbqn7 [119.523161ms]
Feb 20 22:58:50.705: INFO: Got endpoints: latency-svc-wcg6n [126.994898ms]
Feb 20 22:58:50.705: INFO: Created: latency-svc-q2b2q
Feb 20 22:58:50.711: INFO: Created: latency-svc-rwfcb
Feb 20 22:58:50.714: INFO: Got endpoints: latency-svc-q2b2q [135.506769ms]
Feb 20 22:58:50.722: INFO: Created: latency-svc-bxqzc
Feb 20 22:58:50.731: INFO: Created: latency-svc-g2st8
Feb 20 22:58:50.733: INFO: Got endpoints: latency-svc-rwfcb [150.283316ms]
Feb 20 22:58:50.739: INFO: Got endpoints: latency-svc-bxqzc [151.069888ms]
Feb 20 22:58:50.741: INFO: Created: latency-svc-p4f5l
Feb 20 22:58:50.744: INFO: Got endpoints: latency-svc-g2st8 [155.825546ms]
Feb 20 22:58:50.750: INFO: Created: latency-svc-pjhhd
Feb 20 22:58:50.755: INFO: Got endpoints: latency-svc-p4f5l [150.924755ms]
Feb 20 22:58:50.764: INFO: Got endpoints: latency-svc-pjhhd [145.797138ms]
Feb 20 22:58:50.892: INFO: Created: latency-svc-2tq7v
Feb 20 22:58:50.892: INFO: Created: latency-svc-c9thw
Feb 20 22:58:50.892: INFO: Created: latency-svc-n5kxx
Feb 20 22:58:50.892: INFO: Created: latency-svc-mj59w
Feb 20 22:58:50.893: INFO: Created: latency-svc-hfv5n
Feb 20 22:58:50.893: INFO: Created: latency-svc-lll9g
Feb 20 22:58:50.893: INFO: Created: latency-svc-qj7hc
Feb 20 22:58:50.893: INFO: Created: latency-svc-gt52v
Feb 20 22:58:50.893: INFO: Created: latency-svc-g5qdb
Feb 20 22:58:50.894: INFO: Created: latency-svc-wbbvl
Feb 20 22:58:50.897: INFO: Created: latency-svc-hgmlb
Feb 20 22:58:50.897: INFO: Created: latency-svc-hprc4
Feb 20 22:58:50.898: INFO: Created: latency-svc-lrjfz
Feb 20 22:58:50.898: INFO: Created: latency-svc-zvklb
Feb 20 22:58:50.898: INFO: Created: latency-svc-hs7p2
Feb 20 22:58:50.901: INFO: Got endpoints: latency-svc-2tq7v [162.162297ms]
Feb 20 22:58:50.903: INFO: Got endpoints: latency-svc-hprc4 [274.454042ms]
Feb 20 22:58:50.904: INFO: Got endpoints: latency-svc-mj59w [159.872423ms]
Feb 20 22:58:50.906: INFO: Got endpoints: latency-svc-n5kxx [270.729247ms]
Feb 20 22:58:50.906: INFO: Got endpoints: latency-svc-c9thw [151.074625ms]
Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-gt52v [231.45223ms]
Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-hs7p2 [164.732658ms]
Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-g5qdb [263.387288ms]
Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-lrjfz [259.742708ms]
Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-zvklb [263.08477ms]
Feb 20 22:58:50.930: INFO: Got endpoints: latency-svc-lll9g [224.756121ms]
Feb 20 22:58:50.939: INFO: Created: latency-svc-4brlm
Feb 20 22:58:50.948: INFO: Created: latency-svc-gwbf4
Feb 20 22:58:50.952: INFO: Got endpoints: latency-svc-qj7hc [276.853153ms]
Feb 20 22:58:50.958: INFO: Created: latency-svc-kwqqg
Feb 20 22:58:50.959: INFO: Got endpoints: latency-svc-hgmlb [272.693044ms]
Feb 20 22:58:50.962: INFO: Got endpoints: latency-svc-4brlm [60.434705ms]
Feb 20 22:58:50.968: INFO: Got endpoints: latency-svc-hfv5n [253.708448ms]
Feb 20 22:58:50.970: INFO: Got endpoints: latency-svc-wbbvl [237.02699ms]
Feb 20 22:58:50.977: INFO: Created: latency-svc-c4qpp
Feb 20 22:58:50.984: INFO: Created: latency-svc-vmqrh
Feb 20 22:58:50.999: INFO: Got endpoints: latency-svc-gwbf4 [96.422853ms]
Feb 20 22:58:51.002: INFO: Got endpoints: latency-svc-kwqqg [97.182669ms]
Feb 20 22:58:51.003: INFO: Got endpoints: latency-svc-c4qpp [96.769836ms]
Feb 20 22:58:51.007: INFO: Got endpoints: latency-svc-vmqrh [100.202252ms]
Feb 20 22:58:51.007: INFO: Latencies: [36.187155ms 37.312157ms 39.329569ms 52.959771ms 53.907425ms 58.293243ms 59.058886ms 59.09058ms 59.317432ms 60.434705ms 61.193315ms 64.141755ms 64.571234ms 65.60505ms 66.702833ms 66.764034ms 71.117245ms 74.211987ms 76.133648ms 76.44788ms 82.072631ms 82.083578ms 85.614379ms 91.76446ms 92.316208ms 92.372053ms 92.772508ms 95.761225ms 96.422853ms 96.769836ms 97.182669ms 99.433878ms 100.202252ms 105.331304ms 107.375788ms 108.888158ms 108.974402ms 110.002115ms 112.788005ms 113.39582ms 113.534251ms 113.537524ms 113.947546ms 114.161061ms 114.497497ms 115.053392ms 115.471947ms 116.2374ms 116.931208ms 117.053028ms 119.523161ms 120.15751ms 121.522116ms 126.994898ms 127.032729ms 129.718027ms 133.047664ms 134.353683ms 134.537979ms 135.273448ms 135.506769ms 135.8644ms 140.844592ms 141.060065ms 141.188798ms 141.996368ms 145.331804ms 145.797138ms 145.976117ms 146.468027ms 148.626312ms 148.742127ms 149.382189ms 149.414566ms 150.283316ms 150.375012ms 150.924755ms 151.069888ms 151.074625ms 152.957357ms 154.51843ms 154.698821ms 154.901189ms 155.825546ms 156.107387ms 157.225496ms 157.360732ms 157.802738ms 159.392684ms 159.795797ms 159.872423ms 161.296474ms 162.162297ms 163.688225ms 164.046127ms 164.500688ms 164.732658ms 166.209826ms 166.617829ms 167.935863ms 168.164696ms 171.450456ms 171.645124ms 172.49725ms 173.99711ms 174.854329ms 174.956052ms 175.141502ms 175.793901ms 176.064884ms 176.302408ms 176.722003ms 177.177177ms 177.362444ms 177.3975ms 179.207873ms 180.216532ms 180.368723ms 180.848866ms 181.041274ms 185.390016ms 187.575775ms 187.710517ms 187.894883ms 189.864223ms 192.641472ms 194.028656ms 194.742334ms 194.893564ms 195.358297ms 196.124193ms 196.362126ms 196.915622ms 200.272049ms 201.104784ms 201.344715ms 201.583503ms 203.923939ms 204.233097ms 212.738716ms 213.273006ms 213.458961ms 213.695441ms 215.507332ms 218.83ms 219.300662ms 222.847168ms 223.391235ms 224.756121ms 225.574584ms 228.880294ms 229.508407ms 231.45223ms 231.906821ms 234.835712ms 235.706612ms 237.02699ms 238.406434ms 238.617355ms 241.404097ms 242.332038ms 243.289758ms 243.368388ms 244.330604ms 245.871399ms 246.273627ms 250.480475ms 252.651765ms 253.708448ms 253.955974ms 254.246554ms 257.464134ms 259.336464ms 259.742708ms 263.08477ms 263.387288ms 265.609466ms 270.551509ms 270.729247ms 271.134795ms 271.849884ms 271.859738ms 272.693044ms 273.561389ms 274.454042ms 276.853153ms 280.74089ms 285.450682ms 288.320607ms 299.712889ms 305.744966ms 313.293856ms 326.812489ms 366.118617ms 432.460659ms 440.124472ms 466.433829ms 479.782167ms 511.186144ms 513.89303ms]
Feb 20 22:58:51.007: INFO: 50 %ile: 168.164696ms
Feb 20 22:58:51.007: INFO: 90 %ile: 271.849884ms
Feb 20 22:58:51.007: INFO: 99 %ile: 511.186144ms
Feb 20 22:58:51.007: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Feb 20 22:58:51.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7324" for this suite. 02/20/23 22:58:51.018
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":245,"skipped":4489,"failed":0}
------------------------------
• [4.771 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:46.261
    Feb 20 22:58:46.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svc-latency 02/20/23 22:58:46.264
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:46.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:46.343
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 20 22:58:46.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7324 02/20/23 22:58:46.352
    W0220 22:58:46.376957      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0220 22:58:46.377384      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7324, replica count: 1
    I0220 22:58:47.428281      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0220 22:58:48.430006      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 22:58:48.552: INFO: Created: latency-svc-6ztwk
    Feb 20 22:58:48.570: INFO: Got endpoints: latency-svc-6ztwk [39.626155ms]
    Feb 20 22:58:48.597: INFO: Created: latency-svc-chk58
    Feb 20 22:58:48.606: INFO: Created: latency-svc-jqtwf
    Feb 20 22:58:48.608: INFO: Got endpoints: latency-svc-chk58 [36.187155ms]
    Feb 20 22:58:48.616: INFO: Created: latency-svc-jcvsm
    Feb 20 22:58:48.635: INFO: Got endpoints: latency-svc-jqtwf [64.571234ms]
    Feb 20 22:58:48.643: INFO: Got endpoints: latency-svc-jcvsm [71.117245ms]
    Feb 20 22:58:48.644: INFO: Created: latency-svc-9sz2n
    Feb 20 22:58:48.647: INFO: Created: latency-svc-w2zhm
    Feb 20 22:58:48.655: INFO: Got endpoints: latency-svc-9sz2n [82.072631ms]
    Feb 20 22:58:48.655: INFO: Created: latency-svc-xgb8g
    Feb 20 22:58:48.664: INFO: Got endpoints: latency-svc-w2zhm [91.76446ms]
    Feb 20 22:58:48.665: INFO: Created: latency-svc-p7mfc
    Feb 20 22:58:48.674: INFO: Created: latency-svc-l25f6
    Feb 20 22:58:48.684: INFO: Created: latency-svc-m5hqc
    Feb 20 22:58:48.698: INFO: Got endpoints: latency-svc-p7mfc [127.032729ms]
    Feb 20 22:58:48.698: INFO: Created: latency-svc-lhldt
    Feb 20 22:58:48.703: INFO: Created: latency-svc-jq92j
    Feb 20 22:58:48.712: INFO: Created: latency-svc-trhtn
    Feb 20 22:58:48.722: INFO: Created: latency-svc-bngwz
    Feb 20 22:58:48.726: INFO: Got endpoints: latency-svc-m5hqc [154.51843ms]
    Feb 20 22:58:48.729: INFO: Got endpoints: latency-svc-l25f6 [156.107387ms]
    Feb 20 22:58:48.732: INFO: Created: latency-svc-h4hx6
    Feb 20 22:58:48.741: INFO: Created: latency-svc-krzm5
    Feb 20 22:58:48.748: INFO: Got endpoints: latency-svc-lhldt [175.793901ms]
    Feb 20 22:58:48.749: INFO: Got endpoints: latency-svc-xgb8g [177.362444ms]
    Feb 20 22:58:48.751: INFO: Created: latency-svc-xtdp2
    Feb 20 22:58:48.760: INFO: Created: latency-svc-pw97q
    Feb 20 22:58:48.772: INFO: Created: latency-svc-kk7cn
    Feb 20 22:58:48.781: INFO: Created: latency-svc-7m26b
    Feb 20 22:58:48.787: INFO: Got endpoints: latency-svc-jq92j [215.507332ms]
    Feb 20 22:58:48.791: INFO: Created: latency-svc-rg9t6
    Feb 20 22:58:48.800: INFO: Created: latency-svc-pz2dl
    Feb 20 22:58:48.805: INFO: Got endpoints: latency-svc-trhtn [231.906821ms]
    Feb 20 22:58:48.810: INFO: Created: latency-svc-g8fkh
    Feb 20 22:58:48.811: INFO: Got endpoints: latency-svc-krzm5 [238.617355ms]
    Feb 20 22:58:48.819: INFO: Created: latency-svc-zvw2s
    Feb 20 22:58:48.823: INFO: Got endpoints: latency-svc-h4hx6 [250.480475ms]
    Feb 20 22:58:48.824: INFO: Got endpoints: latency-svc-bngwz [252.651765ms]
    Feb 20 22:58:48.842: INFO: Created: latency-svc-stg4t
    Feb 20 22:58:48.842: INFO: Created: latency-svc-b8qpl
    Feb 20 22:58:48.846: INFO: Created: latency-svc-s4crq
    Feb 20 22:58:48.852: INFO: Got endpoints: latency-svc-xtdp2 [243.289758ms]
    Feb 20 22:58:48.861: INFO: Created: latency-svc-7h8pc
    Feb 20 22:58:48.872: INFO: Got endpoints: latency-svc-kk7cn [229.508407ms]
    Feb 20 22:58:48.872: INFO: Created: latency-svc-hln5p
    Feb 20 22:58:48.889: INFO: Created: latency-svc-slxw9
    Feb 20 22:58:48.889: INFO: Got endpoints: latency-svc-pw97q [254.246554ms]
    Feb 20 22:58:48.890: INFO: Got endpoints: latency-svc-7m26b [234.835712ms]
    Feb 20 22:58:48.891: INFO: Got endpoints: latency-svc-pz2dl [192.641472ms]
    Feb 20 22:58:48.891: INFO: Created: latency-svc-cncnr
    Feb 20 22:58:48.901: INFO: Created: latency-svc-pcmcj
    Feb 20 22:58:48.907: INFO: Got endpoints: latency-svc-rg9t6 [243.368388ms]
    Feb 20 22:58:48.911: INFO: Created: latency-svc-5wrdm
    Feb 20 22:58:48.923: INFO: Created: latency-svc-4xb68
    Feb 20 22:58:48.923: INFO: Got endpoints: latency-svc-g8fkh [196.915622ms]
    Feb 20 22:58:48.932: INFO: Created: latency-svc-jw2z5
    Feb 20 22:58:48.941: INFO: Created: latency-svc-7b54h
    Feb 20 22:58:48.942: INFO: Got endpoints: latency-svc-zvw2s [213.458961ms]
    Feb 20 22:58:48.944: INFO: Got endpoints: latency-svc-stg4t [195.358297ms]
    Feb 20 22:58:48.951: INFO: Created: latency-svc-gl2rn
    Feb 20 22:58:48.952: INFO: Got endpoints: latency-svc-b8qpl [203.923939ms]
    Feb 20 22:58:48.960: INFO: Created: latency-svc-mrw75
    Feb 20 22:58:48.973: INFO: Created: latency-svc-6ccp6
    Feb 20 22:58:48.983: INFO: Created: latency-svc-k6n46
    Feb 20 22:58:48.984: INFO: Got endpoints: latency-svc-s4crq [194.893564ms]
    Feb 20 22:58:48.985: INFO: Got endpoints: latency-svc-7h8pc [180.216532ms]
    Feb 20 22:58:48.991: INFO: Created: latency-svc-gcd7x
    Feb 20 22:58:48.991: INFO: Got endpoints: latency-svc-cncnr [166.617829ms]
    Feb 20 22:58:48.991: INFO: Got endpoints: latency-svc-slxw9 [168.164696ms]
    Feb 20 22:58:48.991: INFO: Got endpoints: latency-svc-hln5p [180.368723ms]
    Feb 20 22:58:48.993: INFO: Got endpoints: latency-svc-pcmcj [141.060065ms]
    Feb 20 22:58:48.994: INFO: Got endpoints: latency-svc-5wrdm [121.522116ms]
    Feb 20 22:58:49.006: INFO: Created: latency-svc-cn98p
    Feb 20 22:58:49.016: INFO: Created: latency-svc-lmss6
    Feb 20 22:58:49.020: INFO: Got endpoints: latency-svc-jw2z5 [129.718027ms]
    Feb 20 22:58:49.023: INFO: Got endpoints: latency-svc-7b54h [133.047664ms]
    Feb 20 22:58:49.023: INFO: Got endpoints: latency-svc-mrw75 [99.433878ms]
    Feb 20 22:58:49.024: INFO: Got endpoints: latency-svc-gl2rn [117.053028ms]
    Feb 20 22:58:49.025: INFO: Got endpoints: latency-svc-4xb68 [135.273448ms]
    Feb 20 22:58:49.031: INFO: Created: latency-svc-m9p9n
    Feb 20 22:58:49.034: INFO: Got endpoints: latency-svc-6ccp6 [92.316208ms]
    Feb 20 22:58:49.037: INFO: Got endpoints: latency-svc-gcd7x [85.614379ms]
    Feb 20 22:58:49.038: INFO: Got endpoints: latency-svc-cn98p [53.907425ms]
    Feb 20 22:58:49.040: INFO: Got endpoints: latency-svc-k6n46 [95.761225ms]
    Feb 20 22:58:49.044: INFO: Got endpoints: latency-svc-lmss6 [59.09058ms]
    Feb 20 22:58:49.055: INFO: Got endpoints: latency-svc-m9p9n [64.141755ms]
    Feb 20 22:58:49.188: INFO: Created: latency-svc-7pjrj
    Feb 20 22:58:49.194: INFO: Created: latency-svc-p4tpc
    Feb 20 22:58:49.195: INFO: Created: latency-svc-27fh7
    Feb 20 22:58:49.195: INFO: Created: latency-svc-9bcrb
    Feb 20 22:58:49.199: INFO: Created: latency-svc-hrz5f
    Feb 20 22:58:49.199: INFO: Created: latency-svc-7bdjs
    Feb 20 22:58:49.199: INFO: Created: latency-svc-94b9v
    Feb 20 22:58:49.200: INFO: Created: latency-svc-sf2kd
    Feb 20 22:58:49.201: INFO: Created: latency-svc-k2p5m
    Feb 20 22:58:49.201: INFO: Created: latency-svc-twlff
    Feb 20 22:58:49.202: INFO: Created: latency-svc-pzljs
    Feb 20 22:58:49.202: INFO: Created: latency-svc-cn786
    Feb 20 22:58:49.202: INFO: Got endpoints: latency-svc-p4tpc [181.041274ms]
    Feb 20 22:58:49.202: INFO: Created: latency-svc-7f2jc
    Feb 20 22:58:49.202: INFO: Created: latency-svc-t4ln8
    Feb 20 22:58:49.202: INFO: Got endpoints: latency-svc-7pjrj [177.3975ms]
    Feb 20 22:58:49.204: INFO: Got endpoints: latency-svc-94b9v [148.626312ms]
    Feb 20 22:58:49.206: INFO: Created: latency-svc-wj2qw
    Feb 20 22:58:49.210: INFO: Got endpoints: latency-svc-t4ln8 [171.450456ms]
    Feb 20 22:58:49.210: INFO: Got endpoints: latency-svc-27fh7 [218.83ms]
    Feb 20 22:58:49.210: INFO: Got endpoints: latency-svc-wj2qw [187.575775ms]
    Feb 20 22:58:49.226: INFO: Created: latency-svc-6lb5w
    Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-sf2kd [201.104784ms]
    Feb 20 22:58:49.236: INFO: Created: latency-svc-jzd7s
    Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-cn786 [244.330604ms]
    Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-hrz5f [213.273006ms]
    Feb 20 22:58:49.236: INFO: Got endpoints: latency-svc-7bdjs [242.332038ms]
    Feb 20 22:58:49.238: INFO: Got endpoints: latency-svc-9bcrb [200.272049ms]
    Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-7f2jc [222.847168ms]
    Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-6lb5w [61.193315ms]
    Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-twlff [238.406434ms]
    Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-k2p5m [270.551509ms]
    Feb 20 22:58:49.263: INFO: Created: latency-svc-cscl6
    Feb 20 22:58:49.263: INFO: Got endpoints: latency-svc-pzljs [219.300662ms]
    Feb 20 22:58:49.318: INFO: Got endpoints: latency-svc-cscl6 [113.537524ms]
    Feb 20 22:58:49.318: INFO: Created: latency-svc-vxzzx
    Feb 20 22:58:49.318: INFO: Got endpoints: latency-svc-jzd7s [116.2374ms]
    Feb 20 22:58:49.320: INFO: Created: latency-svc-2vvcw
    Feb 20 22:58:49.320: INFO: Created: latency-svc-wqrwn
    Feb 20 22:58:49.324: INFO: Got endpoints: latency-svc-2vvcw [113.534251ms]
    Feb 20 22:58:49.324: INFO: Got endpoints: latency-svc-wqrwn [113.39582ms]
    Feb 20 22:58:49.324: INFO: Got endpoints: latency-svc-vxzzx [113.947546ms]
    Feb 20 22:58:49.371: INFO: Created: latency-svc-7zftg
    Feb 20 22:58:49.423: INFO: Got endpoints: latency-svc-7zftg [187.710517ms]
    Feb 20 22:58:49.423: INFO: Created: latency-svc-th8md
    Feb 20 22:58:49.461: INFO: Got endpoints: latency-svc-th8md [225.574584ms]
    Feb 20 22:58:49.462: INFO: Created: latency-svc-nflth
    Feb 20 22:58:49.465: INFO: Got endpoints: latency-svc-nflth [228.880294ms]
    Feb 20 22:58:49.465: INFO: Created: latency-svc-zchrq
    Feb 20 22:58:49.495: INFO: Created: latency-svc-s2hjr
    Feb 20 22:58:49.495: INFO: Got endpoints: latency-svc-s2hjr [257.464134ms]
    Feb 20 22:58:49.495: INFO: Created: latency-svc-j8pkn
    Feb 20 22:58:49.495: INFO: Got endpoints: latency-svc-zchrq [259.336464ms]
    Feb 20 22:58:49.498: INFO: Created: latency-svc-fkbmj
    Feb 20 22:58:49.509: INFO: Got endpoints: latency-svc-j8pkn [245.871399ms]
    Feb 20 22:58:49.509: INFO: Got endpoints: latency-svc-fkbmj [246.273627ms]
    Feb 20 22:58:49.510: INFO: Created: latency-svc-bql8m
    Feb 20 22:58:49.534: INFO: Created: latency-svc-gn8j2
    Feb 20 22:58:49.534: INFO: Got endpoints: latency-svc-gn8j2 [271.134795ms]
    Feb 20 22:58:49.535: INFO: Got endpoints: latency-svc-bql8m [271.849884ms]
    Feb 20 22:58:49.746: INFO: Created: latency-svc-vmhqh
    Feb 20 22:58:49.746: INFO: Created: latency-svc-6jc9d
    Feb 20 22:58:49.746: INFO: Created: latency-svc-zjbdp
    Feb 20 22:58:49.746: INFO: Created: latency-svc-x7628
    Feb 20 22:58:49.746: INFO: Created: latency-svc-c26cp
    Feb 20 22:58:49.747: INFO: Created: latency-svc-rnjzp
    Feb 20 22:58:49.747: INFO: Created: latency-svc-klfh5
    Feb 20 22:58:49.747: INFO: Created: latency-svc-qxxfn
    Feb 20 22:58:49.747: INFO: Created: latency-svc-gb9zf
    Feb 20 22:58:49.747: INFO: Created: latency-svc-mlg4z
    Feb 20 22:58:49.747: INFO: Created: latency-svc-gr4kf
    Feb 20 22:58:49.747: INFO: Created: latency-svc-h2h2z
    Feb 20 22:58:49.747: INFO: Created: latency-svc-l5gml
    Feb 20 22:58:49.748: INFO: Created: latency-svc-9wht9
    Feb 20 22:58:49.748: INFO: Created: latency-svc-qfqt7
    Feb 20 22:58:49.762: INFO: Got endpoints: latency-svc-vmhqh [432.460659ms]
    Feb 20 22:58:49.770: INFO: Got endpoints: latency-svc-6jc9d [440.124472ms]
    Feb 20 22:58:49.770: INFO: Got endpoints: latency-svc-qfqt7 [273.561389ms]
    Feb 20 22:58:49.770: INFO: Got endpoints: latency-svc-zjbdp [235.706612ms]
    Feb 20 22:58:49.771: INFO: Got endpoints: latency-svc-klfh5 [305.744966ms]
    Feb 20 22:58:49.777: INFO: Got endpoints: latency-svc-gb9zf [513.89303ms]
    Feb 20 22:58:49.784: INFO: Got endpoints: latency-svc-c26cp [466.433829ms]
    Feb 20 22:58:49.788: INFO: Created: latency-svc-bdxkk
    Feb 20 22:58:49.788: INFO: Got endpoints: latency-svc-gr4kf [326.812489ms]
    Feb 20 22:58:49.789: INFO: Got endpoints: latency-svc-rnjzp [253.955974ms]
    Feb 20 22:58:49.789: INFO: Got endpoints: latency-svc-x7628 [280.74089ms]
    Feb 20 22:58:49.790: INFO: Got endpoints: latency-svc-l5gml [366.118617ms]
    Feb 20 22:58:49.797: INFO: Created: latency-svc-4pz6k
    Feb 20 22:58:49.809: INFO: Got endpoints: latency-svc-mlg4z [313.293856ms]
    Feb 20 22:58:49.809: INFO: Got endpoints: latency-svc-h2h2z [299.712889ms]
    Feb 20 22:58:49.809: INFO: Got endpoints: latency-svc-9wht9 [479.782167ms]
    Feb 20 22:58:49.826: INFO: Created: latency-svc-ckfl5
    Feb 20 22:58:49.826: INFO: Created: latency-svc-ngp8p
    Feb 20 22:58:49.827: INFO: Created: latency-svc-gw5k5
    Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-gw5k5 [58.293243ms]
    Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-qxxfn [511.186144ms]
    Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-bdxkk [66.764034ms]
    Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-4pz6k [59.317432ms]
    Feb 20 22:58:49.829: INFO: Got endpoints: latency-svc-ckfl5 [59.058886ms]
    Feb 20 22:58:49.836: INFO: Created: latency-svc-6rbp5
    Feb 20 22:58:49.836: INFO: Got endpoints: latency-svc-ngp8p [65.60505ms]
    Feb 20 22:58:49.854: INFO: Got endpoints: latency-svc-6rbp5 [76.44788ms]
    Feb 20 22:58:49.854: INFO: Created: latency-svc-f95g7
    Feb 20 22:58:49.855: INFO: Created: latency-svc-rv57k
    Feb 20 22:58:49.859: INFO: Got endpoints: latency-svc-rv57k [74.211987ms]
    Feb 20 22:58:49.864: INFO: Created: latency-svc-xrlxx
    Feb 20 22:58:49.881: INFO: Created: latency-svc-4hlz9
    Feb 20 22:58:49.881: INFO: Got endpoints: latency-svc-xrlxx [92.772508ms]
    Feb 20 22:58:49.881: INFO: Got endpoints: latency-svc-f95g7 [92.372053ms]
    Feb 20 22:58:49.900: INFO: Created: latency-svc-jt6mg
    Feb 20 22:58:49.901: INFO: Created: latency-svc-6xlxp
    Feb 20 22:58:49.904: INFO: Got endpoints: latency-svc-jt6mg [114.497497ms]
    Feb 20 22:58:49.905: INFO: Got endpoints: latency-svc-4hlz9 [115.471947ms]
    Feb 20 22:58:49.906: INFO: Created: latency-svc-pzvn5
    Feb 20 22:58:49.915: INFO: Got endpoints: latency-svc-6xlxp [105.331304ms]
    Feb 20 22:58:49.915: INFO: Created: latency-svc-7lsdw
    Feb 20 22:58:49.918: INFO: Got endpoints: latency-svc-pzvn5 [108.974402ms]
    Feb 20 22:58:49.926: INFO: Got endpoints: latency-svc-7lsdw [116.931208ms]
    Feb 20 22:58:49.927: INFO: Created: latency-svc-pfn2j
    Feb 20 22:58:49.934: INFO: Created: latency-svc-b6mrn
    Feb 20 22:58:49.938: INFO: Got endpoints: latency-svc-pfn2j [108.888158ms]
    Feb 20 22:58:49.943: INFO: Created: latency-svc-xgx9h
    Feb 20 22:58:49.949: INFO: Got endpoints: latency-svc-b6mrn [120.15751ms]
    Feb 20 22:58:49.964: INFO: Got endpoints: latency-svc-xgx9h [134.537979ms]
    Feb 20 22:58:49.964: INFO: Created: latency-svc-jtcls
    Feb 20 22:58:49.964: INFO: Created: latency-svc-7wctq
    Feb 20 22:58:49.964: INFO: Got endpoints: latency-svc-7wctq [135.8644ms]
    Feb 20 22:58:49.975: INFO: Created: latency-svc-kbmgp
    Feb 20 22:58:49.978: INFO: Got endpoints: latency-svc-jtcls [149.414566ms]
    Feb 20 22:58:49.983: INFO: Got endpoints: latency-svc-kbmgp [146.468027ms]
    Feb 20 22:58:49.987: INFO: Created: latency-svc-fgngc
    Feb 20 22:58:49.996: INFO: Got endpoints: latency-svc-fgngc [141.996368ms]
    Feb 20 22:58:49.996: INFO: Created: latency-svc-hmn22
    Feb 20 22:58:50.009: INFO: Got endpoints: latency-svc-hmn22 [150.375012ms]
    Feb 20 22:58:50.009: INFO: Created: latency-svc-l95sm
    Feb 20 22:58:50.022: INFO: Got endpoints: latency-svc-l95sm [140.844592ms]
    Feb 20 22:58:50.025: INFO: Created: latency-svc-8dhtx
    Feb 20 22:58:50.031: INFO: Created: latency-svc-w22wv
    Feb 20 22:58:50.036: INFO: Got endpoints: latency-svc-8dhtx [154.698821ms]
    Feb 20 22:58:50.041: INFO: Created: latency-svc-bq6v8
    Feb 20 22:58:50.045: INFO: Got endpoints: latency-svc-w22wv [141.188798ms]
    Feb 20 22:58:50.050: INFO: Created: latency-svc-8nj6v
    Feb 20 22:58:50.060: INFO: Created: latency-svc-pdj5g
    Feb 20 22:58:50.065: INFO: Got endpoints: latency-svc-bq6v8 [159.795797ms]
    Feb 20 22:58:50.068: INFO: Got endpoints: latency-svc-8nj6v [152.957357ms]
    Feb 20 22:58:50.075: INFO: Got endpoints: latency-svc-pdj5g [157.360732ms]
    Feb 20 22:58:50.075: INFO: Created: latency-svc-nhrmj
    Feb 20 22:58:50.086: INFO: Got endpoints: latency-svc-nhrmj [159.392684ms]
    Feb 20 22:58:50.088: INFO: Created: latency-svc-xkb4w
    Feb 20 22:58:50.100: INFO: Created: latency-svc-7v9j6
    Feb 20 22:58:50.106: INFO: Got endpoints: latency-svc-xkb4w [167.935863ms]
    Feb 20 22:58:50.109: INFO: Created: latency-svc-jlc5m
    Feb 20 22:58:50.113: INFO: Got endpoints: latency-svc-7v9j6 [163.688225ms]
    Feb 20 22:58:50.120: INFO: Created: latency-svc-kqr7g
    Feb 20 22:58:50.121: INFO: Got endpoints: latency-svc-jlc5m [157.225496ms]
    Feb 20 22:58:50.129: INFO: Created: latency-svc-dbrjs
    Feb 20 22:58:50.139: INFO: Created: latency-svc-nthpv
    Feb 20 22:58:50.140: INFO: Got endpoints: latency-svc-kqr7g [175.141502ms]
    Feb 20 22:58:50.142: INFO: Got endpoints: latency-svc-dbrjs [164.046127ms]
    Feb 20 22:58:50.150: INFO: Created: latency-svc-g7pnw
    Feb 20 22:58:50.154: INFO: Created: latency-svc-lv7j5
    Feb 20 22:58:50.165: INFO: Created: latency-svc-z9nmc
    Feb 20 22:58:50.171: INFO: Created: latency-svc-ndnqw
    Feb 20 22:58:50.177: INFO: Got endpoints: latency-svc-nthpv [194.742334ms]
    Feb 20 22:58:50.180: INFO: Got endpoints: latency-svc-z9nmc [157.802738ms]
    Feb 20 22:58:50.181: INFO: Got endpoints: latency-svc-g7pnw [185.390016ms]
    Feb 20 22:58:50.182: INFO: Got endpoints: latency-svc-lv7j5 [172.49725ms]
    Feb 20 22:58:50.184: INFO: Created: latency-svc-7jsxx
    Feb 20 22:58:50.185: INFO: Got endpoints: latency-svc-ndnqw [149.382189ms]
    Feb 20 22:58:50.191: INFO: Created: latency-svc-sbhcg
    Feb 20 22:58:50.194: INFO: Got endpoints: latency-svc-7jsxx [148.742127ms]
    Feb 20 22:58:50.199: INFO: Got endpoints: latency-svc-sbhcg [134.353683ms]
    Feb 20 22:58:50.201: INFO: Created: latency-svc-j7plp
    Feb 20 22:58:50.213: INFO: Created: latency-svc-c7pz4
    Feb 20 22:58:50.213: INFO: Got endpoints: latency-svc-j7plp [145.331804ms]
    Feb 20 22:58:50.221: INFO: Got endpoints: latency-svc-c7pz4 [145.976117ms]
    Feb 20 22:58:50.353: INFO: Created: latency-svc-czmgl
    Feb 20 22:58:50.359: INFO: Created: latency-svc-x7klv
    Feb 20 22:58:50.359: INFO: Created: latency-svc-s54xk
    Feb 20 22:58:50.360: INFO: Created: latency-svc-v5b4p
    Feb 20 22:58:50.360: INFO: Created: latency-svc-rvjkm
    Feb 20 22:58:50.360: INFO: Created: latency-svc-shklr
    Feb 20 22:58:50.363: INFO: Created: latency-svc-6xgcl
    Feb 20 22:58:50.363: INFO: Created: latency-svc-xbknv
    Feb 20 22:58:50.360: INFO: Created: latency-svc-lxlnw
    Feb 20 22:58:50.360: INFO: Created: latency-svc-t9cp7
    Feb 20 22:58:50.362: INFO: Created: latency-svc-8fdtt
    Feb 20 22:58:50.362: INFO: Created: latency-svc-xjv52
    Feb 20 22:58:50.363: INFO: Created: latency-svc-48pps
    Feb 20 22:58:50.363: INFO: Created: latency-svc-w2zsc
    Feb 20 22:58:50.364: INFO: Created: latency-svc-v88q5
    Feb 20 22:58:50.365: INFO: Got endpoints: latency-svc-czmgl [187.894883ms]
    Feb 20 22:58:50.366: INFO: Got endpoints: latency-svc-x7klv [223.391235ms]
    Feb 20 22:58:50.366: INFO: Got endpoints: latency-svc-v5b4p [180.848866ms]
    Feb 20 22:58:50.374: INFO: Got endpoints: latency-svc-v88q5 [174.854329ms]
    Feb 20 22:58:50.374: INFO: Got endpoints: latency-svc-s54xk [288.320607ms]
    Feb 20 22:58:50.376: INFO: Got endpoints: latency-svc-shklr [154.901189ms]
    Feb 20 22:58:50.376: INFO: Got endpoints: latency-svc-8fdtt [196.362126ms]
    Feb 20 22:58:50.378: INFO: Got endpoints: latency-svc-t9cp7 [196.124193ms]
    Feb 20 22:58:50.378: INFO: Got endpoints: latency-svc-6xgcl [271.859738ms]
    Feb 20 22:58:50.381: INFO: Got endpoints: latency-svc-rvjkm [241.404097ms]
    Feb 20 22:58:50.386: INFO: Got endpoints: latency-svc-48pps [204.233097ms]
    Feb 20 22:58:50.387: INFO: Got endpoints: latency-svc-xbknv [265.609466ms]
    Feb 20 22:58:50.388: INFO: Got endpoints: latency-svc-xjv52 [194.028656ms]
    Feb 20 22:58:50.390: INFO: Got endpoints: latency-svc-lxlnw [176.722003ms]
    Feb 20 22:58:50.396: INFO: Created: latency-svc-67lhv
    Feb 20 22:58:50.398: INFO: Got endpoints: latency-svc-w2zsc [285.450682ms]
    Feb 20 22:58:50.405: INFO: Got endpoints: latency-svc-67lhv [39.329569ms]
    Feb 20 22:58:50.532: INFO: Created: latency-svc-27q4n
    Feb 20 22:58:50.533: INFO: Created: latency-svc-2f2rx
    Feb 20 22:58:50.535: INFO: Created: latency-svc-vgtt2
    Feb 20 22:58:50.536: INFO: Created: latency-svc-292tn
    Feb 20 22:58:50.539: INFO: Created: latency-svc-bd9rq
    Feb 20 22:58:50.540: INFO: Created: latency-svc-cgrl5
    Feb 20 22:58:50.540: INFO: Created: latency-svc-m442v
    Feb 20 22:58:50.541: INFO: Created: latency-svc-4l87c
    Feb 20 22:58:50.541: INFO: Created: latency-svc-779b8
    Feb 20 22:58:50.542: INFO: Created: latency-svc-nzn2h
    Feb 20 22:58:50.543: INFO: Created: latency-svc-lzbg8
    Feb 20 22:58:50.543: INFO: Created: latency-svc-k7x4n
    Feb 20 22:58:50.543: INFO: Created: latency-svc-wk2z9
    Feb 20 22:58:50.543: INFO: Created: latency-svc-gd9dc
    Feb 20 22:58:50.543: INFO: Created: latency-svc-825p9
    Feb 20 22:58:50.551: INFO: Got endpoints: latency-svc-27q4n [176.064884ms]
    Feb 20 22:58:50.551: INFO: Got endpoints: latency-svc-2f2rx [174.956052ms]
    Feb 20 22:58:50.551: INFO: Got endpoints: latency-svc-vgtt2 [161.296474ms]
    Feb 20 22:58:50.552: INFO: Got endpoints: latency-svc-292tn [173.99711ms]
    Feb 20 22:58:50.552: INFO: Got endpoints: latency-svc-bd9rq [166.209826ms]
    Feb 20 22:58:50.553: INFO: Got endpoints: latency-svc-k7x4n [171.645124ms]
    Feb 20 22:58:50.553: INFO: Got endpoints: latency-svc-cgrl5 [164.500688ms]
    Feb 20 22:58:50.554: INFO: Got endpoints: latency-svc-825p9 [176.302408ms]
    Feb 20 22:58:50.567: INFO: Got endpoints: latency-svc-lzbg8 [201.344715ms]
    Feb 20 22:58:50.576: INFO: Created: latency-svc-q6qxt
    Feb 20 22:58:50.577: INFO: Got endpoints: latency-svc-wk2z9 [189.864223ms]
    Feb 20 22:58:50.577: INFO: Got endpoints: latency-svc-4l87c [179.207873ms]
    Feb 20 22:58:50.578: INFO: Got endpoints: latency-svc-m442v [201.583503ms]
    Feb 20 22:58:50.578: INFO: Got endpoints: latency-svc-gd9dc [212.738716ms]
    Feb 20 22:58:50.582: INFO: Got endpoints: latency-svc-nzn2h [177.177177ms]
    Feb 20 22:58:50.587: INFO: Created: latency-svc-bl94d
    Feb 20 22:58:50.588: INFO: Got endpoints: latency-svc-779b8 [213.695441ms]
    Feb 20 22:58:50.588: INFO: Got endpoints: latency-svc-q6qxt [37.312157ms]
    Feb 20 22:58:50.603: INFO: Created: latency-svc-xzmrl
    Feb 20 22:58:50.604: INFO: Got endpoints: latency-svc-bl94d [52.959771ms]
    Feb 20 22:58:50.613: INFO: Created: latency-svc-mzqjr
    Feb 20 22:58:50.618: INFO: Got endpoints: latency-svc-xzmrl [66.702833ms]
    Feb 20 22:58:50.624: INFO: Created: latency-svc-lh6ws
    Feb 20 22:58:50.628: INFO: Got endpoints: latency-svc-mzqjr [76.133648ms]
    Feb 20 22:58:50.634: INFO: Created: latency-svc-8nrmx
    Feb 20 22:58:50.635: INFO: Got endpoints: latency-svc-lh6ws [82.083578ms]
    Feb 20 22:58:50.644: INFO: Created: latency-svc-gt5tm
    Feb 20 22:58:50.654: INFO: Created: latency-svc-8twt2
    Feb 20 22:58:50.661: INFO: Created: latency-svc-d5hnk
    Feb 20 22:58:50.666: INFO: Got endpoints: latency-svc-gt5tm [112.788005ms]
    Feb 20 22:58:50.666: INFO: Got endpoints: latency-svc-8nrmx [114.161061ms]
    Feb 20 22:58:50.670: INFO: Got endpoints: latency-svc-8twt2 [115.053392ms]
    Feb 20 22:58:50.671: INFO: Created: latency-svc-nd8xj
    Feb 20 22:58:50.675: INFO: Got endpoints: latency-svc-d5hnk [107.375788ms]
    Feb 20 22:58:50.683: INFO: Created: latency-svc-lbqn7
    Feb 20 22:58:50.687: INFO: Got endpoints: latency-svc-nd8xj [110.002115ms]
    Feb 20 22:58:50.695: INFO: Created: latency-svc-wcg6n
    Feb 20 22:58:50.697: INFO: Got endpoints: latency-svc-lbqn7 [119.523161ms]
    Feb 20 22:58:50.705: INFO: Got endpoints: latency-svc-wcg6n [126.994898ms]
    Feb 20 22:58:50.705: INFO: Created: latency-svc-q2b2q
    Feb 20 22:58:50.711: INFO: Created: latency-svc-rwfcb
    Feb 20 22:58:50.714: INFO: Got endpoints: latency-svc-q2b2q [135.506769ms]
    Feb 20 22:58:50.722: INFO: Created: latency-svc-bxqzc
    Feb 20 22:58:50.731: INFO: Created: latency-svc-g2st8
    Feb 20 22:58:50.733: INFO: Got endpoints: latency-svc-rwfcb [150.283316ms]
    Feb 20 22:58:50.739: INFO: Got endpoints: latency-svc-bxqzc [151.069888ms]
    Feb 20 22:58:50.741: INFO: Created: latency-svc-p4f5l
    Feb 20 22:58:50.744: INFO: Got endpoints: latency-svc-g2st8 [155.825546ms]
    Feb 20 22:58:50.750: INFO: Created: latency-svc-pjhhd
    Feb 20 22:58:50.755: INFO: Got endpoints: latency-svc-p4f5l [150.924755ms]
    Feb 20 22:58:50.764: INFO: Got endpoints: latency-svc-pjhhd [145.797138ms]
    Feb 20 22:58:50.892: INFO: Created: latency-svc-2tq7v
    Feb 20 22:58:50.892: INFO: Created: latency-svc-c9thw
    Feb 20 22:58:50.892: INFO: Created: latency-svc-n5kxx
    Feb 20 22:58:50.892: INFO: Created: latency-svc-mj59w
    Feb 20 22:58:50.893: INFO: Created: latency-svc-hfv5n
    Feb 20 22:58:50.893: INFO: Created: latency-svc-lll9g
    Feb 20 22:58:50.893: INFO: Created: latency-svc-qj7hc
    Feb 20 22:58:50.893: INFO: Created: latency-svc-gt52v
    Feb 20 22:58:50.893: INFO: Created: latency-svc-g5qdb
    Feb 20 22:58:50.894: INFO: Created: latency-svc-wbbvl
    Feb 20 22:58:50.897: INFO: Created: latency-svc-hgmlb
    Feb 20 22:58:50.897: INFO: Created: latency-svc-hprc4
    Feb 20 22:58:50.898: INFO: Created: latency-svc-lrjfz
    Feb 20 22:58:50.898: INFO: Created: latency-svc-zvklb
    Feb 20 22:58:50.898: INFO: Created: latency-svc-hs7p2
    Feb 20 22:58:50.901: INFO: Got endpoints: latency-svc-2tq7v [162.162297ms]
    Feb 20 22:58:50.903: INFO: Got endpoints: latency-svc-hprc4 [274.454042ms]
    Feb 20 22:58:50.904: INFO: Got endpoints: latency-svc-mj59w [159.872423ms]
    Feb 20 22:58:50.906: INFO: Got endpoints: latency-svc-n5kxx [270.729247ms]
    Feb 20 22:58:50.906: INFO: Got endpoints: latency-svc-c9thw [151.074625ms]
    Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-gt52v [231.45223ms]
    Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-hs7p2 [164.732658ms]
    Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-g5qdb [263.387288ms]
    Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-lrjfz [259.742708ms]
    Feb 20 22:58:50.929: INFO: Got endpoints: latency-svc-zvklb [263.08477ms]
    Feb 20 22:58:50.930: INFO: Got endpoints: latency-svc-lll9g [224.756121ms]
    Feb 20 22:58:50.939: INFO: Created: latency-svc-4brlm
    Feb 20 22:58:50.948: INFO: Created: latency-svc-gwbf4
    Feb 20 22:58:50.952: INFO: Got endpoints: latency-svc-qj7hc [276.853153ms]
    Feb 20 22:58:50.958: INFO: Created: latency-svc-kwqqg
    Feb 20 22:58:50.959: INFO: Got endpoints: latency-svc-hgmlb [272.693044ms]
    Feb 20 22:58:50.962: INFO: Got endpoints: latency-svc-4brlm [60.434705ms]
    Feb 20 22:58:50.968: INFO: Got endpoints: latency-svc-hfv5n [253.708448ms]
    Feb 20 22:58:50.970: INFO: Got endpoints: latency-svc-wbbvl [237.02699ms]
    Feb 20 22:58:50.977: INFO: Created: latency-svc-c4qpp
    Feb 20 22:58:50.984: INFO: Created: latency-svc-vmqrh
    Feb 20 22:58:50.999: INFO: Got endpoints: latency-svc-gwbf4 [96.422853ms]
    Feb 20 22:58:51.002: INFO: Got endpoints: latency-svc-kwqqg [97.182669ms]
    Feb 20 22:58:51.003: INFO: Got endpoints: latency-svc-c4qpp [96.769836ms]
    Feb 20 22:58:51.007: INFO: Got endpoints: latency-svc-vmqrh [100.202252ms]
    Feb 20 22:58:51.007: INFO: Latencies: [36.187155ms 37.312157ms 39.329569ms 52.959771ms 53.907425ms 58.293243ms 59.058886ms 59.09058ms 59.317432ms 60.434705ms 61.193315ms 64.141755ms 64.571234ms 65.60505ms 66.702833ms 66.764034ms 71.117245ms 74.211987ms 76.133648ms 76.44788ms 82.072631ms 82.083578ms 85.614379ms 91.76446ms 92.316208ms 92.372053ms 92.772508ms 95.761225ms 96.422853ms 96.769836ms 97.182669ms 99.433878ms 100.202252ms 105.331304ms 107.375788ms 108.888158ms 108.974402ms 110.002115ms 112.788005ms 113.39582ms 113.534251ms 113.537524ms 113.947546ms 114.161061ms 114.497497ms 115.053392ms 115.471947ms 116.2374ms 116.931208ms 117.053028ms 119.523161ms 120.15751ms 121.522116ms 126.994898ms 127.032729ms 129.718027ms 133.047664ms 134.353683ms 134.537979ms 135.273448ms 135.506769ms 135.8644ms 140.844592ms 141.060065ms 141.188798ms 141.996368ms 145.331804ms 145.797138ms 145.976117ms 146.468027ms 148.626312ms 148.742127ms 149.382189ms 149.414566ms 150.283316ms 150.375012ms 150.924755ms 151.069888ms 151.074625ms 152.957357ms 154.51843ms 154.698821ms 154.901189ms 155.825546ms 156.107387ms 157.225496ms 157.360732ms 157.802738ms 159.392684ms 159.795797ms 159.872423ms 161.296474ms 162.162297ms 163.688225ms 164.046127ms 164.500688ms 164.732658ms 166.209826ms 166.617829ms 167.935863ms 168.164696ms 171.450456ms 171.645124ms 172.49725ms 173.99711ms 174.854329ms 174.956052ms 175.141502ms 175.793901ms 176.064884ms 176.302408ms 176.722003ms 177.177177ms 177.362444ms 177.3975ms 179.207873ms 180.216532ms 180.368723ms 180.848866ms 181.041274ms 185.390016ms 187.575775ms 187.710517ms 187.894883ms 189.864223ms 192.641472ms 194.028656ms 194.742334ms 194.893564ms 195.358297ms 196.124193ms 196.362126ms 196.915622ms 200.272049ms 201.104784ms 201.344715ms 201.583503ms 203.923939ms 204.233097ms 212.738716ms 213.273006ms 213.458961ms 213.695441ms 215.507332ms 218.83ms 219.300662ms 222.847168ms 223.391235ms 224.756121ms 225.574584ms 228.880294ms 229.508407ms 231.45223ms 231.906821ms 234.835712ms 235.706612ms 237.02699ms 238.406434ms 238.617355ms 241.404097ms 242.332038ms 243.289758ms 243.368388ms 244.330604ms 245.871399ms 246.273627ms 250.480475ms 252.651765ms 253.708448ms 253.955974ms 254.246554ms 257.464134ms 259.336464ms 259.742708ms 263.08477ms 263.387288ms 265.609466ms 270.551509ms 270.729247ms 271.134795ms 271.849884ms 271.859738ms 272.693044ms 273.561389ms 274.454042ms 276.853153ms 280.74089ms 285.450682ms 288.320607ms 299.712889ms 305.744966ms 313.293856ms 326.812489ms 366.118617ms 432.460659ms 440.124472ms 466.433829ms 479.782167ms 511.186144ms 513.89303ms]
    Feb 20 22:58:51.007: INFO: 50 %ile: 168.164696ms
    Feb 20 22:58:51.007: INFO: 90 %ile: 271.849884ms
    Feb 20 22:58:51.007: INFO: 99 %ile: 511.186144ms
    Feb 20 22:58:51.007: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Feb 20 22:58:51.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-7324" for this suite. 02/20/23 22:58:51.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:51.034
Feb 20 22:58:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 22:58:51.035
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:51.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:51.133
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 02/20/23 22:58:51.141
Feb 20 22:58:51.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669" in namespace "projected-2939" to be "Succeeded or Failed"
Feb 20 22:58:51.182: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Pending", Reason="", readiness=false. Elapsed: 9.545894ms
Feb 20 22:58:53.190: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018143936s
Feb 20 22:58:55.192: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019214683s
Feb 20 22:58:57.192: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019410475s
STEP: Saw pod success 02/20/23 22:58:57.192
Feb 20 22:58:57.192: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669" satisfied condition "Succeeded or Failed"
Feb 20 22:58:57.200: INFO: Trying to get logs from node 10.8.38.70 pod downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669 container client-container: <nil>
STEP: delete the pod 02/20/23 22:58:57.218
Feb 20 22:58:57.238: INFO: Waiting for pod downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669 to disappear
Feb 20 22:58:57.246: INFO: Pod downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 22:58:57.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2939" for this suite. 02/20/23 22:58:57.259
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":246,"skipped":4509,"failed":0}
------------------------------
• [SLOW TEST] [6.240 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:51.034
    Feb 20 22:58:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 22:58:51.035
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:51.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:51.133
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 02/20/23 22:58:51.141
    Feb 20 22:58:51.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669" in namespace "projected-2939" to be "Succeeded or Failed"
    Feb 20 22:58:51.182: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Pending", Reason="", readiness=false. Elapsed: 9.545894ms
    Feb 20 22:58:53.190: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018143936s
    Feb 20 22:58:55.192: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019214683s
    Feb 20 22:58:57.192: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019410475s
    STEP: Saw pod success 02/20/23 22:58:57.192
    Feb 20 22:58:57.192: INFO: Pod "downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669" satisfied condition "Succeeded or Failed"
    Feb 20 22:58:57.200: INFO: Trying to get logs from node 10.8.38.70 pod downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669 container client-container: <nil>
    STEP: delete the pod 02/20/23 22:58:57.218
    Feb 20 22:58:57.238: INFO: Waiting for pod downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669 to disappear
    Feb 20 22:58:57.246: INFO: Pod downwardapi-volume-0896f0ce-6df6-4fbf-8064-5aedf9571669 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 22:58:57.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2939" for this suite. 02/20/23 22:58:57.259
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:58:57.275
Feb 20 22:58:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 22:58:57.28
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:57.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:57.38
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Feb 20 22:58:57.428: INFO: Waiting up to 5m0s for pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81" in namespace "container-probe-5323" to be "running and ready"
Feb 20 22:58:57.438: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 9.408652ms
Feb 20 22:58:57.438: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:58:59.446: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01773226s
Feb 20 22:58:59.447: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:59:01.481: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 4.052198536s
Feb 20 22:59:01.481: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:03.451: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 6.02209528s
Feb 20 22:59:03.451: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:05.450: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 8.020869211s
Feb 20 22:59:05.450: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:07.449: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 10.020280258s
Feb 20 22:59:07.449: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:09.451: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 12.022150254s
Feb 20 22:59:09.451: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:11.452: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 14.023122453s
Feb 20 22:59:11.452: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:13.449: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 16.020566525s
Feb 20 22:59:13.449: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:15.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 18.019415638s
Feb 20 22:59:15.448: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:17.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 20.019560546s
Feb 20 22:59:17.448: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
Feb 20 22:59:19.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=true. Elapsed: 22.019203703s
Feb 20 22:59:19.448: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = true)
Feb 20 22:59:19.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81" satisfied condition "running and ready"
Feb 20 22:59:19.457: INFO: Container started at 2023-02-20 22:58:58 +0000 UTC, pod became ready at 2023-02-20 22:59:17 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 22:59:19.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5323" for this suite. 02/20/23 22:59:19.469
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":247,"skipped":4509,"failed":0}
------------------------------
• [SLOW TEST] [22.210 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:58:57.275
    Feb 20 22:58:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 22:58:57.28
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:58:57.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:58:57.38
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Feb 20 22:58:57.428: INFO: Waiting up to 5m0s for pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81" in namespace "container-probe-5323" to be "running and ready"
    Feb 20 22:58:57.438: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 9.408652ms
    Feb 20 22:58:57.438: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:58:59.446: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01773226s
    Feb 20 22:58:59.447: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:59:01.481: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 4.052198536s
    Feb 20 22:59:01.481: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:03.451: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 6.02209528s
    Feb 20 22:59:03.451: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:05.450: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 8.020869211s
    Feb 20 22:59:05.450: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:07.449: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 10.020280258s
    Feb 20 22:59:07.449: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:09.451: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 12.022150254s
    Feb 20 22:59:09.451: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:11.452: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 14.023122453s
    Feb 20 22:59:11.452: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:13.449: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 16.020566525s
    Feb 20 22:59:13.449: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:15.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 18.019415638s
    Feb 20 22:59:15.448: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:17.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=false. Elapsed: 20.019560546s
    Feb 20 22:59:17.448: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = false)
    Feb 20 22:59:19.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81": Phase="Running", Reason="", readiness=true. Elapsed: 22.019203703s
    Feb 20 22:59:19.448: INFO: The phase of Pod test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81 is Running (Ready = true)
    Feb 20 22:59:19.448: INFO: Pod "test-webserver-99bd49ed-96e8-493a-bb5c-3b701ffb1c81" satisfied condition "running and ready"
    Feb 20 22:59:19.457: INFO: Container started at 2023-02-20 22:58:58 +0000 UTC, pod became ready at 2023-02-20 22:59:17 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 22:59:19.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5323" for this suite. 02/20/23 22:59:19.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:59:19.49
Feb 20 22:59:19.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename security-context-test 02/20/23 22:59:19.494
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:19.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:19.545
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
W0220 22:59:19.638385      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 22:59:19.638: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" in namespace "security-context-test-82" to be "Succeeded or Failed"
Feb 20 22:59:19.647: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.031777ms
Feb 20 22:59:21.657: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018958544s
Feb 20 22:59:23.657: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019308054s
Feb 20 22:59:23.658: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 20 22:59:23.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-82" for this suite. 02/20/23 22:59:23.671
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":248,"skipped":4519,"failed":0}
------------------------------
• [4.195 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:59:19.49
    Feb 20 22:59:19.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename security-context-test 02/20/23 22:59:19.494
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:19.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:19.545
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    W0220 22:59:19.638385      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 22:59:19.638: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" in namespace "security-context-test-82" to be "Succeeded or Failed"
    Feb 20 22:59:19.647: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.031777ms
    Feb 20 22:59:21.657: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018958544s
    Feb 20 22:59:23.657: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019308054s
    Feb 20 22:59:23.658: INFO: Pod "busybox-user-65534-5e8d8e13-548b-4570-85a1-51900353b8e0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 20 22:59:23.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-82" for this suite. 02/20/23 22:59:23.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:59:23.695
Feb 20 22:59:23.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 22:59:23.698
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:23.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:23.75
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 02/20/23 22:59:23.891
STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:59:23.91
Feb 20 22:59:23.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:59:23.928: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:59:24.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:59:24.970: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:59:25.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 22:59:25.982: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 22:59:26.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 22:59:26.950: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/20/23 22:59:26.961
Feb 20 22:59:26.975: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/20/23 22:59:26.975
Feb 20 22:59:27.002: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/20/23 22:59:27.002
Feb 20 22:59:27.007: INFO: Observed &DaemonSet event: ADDED
Feb 20 22:59:27.008: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.008: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.009: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.009: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.009: INFO: Found daemon set daemon-set in namespace daemonsets-2057 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 20 22:59:27.009: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/20/23 22:59:27.009
STEP: watching for the daemon set status to be patched 02/20/23 22:59:27.032
Feb 20 22:59:27.037: INFO: Observed &DaemonSet event: ADDED
Feb 20 22:59:27.038: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.038: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.038: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.039: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.039: INFO: Observed daemon set daemon-set in namespace daemonsets-2057 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 20 22:59:27.039: INFO: Observed &DaemonSet event: MODIFIED
Feb 20 22:59:27.039: INFO: Found daemon set daemon-set in namespace daemonsets-2057 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 20 22:59:27.039: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/20/23 22:59:27.053
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2057, will wait for the garbage collector to delete the pods 02/20/23 22:59:27.053
Feb 20 22:59:27.135: INFO: Deleting DaemonSet.extensions daemon-set took: 20.602565ms
Feb 20 22:59:27.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.905071ms
Feb 20 22:59:29.755: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 22:59:29.755: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 20 22:59:29.765: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"119330"},"items":null}

Feb 20 22:59:29.799: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"119330"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 22:59:29.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2057" for this suite. 02/20/23 22:59:29.843
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":249,"skipped":4546,"failed":0}
------------------------------
• [SLOW TEST] [6.180 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:59:23.695
    Feb 20 22:59:23.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 22:59:23.698
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:23.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:23.75
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 02/20/23 22:59:23.891
    STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 22:59:23.91
    Feb 20 22:59:23.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:59:23.928: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:59:24.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:59:24.970: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:59:25.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 22:59:25.982: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 22:59:26.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 22:59:26.950: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/20/23 22:59:26.961
    Feb 20 22:59:26.975: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/20/23 22:59:26.975
    Feb 20 22:59:27.002: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/20/23 22:59:27.002
    Feb 20 22:59:27.007: INFO: Observed &DaemonSet event: ADDED
    Feb 20 22:59:27.008: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.008: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.009: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.009: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.009: INFO: Found daemon set daemon-set in namespace daemonsets-2057 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 20 22:59:27.009: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/20/23 22:59:27.009
    STEP: watching for the daemon set status to be patched 02/20/23 22:59:27.032
    Feb 20 22:59:27.037: INFO: Observed &DaemonSet event: ADDED
    Feb 20 22:59:27.038: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.038: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.038: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.039: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.039: INFO: Observed daemon set daemon-set in namespace daemonsets-2057 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 20 22:59:27.039: INFO: Observed &DaemonSet event: MODIFIED
    Feb 20 22:59:27.039: INFO: Found daemon set daemon-set in namespace daemonsets-2057 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 20 22:59:27.039: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/20/23 22:59:27.053
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2057, will wait for the garbage collector to delete the pods 02/20/23 22:59:27.053
    Feb 20 22:59:27.135: INFO: Deleting DaemonSet.extensions daemon-set took: 20.602565ms
    Feb 20 22:59:27.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.905071ms
    Feb 20 22:59:29.755: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 22:59:29.755: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 20 22:59:29.765: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"119330"},"items":null}

    Feb 20 22:59:29.799: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"119330"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 22:59:29.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2057" for this suite. 02/20/23 22:59:29.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:59:29.898
Feb 20 22:59:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-runtime 02/20/23 22:59:29.902
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:29.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:29.999
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 02/20/23 22:59:30.019
STEP: wait for the container to reach Failed 02/20/23 22:59:30.051
STEP: get the container status 02/20/23 22:59:35.107
STEP: the container should be terminated 02/20/23 22:59:35.115
STEP: the termination message should be set 02/20/23 22:59:35.116
Feb 20 22:59:35.116: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/20/23 22:59:35.116
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 20 22:59:35.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3470" for this suite. 02/20/23 22:59:35.159
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":250,"skipped":4614,"failed":0}
------------------------------
• [SLOW TEST] [5.275 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:59:29.898
    Feb 20 22:59:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-runtime 02/20/23 22:59:29.902
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:29.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:29.999
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 02/20/23 22:59:30.019
    STEP: wait for the container to reach Failed 02/20/23 22:59:30.051
    STEP: get the container status 02/20/23 22:59:35.107
    STEP: the container should be terminated 02/20/23 22:59:35.115
    STEP: the termination message should be set 02/20/23 22:59:35.116
    Feb 20 22:59:35.116: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/20/23 22:59:35.116
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 20 22:59:35.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3470" for this suite. 02/20/23 22:59:35.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:59:35.179
Feb 20 22:59:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:59:35.18
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:35.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:35.237
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Feb 20 22:59:35.318: INFO: Waiting up to 5m0s for pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0" in namespace "svcaccounts-3781" to be "running"
Feb 20 22:59:35.331: INFO: Pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.951796ms
Feb 20 22:59:37.342: INFO: Pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.024008348s
Feb 20 22:59:37.342: INFO: Pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0" satisfied condition "running"
STEP: reading a file in the container 02/20/23 22:59:37.342
Feb 20 22:59:37.343: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3781 pod-service-account-b9240938-574d-4bba-a725-63aedff577e0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/20/23 22:59:37.637
Feb 20 22:59:37.638: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3781 pod-service-account-b9240938-574d-4bba-a725-63aedff577e0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/20/23 22:59:37.947
Feb 20 22:59:37.948: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3781 pod-service-account-b9240938-574d-4bba-a725-63aedff577e0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 20 22:59:38.250: INFO: Got root ca configmap in namespace "svcaccounts-3781"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 20 22:59:38.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3781" for this suite. 02/20/23 22:59:38.27
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":251,"skipped":4645,"failed":0}
------------------------------
• [3.105 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:59:35.179
    Feb 20 22:59:35.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename svcaccounts 02/20/23 22:59:35.18
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:35.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:35.237
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Feb 20 22:59:35.318: INFO: Waiting up to 5m0s for pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0" in namespace "svcaccounts-3781" to be "running"
    Feb 20 22:59:35.331: INFO: Pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.951796ms
    Feb 20 22:59:37.342: INFO: Pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.024008348s
    Feb 20 22:59:37.342: INFO: Pod "pod-service-account-b9240938-574d-4bba-a725-63aedff577e0" satisfied condition "running"
    STEP: reading a file in the container 02/20/23 22:59:37.342
    Feb 20 22:59:37.343: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3781 pod-service-account-b9240938-574d-4bba-a725-63aedff577e0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/20/23 22:59:37.637
    Feb 20 22:59:37.638: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3781 pod-service-account-b9240938-574d-4bba-a725-63aedff577e0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/20/23 22:59:37.947
    Feb 20 22:59:37.948: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3781 pod-service-account-b9240938-574d-4bba-a725-63aedff577e0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 20 22:59:38.250: INFO: Got root ca configmap in namespace "svcaccounts-3781"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 20 22:59:38.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3781" for this suite. 02/20/23 22:59:38.27
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:59:38.285
Feb 20 22:59:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename hostport 02/20/23 22:59:38.29
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:38.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:38.338
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/20/23 22:59:38.363
W0220 22:59:38.398097      20 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54323)
Feb 20 22:59:38.398: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4249" to be "running and ready"
Feb 20 22:59:38.407: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.042713ms
Feb 20 22:59:38.407: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:59:40.415: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017483002s
Feb 20 22:59:40.415: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 20 22:59:40.415: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.8.38.69 on the node which pod1 resides and expect scheduled 02/20/23 22:59:40.415
Feb 20 22:59:40.443: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4249" to be "running and ready"
Feb 20 22:59:40.456: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.092295ms
Feb 20 22:59:40.456: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:59:42.466: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.023725997s
Feb 20 22:59:42.466: INFO: The phase of Pod pod2 is Running (Ready = false)
Feb 20 22:59:44.465: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.021885886s
Feb 20 22:59:44.465: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 20 22:59:44.465: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.8.38.69 but use UDP protocol on the node which pod2 resides 02/20/23 22:59:44.465
Feb 20 22:59:44.489: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4249" to be "running and ready"
Feb 20 22:59:44.501: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.814696ms
Feb 20 22:59:44.501: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:59:46.511: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.021692653s
Feb 20 22:59:46.511: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 20 22:59:46.511: INFO: Pod "pod3" satisfied condition "running and ready"
Feb 20 22:59:46.535: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4249" to be "running and ready"
Feb 20 22:59:46.545: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022697ms
Feb 20 22:59:46.545: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 20 22:59:48.555: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.019914668s
Feb 20 22:59:48.555: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 20 22:59:48.555: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/20/23 22:59:48.563
Feb 20 22:59:48.563: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.8.38.69 http://127.0.0.1:54323/hostname] Namespace:hostport-4249 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:59:48.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:59:48.564: INFO: ExecWithOptions: Clientset creation
Feb 20 22:59:48.564: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4249/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.8.38.69+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.8.38.69, port: 54323 02/20/23 22:59:48.836
Feb 20 22:59:48.837: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.8.38.69:54323/hostname] Namespace:hostport-4249 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:59:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:59:48.838: INFO: ExecWithOptions: Clientset creation
Feb 20 22:59:48.846: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4249/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.8.38.69%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.8.38.69, port: 54323 UDP 02/20/23 22:59:49.112
Feb 20 22:59:49.112: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.8.38.69 54323] Namespace:hostport-4249 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 22:59:49.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 22:59:49.113: INFO: ExecWithOptions: Clientset creation
Feb 20 22:59:49.113: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4249/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.8.38.69+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Feb 20 22:59:54.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4249" for this suite. 02/20/23 22:59:54.421
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":252,"skipped":4646,"failed":0}
------------------------------
• [SLOW TEST] [16.155 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:59:38.285
    Feb 20 22:59:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename hostport 02/20/23 22:59:38.29
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:38.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:38.338
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/20/23 22:59:38.363
    W0220 22:59:38.398097      20 warnings.go:70] would violate PodSecurity "restricted:latest": hostPort (container "agnhost" uses hostPort 54323)
    Feb 20 22:59:38.398: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4249" to be "running and ready"
    Feb 20 22:59:38.407: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.042713ms
    Feb 20 22:59:38.407: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:59:40.415: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017483002s
    Feb 20 22:59:40.415: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 20 22:59:40.415: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.8.38.69 on the node which pod1 resides and expect scheduled 02/20/23 22:59:40.415
    Feb 20 22:59:40.443: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4249" to be "running and ready"
    Feb 20 22:59:40.456: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.092295ms
    Feb 20 22:59:40.456: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:59:42.466: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.023725997s
    Feb 20 22:59:42.466: INFO: The phase of Pod pod2 is Running (Ready = false)
    Feb 20 22:59:44.465: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.021885886s
    Feb 20 22:59:44.465: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 20 22:59:44.465: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.8.38.69 but use UDP protocol on the node which pod2 resides 02/20/23 22:59:44.465
    Feb 20 22:59:44.489: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4249" to be "running and ready"
    Feb 20 22:59:44.501: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.814696ms
    Feb 20 22:59:44.501: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:59:46.511: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.021692653s
    Feb 20 22:59:46.511: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 20 22:59:46.511: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb 20 22:59:46.535: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4249" to be "running and ready"
    Feb 20 22:59:46.545: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022697ms
    Feb 20 22:59:46.545: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 22:59:48.555: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.019914668s
    Feb 20 22:59:48.555: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 20 22:59:48.555: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/20/23 22:59:48.563
    Feb 20 22:59:48.563: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.8.38.69 http://127.0.0.1:54323/hostname] Namespace:hostport-4249 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:59:48.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:59:48.564: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:59:48.564: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4249/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.8.38.69+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.8.38.69, port: 54323 02/20/23 22:59:48.836
    Feb 20 22:59:48.837: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.8.38.69:54323/hostname] Namespace:hostport-4249 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:59:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:59:48.838: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:59:48.846: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4249/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.8.38.69%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.8.38.69, port: 54323 UDP 02/20/23 22:59:49.112
    Feb 20 22:59:49.112: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.8.38.69 54323] Namespace:hostport-4249 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 22:59:49.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 22:59:49.113: INFO: ExecWithOptions: Clientset creation
    Feb 20 22:59:49.113: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4249/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.8.38.69+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Feb 20 22:59:54.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-4249" for this suite. 02/20/23 22:59:54.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 22:59:54.447
Feb 20 22:59:54.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename init-container 02/20/23 22:59:54.449
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:54.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:54.541
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 02/20/23 22:59:54.559
Feb 20 22:59:54.559: INFO: PodSpec: initContainers in spec.initContainers
Feb 20 23:00:37.665: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-18b32bdb-be6e-459c-9224-e1e89204ff69", GenerateName:"", Namespace:"init-container-4785", SelfLink:"", UID:"3454080b-f557-49d8-83c1-bf7a8f4d57c7", ResourceVersion:"120156", Generation:0, CreationTimestamp:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"559684143"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"854f6a4f9458f5b2ff7bf90469cf97a6138a55dd5a90be9f690bcd40baf298be", "cni.projectcalico.org/podIP":"172.30.181.240/32", "cni.projectcalico.org/podIPs":"172.30.181.240/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.240\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.240\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e150), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 22, 59, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e180), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 22, 59, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e1c8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 23, 0, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e1f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-sh5hd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00c3c1f20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sh5hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003a714a0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sh5hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003a71500), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sh5hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003a71440), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002af3788), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.8.38.66", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00094a770), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002af3840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002af3860)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002af387c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002af3880), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00106cef0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.8.38.66", PodIP:"172.30.181.240", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.181.240"}}, StartTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00094a850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00094a8c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://4e9660da5e3179ffb9f968ea54fafeb1c397aa0735dbd678b7e0ed0f794b69d0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c3c1fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c3c1f80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002af38ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 23:00:37.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4785" for this suite. 02/20/23 23:00:37.68
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":253,"skipped":4654,"failed":0}
------------------------------
• [SLOW TEST] [43.248 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 22:59:54.447
    Feb 20 22:59:54.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename init-container 02/20/23 22:59:54.449
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 22:59:54.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 22:59:54.541
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 02/20/23 22:59:54.559
    Feb 20 22:59:54.559: INFO: PodSpec: initContainers in spec.initContainers
    Feb 20 23:00:37.665: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-18b32bdb-be6e-459c-9224-e1e89204ff69", GenerateName:"", Namespace:"init-container-4785", SelfLink:"", UID:"3454080b-f557-49d8-83c1-bf7a8f4d57c7", ResourceVersion:"120156", Generation:0, CreationTimestamp:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"559684143"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"854f6a4f9458f5b2ff7bf90469cf97a6138a55dd5a90be9f690bcd40baf298be", "cni.projectcalico.org/podIP":"172.30.181.240/32", "cni.projectcalico.org/podIPs":"172.30.181.240/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.240\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.181.240\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e150), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 22, 59, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e180), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 22, 59, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e1c8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 20, 23, 0, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00130e1f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-sh5hd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00c3c1f20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sh5hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003a714a0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sh5hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003a71500), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sh5hd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003a71440), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002af3788), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.8.38.66", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00094a770), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002af3840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002af3860)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002af387c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002af3880), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00106cef0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.8.38.66", PodIP:"172.30.181.240", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.181.240"}}, StartTime:time.Date(2023, time.February, 20, 22, 59, 54, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00094a850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00094a8c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://4e9660da5e3179ffb9f968ea54fafeb1c397aa0735dbd678b7e0ed0f794b69d0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c3c1fa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c3c1f80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc002af38ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 23:00:37.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4785" for this suite. 02/20/23 23:00:37.68
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:00:37.701
Feb 20 23:00:37.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename server-version 02/20/23 23:00:37.703
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:37.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:37.759
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/20/23 23:00:37.768
STEP: Confirm major version 02/20/23 23:00:37.771
Feb 20 23:00:37.772: INFO: Major version: 1
STEP: Confirm minor version 02/20/23 23:00:37.772
Feb 20 23:00:37.772: INFO: cleanMinorVersion: 25
Feb 20 23:00:37.772: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Feb 20 23:00:37.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2079" for this suite. 02/20/23 23:00:37.781
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":254,"skipped":4656,"failed":0}
------------------------------
• [0.095 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:00:37.701
    Feb 20 23:00:37.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename server-version 02/20/23 23:00:37.703
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:37.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:37.759
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/20/23 23:00:37.768
    STEP: Confirm major version 02/20/23 23:00:37.771
    Feb 20 23:00:37.772: INFO: Major version: 1
    STEP: Confirm minor version 02/20/23 23:00:37.772
    Feb 20 23:00:37.772: INFO: cleanMinorVersion: 25
    Feb 20 23:00:37.772: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Feb 20 23:00:37.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-2079" for this suite. 02/20/23 23:00:37.781
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:00:37.796
Feb 20 23:00:37.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 23:00:37.799
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:37.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:37.858
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7682 02/20/23 23:00:37.875
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/20/23 23:00:37.903
STEP: creating service externalsvc in namespace services-7682 02/20/23 23:00:37.904
STEP: creating replication controller externalsvc in namespace services-7682 02/20/23 23:00:37.934
I0220 23:00:37.952667      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7682, replica count: 2
I0220 23:00:41.004915      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/20/23 23:00:41.014
Feb 20 23:00:41.043: INFO: Creating new exec pod
Feb 20 23:00:41.125: INFO: Waiting up to 5m0s for pod "execpodwb5bp" in namespace "services-7682" to be "running"
Feb 20 23:00:41.134: INFO: Pod "execpodwb5bp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.643655ms
Feb 20 23:00:43.146: INFO: Pod "execpodwb5bp": Phase="Running", Reason="", readiness=true. Elapsed: 2.020725711s
Feb 20 23:00:43.146: INFO: Pod "execpodwb5bp" satisfied condition "running"
Feb 20 23:00:43.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7682 exec execpodwb5bp -- /bin/sh -x -c nslookup clusterip-service.services-7682.svc.cluster.local'
Feb 20 23:00:43.468: INFO: stderr: "+ nslookup clusterip-service.services-7682.svc.cluster.local\n"
Feb 20 23:00:43.468: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7682.svc.cluster.local\tcanonical name = externalsvc.services-7682.svc.cluster.local.\nName:\texternalsvc.services-7682.svc.cluster.local\nAddress: 172.21.177.98\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7682, will wait for the garbage collector to delete the pods 02/20/23 23:00:43.468
Feb 20 23:00:43.543: INFO: Deleting ReplicationController externalsvc took: 13.232006ms
Feb 20 23:00:43.744: INFO: Terminating ReplicationController externalsvc pods took: 201.319054ms
Feb 20 23:00:46.380: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 23:00:46.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7682" for this suite. 02/20/23 23:00:46.416
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":255,"skipped":4656,"failed":0}
------------------------------
• [SLOW TEST] [8.636 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:00:37.796
    Feb 20 23:00:37.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 23:00:37.799
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:37.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:37.858
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7682 02/20/23 23:00:37.875
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/20/23 23:00:37.903
    STEP: creating service externalsvc in namespace services-7682 02/20/23 23:00:37.904
    STEP: creating replication controller externalsvc in namespace services-7682 02/20/23 23:00:37.934
    I0220 23:00:37.952667      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7682, replica count: 2
    I0220 23:00:41.004915      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/20/23 23:00:41.014
    Feb 20 23:00:41.043: INFO: Creating new exec pod
    Feb 20 23:00:41.125: INFO: Waiting up to 5m0s for pod "execpodwb5bp" in namespace "services-7682" to be "running"
    Feb 20 23:00:41.134: INFO: Pod "execpodwb5bp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.643655ms
    Feb 20 23:00:43.146: INFO: Pod "execpodwb5bp": Phase="Running", Reason="", readiness=true. Elapsed: 2.020725711s
    Feb 20 23:00:43.146: INFO: Pod "execpodwb5bp" satisfied condition "running"
    Feb 20 23:00:43.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-7682 exec execpodwb5bp -- /bin/sh -x -c nslookup clusterip-service.services-7682.svc.cluster.local'
    Feb 20 23:00:43.468: INFO: stderr: "+ nslookup clusterip-service.services-7682.svc.cluster.local\n"
    Feb 20 23:00:43.468: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7682.svc.cluster.local\tcanonical name = externalsvc.services-7682.svc.cluster.local.\nName:\texternalsvc.services-7682.svc.cluster.local\nAddress: 172.21.177.98\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7682, will wait for the garbage collector to delete the pods 02/20/23 23:00:43.468
    Feb 20 23:00:43.543: INFO: Deleting ReplicationController externalsvc took: 13.232006ms
    Feb 20 23:00:43.744: INFO: Terminating ReplicationController externalsvc pods took: 201.319054ms
    Feb 20 23:00:46.380: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 23:00:46.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7682" for this suite. 02/20/23 23:00:46.416
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:00:46.435
Feb 20 23:00:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 23:00:46.436
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:46.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:46.541
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 23:00:46.596
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:00:47.534
STEP: Deploying the webhook pod 02/20/23 23:00:47.561
STEP: Wait for the deployment to be ready 02/20/23 23:00:47.583
Feb 20 23:00:47.601: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/20/23 23:00:49.627
STEP: Verifying the service has paired with the endpoint 02/20/23 23:00:49.701
Feb 20 23:00:50.702: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 02/20/23 23:00:50.712
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/20/23 23:00:50.792
STEP: Creating a configMap that should not be mutated 02/20/23 23:00:50.843
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/20/23 23:00:50.944
STEP: Creating a configMap that should be mutated 02/20/23 23:00:50.966
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:00:51.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5895" for this suite. 02/20/23 23:00:51.128
STEP: Destroying namespace "webhook-5895-markers" for this suite. 02/20/23 23:00:51.146
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":256,"skipped":4703,"failed":0}
------------------------------
• [4.872 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:00:46.435
    Feb 20 23:00:46.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 23:00:46.436
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:46.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:46.541
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 23:00:46.596
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:00:47.534
    STEP: Deploying the webhook pod 02/20/23 23:00:47.561
    STEP: Wait for the deployment to be ready 02/20/23 23:00:47.583
    Feb 20 23:00:47.601: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/20/23 23:00:49.627
    STEP: Verifying the service has paired with the endpoint 02/20/23 23:00:49.701
    Feb 20 23:00:50.702: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 02/20/23 23:00:50.712
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/20/23 23:00:50.792
    STEP: Creating a configMap that should not be mutated 02/20/23 23:00:50.843
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/20/23 23:00:50.944
    STEP: Creating a configMap that should be mutated 02/20/23 23:00:50.966
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:00:51.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5895" for this suite. 02/20/23 23:00:51.128
    STEP: Destroying namespace "webhook-5895-markers" for this suite. 02/20/23 23:00:51.146
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:00:51.323
Feb 20 23:00:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 23:00:51.324
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:51.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:51.403
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 02/20/23 23:00:51.417
Feb 20 23:00:51.446: INFO: created test-pod-1
Feb 20 23:00:51.474: INFO: created test-pod-2
Feb 20 23:00:51.499: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/20/23 23:00:51.499
Feb 20 23:00:51.499: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-355' to be running and ready
Feb 20 23:00:51.530: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 20 23:00:51.530: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 20 23:00:51.530: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 20 23:00:51.530: INFO: 0 / 3 pods in namespace 'pods-355' are running and ready (0 seconds elapsed)
Feb 20 23:00:51.530: INFO: expected 0 pod replicas in namespace 'pods-355', 0 are Running and Ready.
Feb 20 23:00:51.530: INFO: POD         NODE        PHASE    GRACE  CONDITIONS
Feb 20 23:00:51.530: INFO: test-pod-1  10.8.38.70  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
Feb 20 23:00:51.530: INFO: test-pod-2  10.8.38.66  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
Feb 20 23:00:51.531: INFO: test-pod-3  10.8.38.69  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
Feb 20 23:00:51.531: INFO: 
Feb 20 23:00:53.560: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 20 23:00:53.560: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 20 23:00:53.560: INFO: 1 / 3 pods in namespace 'pods-355' are running and ready (2 seconds elapsed)
Feb 20 23:00:53.560: INFO: expected 0 pod replicas in namespace 'pods-355', 0 are Running and Ready.
Feb 20 23:00:53.560: INFO: POD         NODE        PHASE    GRACE  CONDITIONS
Feb 20 23:00:53.560: INFO: test-pod-2  10.8.38.66  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
Feb 20 23:00:53.560: INFO: test-pod-3  10.8.38.69  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
Feb 20 23:00:53.560: INFO: 
Feb 20 23:00:55.556: INFO: 3 / 3 pods in namespace 'pods-355' are running and ready (4 seconds elapsed)
Feb 20 23:00:55.556: INFO: expected 0 pod replicas in namespace 'pods-355', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/20/23 23:00:55.598
Feb 20 23:00:55.613: INFO: Pod quantity 3 is different from expected quantity 0
Feb 20 23:00:56.622: INFO: Pod quantity 3 is different from expected quantity 0
Feb 20 23:00:57.624: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 23:00:58.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-355" for this suite. 02/20/23 23:00:58.639
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":257,"skipped":4739,"failed":0}
------------------------------
• [SLOW TEST] [7.351 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:00:51.323
    Feb 20 23:00:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 23:00:51.324
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:51.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:51.403
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 02/20/23 23:00:51.417
    Feb 20 23:00:51.446: INFO: created test-pod-1
    Feb 20 23:00:51.474: INFO: created test-pod-2
    Feb 20 23:00:51.499: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/20/23 23:00:51.499
    Feb 20 23:00:51.499: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-355' to be running and ready
    Feb 20 23:00:51.530: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 20 23:00:51.530: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 20 23:00:51.530: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 20 23:00:51.530: INFO: 0 / 3 pods in namespace 'pods-355' are running and ready (0 seconds elapsed)
    Feb 20 23:00:51.530: INFO: expected 0 pod replicas in namespace 'pods-355', 0 are Running and Ready.
    Feb 20 23:00:51.530: INFO: POD         NODE        PHASE    GRACE  CONDITIONS
    Feb 20 23:00:51.530: INFO: test-pod-1  10.8.38.70  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
    Feb 20 23:00:51.530: INFO: test-pod-2  10.8.38.66  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
    Feb 20 23:00:51.531: INFO: test-pod-3  10.8.38.69  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
    Feb 20 23:00:51.531: INFO: 
    Feb 20 23:00:53.560: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 20 23:00:53.560: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 20 23:00:53.560: INFO: 1 / 3 pods in namespace 'pods-355' are running and ready (2 seconds elapsed)
    Feb 20 23:00:53.560: INFO: expected 0 pod replicas in namespace 'pods-355', 0 are Running and Ready.
    Feb 20 23:00:53.560: INFO: POD         NODE        PHASE    GRACE  CONDITIONS
    Feb 20 23:00:53.560: INFO: test-pod-2  10.8.38.66  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
    Feb 20 23:00:53.560: INFO: test-pod-3  10.8.38.69  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-20 23:00:51 +0000 UTC  }]
    Feb 20 23:00:53.560: INFO: 
    Feb 20 23:00:55.556: INFO: 3 / 3 pods in namespace 'pods-355' are running and ready (4 seconds elapsed)
    Feb 20 23:00:55.556: INFO: expected 0 pod replicas in namespace 'pods-355', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/20/23 23:00:55.598
    Feb 20 23:00:55.613: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 20 23:00:56.622: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 20 23:00:57.624: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 23:00:58.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-355" for this suite. 02/20/23 23:00:58.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:00:58.684
Feb 20 23:00:58.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename disruption 02/20/23 23:00:58.684
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:58.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:58.774
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 02/20/23 23:00:58.782
STEP: Waiting for the pdb to be processed 02/20/23 23:00:58.795
STEP: First trying to evict a pod which shouldn't be evictable 02/20/23 23:01:00.827
STEP: Waiting for all pods to be running 02/20/23 23:01:00.828
Feb 20 23:01:00.836: INFO: pods: 0 < 3
Feb 20 23:01:02.846: INFO: running pods: 0 < 3
STEP: locating a running pod 02/20/23 23:01:04.848
STEP: Updating the pdb to allow a pod to be evicted 02/20/23 23:01:04.874
STEP: Waiting for the pdb to be processed 02/20/23 23:01:04.896
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/20/23 23:01:06.914
STEP: Waiting for all pods to be running 02/20/23 23:01:06.914
STEP: Waiting for the pdb to observed all healthy pods 02/20/23 23:01:06.923
STEP: Patching the pdb to disallow a pod to be evicted 02/20/23 23:01:07.033
STEP: Waiting for the pdb to be processed 02/20/23 23:01:07.08
STEP: Waiting for all pods to be running 02/20/23 23:01:07.097
Feb 20 23:01:07.127: INFO: running pods: 2 < 3
STEP: locating a running pod 02/20/23 23:01:09.137
STEP: Deleting the pdb to allow a pod to be evicted 02/20/23 23:01:09.163
STEP: Waiting for the pdb to be deleted 02/20/23 23:01:09.207
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/20/23 23:01:09.217
STEP: Waiting for all pods to be running 02/20/23 23:01:09.217
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 20 23:01:09.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2613" for this suite. 02/20/23 23:01:09.296
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":258,"skipped":4803,"failed":0}
------------------------------
• [SLOW TEST] [10.627 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:00:58.684
    Feb 20 23:00:58.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename disruption 02/20/23 23:00:58.684
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:00:58.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:00:58.774
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 02/20/23 23:00:58.782
    STEP: Waiting for the pdb to be processed 02/20/23 23:00:58.795
    STEP: First trying to evict a pod which shouldn't be evictable 02/20/23 23:01:00.827
    STEP: Waiting for all pods to be running 02/20/23 23:01:00.828
    Feb 20 23:01:00.836: INFO: pods: 0 < 3
    Feb 20 23:01:02.846: INFO: running pods: 0 < 3
    STEP: locating a running pod 02/20/23 23:01:04.848
    STEP: Updating the pdb to allow a pod to be evicted 02/20/23 23:01:04.874
    STEP: Waiting for the pdb to be processed 02/20/23 23:01:04.896
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/20/23 23:01:06.914
    STEP: Waiting for all pods to be running 02/20/23 23:01:06.914
    STEP: Waiting for the pdb to observed all healthy pods 02/20/23 23:01:06.923
    STEP: Patching the pdb to disallow a pod to be evicted 02/20/23 23:01:07.033
    STEP: Waiting for the pdb to be processed 02/20/23 23:01:07.08
    STEP: Waiting for all pods to be running 02/20/23 23:01:07.097
    Feb 20 23:01:07.127: INFO: running pods: 2 < 3
    STEP: locating a running pod 02/20/23 23:01:09.137
    STEP: Deleting the pdb to allow a pod to be evicted 02/20/23 23:01:09.163
    STEP: Waiting for the pdb to be deleted 02/20/23 23:01:09.207
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/20/23 23:01:09.217
    STEP: Waiting for all pods to be running 02/20/23 23:01:09.217
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 20 23:01:09.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2613" for this suite. 02/20/23 23:01:09.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:01:09.311
Feb 20 23:01:09.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:01:09.317
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:09.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:09.372
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-9a2b0539-5a94-4306-ab0a-cf3240c74ef8 02/20/23 23:01:09.38
STEP: Creating secret with name secret-projected-all-test-volume-5b9c6096-3dcc-401c-b10c-4680975fa8e9 02/20/23 23:01:09.498
STEP: Creating a pod to test Check all projections for projected volume plugin 02/20/23 23:01:09.514
Feb 20 23:01:09.610: INFO: Waiting up to 5m0s for pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644" in namespace "projected-2706" to be "Succeeded or Failed"
Feb 20 23:01:09.623: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Pending", Reason="", readiness=false. Elapsed: 12.001589ms
Feb 20 23:01:11.633: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022542418s
Feb 20 23:01:13.633: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022424908s
Feb 20 23:01:15.632: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02110377s
STEP: Saw pod success 02/20/23 23:01:15.632
Feb 20 23:01:15.632: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644" satisfied condition "Succeeded or Failed"
Feb 20 23:01:15.640: INFO: Trying to get logs from node 10.8.38.66 pod projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644 container projected-all-volume-test: <nil>
STEP: delete the pod 02/20/23 23:01:15.677
Feb 20 23:01:15.701: INFO: Waiting for pod projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644 to disappear
Feb 20 23:01:15.710: INFO: Pod projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Feb 20 23:01:15.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2706" for this suite. 02/20/23 23:01:15.73
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":259,"skipped":4810,"failed":0}
------------------------------
• [SLOW TEST] [6.435 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:01:09.311
    Feb 20 23:01:09.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:01:09.317
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:09.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:09.372
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-9a2b0539-5a94-4306-ab0a-cf3240c74ef8 02/20/23 23:01:09.38
    STEP: Creating secret with name secret-projected-all-test-volume-5b9c6096-3dcc-401c-b10c-4680975fa8e9 02/20/23 23:01:09.498
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/20/23 23:01:09.514
    Feb 20 23:01:09.610: INFO: Waiting up to 5m0s for pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644" in namespace "projected-2706" to be "Succeeded or Failed"
    Feb 20 23:01:09.623: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Pending", Reason="", readiness=false. Elapsed: 12.001589ms
    Feb 20 23:01:11.633: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022542418s
    Feb 20 23:01:13.633: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022424908s
    Feb 20 23:01:15.632: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02110377s
    STEP: Saw pod success 02/20/23 23:01:15.632
    Feb 20 23:01:15.632: INFO: Pod "projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644" satisfied condition "Succeeded or Failed"
    Feb 20 23:01:15.640: INFO: Trying to get logs from node 10.8.38.66 pod projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644 container projected-all-volume-test: <nil>
    STEP: delete the pod 02/20/23 23:01:15.677
    Feb 20 23:01:15.701: INFO: Waiting for pod projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644 to disappear
    Feb 20 23:01:15.710: INFO: Pod projected-volume-8d179f8f-84ad-454f-bb45-9a758d7e1644 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Feb 20 23:01:15.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2706" for this suite. 02/20/23 23:01:15.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:01:15.757
Feb 20 23:01:15.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename disruption 02/20/23 23:01:15.758
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:15.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:15.802
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 02/20/23 23:01:15.823
STEP: Updating PodDisruptionBudget status 02/20/23 23:01:17.841
STEP: Waiting for all pods to be running 02/20/23 23:01:17.87
Feb 20 23:01:17.882: INFO: running pods: 0 < 1
Feb 20 23:01:19.916: INFO: running pods: 0 < 1
STEP: locating a running pod 02/20/23 23:01:21.893
STEP: Waiting for the pdb to be processed 02/20/23 23:01:21.92
STEP: Patching PodDisruptionBudget status 02/20/23 23:01:21.938
STEP: Waiting for the pdb to be processed 02/20/23 23:01:21.965
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 20 23:01:21.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4175" for this suite. 02/20/23 23:01:21.989
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":260,"skipped":4859,"failed":0}
------------------------------
• [SLOW TEST] [6.250 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:01:15.757
    Feb 20 23:01:15.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename disruption 02/20/23 23:01:15.758
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:15.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:15.802
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 02/20/23 23:01:15.823
    STEP: Updating PodDisruptionBudget status 02/20/23 23:01:17.841
    STEP: Waiting for all pods to be running 02/20/23 23:01:17.87
    Feb 20 23:01:17.882: INFO: running pods: 0 < 1
    Feb 20 23:01:19.916: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/20/23 23:01:21.893
    STEP: Waiting for the pdb to be processed 02/20/23 23:01:21.92
    STEP: Patching PodDisruptionBudget status 02/20/23 23:01:21.938
    STEP: Waiting for the pdb to be processed 02/20/23 23:01:21.965
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 20 23:01:21.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4175" for this suite. 02/20/23 23:01:21.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:01:22.009
Feb 20 23:01:22.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename init-container 02/20/23 23:01:22.01
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:22.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:22.065
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 02/20/23 23:01:22.076
Feb 20 23:01:22.076: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 20 23:01:27.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5828" for this suite. 02/20/23 23:01:27.473
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":261,"skipped":4882,"failed":0}
------------------------------
• [SLOW TEST] [5.482 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:01:22.009
    Feb 20 23:01:22.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename init-container 02/20/23 23:01:22.01
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:22.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:22.065
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 02/20/23 23:01:22.076
    Feb 20 23:01:22.076: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 20 23:01:27.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5828" for this suite. 02/20/23 23:01:27.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:01:27.492
Feb 20 23:01:27.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename job 02/20/23 23:01:27.494
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:27.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:27.541
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 02/20/23 23:01:27.564
W0220 23:01:27.577322      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 02/20/23 23:01:27.577
STEP: delete a job 02/20/23 23:01:31.599
STEP: deleting Job.batch foo in namespace job-3948, will wait for the garbage collector to delete the pods 02/20/23 23:01:31.599
Feb 20 23:01:31.671: INFO: Deleting Job.batch foo took: 13.262502ms
Feb 20 23:01:31.773: INFO: Terminating Job.batch foo pods took: 101.308898ms
STEP: Ensuring job was deleted 02/20/23 23:02:03.773
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 20 23:02:03.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3948" for this suite. 02/20/23 23:02:03.794
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":262,"skipped":4894,"failed":0}
------------------------------
• [SLOW TEST] [36.333 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:01:27.492
    Feb 20 23:01:27.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename job 02/20/23 23:01:27.494
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:01:27.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:01:27.541
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 02/20/23 23:01:27.564
    W0220 23:01:27.577322      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 02/20/23 23:01:27.577
    STEP: delete a job 02/20/23 23:01:31.599
    STEP: deleting Job.batch foo in namespace job-3948, will wait for the garbage collector to delete the pods 02/20/23 23:01:31.599
    Feb 20 23:01:31.671: INFO: Deleting Job.batch foo took: 13.262502ms
    Feb 20 23:01:31.773: INFO: Terminating Job.batch foo pods took: 101.308898ms
    STEP: Ensuring job was deleted 02/20/23 23:02:03.773
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 20 23:02:03.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3948" for this suite. 02/20/23 23:02:03.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:03.85
Feb 20 23:02:03.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 23:02:03.852
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:03.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:03.903
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Feb 20 23:02:03.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/20/23 23:02:13.172
Feb 20 23:02:13.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
Feb 20 23:02:15.051: INFO: stderr: ""
Feb 20 23:02:15.051: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 20 23:02:15.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 delete e2e-test-crd-publish-openapi-6759-crds test-foo'
Feb 20 23:02:15.186: INFO: stderr: ""
Feb 20 23:02:15.186: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 20 23:02:15.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 apply -f -'
Feb 20 23:02:15.766: INFO: stderr: ""
Feb 20 23:02:15.766: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 20 23:02:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 delete e2e-test-crd-publish-openapi-6759-crds test-foo'
Feb 20 23:02:15.904: INFO: stderr: ""
Feb 20 23:02:15.905: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/20/23 23:02:15.905
Feb 20 23:02:15.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
Feb 20 23:02:16.441: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/20/23 23:02:16.441
Feb 20 23:02:16.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
Feb 20 23:02:17.858: INFO: rc: 1
Feb 20 23:02:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 apply -f -'
Feb 20 23:02:18.344: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/20/23 23:02:18.344
Feb 20 23:02:18.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
Feb 20 23:02:18.810: INFO: rc: 1
Feb 20 23:02:18.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 apply -f -'
Feb 20 23:02:19.299: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/20/23 23:02:19.299
Feb 20 23:02:19.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds'
Feb 20 23:02:19.770: INFO: stderr: ""
Feb 20 23:02:19.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/20/23 23:02:19.771
Feb 20 23:02:19.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.metadata'
Feb 20 23:02:21.600: INFO: stderr: ""
Feb 20 23:02:21.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 20 23:02:21.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.spec'
Feb 20 23:02:22.117: INFO: stderr: ""
Feb 20 23:02:22.117: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 20 23:02:22.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.spec.bars'
Feb 20 23:02:22.632: INFO: stderr: ""
Feb 20 23:02:22.632: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/20/23 23:02:22.633
Feb 20 23:02:22.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.spec.bars2'
Feb 20 23:02:23.119: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:02:32.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1527" for this suite. 02/20/23 23:02:32.156
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":263,"skipped":4923,"failed":0}
------------------------------
• [SLOW TEST] [28.320 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:03.85
    Feb 20 23:02:03.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 23:02:03.852
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:03.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:03.903
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Feb 20 23:02:03.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/20/23 23:02:13.172
    Feb 20 23:02:13.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
    Feb 20 23:02:15.051: INFO: stderr: ""
    Feb 20 23:02:15.051: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 20 23:02:15.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 delete e2e-test-crd-publish-openapi-6759-crds test-foo'
    Feb 20 23:02:15.186: INFO: stderr: ""
    Feb 20 23:02:15.186: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 20 23:02:15.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 apply -f -'
    Feb 20 23:02:15.766: INFO: stderr: ""
    Feb 20 23:02:15.766: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 20 23:02:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 delete e2e-test-crd-publish-openapi-6759-crds test-foo'
    Feb 20 23:02:15.904: INFO: stderr: ""
    Feb 20 23:02:15.905: INFO: stdout: "e2e-test-crd-publish-openapi-6759-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/20/23 23:02:15.905
    Feb 20 23:02:15.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
    Feb 20 23:02:16.441: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/20/23 23:02:16.441
    Feb 20 23:02:16.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
    Feb 20 23:02:17.858: INFO: rc: 1
    Feb 20 23:02:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 apply -f -'
    Feb 20 23:02:18.344: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/20/23 23:02:18.344
    Feb 20 23:02:18.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 create -f -'
    Feb 20 23:02:18.810: INFO: rc: 1
    Feb 20 23:02:18.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 --namespace=crd-publish-openapi-1527 apply -f -'
    Feb 20 23:02:19.299: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/20/23 23:02:19.299
    Feb 20 23:02:19.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds'
    Feb 20 23:02:19.770: INFO: stderr: ""
    Feb 20 23:02:19.770: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/20/23 23:02:19.771
    Feb 20 23:02:19.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.metadata'
    Feb 20 23:02:21.600: INFO: stderr: ""
    Feb 20 23:02:21.600: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 20 23:02:21.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.spec'
    Feb 20 23:02:22.117: INFO: stderr: ""
    Feb 20 23:02:22.117: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 20 23:02:22.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.spec.bars'
    Feb 20 23:02:22.632: INFO: stderr: ""
    Feb 20 23:02:22.632: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6759-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/20/23 23:02:22.633
    Feb 20 23:02:22.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-1527 explain e2e-test-crd-publish-openapi-6759-crds.spec.bars2'
    Feb 20 23:02:23.119: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:02:32.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1527" for this suite. 02/20/23 23:02:32.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:32.173
Feb 20 23:02:32.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-runtime 02/20/23 23:02:32.175
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:32.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:32.221
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 02/20/23 23:02:32.233
STEP: wait for the container to reach Succeeded 02/20/23 23:02:32.288
STEP: get the container status 02/20/23 23:02:36.336
STEP: the container should be terminated 02/20/23 23:02:36.345
STEP: the termination message should be set 02/20/23 23:02:36.345
Feb 20 23:02:36.346: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/20/23 23:02:36.346
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 20 23:02:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4602" for this suite. 02/20/23 23:02:36.387
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":264,"skipped":4930,"failed":0}
------------------------------
• [4.230 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:32.173
    Feb 20 23:02:32.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-runtime 02/20/23 23:02:32.175
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:32.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:32.221
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 02/20/23 23:02:32.233
    STEP: wait for the container to reach Succeeded 02/20/23 23:02:32.288
    STEP: get the container status 02/20/23 23:02:36.336
    STEP: the container should be terminated 02/20/23 23:02:36.345
    STEP: the termination message should be set 02/20/23 23:02:36.345
    Feb 20 23:02:36.346: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/20/23 23:02:36.346
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 20 23:02:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4602" for this suite. 02/20/23 23:02:36.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:36.41
Feb 20 23:02:36.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename conformance-tests 02/20/23 23:02:36.412
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:36.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:36.473
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/20/23 23:02:36.481
Feb 20 23:02:36.481: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Feb 20 23:02:36.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-2386" for this suite. 02/20/23 23:02:36.512
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":265,"skipped":4940,"failed":0}
------------------------------
• [0.118 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:36.41
    Feb 20 23:02:36.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename conformance-tests 02/20/23 23:02:36.412
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:36.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:36.473
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/20/23 23:02:36.481
    Feb 20 23:02:36.481: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Feb 20 23:02:36.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-2386" for this suite. 02/20/23 23:02:36.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:36.533
Feb 20 23:02:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:02:36.539
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:36.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:36.607
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-869a74e9-229c-4019-8a1f-477d15334085 02/20/23 23:02:36.614
STEP: Creating a pod to test consume configMaps 02/20/23 23:02:36.634
Feb 20 23:02:36.671: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86" in namespace "projected-8126" to be "Succeeded or Failed"
Feb 20 23:02:36.681: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 9.928975ms
Feb 20 23:02:38.690: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019339018s
Feb 20 23:02:40.691: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020571604s
STEP: Saw pod success 02/20/23 23:02:40.691
Feb 20 23:02:40.692: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86" satisfied condition "Succeeded or Failed"
Feb 20 23:02:40.700: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 23:02:40.728
Feb 20 23:02:40.752: INFO: Waiting for pod pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86 to disappear
Feb 20 23:02:40.761: INFO: Pod pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 23:02:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8126" for this suite. 02/20/23 23:02:40.773
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":266,"skipped":4946,"failed":0}
------------------------------
• [4.255 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:36.533
    Feb 20 23:02:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:02:36.539
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:36.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:36.607
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-869a74e9-229c-4019-8a1f-477d15334085 02/20/23 23:02:36.614
    STEP: Creating a pod to test consume configMaps 02/20/23 23:02:36.634
    Feb 20 23:02:36.671: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86" in namespace "projected-8126" to be "Succeeded or Failed"
    Feb 20 23:02:36.681: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 9.928975ms
    Feb 20 23:02:38.690: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019339018s
    Feb 20 23:02:40.691: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020571604s
    STEP: Saw pod success 02/20/23 23:02:40.691
    Feb 20 23:02:40.692: INFO: Pod "pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86" satisfied condition "Succeeded or Failed"
    Feb 20 23:02:40.700: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 23:02:40.728
    Feb 20 23:02:40.752: INFO: Waiting for pod pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86 to disappear
    Feb 20 23:02:40.761: INFO: Pod pod-projected-configmaps-cbebce85-8203-49bc-a03d-ce8119b5aa86 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 23:02:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8126" for this suite. 02/20/23 23:02:40.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:40.794
Feb 20 23:02:40.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 23:02:40.796
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:40.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:40.851
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 23:02:40.9
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:02:41.125
STEP: Deploying the webhook pod 02/20/23 23:02:41.15
STEP: Wait for the deployment to be ready 02/20/23 23:02:41.174
Feb 20 23:02:41.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 20 23:02:43.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/20/23 23:02:45.236
STEP: Verifying the service has paired with the endpoint 02/20/23 23:02:45.274
Feb 20 23:02:46.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Feb 20 23:02:46.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/20/23 23:02:46.829
STEP: Creating a custom resource that should be denied by the webhook 02/20/23 23:02:46.875
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/20/23 23:02:48.957
STEP: Updating the custom resource with disallowed data should be denied 02/20/23 23:02:48.98
STEP: Deleting the custom resource should be denied 02/20/23 23:02:49.011
STEP: Remove the offending key and value from the custom resource data 02/20/23 23:02:49.033
STEP: Deleting the updated custom resource should be successful 02/20/23 23:02:49.064
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:02:49.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4519" for this suite. 02/20/23 23:02:49.652
STEP: Destroying namespace "webhook-4519-markers" for this suite. 02/20/23 23:02:49.668
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":267,"skipped":4954,"failed":0}
------------------------------
• [SLOW TEST] [8.975 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:40.794
    Feb 20 23:02:40.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 23:02:40.796
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:40.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:40.851
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 23:02:40.9
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:02:41.125
    STEP: Deploying the webhook pod 02/20/23 23:02:41.15
    STEP: Wait for the deployment to be ready 02/20/23 23:02:41.174
    Feb 20 23:02:41.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 20 23:02:43.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 20, 23, 2, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/20/23 23:02:45.236
    STEP: Verifying the service has paired with the endpoint 02/20/23 23:02:45.274
    Feb 20 23:02:46.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Feb 20 23:02:46.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/20/23 23:02:46.829
    STEP: Creating a custom resource that should be denied by the webhook 02/20/23 23:02:46.875
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/20/23 23:02:48.957
    STEP: Updating the custom resource with disallowed data should be denied 02/20/23 23:02:48.98
    STEP: Deleting the custom resource should be denied 02/20/23 23:02:49.011
    STEP: Remove the offending key and value from the custom resource data 02/20/23 23:02:49.033
    STEP: Deleting the updated custom resource should be successful 02/20/23 23:02:49.064
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:02:49.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4519" for this suite. 02/20/23 23:02:49.652
    STEP: Destroying namespace "webhook-4519-markers" for this suite. 02/20/23 23:02:49.668
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:49.77
Feb 20 23:02:49.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename job 02/20/23 23:02:49.771
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:49.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:49.829
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 02/20/23 23:02:49.841
W0220 23:02:49.863453      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 02/20/23 23:02:49.863
STEP: Ensuring pods with index for job exist 02/20/23 23:02:59.873
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 20 23:02:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8641" for this suite. 02/20/23 23:02:59.906
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":268,"skipped":4962,"failed":0}
------------------------------
• [SLOW TEST] [10.150 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:49.77
    Feb 20 23:02:49.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename job 02/20/23 23:02:49.771
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:49.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:49.829
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 02/20/23 23:02:49.841
    W0220 23:02:49.863453      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 02/20/23 23:02:49.863
    STEP: Ensuring pods with index for job exist 02/20/23 23:02:59.873
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 20 23:02:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8641" for this suite. 02/20/23 23:02:59.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:02:59.927
Feb 20 23:02:59.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename podtemplate 02/20/23 23:02:59.929
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:59.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:59.986
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/20/23 23:03:00
W0220 23:03:00.017947      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 23:03:00.020: INFO: created test-podtemplate-1
Feb 20 23:03:00.037: INFO: created test-podtemplate-2
Feb 20 23:03:00.082: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/20/23 23:03:00.082
STEP: delete collection of pod templates 02/20/23 23:03:00.093
Feb 20 23:03:00.093: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/20/23 23:03:00.167
Feb 20 23:03:00.168: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 20 23:03:00.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5322" for this suite. 02/20/23 23:03:00.195
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":269,"skipped":4981,"failed":0}
------------------------------
• [0.284 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:02:59.927
    Feb 20 23:02:59.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename podtemplate 02/20/23 23:02:59.929
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:02:59.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:02:59.986
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/20/23 23:03:00
    W0220 23:03:00.017947      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 23:03:00.020: INFO: created test-podtemplate-1
    Feb 20 23:03:00.037: INFO: created test-podtemplate-2
    Feb 20 23:03:00.082: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/20/23 23:03:00.082
    STEP: delete collection of pod templates 02/20/23 23:03:00.093
    Feb 20 23:03:00.093: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/20/23 23:03:00.167
    Feb 20 23:03:00.168: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 20 23:03:00.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-5322" for this suite. 02/20/23 23:03:00.195
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:03:00.214
Feb 20 23:03:00.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-preemption 02/20/23 23:03:00.217
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:03:00.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:03:00.277
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 20 23:03:00.385: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 20 23:04:00.535: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:00.55
Feb 20 23:04:00.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-preemption-path 02/20/23 23:04:00.553
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:00.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:00.606
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 02/20/23 23:04:00.614
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/20/23 23:04:00.615
Feb 20 23:04:00.643: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4359" to be "running"
Feb 20 23:04:00.655: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104872ms
Feb 20 23:04:02.663: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019975072s
Feb 20 23:04:04.665: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.022023339s
Feb 20 23:04:04.666: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/20/23 23:04:04.678
Feb 20 23:04:04.696: INFO: found a healthy node: 10.8.38.66
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Feb 20 23:04:16.913: INFO: pods created so far: [1 1 1]
Feb 20 23:04:16.913: INFO: length of pods created so far: 3
Feb 20 23:04:20.945: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Feb 20 23:04:27.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4359" for this suite. 02/20/23 23:04:27.959
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:04:28.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8364" for this suite. 02/20/23 23:04:28.081
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":270,"skipped":4984,"failed":0}
------------------------------
• [SLOW TEST] [88.002 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:03:00.214
    Feb 20 23:03:00.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-preemption 02/20/23 23:03:00.217
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:03:00.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:03:00.277
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 20 23:03:00.385: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 20 23:04:00.535: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:00.55
    Feb 20 23:04:00.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-preemption-path 02/20/23 23:04:00.553
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:00.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:00.606
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 02/20/23 23:04:00.614
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/20/23 23:04:00.615
    Feb 20 23:04:00.643: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4359" to be "running"
    Feb 20 23:04:00.655: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.104872ms
    Feb 20 23:04:02.663: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019975072s
    Feb 20 23:04:04.665: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.022023339s
    Feb 20 23:04:04.666: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/20/23 23:04:04.678
    Feb 20 23:04:04.696: INFO: found a healthy node: 10.8.38.66
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Feb 20 23:04:16.913: INFO: pods created so far: [1 1 1]
    Feb 20 23:04:16.913: INFO: length of pods created so far: 3
    Feb 20 23:04:20.945: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Feb 20 23:04:27.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-4359" for this suite. 02/20/23 23:04:27.959
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:04:28.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8364" for this suite. 02/20/23 23:04:28.081
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:28.225
Feb 20 23:04:28.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replicaset 02/20/23 23:04:28.227
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:28.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:28.288
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/20/23 23:04:28.297
Feb 20 23:04:28.324: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5235" to be "running and ready"
Feb 20 23:04:28.335: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.496716ms
Feb 20 23:04:28.335: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:04:30.344: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.019155259s
Feb 20 23:04:30.344: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 20 23:04:30.344: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/20/23 23:04:30.351
STEP: Then the orphan pod is adopted 02/20/23 23:04:30.361
STEP: When the matched label of one of its pods change 02/20/23 23:04:31.399
Feb 20 23:04:31.410: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/20/23 23:04:31.445
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 20 23:04:32.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5235" for this suite. 02/20/23 23:04:32.477
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":271,"skipped":5003,"failed":0}
------------------------------
• [4.266 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:28.225
    Feb 20 23:04:28.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replicaset 02/20/23 23:04:28.227
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:28.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:28.288
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/20/23 23:04:28.297
    Feb 20 23:04:28.324: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-5235" to be "running and ready"
    Feb 20 23:04:28.335: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 10.496716ms
    Feb 20 23:04:28.335: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:04:30.344: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.019155259s
    Feb 20 23:04:30.344: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 20 23:04:30.344: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/20/23 23:04:30.351
    STEP: Then the orphan pod is adopted 02/20/23 23:04:30.361
    STEP: When the matched label of one of its pods change 02/20/23 23:04:31.399
    Feb 20 23:04:31.410: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/20/23 23:04:31.445
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 20 23:04:32.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5235" for this suite. 02/20/23 23:04:32.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:32.497
Feb 20 23:04:32.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 23:04:32.499
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:32.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:32.548
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 20 23:04:32.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:04:33.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7113" for this suite. 02/20/23 23:04:33.655
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":272,"skipped":5009,"failed":0}
------------------------------
• [1.174 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:32.497
    Feb 20 23:04:32.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 23:04:32.499
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:32.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:32.548
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 20 23:04:32.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:04:33.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7113" for this suite. 02/20/23 23:04:33.655
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:33.673
Feb 20 23:04:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename security-context-test 02/20/23 23:04:33.678
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:33.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:33.745
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Feb 20 23:04:33.788: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8" in namespace "security-context-test-5934" to be "Succeeded or Failed"
Feb 20 23:04:33.796: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821747ms
Feb 20 23:04:35.807: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018202467s
Feb 20 23:04:37.806: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017887797s
Feb 20 23:04:37.806: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8" satisfied condition "Succeeded or Failed"
Feb 20 23:04:37.946: INFO: Got logs for pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 20 23:04:37.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5934" for this suite. 02/20/23 23:04:37.965
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":273,"skipped":5009,"failed":0}
------------------------------
• [4.307 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:33.673
    Feb 20 23:04:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename security-context-test 02/20/23 23:04:33.678
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:33.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:33.745
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Feb 20 23:04:33.788: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8" in namespace "security-context-test-5934" to be "Succeeded or Failed"
    Feb 20 23:04:33.796: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821747ms
    Feb 20 23:04:35.807: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018202467s
    Feb 20 23:04:37.806: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017887797s
    Feb 20 23:04:37.806: INFO: Pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8" satisfied condition "Succeeded or Failed"
    Feb 20 23:04:37.946: INFO: Got logs for pod "busybox-privileged-false-026df7d1-0a6b-4403-b736-a802775979e8": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 20 23:04:37.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5934" for this suite. 02/20/23 23:04:37.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:37.989
Feb 20 23:04:37.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 23:04:37.993
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:38.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:38.115
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 02/20/23 23:04:38.123
Feb 20 23:04:38.177: INFO: Waiting up to 5m0s for pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e" in namespace "emptydir-3350" to be "Succeeded or Failed"
Feb 20 23:04:38.210: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 31.408516ms
Feb 20 23:04:40.220: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040954413s
Feb 20 23:04:42.221: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0419455s
STEP: Saw pod success 02/20/23 23:04:42.221
Feb 20 23:04:42.221: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e" satisfied condition "Succeeded or Failed"
Feb 20 23:04:42.229: INFO: Trying to get logs from node 10.8.38.66 pod pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e container test-container: <nil>
STEP: delete the pod 02/20/23 23:04:42.263
Feb 20 23:04:42.287: INFO: Waiting for pod pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e to disappear
Feb 20 23:04:42.306: INFO: Pod pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 23:04:42.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3350" for this suite. 02/20/23 23:04:42.322
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":274,"skipped":5035,"failed":0}
------------------------------
• [4.350 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:37.989
    Feb 20 23:04:37.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 23:04:37.993
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:38.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:38.115
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/20/23 23:04:38.123
    Feb 20 23:04:38.177: INFO: Waiting up to 5m0s for pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e" in namespace "emptydir-3350" to be "Succeeded or Failed"
    Feb 20 23:04:38.210: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 31.408516ms
    Feb 20 23:04:40.220: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040954413s
    Feb 20 23:04:42.221: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0419455s
    STEP: Saw pod success 02/20/23 23:04:42.221
    Feb 20 23:04:42.221: INFO: Pod "pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e" satisfied condition "Succeeded or Failed"
    Feb 20 23:04:42.229: INFO: Trying to get logs from node 10.8.38.66 pod pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e container test-container: <nil>
    STEP: delete the pod 02/20/23 23:04:42.263
    Feb 20 23:04:42.287: INFO: Waiting for pod pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e to disappear
    Feb 20 23:04:42.306: INFO: Pod pod-14ea4f33-0693-4dd2-973c-69f48b2a4a4e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 23:04:42.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3350" for this suite. 02/20/23 23:04:42.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:42.35
Feb 20 23:04:42.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 23:04:42.351
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:42.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:42.402
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 02/20/23 23:04:42.41
Feb 20 23:04:42.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a" in namespace "downward-api-9050" to be "Succeeded or Failed"
Feb 20 23:04:42.453: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.98532ms
Feb 20 23:04:44.461: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020460096s
Feb 20 23:04:46.463: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022358071s
STEP: Saw pod success 02/20/23 23:04:46.464
Feb 20 23:04:46.464: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a" satisfied condition "Succeeded or Failed"
Feb 20 23:04:46.474: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a container client-container: <nil>
STEP: delete the pod 02/20/23 23:04:46.513
Feb 20 23:04:46.563: INFO: Waiting for pod downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a to disappear
Feb 20 23:04:46.575: INFO: Pod downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 23:04:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9050" for this suite. 02/20/23 23:04:46.587
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":275,"skipped":5069,"failed":0}
------------------------------
• [4.252 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:42.35
    Feb 20 23:04:42.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 23:04:42.351
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:42.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:42.402
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 02/20/23 23:04:42.41
    Feb 20 23:04:42.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a" in namespace "downward-api-9050" to be "Succeeded or Failed"
    Feb 20 23:04:42.453: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.98532ms
    Feb 20 23:04:44.461: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020460096s
    Feb 20 23:04:46.463: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022358071s
    STEP: Saw pod success 02/20/23 23:04:46.464
    Feb 20 23:04:46.464: INFO: Pod "downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a" satisfied condition "Succeeded or Failed"
    Feb 20 23:04:46.474: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a container client-container: <nil>
    STEP: delete the pod 02/20/23 23:04:46.513
    Feb 20 23:04:46.563: INFO: Waiting for pod downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a to disappear
    Feb 20 23:04:46.575: INFO: Pod downwardapi-volume-df79b4a7-b7e1-47fd-84ed-9b4fd342774a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 23:04:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9050" for this suite. 02/20/23 23:04:46.587
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:46.603
Feb 20 23:04:46.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:04:46.608
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:46.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:46.667
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 02/20/23 23:04:46.689
Feb 20 23:04:46.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878" in namespace "projected-6315" to be "Succeeded or Failed"
Feb 20 23:04:46.738: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Pending", Reason="", readiness=false. Elapsed: 10.094252ms
Feb 20 23:04:48.748: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020164208s
Feb 20 23:04:50.749: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021427758s
Feb 20 23:04:52.750: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021890399s
STEP: Saw pod success 02/20/23 23:04:52.75
Feb 20 23:04:52.750: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878" satisfied condition "Succeeded or Failed"
Feb 20 23:04:52.759: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878 container client-container: <nil>
STEP: delete the pod 02/20/23 23:04:52.776
Feb 20 23:04:52.802: INFO: Waiting for pod downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878 to disappear
Feb 20 23:04:52.821: INFO: Pod downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 23:04:52.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6315" for this suite. 02/20/23 23:04:52.834
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":276,"skipped":5071,"failed":0}
------------------------------
• [SLOW TEST] [6.247 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:46.603
    Feb 20 23:04:46.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:04:46.608
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:46.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:46.667
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 02/20/23 23:04:46.689
    Feb 20 23:04:46.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878" in namespace "projected-6315" to be "Succeeded or Failed"
    Feb 20 23:04:46.738: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Pending", Reason="", readiness=false. Elapsed: 10.094252ms
    Feb 20 23:04:48.748: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020164208s
    Feb 20 23:04:50.749: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021427758s
    Feb 20 23:04:52.750: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021890399s
    STEP: Saw pod success 02/20/23 23:04:52.75
    Feb 20 23:04:52.750: INFO: Pod "downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878" satisfied condition "Succeeded or Failed"
    Feb 20 23:04:52.759: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878 container client-container: <nil>
    STEP: delete the pod 02/20/23 23:04:52.776
    Feb 20 23:04:52.802: INFO: Waiting for pod downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878 to disappear
    Feb 20 23:04:52.821: INFO: Pod downwardapi-volume-e1c9bff6-bb5e-40ea-b808-422045752878 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 23:04:52.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6315" for this suite. 02/20/23 23:04:52.834
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:52.851
Feb 20 23:04:52.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 23:04:52.86
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:52.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:52.91
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 02/20/23 23:04:52.923
Feb 20 23:04:52.958: INFO: Waiting up to 5m0s for pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214" in namespace "emptydir-8" to be "Succeeded or Failed"
Feb 20 23:04:52.971: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Pending", Reason="", readiness=false. Elapsed: 12.554465ms
Feb 20 23:04:54.980: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021276252s
Feb 20 23:04:56.980: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021391483s
Feb 20 23:04:58.981: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022914703s
STEP: Saw pod success 02/20/23 23:04:58.981
Feb 20 23:04:58.982: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214" satisfied condition "Succeeded or Failed"
Feb 20 23:04:58.991: INFO: Trying to get logs from node 10.8.38.66 pod pod-c4d77cf7-1339-42d1-a868-6e8b2761e214 container test-container: <nil>
STEP: delete the pod 02/20/23 23:04:59.009
Feb 20 23:04:59.032: INFO: Waiting for pod pod-c4d77cf7-1339-42d1-a868-6e8b2761e214 to disappear
Feb 20 23:04:59.039: INFO: Pod pod-c4d77cf7-1339-42d1-a868-6e8b2761e214 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 23:04:59.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8" for this suite. 02/20/23 23:04:59.051
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":277,"skipped":5075,"failed":0}
------------------------------
• [SLOW TEST] [6.215 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:52.851
    Feb 20 23:04:52.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 23:04:52.86
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:52.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:52.91
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/20/23 23:04:52.923
    Feb 20 23:04:52.958: INFO: Waiting up to 5m0s for pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214" in namespace "emptydir-8" to be "Succeeded or Failed"
    Feb 20 23:04:52.971: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Pending", Reason="", readiness=false. Elapsed: 12.554465ms
    Feb 20 23:04:54.980: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021276252s
    Feb 20 23:04:56.980: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021391483s
    Feb 20 23:04:58.981: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022914703s
    STEP: Saw pod success 02/20/23 23:04:58.981
    Feb 20 23:04:58.982: INFO: Pod "pod-c4d77cf7-1339-42d1-a868-6e8b2761e214" satisfied condition "Succeeded or Failed"
    Feb 20 23:04:58.991: INFO: Trying to get logs from node 10.8.38.66 pod pod-c4d77cf7-1339-42d1-a868-6e8b2761e214 container test-container: <nil>
    STEP: delete the pod 02/20/23 23:04:59.009
    Feb 20 23:04:59.032: INFO: Waiting for pod pod-c4d77cf7-1339-42d1-a868-6e8b2761e214 to disappear
    Feb 20 23:04:59.039: INFO: Pod pod-c4d77cf7-1339-42d1-a868-6e8b2761e214 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 23:04:59.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8" for this suite. 02/20/23 23:04:59.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:04:59.074
Feb 20 23:04:59.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replicaset 02/20/23 23:04:59.075
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:59.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:59.244
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/20/23 23:04:59.26
STEP: Verify that the required pods have come up 02/20/23 23:04:59.273
Feb 20 23:04:59.282: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 20 23:05:04.294: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/20/23 23:05:04.295
Feb 20 23:05:04.303: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/20/23 23:05:04.304
STEP: DeleteCollection of the ReplicaSets 02/20/23 23:05:04.316
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/20/23 23:05:04.334
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 20 23:05:04.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2376" for this suite. 02/20/23 23:05:04.36
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":278,"skipped":5116,"failed":0}
------------------------------
• [SLOW TEST] [5.300 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:04:59.074
    Feb 20 23:04:59.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replicaset 02/20/23 23:04:59.075
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:04:59.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:04:59.244
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/20/23 23:04:59.26
    STEP: Verify that the required pods have come up 02/20/23 23:04:59.273
    Feb 20 23:04:59.282: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 20 23:05:04.294: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/20/23 23:05:04.295
    Feb 20 23:05:04.303: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/20/23 23:05:04.304
    STEP: DeleteCollection of the ReplicaSets 02/20/23 23:05:04.316
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/20/23 23:05:04.334
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 20 23:05:04.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2376" for this suite. 02/20/23 23:05:04.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:04.388
Feb 20 23:05:04.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename events 02/20/23 23:05:04.389
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:04.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:04.501
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/20/23 23:05:04.507
Feb 20 23:05:04.517: INFO: created test-event-1
Feb 20 23:05:04.526: INFO: created test-event-2
Feb 20 23:05:04.535: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/20/23 23:05:04.535
STEP: delete collection of events 02/20/23 23:05:04.545
Feb 20 23:05:04.545: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/20/23 23:05:04.589
Feb 20 23:05:04.590: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 20 23:05:04.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1076" for this suite. 02/20/23 23:05:04.608
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":279,"skipped":5173,"failed":0}
------------------------------
• [0.235 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:04.388
    Feb 20 23:05:04.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename events 02/20/23 23:05:04.389
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:04.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:04.501
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/20/23 23:05:04.507
    Feb 20 23:05:04.517: INFO: created test-event-1
    Feb 20 23:05:04.526: INFO: created test-event-2
    Feb 20 23:05:04.535: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/20/23 23:05:04.535
    STEP: delete collection of events 02/20/23 23:05:04.545
    Feb 20 23:05:04.545: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/20/23 23:05:04.589
    Feb 20 23:05:04.590: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 20 23:05:04.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1076" for this suite. 02/20/23 23:05:04.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:04.629
Feb 20 23:05:04.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:05:04.632
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:04.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:04.677
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 02/20/23 23:05:04.689
W0220 23:05:04.729453      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 23:05:04.729: INFO: Waiting up to 5m0s for pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca" in namespace "projected-4806" to be "running and ready"
Feb 20 23:05:04.738: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.043448ms
Feb 20 23:05:04.738: INFO: The phase of Pod labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:05:06.747: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017238528s
Feb 20 23:05:06.747: INFO: The phase of Pod labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:05:08.778: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.048583186s
Feb 20 23:05:08.778: INFO: The phase of Pod labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca is Running (Ready = true)
Feb 20 23:05:08.778: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca" satisfied condition "running and ready"
Feb 20 23:05:09.420: INFO: Successfully updated pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 20 23:05:11.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4806" for this suite. 02/20/23 23:05:11.494
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":280,"skipped":5228,"failed":0}
------------------------------
• [SLOW TEST] [6.888 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:04.629
    Feb 20 23:05:04.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:05:04.632
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:04.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:04.677
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 02/20/23 23:05:04.689
    W0220 23:05:04.729453      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 23:05:04.729: INFO: Waiting up to 5m0s for pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca" in namespace "projected-4806" to be "running and ready"
    Feb 20 23:05:04.738: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.043448ms
    Feb 20 23:05:04.738: INFO: The phase of Pod labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:05:06.747: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017238528s
    Feb 20 23:05:06.747: INFO: The phase of Pod labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:05:08.778: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca": Phase="Running", Reason="", readiness=true. Elapsed: 4.048583186s
    Feb 20 23:05:08.778: INFO: The phase of Pod labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca is Running (Ready = true)
    Feb 20 23:05:08.778: INFO: Pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca" satisfied condition "running and ready"
    Feb 20 23:05:09.420: INFO: Successfully updated pod "labelsupdatead54043c-46e1-42d9-8464-9ab5ab0115ca"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 20 23:05:11.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4806" for this suite. 02/20/23 23:05:11.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:11.533
Feb 20 23:05:11.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename runtimeclass 02/20/23 23:05:11.534
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:11.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:11.586
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9593-delete-me 02/20/23 23:05:11.609
STEP: Waiting for the RuntimeClass to disappear 02/20/23 23:05:11.625
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 20 23:05:11.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9593" for this suite. 02/20/23 23:05:11.663
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":281,"skipped":5330,"failed":0}
------------------------------
• [0.143 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:11.533
    Feb 20 23:05:11.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename runtimeclass 02/20/23 23:05:11.534
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:11.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:11.586
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9593-delete-me 02/20/23 23:05:11.609
    STEP: Waiting for the RuntimeClass to disappear 02/20/23 23:05:11.625
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 20 23:05:11.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9593" for this suite. 02/20/23 23:05:11.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:11.679
Feb 20 23:05:11.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:05:11.683
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:11.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:11.772
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-3f04fbf4-35ad-4c55-833d-1727a59f12eb 02/20/23 23:05:11.782
STEP: Creating a pod to test consume secrets 02/20/23 23:05:11.793
Feb 20 23:05:11.828: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad" in namespace "projected-8237" to be "Succeeded or Failed"
Feb 20 23:05:11.837: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.794208ms
Feb 20 23:05:13.849: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021514498s
Feb 20 23:05:15.846: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018253975s
STEP: Saw pod success 02/20/23 23:05:15.847
Feb 20 23:05:15.847: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad" satisfied condition "Succeeded or Failed"
Feb 20 23:05:15.856: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad container projected-secret-volume-test: <nil>
STEP: delete the pod 02/20/23 23:05:15.874
Feb 20 23:05:15.898: INFO: Waiting for pod pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad to disappear
Feb 20 23:05:15.906: INFO: Pod pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 23:05:15.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8237" for this suite. 02/20/23 23:05:15.918
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":282,"skipped":5354,"failed":0}
------------------------------
• [4.253 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:11.679
    Feb 20 23:05:11.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:05:11.683
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:11.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:11.772
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-3f04fbf4-35ad-4c55-833d-1727a59f12eb 02/20/23 23:05:11.782
    STEP: Creating a pod to test consume secrets 02/20/23 23:05:11.793
    Feb 20 23:05:11.828: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad" in namespace "projected-8237" to be "Succeeded or Failed"
    Feb 20 23:05:11.837: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad": Phase="Pending", Reason="", readiness=false. Elapsed: 8.794208ms
    Feb 20 23:05:13.849: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021514498s
    Feb 20 23:05:15.846: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018253975s
    STEP: Saw pod success 02/20/23 23:05:15.847
    Feb 20 23:05:15.847: INFO: Pod "pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad" satisfied condition "Succeeded or Failed"
    Feb 20 23:05:15.856: INFO: Trying to get logs from node 10.8.38.70 pod pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 23:05:15.874
    Feb 20 23:05:15.898: INFO: Waiting for pod pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad to disappear
    Feb 20 23:05:15.906: INFO: Pod pod-projected-secrets-54f570d3-7356-470f-95b7-097b5504d8ad no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 23:05:15.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8237" for this suite. 02/20/23 23:05:15.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:15.938
Feb 20 23:05:15.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 23:05:15.941
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:15.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:15.99
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-021c33c9-1739-427e-9926-1ad036e833b0 in namespace container-probe-3260 02/20/23 23:05:16.001
Feb 20 23:05:16.041: INFO: Waiting up to 5m0s for pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0" in namespace "container-probe-3260" to be "not pending"
Feb 20 23:05:16.050: INFO: Pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.403465ms
Feb 20 23:05:18.060: INFO: Pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.018166943s
Feb 20 23:05:18.061: INFO: Pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0" satisfied condition "not pending"
Feb 20 23:05:18.061: INFO: Started pod liveness-021c33c9-1739-427e-9926-1ad036e833b0 in namespace container-probe-3260
STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 23:05:18.061
Feb 20 23:05:18.070: INFO: Initial restart count of pod liveness-021c33c9-1739-427e-9926-1ad036e833b0 is 0
Feb 20 23:05:38.174: INFO: Restart count of pod container-probe-3260/liveness-021c33c9-1739-427e-9926-1ad036e833b0 is now 1 (20.104231984s elapsed)
STEP: deleting the pod 02/20/23 23:05:38.174
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 23:05:38.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3260" for this suite. 02/20/23 23:05:38.215
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":283,"skipped":5366,"failed":0}
------------------------------
• [SLOW TEST] [22.291 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:15.938
    Feb 20 23:05:15.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 23:05:15.941
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:15.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:15.99
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-021c33c9-1739-427e-9926-1ad036e833b0 in namespace container-probe-3260 02/20/23 23:05:16.001
    Feb 20 23:05:16.041: INFO: Waiting up to 5m0s for pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0" in namespace "container-probe-3260" to be "not pending"
    Feb 20 23:05:16.050: INFO: Pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.403465ms
    Feb 20 23:05:18.060: INFO: Pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.018166943s
    Feb 20 23:05:18.061: INFO: Pod "liveness-021c33c9-1739-427e-9926-1ad036e833b0" satisfied condition "not pending"
    Feb 20 23:05:18.061: INFO: Started pod liveness-021c33c9-1739-427e-9926-1ad036e833b0 in namespace container-probe-3260
    STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 23:05:18.061
    Feb 20 23:05:18.070: INFO: Initial restart count of pod liveness-021c33c9-1739-427e-9926-1ad036e833b0 is 0
    Feb 20 23:05:38.174: INFO: Restart count of pod container-probe-3260/liveness-021c33c9-1739-427e-9926-1ad036e833b0 is now 1 (20.104231984s elapsed)
    STEP: deleting the pod 02/20/23 23:05:38.174
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 23:05:38.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3260" for this suite. 02/20/23 23:05:38.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:38.234
Feb 20 23:05:38.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 23:05:38.237
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:38.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:38.294
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 02/20/23 23:05:38.353
STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 23:05:38.374
Feb 20 23:05:38.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:05:38.394: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:05:39.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:05:39.443: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:05:40.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 23:05:40.416: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:05:41.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 23:05:41.503: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/20/23 23:05:41.533
Feb 20 23:05:41.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:05:41.615: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
Feb 20 23:05:42.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:05:42.647: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
Feb 20 23:05:43.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 23:05:43.638: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/20/23 23:05:43.638
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:05:43.657
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9563, will wait for the garbage collector to delete the pods 02/20/23 23:05:43.657
Feb 20 23:05:43.744: INFO: Deleting DaemonSet.extensions daemon-set took: 18.358498ms
Feb 20 23:05:43.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.306224ms
Feb 20 23:05:46.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:05:46.655: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 20 23:05:46.670: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125248"},"items":null}

Feb 20 23:05:46.679: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125249"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:05:46.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9563" for this suite. 02/20/23 23:05:46.728
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":284,"skipped":5377,"failed":0}
------------------------------
• [SLOW TEST] [8.541 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:38.234
    Feb 20 23:05:38.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 23:05:38.237
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:38.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:38.294
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 02/20/23 23:05:38.353
    STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 23:05:38.374
    Feb 20 23:05:38.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:05:38.394: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:05:39.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:05:39.443: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:05:40.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 23:05:40.416: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:05:41.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 23:05:41.503: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/20/23 23:05:41.533
    Feb 20 23:05:41.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:05:41.615: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
    Feb 20 23:05:42.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:05:42.647: INFO: Node 10.8.38.70 is running 0 daemon pod, expected 1
    Feb 20 23:05:43.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 23:05:43.638: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/20/23 23:05:43.638
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:05:43.657
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9563, will wait for the garbage collector to delete the pods 02/20/23 23:05:43.657
    Feb 20 23:05:43.744: INFO: Deleting DaemonSet.extensions daemon-set took: 18.358498ms
    Feb 20 23:05:43.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.306224ms
    Feb 20 23:05:46.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:05:46.655: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 20 23:05:46.670: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125248"},"items":null}

    Feb 20 23:05:46.679: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125249"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:05:46.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9563" for this suite. 02/20/23 23:05:46.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:46.782
Feb 20 23:05:46.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 23:05:46.784
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:46.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:46.847
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/20/23 23:05:46.856
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_udp@PTR;check="$$(dig +tcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_tcp@PTR;sleep 1; done
 02/20/23 23:05:46.899
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_udp@PTR;check="$$(dig +tcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_tcp@PTR;sleep 1; done
 02/20/23 23:05:46.903
STEP: creating a pod to probe DNS 02/20/23 23:05:46.903
STEP: submitting the pod to kubernetes 02/20/23 23:05:46.904
Feb 20 23:05:46.934: INFO: Waiting up to 15m0s for pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078" in namespace "dns-8703" to be "running"
Feb 20 23:05:46.943: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078": Phase="Pending", Reason="", readiness=false. Elapsed: 9.350188ms
Feb 20 23:05:48.953: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018968514s
Feb 20 23:05:50.953: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078": Phase="Running", Reason="", readiness=true. Elapsed: 4.019479484s
Feb 20 23:05:50.953: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078" satisfied condition "running"
STEP: retrieving the pod 02/20/23 23:05:50.953
STEP: looking for the results for each expected name from probers 02/20/23 23:05:50.962
Feb 20 23:05:50.978: INFO: Unable to read wheezy_udp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
Feb 20 23:05:50.990: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
Feb 20 23:05:51.002: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
Feb 20 23:05:51.077: INFO: Unable to read jessie_udp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
Feb 20 23:05:51.089: INFO: Unable to read jessie_tcp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
Feb 20 23:05:51.100: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
Feb 20 23:05:51.160: INFO: Lookups using dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078 failed for: [wheezy_udp@dns-test-service.dns-8703.svc.cluster.local wheezy_tcp@dns-test-service.dns-8703.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local jessie_udp@dns-test-service.dns-8703.svc.cluster.local jessie_tcp@dns-test-service.dns-8703.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local]

Feb 20 23:05:56.353: INFO: DNS probes using dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078 succeeded

STEP: deleting the pod 02/20/23 23:05:56.353
STEP: deleting the test service 02/20/23 23:05:56.375
STEP: deleting the test headless service 02/20/23 23:05:56.408
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 23:05:56.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8703" for this suite. 02/20/23 23:05:56.443
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":285,"skipped":5388,"failed":0}
------------------------------
• [SLOW TEST] [9.676 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:46.782
    Feb 20 23:05:46.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 23:05:46.784
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:46.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:46.847
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/20/23 23:05:46.856
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_udp@PTR;check="$$(dig +tcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_tcp@PTR;sleep 1; done
     02/20/23 23:05:46.899
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8703.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8703.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8703.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_udp@PTR;check="$$(dig +tcp +noall +answer +search 174.198.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.198.174_tcp@PTR;sleep 1; done
     02/20/23 23:05:46.903
    STEP: creating a pod to probe DNS 02/20/23 23:05:46.903
    STEP: submitting the pod to kubernetes 02/20/23 23:05:46.904
    Feb 20 23:05:46.934: INFO: Waiting up to 15m0s for pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078" in namespace "dns-8703" to be "running"
    Feb 20 23:05:46.943: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078": Phase="Pending", Reason="", readiness=false. Elapsed: 9.350188ms
    Feb 20 23:05:48.953: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018968514s
    Feb 20 23:05:50.953: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078": Phase="Running", Reason="", readiness=true. Elapsed: 4.019479484s
    Feb 20 23:05:50.953: INFO: Pod "dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 23:05:50.953
    STEP: looking for the results for each expected name from probers 02/20/23 23:05:50.962
    Feb 20 23:05:50.978: INFO: Unable to read wheezy_udp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
    Feb 20 23:05:50.990: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
    Feb 20 23:05:51.002: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
    Feb 20 23:05:51.077: INFO: Unable to read jessie_udp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
    Feb 20 23:05:51.089: INFO: Unable to read jessie_tcp@dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
    Feb 20 23:05:51.100: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local from pod dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078: the server could not find the requested resource (get pods dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078)
    Feb 20 23:05:51.160: INFO: Lookups using dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078 failed for: [wheezy_udp@dns-test-service.dns-8703.svc.cluster.local wheezy_tcp@dns-test-service.dns-8703.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local jessie_udp@dns-test-service.dns-8703.svc.cluster.local jessie_tcp@dns-test-service.dns-8703.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8703.svc.cluster.local]

    Feb 20 23:05:56.353: INFO: DNS probes using dns-8703/dns-test-ea1f3fb4-8f28-49f9-b407-e34e50ec0078 succeeded

    STEP: deleting the pod 02/20/23 23:05:56.353
    STEP: deleting the test service 02/20/23 23:05:56.375
    STEP: deleting the test headless service 02/20/23 23:05:56.408
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 23:05:56.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8703" for this suite. 02/20/23 23:05:56.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:56.468
Feb 20 23:05:56.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 23:05:56.469
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:56.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:56.531
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Feb 20 23:05:56.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: creating the pod 02/20/23 23:05:56.54
STEP: submitting the pod to kubernetes 02/20/23 23:05:56.54
Feb 20 23:05:56.569: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6" in namespace "pods-958" to be "running and ready"
Feb 20 23:05:56.589: INFO: Pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.770428ms
Feb 20 23:05:56.589: INFO: The phase of Pod pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:05:58.600: INFO: Pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.030712763s
Feb 20 23:05:58.600: INFO: The phase of Pod pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6 is Running (Ready = true)
Feb 20 23:05:58.600: INFO: Pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 23:05:58.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-958" for this suite. 02/20/23 23:05:58.8
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":286,"skipped":5421,"failed":0}
------------------------------
• [2.346 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:56.468
    Feb 20 23:05:56.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 23:05:56.469
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:56.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:56.531
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Feb 20 23:05:56.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: creating the pod 02/20/23 23:05:56.54
    STEP: submitting the pod to kubernetes 02/20/23 23:05:56.54
    Feb 20 23:05:56.569: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6" in namespace "pods-958" to be "running and ready"
    Feb 20 23:05:56.589: INFO: Pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.770428ms
    Feb 20 23:05:56.589: INFO: The phase of Pod pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:05:58.600: INFO: Pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.030712763s
    Feb 20 23:05:58.600: INFO: The phase of Pod pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6 is Running (Ready = true)
    Feb 20 23:05:58.600: INFO: Pod "pod-exec-websocket-aacf8674-9491-45e0-8bdc-4057134483e6" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 23:05:58.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-958" for this suite. 02/20/23 23:05:58.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:05:58.827
Feb 20 23:05:58.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename security-context 02/20/23 23:05:58.829
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:58.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:58.878
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/20/23 23:05:58.886
W0220 23:05:58.918485      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 23:05:58.919: INFO: Waiting up to 5m0s for pod "security-context-d4206684-d054-434b-909b-f44f071217c4" in namespace "security-context-4616" to be "Succeeded or Failed"
Feb 20 23:05:58.931: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.964317ms
Feb 20 23:06:00.942: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023199115s
Feb 20 23:06:02.940: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021450697s
STEP: Saw pod success 02/20/23 23:06:02.941
Feb 20 23:06:02.941: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4" satisfied condition "Succeeded or Failed"
Feb 20 23:06:02.949: INFO: Trying to get logs from node 10.8.38.70 pod security-context-d4206684-d054-434b-909b-f44f071217c4 container test-container: <nil>
STEP: delete the pod 02/20/23 23:06:02.973
Feb 20 23:06:02.997: INFO: Waiting for pod security-context-d4206684-d054-434b-909b-f44f071217c4 to disappear
Feb 20 23:06:03.005: INFO: Pod security-context-d4206684-d054-434b-909b-f44f071217c4 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 20 23:06:03.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4616" for this suite. 02/20/23 23:06:03.018
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":287,"skipped":5439,"failed":0}
------------------------------
• [4.211 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:05:58.827
    Feb 20 23:05:58.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename security-context 02/20/23 23:05:58.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:05:58.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:05:58.878
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/20/23 23:05:58.886
    W0220 23:05:58.918485      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 23:05:58.919: INFO: Waiting up to 5m0s for pod "security-context-d4206684-d054-434b-909b-f44f071217c4" in namespace "security-context-4616" to be "Succeeded or Failed"
    Feb 20 23:05:58.931: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.964317ms
    Feb 20 23:06:00.942: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023199115s
    Feb 20 23:06:02.940: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021450697s
    STEP: Saw pod success 02/20/23 23:06:02.941
    Feb 20 23:06:02.941: INFO: Pod "security-context-d4206684-d054-434b-909b-f44f071217c4" satisfied condition "Succeeded or Failed"
    Feb 20 23:06:02.949: INFO: Trying to get logs from node 10.8.38.70 pod security-context-d4206684-d054-434b-909b-f44f071217c4 container test-container: <nil>
    STEP: delete the pod 02/20/23 23:06:02.973
    Feb 20 23:06:02.997: INFO: Waiting for pod security-context-d4206684-d054-434b-909b-f44f071217c4 to disappear
    Feb 20 23:06:03.005: INFO: Pod security-context-d4206684-d054-434b-909b-f44f071217c4 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 20 23:06:03.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4616" for this suite. 02/20/23 23:06:03.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:06:03.039
Feb 20 23:06:03.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 23:06:03.041
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:03.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:03.134
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Feb 20 23:06:03.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/20/23 23:06:13.117
Feb 20 23:06:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 create -f -'
Feb 20 23:06:15.012: INFO: stderr: ""
Feb 20 23:06:15.012: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 20 23:06:15.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 delete e2e-test-crd-publish-openapi-2572-crds test-cr'
Feb 20 23:06:15.179: INFO: stderr: ""
Feb 20 23:06:15.179: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 20 23:06:15.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 apply -f -'
Feb 20 23:06:15.772: INFO: stderr: ""
Feb 20 23:06:15.772: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 20 23:06:15.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 delete e2e-test-crd-publish-openapi-2572-crds test-cr'
Feb 20 23:06:15.888: INFO: stderr: ""
Feb 20 23:06:15.888: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/20/23 23:06:15.888
Feb 20 23:06:15.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 explain e2e-test-crd-publish-openapi-2572-crds'
Feb 20 23:06:17.479: INFO: stderr: ""
Feb 20 23:06:17.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2572-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:06:25.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8761" for this suite. 02/20/23 23:06:25.754
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":288,"skipped":5445,"failed":0}
------------------------------
• [SLOW TEST] [22.737 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:06:03.039
    Feb 20 23:06:03.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 23:06:03.041
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:03.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:03.134
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Feb 20 23:06:03.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/20/23 23:06:13.117
    Feb 20 23:06:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 create -f -'
    Feb 20 23:06:15.012: INFO: stderr: ""
    Feb 20 23:06:15.012: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 20 23:06:15.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 delete e2e-test-crd-publish-openapi-2572-crds test-cr'
    Feb 20 23:06:15.179: INFO: stderr: ""
    Feb 20 23:06:15.179: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 20 23:06:15.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 apply -f -'
    Feb 20 23:06:15.772: INFO: stderr: ""
    Feb 20 23:06:15.772: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 20 23:06:15.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 --namespace=crd-publish-openapi-8761 delete e2e-test-crd-publish-openapi-2572-crds test-cr'
    Feb 20 23:06:15.888: INFO: stderr: ""
    Feb 20 23:06:15.888: INFO: stdout: "e2e-test-crd-publish-openapi-2572-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/20/23 23:06:15.888
    Feb 20 23:06:15.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=crd-publish-openapi-8761 explain e2e-test-crd-publish-openapi-2572-crds'
    Feb 20 23:06:17.479: INFO: stderr: ""
    Feb 20 23:06:17.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2572-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:06:25.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8761" for this suite. 02/20/23 23:06:25.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:06:25.783
Feb 20 23:06:25.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename endpointslicemirroring 02/20/23 23:06:25.786
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:25.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:25.846
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/20/23 23:06:25.889
Feb 20 23:06:25.912: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/20/23 23:06:27.921
Feb 20 23:06:27.941: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 02/20/23 23:06:29.95
Feb 20 23:06:29.974: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Feb 20 23:06:31.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4302" for this suite. 02/20/23 23:06:32.005
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":289,"skipped":5504,"failed":0}
------------------------------
• [SLOW TEST] [6.247 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:06:25.783
    Feb 20 23:06:25.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename endpointslicemirroring 02/20/23 23:06:25.786
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:25.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:25.846
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/20/23 23:06:25.889
    Feb 20 23:06:25.912: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/20/23 23:06:27.921
    Feb 20 23:06:27.941: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 02/20/23 23:06:29.95
    Feb 20 23:06:29.974: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Feb 20 23:06:31.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-4302" for this suite. 02/20/23 23:06:32.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:06:32.039
Feb 20 23:06:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename runtimeclass 02/20/23 23:06:32.04
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:32.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:32.185
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 20 23:06:32.324: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9955 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 20 23:06:32.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9955" for this suite. 02/20/23 23:06:32.43
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":290,"skipped":5531,"failed":0}
------------------------------
• [0.418 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:06:32.039
    Feb 20 23:06:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename runtimeclass 02/20/23 23:06:32.04
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:32.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:32.185
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 20 23:06:32.324: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9955 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 20 23:06:32.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9955" for this suite. 02/20/23 23:06:32.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:06:32.461
Feb 20 23:06:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 23:06:32.465
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:32.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:32.602
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-912ca8d3-a344-47f1-9473-745e4f2aa764 02/20/23 23:06:32.616
STEP: Creating a pod to test consume configMaps 02/20/23 23:06:32.649
Feb 20 23:06:32.706: INFO: Waiting up to 5m0s for pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130" in namespace "configmap-9185" to be "Succeeded or Failed"
Feb 20 23:06:32.718: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490708ms
Feb 20 23:06:34.730: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024073511s
Feb 20 23:06:36.752: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045614168s
STEP: Saw pod success 02/20/23 23:06:36.752
Feb 20 23:06:36.752: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130" satisfied condition "Succeeded or Failed"
Feb 20 23:06:36.762: INFO: Trying to get logs from node 10.8.38.69 pod pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130 container agnhost-container: <nil>
STEP: delete the pod 02/20/23 23:06:36.814
Feb 20 23:06:36.852: INFO: Waiting for pod pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130 to disappear
Feb 20 23:06:36.889: INFO: Pod pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 23:06:36.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9185" for this suite. 02/20/23 23:06:36.914
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":291,"skipped":5536,"failed":0}
------------------------------
• [4.474 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:06:32.461
    Feb 20 23:06:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 23:06:32.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:32.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:32.602
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-912ca8d3-a344-47f1-9473-745e4f2aa764 02/20/23 23:06:32.616
    STEP: Creating a pod to test consume configMaps 02/20/23 23:06:32.649
    Feb 20 23:06:32.706: INFO: Waiting up to 5m0s for pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130" in namespace "configmap-9185" to be "Succeeded or Failed"
    Feb 20 23:06:32.718: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490708ms
    Feb 20 23:06:34.730: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024073511s
    Feb 20 23:06:36.752: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045614168s
    STEP: Saw pod success 02/20/23 23:06:36.752
    Feb 20 23:06:36.752: INFO: Pod "pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130" satisfied condition "Succeeded or Failed"
    Feb 20 23:06:36.762: INFO: Trying to get logs from node 10.8.38.69 pod pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130 container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 23:06:36.814
    Feb 20 23:06:36.852: INFO: Waiting for pod pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130 to disappear
    Feb 20 23:06:36.889: INFO: Pod pod-configmaps-606637c2-c264-4815-ae0a-7027d5a15130 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 23:06:36.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9185" for this suite. 02/20/23 23:06:36.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:06:36.938
Feb 20 23:06:36.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 23:06:36.94
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:37.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:37.041
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-68 02/20/23 23:06:37.058
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 02/20/23 23:06:37.091
STEP: Creating stateful set ss in namespace statefulset-68 02/20/23 23:06:37.107
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-68 02/20/23 23:06:37.127
Feb 20 23:06:37.141: INFO: Found 0 stateful pods, waiting for 1
Feb 20 23:06:47.156: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/20/23 23:06:47.156
Feb 20 23:06:47.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 23:06:47.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 23:06:47.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 23:06:47.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 23:06:47.480: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 23:06:57.493: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 23:06:57.493: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 23:06:57.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996337s
Feb 20 23:06:58.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98826886s
Feb 20 23:06:59.557: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975862864s
Feb 20 23:07:00.579: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96215537s
Feb 20 23:07:01.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.940419289s
Feb 20 23:07:02.615: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.918612951s
Feb 20 23:07:03.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.905057299s
Feb 20 23:07:04.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.891878701s
Feb 20 23:07:05.653: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.879209849s
Feb 20 23:07:06.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 866.565317ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-68 02/20/23 23:07:07.668
Feb 20 23:07:07.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 23:07:07.960: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 23:07:07.960: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 23:07:07.960: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 23:07:07.971: INFO: Found 1 stateful pods, waiting for 3
Feb 20 23:07:17.990: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 23:07:17.990: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 23:07:17.990: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/20/23 23:07:17.99
STEP: Scale down will halt with unhealthy stateful pod 02/20/23 23:07:17.99
Feb 20 23:07:18.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 23:07:18.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 23:07:18.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 23:07:18.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 23:07:18.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 23:07:18.625: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 23:07:18.625: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 23:07:18.625: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 23:07:18.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 20 23:07:18.930: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 20 23:07:18.930: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 20 23:07:18.930: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 20 23:07:18.930: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 23:07:18.938: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 20 23:07:28.964: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 23:07:28.964: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 23:07:28.964: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 23:07:29.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994676s
Feb 20 23:07:30.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980817292s
Feb 20 23:07:31.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962901008s
Feb 20 23:07:32.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.941004168s
Feb 20 23:07:33.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925430533s
Feb 20 23:07:34.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.883252947s
Feb 20 23:07:35.132: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.867571105s
Feb 20 23:07:36.148: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.849922981s
Feb 20 23:07:37.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.833108022s
Feb 20 23:07:38.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 817.433521ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-68 02/20/23 23:07:39.184
Feb 20 23:07:39.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 23:07:39.500: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 23:07:39.500: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 23:07:39.500: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 23:07:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 23:07:39.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 23:07:39.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 23:07:39.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 23:07:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 20 23:07:40.092: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 20 23:07:40.092: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 20 23:07:40.092: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 20 23:07:40.092: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/20/23 23:07:50.14
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 23:07:50.141: INFO: Deleting all statefulset in ns statefulset-68
Feb 20 23:07:50.149: INFO: Scaling statefulset ss to 0
Feb 20 23:07:50.184: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 23:07:50.192: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 23:07:50.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-68" for this suite. 02/20/23 23:07:50.238
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":292,"skipped":5550,"failed":0}
------------------------------
• [SLOW TEST] [73.336 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:06:36.938
    Feb 20 23:06:36.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 23:06:36.94
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:06:37.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:06:37.041
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-68 02/20/23 23:06:37.058
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/20/23 23:06:37.091
    STEP: Creating stateful set ss in namespace statefulset-68 02/20/23 23:06:37.107
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-68 02/20/23 23:06:37.127
    Feb 20 23:06:37.141: INFO: Found 0 stateful pods, waiting for 1
    Feb 20 23:06:47.156: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/20/23 23:06:47.156
    Feb 20 23:06:47.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 23:06:47.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 23:06:47.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 23:06:47.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 23:06:47.480: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 20 23:06:57.493: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 23:06:57.493: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 23:06:57.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996337s
    Feb 20 23:06:58.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98826886s
    Feb 20 23:06:59.557: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975862864s
    Feb 20 23:07:00.579: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96215537s
    Feb 20 23:07:01.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.940419289s
    Feb 20 23:07:02.615: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.918612951s
    Feb 20 23:07:03.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.905057299s
    Feb 20 23:07:04.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.891878701s
    Feb 20 23:07:05.653: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.879209849s
    Feb 20 23:07:06.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 866.565317ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-68 02/20/23 23:07:07.668
    Feb 20 23:07:07.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 23:07:07.960: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 23:07:07.960: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 23:07:07.960: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 23:07:07.971: INFO: Found 1 stateful pods, waiting for 3
    Feb 20 23:07:17.990: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 23:07:17.990: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 23:07:17.990: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/20/23 23:07:17.99
    STEP: Scale down will halt with unhealthy stateful pod 02/20/23 23:07:17.99
    Feb 20 23:07:18.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 23:07:18.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 23:07:18.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 23:07:18.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 23:07:18.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 23:07:18.625: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 23:07:18.625: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 23:07:18.625: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 23:07:18.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 20 23:07:18.930: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 20 23:07:18.930: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 20 23:07:18.930: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 20 23:07:18.930: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 23:07:18.938: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Feb 20 23:07:28.964: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 23:07:28.964: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 23:07:28.964: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 20 23:07:29.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999994676s
    Feb 20 23:07:30.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980817292s
    Feb 20 23:07:31.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962901008s
    Feb 20 23:07:32.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.941004168s
    Feb 20 23:07:33.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925430533s
    Feb 20 23:07:34.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.883252947s
    Feb 20 23:07:35.132: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.867571105s
    Feb 20 23:07:36.148: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.849922981s
    Feb 20 23:07:37.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.833108022s
    Feb 20 23:07:38.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 817.433521ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-68 02/20/23 23:07:39.184
    Feb 20 23:07:39.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 23:07:39.500: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 23:07:39.500: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 23:07:39.500: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 23:07:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 23:07:39.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 23:07:39.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 23:07:39.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 23:07:39.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=statefulset-68 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 20 23:07:40.092: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 20 23:07:40.092: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 20 23:07:40.092: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 20 23:07:40.092: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/20/23 23:07:50.14
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 23:07:50.141: INFO: Deleting all statefulset in ns statefulset-68
    Feb 20 23:07:50.149: INFO: Scaling statefulset ss to 0
    Feb 20 23:07:50.184: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 23:07:50.192: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 23:07:50.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-68" for this suite. 02/20/23 23:07:50.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:07:50.277
Feb 20 23:07:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 23:07:50.279
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:07:50.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:07:50.349
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 02/20/23 23:07:50.36
STEP: Creating a ResourceQuota 02/20/23 23:07:55.369
STEP: Ensuring resource quota status is calculated 02/20/23 23:07:55.38
STEP: Creating a ReplicationController 02/20/23 23:07:57.39
STEP: Ensuring resource quota status captures replication controller creation 02/20/23 23:07:57.447
STEP: Deleting a ReplicationController 02/20/23 23:07:59.458
STEP: Ensuring resource quota status released usage 02/20/23 23:07:59.47
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 23:08:01.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6758" for this suite. 02/20/23 23:08:01.5
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":293,"skipped":5556,"failed":0}
------------------------------
• [SLOW TEST] [11.262 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:07:50.277
    Feb 20 23:07:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 23:07:50.279
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:07:50.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:07:50.349
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 02/20/23 23:07:50.36
    STEP: Creating a ResourceQuota 02/20/23 23:07:55.369
    STEP: Ensuring resource quota status is calculated 02/20/23 23:07:55.38
    STEP: Creating a ReplicationController 02/20/23 23:07:57.39
    STEP: Ensuring resource quota status captures replication controller creation 02/20/23 23:07:57.447
    STEP: Deleting a ReplicationController 02/20/23 23:07:59.458
    STEP: Ensuring resource quota status released usage 02/20/23 23:07:59.47
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 23:08:01.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6758" for this suite. 02/20/23 23:08:01.5
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:08:01.54
Feb 20 23:08:01.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 23:08:01.545
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:08:01.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:08:01.626
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662 in namespace container-probe-7918 02/20/23 23:08:01.697
Feb 20 23:08:01.742: INFO: Waiting up to 5m0s for pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662" in namespace "container-probe-7918" to be "not pending"
Feb 20 23:08:01.786: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662": Phase="Pending", Reason="", readiness=false. Elapsed: 44.226758ms
Feb 20 23:08:03.798: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055442315s
Feb 20 23:08:05.799: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662": Phase="Running", Reason="", readiness=true. Elapsed: 4.056989397s
Feb 20 23:08:05.799: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662" satisfied condition "not pending"
Feb 20 23:08:05.799: INFO: Started pod liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662 in namespace container-probe-7918
STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 23:08:05.8
Feb 20 23:08:05.812: INFO: Initial restart count of pod liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662 is 0
STEP: deleting the pod 02/20/23 23:12:07.561
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 23:12:07.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7918" for this suite. 02/20/23 23:12:07.654
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":294,"skipped":5557,"failed":0}
------------------------------
• [SLOW TEST] [246.139 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:08:01.54
    Feb 20 23:08:01.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 23:08:01.545
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:08:01.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:08:01.626
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662 in namespace container-probe-7918 02/20/23 23:08:01.697
    Feb 20 23:08:01.742: INFO: Waiting up to 5m0s for pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662" in namespace "container-probe-7918" to be "not pending"
    Feb 20 23:08:01.786: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662": Phase="Pending", Reason="", readiness=false. Elapsed: 44.226758ms
    Feb 20 23:08:03.798: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055442315s
    Feb 20 23:08:05.799: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662": Phase="Running", Reason="", readiness=true. Elapsed: 4.056989397s
    Feb 20 23:08:05.799: INFO: Pod "liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662" satisfied condition "not pending"
    Feb 20 23:08:05.799: INFO: Started pod liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662 in namespace container-probe-7918
    STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 23:08:05.8
    Feb 20 23:08:05.812: INFO: Initial restart count of pod liveness-4cf4b9ce-5cfc-4f89-a4cc-19786b479662 is 0
    STEP: deleting the pod 02/20/23 23:12:07.561
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 23:12:07.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7918" for this suite. 02/20/23 23:12:07.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:12:07.687
Feb 20 23:12:07.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 23:12:07.69
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:12:07.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:12:07.751
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 02/20/23 23:12:07.761
Feb 20 23:12:07.818: INFO: Waiting up to 5m0s for pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3" in namespace "emptydir-8729" to be "Succeeded or Failed"
Feb 20 23:12:07.852: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Pending", Reason="", readiness=false. Elapsed: 33.706984ms
Feb 20 23:12:09.865: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047234299s
Feb 20 23:12:11.865: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046759124s
Feb 20 23:12:13.869: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05088114s
STEP: Saw pod success 02/20/23 23:12:13.869
Feb 20 23:12:13.869: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3" satisfied condition "Succeeded or Failed"
Feb 20 23:12:13.880: INFO: Trying to get logs from node 10.8.38.66 pod pod-4ca8a629-de78-4845-9840-59e5d84f54b3 container test-container: <nil>
STEP: delete the pod 02/20/23 23:12:13.93
Feb 20 23:12:13.961: INFO: Waiting for pod pod-4ca8a629-de78-4845-9840-59e5d84f54b3 to disappear
Feb 20 23:12:13.971: INFO: Pod pod-4ca8a629-de78-4845-9840-59e5d84f54b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 23:12:13.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8729" for this suite. 02/20/23 23:12:14.01
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":295,"skipped":5579,"failed":0}
------------------------------
• [SLOW TEST] [6.375 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:12:07.687
    Feb 20 23:12:07.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 23:12:07.69
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:12:07.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:12:07.751
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/20/23 23:12:07.761
    Feb 20 23:12:07.818: INFO: Waiting up to 5m0s for pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3" in namespace "emptydir-8729" to be "Succeeded or Failed"
    Feb 20 23:12:07.852: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Pending", Reason="", readiness=false. Elapsed: 33.706984ms
    Feb 20 23:12:09.865: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047234299s
    Feb 20 23:12:11.865: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046759124s
    Feb 20 23:12:13.869: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05088114s
    STEP: Saw pod success 02/20/23 23:12:13.869
    Feb 20 23:12:13.869: INFO: Pod "pod-4ca8a629-de78-4845-9840-59e5d84f54b3" satisfied condition "Succeeded or Failed"
    Feb 20 23:12:13.880: INFO: Trying to get logs from node 10.8.38.66 pod pod-4ca8a629-de78-4845-9840-59e5d84f54b3 container test-container: <nil>
    STEP: delete the pod 02/20/23 23:12:13.93
    Feb 20 23:12:13.961: INFO: Waiting for pod pod-4ca8a629-de78-4845-9840-59e5d84f54b3 to disappear
    Feb 20 23:12:13.971: INFO: Pod pod-4ca8a629-de78-4845-9840-59e5d84f54b3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 23:12:13.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8729" for this suite. 02/20/23 23:12:14.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:12:14.063
Feb 20 23:12:14.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 23:12:14.066
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:12:14.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:12:14.122
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/20/23 23:12:14.134
Feb 20 23:12:14.216: INFO: Waiting up to 5m0s for pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a" in namespace "emptydir-2372" to be "Succeeded or Failed"
Feb 20 23:12:14.233: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.065338ms
Feb 20 23:12:16.244: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0278551s
Feb 20 23:12:18.245: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028718271s
STEP: Saw pod success 02/20/23 23:12:18.245
Feb 20 23:12:18.246: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a" satisfied condition "Succeeded or Failed"
Feb 20 23:12:18.258: INFO: Trying to get logs from node 10.8.38.66 pod pod-2805ff96-cf3e-4638-88c4-d53da69edc8a container test-container: <nil>
STEP: delete the pod 02/20/23 23:12:18.279
Feb 20 23:12:18.305: INFO: Waiting for pod pod-2805ff96-cf3e-4638-88c4-d53da69edc8a to disappear
Feb 20 23:12:18.316: INFO: Pod pod-2805ff96-cf3e-4638-88c4-d53da69edc8a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 23:12:18.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2372" for this suite. 02/20/23 23:12:18.337
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":296,"skipped":5585,"failed":0}
------------------------------
• [4.325 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:12:14.063
    Feb 20 23:12:14.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 23:12:14.066
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:12:14.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:12:14.122
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/20/23 23:12:14.134
    Feb 20 23:12:14.216: INFO: Waiting up to 5m0s for pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a" in namespace "emptydir-2372" to be "Succeeded or Failed"
    Feb 20 23:12:14.233: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.065338ms
    Feb 20 23:12:16.244: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0278551s
    Feb 20 23:12:18.245: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028718271s
    STEP: Saw pod success 02/20/23 23:12:18.245
    Feb 20 23:12:18.246: INFO: Pod "pod-2805ff96-cf3e-4638-88c4-d53da69edc8a" satisfied condition "Succeeded or Failed"
    Feb 20 23:12:18.258: INFO: Trying to get logs from node 10.8.38.66 pod pod-2805ff96-cf3e-4638-88c4-d53da69edc8a container test-container: <nil>
    STEP: delete the pod 02/20/23 23:12:18.279
    Feb 20 23:12:18.305: INFO: Waiting for pod pod-2805ff96-cf3e-4638-88c4-d53da69edc8a to disappear
    Feb 20 23:12:18.316: INFO: Pod pod-2805ff96-cf3e-4638-88c4-d53da69edc8a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 23:12:18.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2372" for this suite. 02/20/23 23:12:18.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:12:18.399
Feb 20 23:12:18.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-probe 02/20/23 23:12:18.401
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:12:18.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:12:18.458
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-3bdfdc1e-3494-4295-abb5-71518306e82e in namespace container-probe-7268 02/20/23 23:12:18.469
Feb 20 23:12:18.527: INFO: Waiting up to 5m0s for pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e" in namespace "container-probe-7268" to be "not pending"
Feb 20 23:12:18.545: INFO: Pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.133149ms
Feb 20 23:12:20.576: INFO: Pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e": Phase="Running", Reason="", readiness=true. Elapsed: 2.049334561s
Feb 20 23:12:20.576: INFO: Pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e" satisfied condition "not pending"
Feb 20 23:12:20.576: INFO: Started pod liveness-3bdfdc1e-3494-4295-abb5-71518306e82e in namespace container-probe-7268
STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 23:12:20.576
Feb 20 23:12:20.604: INFO: Initial restart count of pod liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is 0
Feb 20 23:12:40.809: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 1 (20.204193124s elapsed)
Feb 20 23:13:00.946: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 2 (40.341541868s elapsed)
Feb 20 23:13:21.079: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 3 (1m0.474706612s elapsed)
Feb 20 23:13:41.228: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 4 (1m20.624049947s elapsed)
Feb 20 23:14:45.666: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 5 (2m25.061213432s elapsed)
STEP: deleting the pod 02/20/23 23:14:45.666
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 20 23:14:45.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7268" for this suite. 02/20/23 23:14:45.754
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":297,"skipped":5643,"failed":0}
------------------------------
• [SLOW TEST] [147.393 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:12:18.399
    Feb 20 23:12:18.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-probe 02/20/23 23:12:18.401
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:12:18.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:12:18.458
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-3bdfdc1e-3494-4295-abb5-71518306e82e in namespace container-probe-7268 02/20/23 23:12:18.469
    Feb 20 23:12:18.527: INFO: Waiting up to 5m0s for pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e" in namespace "container-probe-7268" to be "not pending"
    Feb 20 23:12:18.545: INFO: Pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.133149ms
    Feb 20 23:12:20.576: INFO: Pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e": Phase="Running", Reason="", readiness=true. Elapsed: 2.049334561s
    Feb 20 23:12:20.576: INFO: Pod "liveness-3bdfdc1e-3494-4295-abb5-71518306e82e" satisfied condition "not pending"
    Feb 20 23:12:20.576: INFO: Started pod liveness-3bdfdc1e-3494-4295-abb5-71518306e82e in namespace container-probe-7268
    STEP: checking the pod's current state and verifying that restartCount is present 02/20/23 23:12:20.576
    Feb 20 23:12:20.604: INFO: Initial restart count of pod liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is 0
    Feb 20 23:12:40.809: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 1 (20.204193124s elapsed)
    Feb 20 23:13:00.946: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 2 (40.341541868s elapsed)
    Feb 20 23:13:21.079: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 3 (1m0.474706612s elapsed)
    Feb 20 23:13:41.228: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 4 (1m20.624049947s elapsed)
    Feb 20 23:14:45.666: INFO: Restart count of pod container-probe-7268/liveness-3bdfdc1e-3494-4295-abb5-71518306e82e is now 5 (2m25.061213432s elapsed)
    STEP: deleting the pod 02/20/23 23:14:45.666
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 20 23:14:45.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7268" for this suite. 02/20/23 23:14:45.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:14:45.838
Feb 20 23:14:45.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-runtime 02/20/23 23:14:45.839
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:14:45.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:14:45.926
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/20/23 23:14:46.079
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/20/23 23:15:04.323
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/20/23 23:15:04.335
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/20/23 23:15:04.358
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/20/23 23:15:04.358
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/20/23 23:15:04.437
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/20/23 23:15:07.499
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/20/23 23:15:09.536
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/20/23 23:15:09.559
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/20/23 23:15:09.559
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/20/23 23:15:09.641
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/20/23 23:15:10.664
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/20/23 23:15:14.733
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/20/23 23:15:14.756
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/20/23 23:15:14.756
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 20 23:15:14.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5797" for this suite. 02/20/23 23:15:14.842
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":298,"skipped":5667,"failed":0}
------------------------------
• [SLOW TEST] [29.035 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:14:45.838
    Feb 20 23:14:45.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-runtime 02/20/23 23:14:45.839
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:14:45.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:14:45.926
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/20/23 23:14:46.079
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/20/23 23:15:04.323
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/20/23 23:15:04.335
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/20/23 23:15:04.358
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/20/23 23:15:04.358
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/20/23 23:15:04.437
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/20/23 23:15:07.499
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/20/23 23:15:09.536
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/20/23 23:15:09.559
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/20/23 23:15:09.559
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/20/23 23:15:09.641
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/20/23 23:15:10.664
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/20/23 23:15:14.733
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/20/23 23:15:14.756
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/20/23 23:15:14.756
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 20 23:15:14.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5797" for this suite. 02/20/23 23:15:14.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:15:14.878
Feb 20 23:15:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 23:15:14.88
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:15:14.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:15:14.939
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/20/23 23:15:14.95
Feb 20 23:15:14.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/20/23 23:15:46.607
Feb 20 23:15:46.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 23:15:55.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:16:30.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2933" for this suite. 02/20/23 23:16:30.472
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":299,"skipped":5675,"failed":0}
------------------------------
• [SLOW TEST] [75.618 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:15:14.878
    Feb 20 23:15:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename crd-publish-openapi 02/20/23 23:15:14.88
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:15:14.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:15:14.939
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/20/23 23:15:14.95
    Feb 20 23:15:14.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/20/23 23:15:46.607
    Feb 20 23:15:46.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 23:15:55.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:16:30.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2933" for this suite. 02/20/23 23:16:30.472
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:16:30.499
Feb 20 23:16:30.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename var-expansion 02/20/23 23:16:30.501
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:30.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:30.549
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Feb 20 23:16:30.613: INFO: Waiting up to 2m0s for pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" in namespace "var-expansion-3452" to be "container 0 failed with reason CreateContainerConfigError"
Feb 20 23:16:30.622: INFO: Pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2253ms
Feb 20 23:16:32.633: INFO: Pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020256156s
Feb 20 23:16:32.633: INFO: Pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 20 23:16:32.633: INFO: Deleting pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" in namespace "var-expansion-3452"
Feb 20 23:16:32.652: INFO: Wait up to 5m0s for pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 20 23:16:36.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3452" for this suite. 02/20/23 23:16:36.689
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":300,"skipped":5676,"failed":0}
------------------------------
• [SLOW TEST] [6.218 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:16:30.499
    Feb 20 23:16:30.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename var-expansion 02/20/23 23:16:30.501
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:30.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:30.549
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Feb 20 23:16:30.613: INFO: Waiting up to 2m0s for pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" in namespace "var-expansion-3452" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 20 23:16:30.622: INFO: Pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38": Phase="Pending", Reason="", readiness=false. Elapsed: 9.2253ms
    Feb 20 23:16:32.633: INFO: Pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020256156s
    Feb 20 23:16:32.633: INFO: Pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 20 23:16:32.633: INFO: Deleting pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" in namespace "var-expansion-3452"
    Feb 20 23:16:32.652: INFO: Wait up to 5m0s for pod "var-expansion-da4d2957-1859-4242-89c3-623c07b8ff38" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 20 23:16:36.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3452" for this suite. 02/20/23 23:16:36.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:16:36.719
Feb 20 23:16:36.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 23:16:36.721
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:36.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:36.773
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-3648 02/20/23 23:16:36.781
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[] 02/20/23 23:16:36.843
Feb 20 23:16:36.863: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3648 02/20/23 23:16:36.863
Feb 20 23:16:36.912: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3648" to be "running and ready"
Feb 20 23:16:36.921: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.592597ms
Feb 20 23:16:36.921: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:16:38.932: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019474923s
Feb 20 23:16:38.932: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:16:40.932: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01971538s
Feb 20 23:16:40.932: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 20 23:16:40.933: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[pod1:[100]] 02/20/23 23:16:40.942
Feb 20 23:16:40.969: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3648 02/20/23 23:16:40.969
Feb 20 23:16:41.001: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3648" to be "running and ready"
Feb 20 23:16:41.013: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.66949ms
Feb 20 23:16:41.013: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:16:43.024: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023098468s
Feb 20 23:16:43.024: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:16:45.024: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023112288s
Feb 20 23:16:45.024: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 20 23:16:45.024: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[pod1:[100] pod2:[101]] 02/20/23 23:16:45.034
Feb 20 23:16:45.070: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/20/23 23:16:45.07
Feb 20 23:16:45.070: INFO: Creating new exec pod
Feb 20 23:16:45.100: INFO: Waiting up to 5m0s for pod "execpod9svc4" in namespace "services-3648" to be "running"
Feb 20 23:16:45.113: INFO: Pod "execpod9svc4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.198485ms
Feb 20 23:16:47.125: INFO: Pod "execpod9svc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02467219s
Feb 20 23:16:49.123: INFO: Pod "execpod9svc4": Phase="Running", Reason="", readiness=true. Elapsed: 4.022708676s
Feb 20 23:16:49.123: INFO: Pod "execpod9svc4" satisfied condition "running"
Feb 20 23:16:50.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 20 23:16:50.412: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 20 23:16:50.412: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:16:50.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.249.5 80'
Feb 20 23:16:50.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.249.5 80\nConnection to 172.21.249.5 80 port [tcp/http] succeeded!\n"
Feb 20 23:16:50.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:16:50.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 20 23:16:51.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 20 23:16:51.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:16:51.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.249.5 81'
Feb 20 23:16:51.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.249.5 81\nConnection to 172.21.249.5 81 port [tcp/*] succeeded!\n"
Feb 20 23:16:51.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3648 02/20/23 23:16:51.293
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[pod2:[101]] 02/20/23 23:16:51.327
Feb 20 23:16:52.378: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3648 02/20/23 23:16:52.378
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[] 02/20/23 23:16:52.409
Feb 20 23:16:52.463: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 23:16:52.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3648" for this suite. 02/20/23 23:16:52.521
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":301,"skipped":5718,"failed":0}
------------------------------
• [SLOW TEST] [15.825 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:16:36.719
    Feb 20 23:16:36.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 23:16:36.721
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:36.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:36.773
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-3648 02/20/23 23:16:36.781
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[] 02/20/23 23:16:36.843
    Feb 20 23:16:36.863: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3648 02/20/23 23:16:36.863
    Feb 20 23:16:36.912: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3648" to be "running and ready"
    Feb 20 23:16:36.921: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.592597ms
    Feb 20 23:16:36.921: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:16:38.932: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019474923s
    Feb 20 23:16:38.932: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:16:40.932: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.01971538s
    Feb 20 23:16:40.932: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 20 23:16:40.933: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[pod1:[100]] 02/20/23 23:16:40.942
    Feb 20 23:16:40.969: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3648 02/20/23 23:16:40.969
    Feb 20 23:16:41.001: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3648" to be "running and ready"
    Feb 20 23:16:41.013: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.66949ms
    Feb 20 23:16:41.013: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:16:43.024: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023098468s
    Feb 20 23:16:43.024: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:16:45.024: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023112288s
    Feb 20 23:16:45.024: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 20 23:16:45.024: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[pod1:[100] pod2:[101]] 02/20/23 23:16:45.034
    Feb 20 23:16:45.070: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/20/23 23:16:45.07
    Feb 20 23:16:45.070: INFO: Creating new exec pod
    Feb 20 23:16:45.100: INFO: Waiting up to 5m0s for pod "execpod9svc4" in namespace "services-3648" to be "running"
    Feb 20 23:16:45.113: INFO: Pod "execpod9svc4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.198485ms
    Feb 20 23:16:47.125: INFO: Pod "execpod9svc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02467219s
    Feb 20 23:16:49.123: INFO: Pod "execpod9svc4": Phase="Running", Reason="", readiness=true. Elapsed: 4.022708676s
    Feb 20 23:16:49.123: INFO: Pod "execpod9svc4" satisfied condition "running"
    Feb 20 23:16:50.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Feb 20 23:16:50.412: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 20 23:16:50.412: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:16:50.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.249.5 80'
    Feb 20 23:16:50.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.249.5 80\nConnection to 172.21.249.5 80 port [tcp/http] succeeded!\n"
    Feb 20 23:16:50.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:16:50.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Feb 20 23:16:51.033: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 20 23:16:51.033: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:16:51.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-3648 exec execpod9svc4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.249.5 81'
    Feb 20 23:16:51.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.249.5 81\nConnection to 172.21.249.5 81 port [tcp/*] succeeded!\n"
    Feb 20 23:16:51.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3648 02/20/23 23:16:51.293
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[pod2:[101]] 02/20/23 23:16:51.327
    Feb 20 23:16:52.378: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3648 02/20/23 23:16:52.378
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3648 to expose endpoints map[] 02/20/23 23:16:52.409
    Feb 20 23:16:52.463: INFO: successfully validated that service multi-endpoint-test in namespace services-3648 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 23:16:52.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3648" for this suite. 02/20/23 23:16:52.521
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:16:52.544
Feb 20 23:16:52.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename namespaces 02/20/23 23:16:52.547
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:52.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:52.638
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 02/20/23 23:16:52.653
Feb 20 23:16:52.668: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/20/23 23:16:52.668
Feb 20 23:16:52.688: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/20/23 23:16:52.688
Feb 20 23:16:52.719: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:16:52.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3852" for this suite. 02/20/23 23:16:52.732
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":302,"skipped":5719,"failed":0}
------------------------------
• [0.207 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:16:52.544
    Feb 20 23:16:52.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename namespaces 02/20/23 23:16:52.547
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:52.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:52.638
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 02/20/23 23:16:52.653
    Feb 20 23:16:52.668: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/20/23 23:16:52.668
    Feb 20 23:16:52.688: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/20/23 23:16:52.688
    Feb 20 23:16:52.719: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:16:52.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3852" for this suite. 02/20/23 23:16:52.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:16:52.753
Feb 20 23:16:52.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename security-context-test 02/20/23 23:16:52.754
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:52.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:52.803
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Feb 20 23:16:52.895: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76" in namespace "security-context-test-2642" to be "Succeeded or Failed"
Feb 20 23:16:52.907: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039906ms
Feb 20 23:16:54.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023337837s
Feb 20 23:16:56.918: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023035831s
Feb 20 23:16:58.937: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041277762s
Feb 20 23:17:00.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023442218s
Feb 20 23:17:02.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023746079s
Feb 20 23:17:02.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 20 23:17:02.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2642" for this suite. 02/20/23 23:17:02.977
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":303,"skipped":5743,"failed":0}
------------------------------
• [SLOW TEST] [10.245 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:16:52.753
    Feb 20 23:16:52.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename security-context-test 02/20/23 23:16:52.754
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:16:52.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:16:52.803
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Feb 20 23:16:52.895: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76" in namespace "security-context-test-2642" to be "Succeeded or Failed"
    Feb 20 23:16:52.907: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.039906ms
    Feb 20 23:16:54.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023337837s
    Feb 20 23:16:56.918: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023035831s
    Feb 20 23:16:58.937: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041277762s
    Feb 20 23:17:00.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023442218s
    Feb 20 23:17:02.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023746079s
    Feb 20 23:17:02.919: INFO: Pod "alpine-nnp-false-57641199-f98c-4dcb-bcd6-70934612bf76" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 20 23:17:02.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2642" for this suite. 02/20/23 23:17:02.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:17:03
Feb 20 23:17:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename deployment 02/20/23 23:17:03.003
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:03.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:03.082
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 20 23:17:03.134: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 20 23:17:08.148: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/20/23 23:17:08.148
Feb 20 23:17:08.149: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/20/23 23:17:08.171
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 20 23:17:08.191: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4790  c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3 130736 1 2023-02-20 23:17:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-20 23:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d66eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 23:17:08.226: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4790  16291f32-7ab8-4670-92ae-907adf8b9793 130742 1 2023-02-20 23:17:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3 0xc00d66e207 0xc00d66e208}] [] [{kube-controller-manager Update apps/v1 2023-02-20 23:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d66e298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 20 23:17:08.226: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 20 23:17:08.226: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4790  1686ffe8-fed1-42ea-96a8-b39f90f0d38d 130739 1 2023-02-20 23:17:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3 0xc00d66e0d7 0xc00d66e0d8}] [] [{e2e.test Update apps/v1 2023-02-20 23:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 23:17:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 23:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00d66e198 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 20 23:17:08.254: INFO: Pod "test-cleanup-controller-hlz9x" is available:
&Pod{ObjectMeta:{test-cleanup-controller-hlz9x test-cleanup-controller- deployment-4790  135c0673-b1c5-4bed-8a4e-1da24287b248 130716 0 2023-02-20 23:17:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:2b08abe9ba1f8b2669252cb1cdf2edee4271fe212d451038c7c5358c7ce17d77 cni.projectcalico.org/podIP:172.30.181.219/32 cni.projectcalico.org/podIPs:172.30.181.219/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.219"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.181.219"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 1686ffe8-fed1-42ea-96a8-b39f90f0d38d 0xc00d66e757 0xc00d66e758}] [] [{calico Update v1 2023-02-20 23:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-20 23:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1686ffe8-fed1-42ea-96a8-b39f90f0d38d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-20 23:17:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 23:17:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbd5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbd5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-p7dhr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.219,StartTime:2023-02-20 23:17:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 23:17:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5b0b3c9badb5071bf62533c4f25db8254bc4423c02e3b82f75a813fcff3f8f10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 20 23:17:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4790" for this suite. 02/20/23 23:17:08.299
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":304,"skipped":5756,"failed":0}
------------------------------
• [SLOW TEST] [5.320 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:17:03
    Feb 20 23:17:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename deployment 02/20/23 23:17:03.003
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:03.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:03.082
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 20 23:17:03.134: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 20 23:17:08.148: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/20/23 23:17:08.148
    Feb 20 23:17:08.149: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/20/23 23:17:08.171
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 20 23:17:08.191: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4790  c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3 130736 1 2023-02-20 23:17:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-20 23:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d66eeb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 20 23:17:08.226: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4790  16291f32-7ab8-4670-92ae-907adf8b9793 130742 1 2023-02-20 23:17:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3 0xc00d66e207 0xc00d66e208}] [] [{kube-controller-manager Update apps/v1 2023-02-20 23:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d66e298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 23:17:08.226: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Feb 20 23:17:08.226: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4790  1686ffe8-fed1-42ea-96a8-b39f90f0d38d 130739 1 2023-02-20 23:17:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3 0xc00d66e0d7 0xc00d66e0d8}] [] [{e2e.test Update apps/v1 2023-02-20 23:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-20 23:17:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-20 23:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c7fe5143-2cd9-44f0-bd59-416b7ed7a8d3\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00d66e198 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 20 23:17:08.254: INFO: Pod "test-cleanup-controller-hlz9x" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-hlz9x test-cleanup-controller- deployment-4790  135c0673-b1c5-4bed-8a4e-1da24287b248 130716 0 2023-02-20 23:17:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:2b08abe9ba1f8b2669252cb1cdf2edee4271fe212d451038c7c5358c7ce17d77 cni.projectcalico.org/podIP:172.30.181.219/32 cni.projectcalico.org/podIPs:172.30.181.219/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.219"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.181.219"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 1686ffe8-fed1-42ea-96a8-b39f90f0d38d 0xc00d66e757 0xc00d66e758}] [] [{calico Update v1 2023-02-20 23:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-20 23:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1686ffe8-fed1-42ea-96a8-b39f90f0d38d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-20 23:17:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-20 23:17:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.181.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbd5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbd5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.8.38.66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-p7dhr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-20 23:17:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.8.38.66,PodIP:172.30.181.219,StartTime:2023-02-20 23:17:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-20 23:17:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5b0b3c9badb5071bf62533c4f25db8254bc4423c02e3b82f75a813fcff3f8f10,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.181.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 20 23:17:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4790" for this suite. 02/20/23 23:17:08.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:17:08.327
Feb 20 23:17:08.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename endpointslice 02/20/23 23:17:08.329
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:08.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:08.377
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 02/20/23 23:17:08.385
STEP: getting /apis/discovery.k8s.io 02/20/23 23:17:08.395
STEP: getting /apis/discovery.k8s.iov1 02/20/23 23:17:08.399
STEP: creating 02/20/23 23:17:08.402
STEP: getting 02/20/23 23:17:08.428
STEP: listing 02/20/23 23:17:08.438
STEP: watching 02/20/23 23:17:08.445
Feb 20 23:17:08.445: INFO: starting watch
STEP: cluster-wide listing 02/20/23 23:17:08.449
STEP: cluster-wide watching 02/20/23 23:17:08.457
Feb 20 23:17:08.457: INFO: starting watch
STEP: patching 02/20/23 23:17:08.46
STEP: updating 02/20/23 23:17:08.469
Feb 20 23:17:08.483: INFO: waiting for watch events with expected annotations
Feb 20 23:17:08.483: INFO: saw patched and updated annotations
STEP: deleting 02/20/23 23:17:08.483
STEP: deleting a collection 02/20/23 23:17:08.508
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 20 23:17:08.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2675" for this suite. 02/20/23 23:17:08.556
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":305,"skipped":5764,"failed":0}
------------------------------
• [0.253 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:17:08.327
    Feb 20 23:17:08.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename endpointslice 02/20/23 23:17:08.329
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:08.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:08.377
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 02/20/23 23:17:08.385
    STEP: getting /apis/discovery.k8s.io 02/20/23 23:17:08.395
    STEP: getting /apis/discovery.k8s.iov1 02/20/23 23:17:08.399
    STEP: creating 02/20/23 23:17:08.402
    STEP: getting 02/20/23 23:17:08.428
    STEP: listing 02/20/23 23:17:08.438
    STEP: watching 02/20/23 23:17:08.445
    Feb 20 23:17:08.445: INFO: starting watch
    STEP: cluster-wide listing 02/20/23 23:17:08.449
    STEP: cluster-wide watching 02/20/23 23:17:08.457
    Feb 20 23:17:08.457: INFO: starting watch
    STEP: patching 02/20/23 23:17:08.46
    STEP: updating 02/20/23 23:17:08.469
    Feb 20 23:17:08.483: INFO: waiting for watch events with expected annotations
    Feb 20 23:17:08.483: INFO: saw patched and updated annotations
    STEP: deleting 02/20/23 23:17:08.483
    STEP: deleting a collection 02/20/23 23:17:08.508
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 20 23:17:08.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2675" for this suite. 02/20/23 23:17:08.556
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:17:08.581
Feb 20 23:17:08.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 23:17:08.582
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:08.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:08.683
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/20/23 23:17:08.723
Feb 20 23:17:08.779: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7074" to be "running and ready"
Feb 20 23:17:08.821: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 41.151454ms
Feb 20 23:17:08.821: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:17:10.832: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05209743s
Feb 20 23:17:10.832: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:17:12.836: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.056095698s
Feb 20 23:17:12.836: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 20 23:17:12.836: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 02/20/23 23:17:12.848
Feb 20 23:17:12.882: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7074" to be "running and ready"
Feb 20 23:17:12.891: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.977738ms
Feb 20 23:17:12.891: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:17:14.901: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018888863s
Feb 20 23:17:14.901: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 20 23:17:14.901: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/20/23 23:17:14.91
Feb 20 23:17:14.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 23:17:14.936: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 23:17:16.939: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 23:17:16.949: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 23:17:18.937: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 23:17:18.947: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/20/23 23:17:18.947
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 20 23:17:19.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7074" for this suite. 02/20/23 23:17:19.022
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":306,"skipped":5768,"failed":0}
------------------------------
• [SLOW TEST] [10.476 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:17:08.581
    Feb 20 23:17:08.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/20/23 23:17:08.582
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:08.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:08.683
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/20/23 23:17:08.723
    Feb 20 23:17:08.779: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7074" to be "running and ready"
    Feb 20 23:17:08.821: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 41.151454ms
    Feb 20 23:17:08.821: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:17:10.832: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05209743s
    Feb 20 23:17:10.832: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:17:12.836: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.056095698s
    Feb 20 23:17:12.836: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 20 23:17:12.836: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 02/20/23 23:17:12.848
    Feb 20 23:17:12.882: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7074" to be "running and ready"
    Feb 20 23:17:12.891: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.977738ms
    Feb 20 23:17:12.891: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:17:14.901: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018888863s
    Feb 20 23:17:14.901: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 20 23:17:14.901: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/20/23 23:17:14.91
    Feb 20 23:17:14.927: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 20 23:17:14.936: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 20 23:17:16.939: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 20 23:17:16.949: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 20 23:17:18.937: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 20 23:17:18.947: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/20/23 23:17:18.947
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 20 23:17:19.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7074" for this suite. 02/20/23 23:17:19.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:17:19.058
Feb 20 23:17:19.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 23:17:19.06
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:19.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:19.109
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2961 02/20/23 23:17:19.12
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-2961 02/20/23 23:17:19.154
Feb 20 23:17:19.175: INFO: Found 0 stateful pods, waiting for 1
Feb 20 23:17:29.186: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/20/23 23:17:29.205
STEP: updating a scale subresource 02/20/23 23:17:29.214
STEP: verifying the statefulset Spec.Replicas was modified 02/20/23 23:17:29.223
STEP: Patch a scale subresource 02/20/23 23:17:29.228
STEP: verifying the statefulset Spec.Replicas was modified 02/20/23 23:17:29.237
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 23:17:29.244: INFO: Deleting all statefulset in ns statefulset-2961
Feb 20 23:17:29.252: INFO: Scaling statefulset ss to 0
Feb 20 23:17:39.313: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 23:17:39.319: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 23:17:39.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2961" for this suite. 02/20/23 23:17:39.355
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":307,"skipped":5781,"failed":0}
------------------------------
• [SLOW TEST] [20.321 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:17:19.058
    Feb 20 23:17:19.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 23:17:19.06
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:19.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:19.109
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2961 02/20/23 23:17:19.12
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-2961 02/20/23 23:17:19.154
    Feb 20 23:17:19.175: INFO: Found 0 stateful pods, waiting for 1
    Feb 20 23:17:29.186: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/20/23 23:17:29.205
    STEP: updating a scale subresource 02/20/23 23:17:29.214
    STEP: verifying the statefulset Spec.Replicas was modified 02/20/23 23:17:29.223
    STEP: Patch a scale subresource 02/20/23 23:17:29.228
    STEP: verifying the statefulset Spec.Replicas was modified 02/20/23 23:17:29.237
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 23:17:29.244: INFO: Deleting all statefulset in ns statefulset-2961
    Feb 20 23:17:29.252: INFO: Scaling statefulset ss to 0
    Feb 20 23:17:39.313: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 23:17:39.319: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 23:17:39.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2961" for this suite. 02/20/23 23:17:39.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:17:39.382
Feb 20 23:17:39.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-preemption 02/20/23 23:17:39.385
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:39.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:39.437
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 20 23:17:39.469: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 20 23:18:39.618: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 02/20/23 23:18:39.65
Feb 20 23:18:39.728: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 20 23:18:39.797: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 20 23:18:39.860: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 20 23:18:39.903: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 20 23:18:39.990: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 20 23:18:40.023: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/20/23 23:18:40.023
Feb 20 23:18:40.024: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:40.035: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.665739ms
Feb 20 23:18:42.047: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02243128s
Feb 20 23:18:44.056: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031957712s
Feb 20 23:18:46.047: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022415244s
Feb 20 23:18:48.048: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023492754s
Feb 20 23:18:50.068: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044014208s
Feb 20 23:18:52.045: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.020457625s
Feb 20 23:18:52.045: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 20 23:18:52.045: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:52.054: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.383272ms
Feb 20 23:18:52.054: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:18:52.054: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:52.064: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.339493ms
Feb 20 23:18:52.064: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:18:52.064: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:52.075: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.093402ms
Feb 20 23:18:52.075: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:18:52.075: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:52.084: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.473194ms
Feb 20 23:18:52.084: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:18:52.084: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:52.093: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.272543ms
Feb 20 23:18:52.093: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/20/23 23:18:52.093
Feb 20 23:18:52.129: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8991" to be "running"
Feb 20 23:18:52.140: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.591598ms
Feb 20 23:18:54.150: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020443945s
Feb 20 23:18:56.151: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021819147s
Feb 20 23:18:58.151: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.02136946s
Feb 20 23:18:58.151: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:18:58.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8991" for this suite. 02/20/23 23:18:58.228
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":308,"skipped":5799,"failed":0}
------------------------------
• [SLOW TEST] [78.991 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:17:39.382
    Feb 20 23:17:39.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-preemption 02/20/23 23:17:39.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:17:39.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:17:39.437
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 20 23:17:39.469: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 20 23:18:39.618: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 02/20/23 23:18:39.65
    Feb 20 23:18:39.728: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 20 23:18:39.797: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 20 23:18:39.860: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 20 23:18:39.903: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 20 23:18:39.990: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 20 23:18:40.023: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/20/23 23:18:40.023
    Feb 20 23:18:40.024: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:40.035: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.665739ms
    Feb 20 23:18:42.047: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02243128s
    Feb 20 23:18:44.056: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031957712s
    Feb 20 23:18:46.047: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022415244s
    Feb 20 23:18:48.048: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023492754s
    Feb 20 23:18:50.068: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044014208s
    Feb 20 23:18:52.045: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.020457625s
    Feb 20 23:18:52.045: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 20 23:18:52.045: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:52.054: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.383272ms
    Feb 20 23:18:52.054: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:18:52.054: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:52.064: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.339493ms
    Feb 20 23:18:52.064: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:18:52.064: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:52.075: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.093402ms
    Feb 20 23:18:52.075: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:18:52.075: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:52.084: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.473194ms
    Feb 20 23:18:52.084: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:18:52.084: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:52.093: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.272543ms
    Feb 20 23:18:52.093: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/20/23 23:18:52.093
    Feb 20 23:18:52.129: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8991" to be "running"
    Feb 20 23:18:52.140: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.591598ms
    Feb 20 23:18:54.150: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020443945s
    Feb 20 23:18:56.151: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021819147s
    Feb 20 23:18:58.151: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.02136946s
    Feb 20 23:18:58.151: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:18:58.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8991" for this suite. 02/20/23 23:18:58.228
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:18:58.375
Feb 20 23:18:58.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 23:18:58.377
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:18:58.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:18:58.436
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/20/23 23:18:58.443
Feb 20 23:18:58.493: INFO: Waiting up to 5m0s for pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5" in namespace "emptydir-9308" to be "Succeeded or Failed"
Feb 20 23:18:58.530: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.298888ms
Feb 20 23:19:00.541: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047540337s
Feb 20 23:19:02.566: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072840257s
Feb 20 23:19:04.548: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055171496s
STEP: Saw pod success 02/20/23 23:19:04.548
Feb 20 23:19:04.549: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5" satisfied condition "Succeeded or Failed"
Feb 20 23:19:04.558: INFO: Trying to get logs from node 10.8.38.66 pod pod-0425eb05-02fd-4393-a39e-449502c7a7b5 container test-container: <nil>
STEP: delete the pod 02/20/23 23:19:04.599
Feb 20 23:19:04.630: INFO: Waiting for pod pod-0425eb05-02fd-4393-a39e-449502c7a7b5 to disappear
Feb 20 23:19:04.640: INFO: Pod pod-0425eb05-02fd-4393-a39e-449502c7a7b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 23:19:04.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9308" for this suite. 02/20/23 23:19:04.654
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":309,"skipped":5806,"failed":0}
------------------------------
• [SLOW TEST] [6.300 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:18:58.375
    Feb 20 23:18:58.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 23:18:58.377
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:18:58.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:18:58.436
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/20/23 23:18:58.443
    Feb 20 23:18:58.493: INFO: Waiting up to 5m0s for pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5" in namespace "emptydir-9308" to be "Succeeded or Failed"
    Feb 20 23:18:58.530: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.298888ms
    Feb 20 23:19:00.541: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047540337s
    Feb 20 23:19:02.566: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072840257s
    Feb 20 23:19:04.548: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055171496s
    STEP: Saw pod success 02/20/23 23:19:04.548
    Feb 20 23:19:04.549: INFO: Pod "pod-0425eb05-02fd-4393-a39e-449502c7a7b5" satisfied condition "Succeeded or Failed"
    Feb 20 23:19:04.558: INFO: Trying to get logs from node 10.8.38.66 pod pod-0425eb05-02fd-4393-a39e-449502c7a7b5 container test-container: <nil>
    STEP: delete the pod 02/20/23 23:19:04.599
    Feb 20 23:19:04.630: INFO: Waiting for pod pod-0425eb05-02fd-4393-a39e-449502c7a7b5 to disappear
    Feb 20 23:19:04.640: INFO: Pod pod-0425eb05-02fd-4393-a39e-449502c7a7b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 23:19:04.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9308" for this suite. 02/20/23 23:19:04.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:19:04.681
Feb 20 23:19:04.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 23:19:04.684
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:19:04.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:19:04.821
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 02/20/23 23:19:04.828
STEP: submitting the pod to kubernetes 02/20/23 23:19:04.829
Feb 20 23:19:04.881: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" in namespace "pods-5116" to be "running and ready"
Feb 20 23:19:04.902: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Pending", Reason="", readiness=false. Elapsed: 20.959536ms
Feb 20 23:19:04.902: INFO: The phase of Pod pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:19:06.916: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=true. Elapsed: 2.035373736s
Feb 20 23:19:06.916: INFO: The phase of Pod pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97 is Running (Ready = true)
Feb 20 23:19:06.916: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/20/23 23:19:06.926
STEP: updating the pod 02/20/23 23:19:06.936
Feb 20 23:19:07.483: INFO: Successfully updated pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97"
Feb 20 23:19:07.486: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" in namespace "pods-5116" to be "terminated with reason DeadlineExceeded"
Feb 20 23:19:07.494: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=true. Elapsed: 8.596288ms
Feb 20 23:19:09.505: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=true. Elapsed: 2.019311772s
Feb 20 23:19:11.504: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=false. Elapsed: 4.018497135s
Feb 20 23:19:13.508: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.021855345s
Feb 20 23:19:13.508: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 23:19:13.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5116" for this suite. 02/20/23 23:19:13.527
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":310,"skipped":5811,"failed":0}
------------------------------
• [SLOW TEST] [8.871 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:19:04.681
    Feb 20 23:19:04.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 23:19:04.684
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:19:04.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:19:04.821
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 02/20/23 23:19:04.828
    STEP: submitting the pod to kubernetes 02/20/23 23:19:04.829
    Feb 20 23:19:04.881: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" in namespace "pods-5116" to be "running and ready"
    Feb 20 23:19:04.902: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Pending", Reason="", readiness=false. Elapsed: 20.959536ms
    Feb 20 23:19:04.902: INFO: The phase of Pod pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:19:06.916: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=true. Elapsed: 2.035373736s
    Feb 20 23:19:06.916: INFO: The phase of Pod pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97 is Running (Ready = true)
    Feb 20 23:19:06.916: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/20/23 23:19:06.926
    STEP: updating the pod 02/20/23 23:19:06.936
    Feb 20 23:19:07.483: INFO: Successfully updated pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97"
    Feb 20 23:19:07.486: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" in namespace "pods-5116" to be "terminated with reason DeadlineExceeded"
    Feb 20 23:19:07.494: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=true. Elapsed: 8.596288ms
    Feb 20 23:19:09.505: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=true. Elapsed: 2.019311772s
    Feb 20 23:19:11.504: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Running", Reason="", readiness=false. Elapsed: 4.018497135s
    Feb 20 23:19:13.508: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.021855345s
    Feb 20 23:19:13.508: INFO: Pod "pod-update-activedeadlineseconds-73f02e86-3bcc-48db-aeda-dafd44a67c97" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 23:19:13.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5116" for this suite. 02/20/23 23:19:13.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:19:13.569
Feb 20 23:19:13.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 23:19:13.57
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:19:13.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:19:13.619
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-a357092a-ca34-469a-ba27-7169c04266d6 02/20/23 23:19:13.629
STEP: Creating a pod to test consume secrets 02/20/23 23:19:13.661
Feb 20 23:19:13.724: INFO: Waiting up to 5m0s for pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49" in namespace "secrets-5611" to be "Succeeded or Failed"
Feb 20 23:19:13.733: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.523526ms
Feb 20 23:19:15.743: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019227047s
Feb 20 23:19:17.744: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020612159s
Feb 20 23:19:19.745: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021568552s
STEP: Saw pod success 02/20/23 23:19:19.745
Feb 20 23:19:19.746: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49" satisfied condition "Succeeded or Failed"
Feb 20 23:19:19.754: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 23:19:19.781
Feb 20 23:19:19.804: INFO: Waiting for pod pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49 to disappear
Feb 20 23:19:19.811: INFO: Pod pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 23:19:19.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5611" for this suite. 02/20/23 23:19:19.85
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":311,"skipped":5862,"failed":0}
------------------------------
• [SLOW TEST] [6.317 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:19:13.569
    Feb 20 23:19:13.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 23:19:13.57
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:19:13.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:19:13.619
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-a357092a-ca34-469a-ba27-7169c04266d6 02/20/23 23:19:13.629
    STEP: Creating a pod to test consume secrets 02/20/23 23:19:13.661
    Feb 20 23:19:13.724: INFO: Waiting up to 5m0s for pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49" in namespace "secrets-5611" to be "Succeeded or Failed"
    Feb 20 23:19:13.733: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.523526ms
    Feb 20 23:19:15.743: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019227047s
    Feb 20 23:19:17.744: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020612159s
    Feb 20 23:19:19.745: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021568552s
    STEP: Saw pod success 02/20/23 23:19:19.745
    Feb 20 23:19:19.746: INFO: Pod "pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49" satisfied condition "Succeeded or Failed"
    Feb 20 23:19:19.754: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 23:19:19.781
    Feb 20 23:19:19.804: INFO: Waiting for pod pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49 to disappear
    Feb 20 23:19:19.811: INFO: Pod pod-secrets-27608223-bfa4-4d2d-a437-66b1c243ed49 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 23:19:19.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5611" for this suite. 02/20/23 23:19:19.85
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:19:19.887
Feb 20 23:19:19.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename cronjob 02/20/23 23:19:19.888
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:19:19.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:19:19.978
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/20/23 23:19:19.984
STEP: Ensuring more than one job is running at a time 02/20/23 23:19:19.996
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/20/23 23:21:02.006
STEP: Removing cronjob 02/20/23 23:21:02.016
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 20 23:21:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7566" for this suite. 02/20/23 23:21:02.047
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":312,"skipped":5863,"failed":0}
------------------------------
• [SLOW TEST] [102.184 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:19:19.887
    Feb 20 23:19:19.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename cronjob 02/20/23 23:19:19.888
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:19:19.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:19:19.978
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/20/23 23:19:19.984
    STEP: Ensuring more than one job is running at a time 02/20/23 23:19:19.996
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/20/23 23:21:02.006
    STEP: Removing cronjob 02/20/23 23:21:02.016
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 20 23:21:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7566" for this suite. 02/20/23 23:21:02.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:02.078
Feb 20 23:21:02.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename gc 02/20/23 23:21:02.083
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:02.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:02.151
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 20 23:21:02.362: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"437ed843-c6ed-41fd-994d-55b697ed733d", Controller:(*bool)(0xc00aea8d32), BlockOwnerDeletion:(*bool)(0xc00aea8d33)}}
Feb 20 23:21:02.377: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"115df762-56c6-474d-a11e-bb11a39eead4", Controller:(*bool)(0xc00aea9032), BlockOwnerDeletion:(*bool)(0xc00aea9033)}}
Feb 20 23:21:02.390: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e7d6d172-5510-47b9-a68c-475e76a5a842", Controller:(*bool)(0xc0001ab202), BlockOwnerDeletion:(*bool)(0xc0001ab203)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 20 23:21:07.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7915" for this suite. 02/20/23 23:21:07.432
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":313,"skipped":5916,"failed":0}
------------------------------
• [SLOW TEST] [5.375 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:02.078
    Feb 20 23:21:02.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename gc 02/20/23 23:21:02.083
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:02.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:02.151
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 20 23:21:02.362: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"437ed843-c6ed-41fd-994d-55b697ed733d", Controller:(*bool)(0xc00aea8d32), BlockOwnerDeletion:(*bool)(0xc00aea8d33)}}
    Feb 20 23:21:02.377: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"115df762-56c6-474d-a11e-bb11a39eead4", Controller:(*bool)(0xc00aea9032), BlockOwnerDeletion:(*bool)(0xc00aea9033)}}
    Feb 20 23:21:02.390: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e7d6d172-5510-47b9-a68c-475e76a5a842", Controller:(*bool)(0xc0001ab202), BlockOwnerDeletion:(*bool)(0xc0001ab203)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 20 23:21:07.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7915" for this suite. 02/20/23 23:21:07.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:07.459
Feb 20 23:21:07.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 23:21:07.461
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:07.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:07.523
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 02/20/23 23:21:07.531
STEP: Getting a ResourceQuota 02/20/23 23:21:07.541
STEP: Listing all ResourceQuotas with LabelSelector 02/20/23 23:21:07.55
STEP: Patching the ResourceQuota 02/20/23 23:21:07.56
STEP: Deleting a Collection of ResourceQuotas 02/20/23 23:21:07.58
STEP: Verifying the deleted ResourceQuota 02/20/23 23:21:07.597
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 23:21:07.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9864" for this suite. 02/20/23 23:21:07.617
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":314,"skipped":5924,"failed":0}
------------------------------
• [0.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:07.459
    Feb 20 23:21:07.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 23:21:07.461
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:07.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:07.523
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 02/20/23 23:21:07.531
    STEP: Getting a ResourceQuota 02/20/23 23:21:07.541
    STEP: Listing all ResourceQuotas with LabelSelector 02/20/23 23:21:07.55
    STEP: Patching the ResourceQuota 02/20/23 23:21:07.56
    STEP: Deleting a Collection of ResourceQuotas 02/20/23 23:21:07.58
    STEP: Verifying the deleted ResourceQuota 02/20/23 23:21:07.597
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 23:21:07.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9864" for this suite. 02/20/23 23:21:07.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:07.645
Feb 20 23:21:07.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 23:21:07.648
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:07.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:07.703
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 02/20/23 23:21:07.712
Feb 20 23:21:07.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8720 create -f -'
Feb 20 23:21:08.372: INFO: stderr: ""
Feb 20 23:21:08.372: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/20/23 23:21:08.372
Feb 20 23:21:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8720 diff -f -'
Feb 20 23:21:08.861: INFO: rc: 1
Feb 20 23:21:08.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8720 delete -f -'
Feb 20 23:21:08.983: INFO: stderr: ""
Feb 20 23:21:08.983: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 23:21:08.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8720" for this suite. 02/20/23 23:21:08.996
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":315,"skipped":5930,"failed":0}
------------------------------
• [1.370 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:07.645
    Feb 20 23:21:07.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 23:21:07.648
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:07.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:07.703
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 02/20/23 23:21:07.712
    Feb 20 23:21:07.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8720 create -f -'
    Feb 20 23:21:08.372: INFO: stderr: ""
    Feb 20 23:21:08.372: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/20/23 23:21:08.372
    Feb 20 23:21:08.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8720 diff -f -'
    Feb 20 23:21:08.861: INFO: rc: 1
    Feb 20 23:21:08.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-8720 delete -f -'
    Feb 20 23:21:08.983: INFO: stderr: ""
    Feb 20 23:21:08.983: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 23:21:08.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8720" for this suite. 02/20/23 23:21:08.996
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:09.015
Feb 20 23:21:09.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename ingressclass 02/20/23 23:21:09.019
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.116
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/20/23 23:21:09.133
STEP: getting /apis/networking.k8s.io 02/20/23 23:21:09.139
STEP: getting /apis/networking.k8s.iov1 02/20/23 23:21:09.145
STEP: creating 02/20/23 23:21:09.158
STEP: getting 02/20/23 23:21:09.193
STEP: listing 02/20/23 23:21:09.202
STEP: watching 02/20/23 23:21:09.211
Feb 20 23:21:09.211: INFO: starting watch
STEP: patching 02/20/23 23:21:09.214
STEP: updating 02/20/23 23:21:09.227
Feb 20 23:21:09.238: INFO: waiting for watch events with expected annotations
Feb 20 23:21:09.238: INFO: saw patched and updated annotations
STEP: deleting 02/20/23 23:21:09.243
STEP: deleting a collection 02/20/23 23:21:09.281
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Feb 20 23:21:09.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3216" for this suite. 02/20/23 23:21:09.344
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":316,"skipped":5930,"failed":0}
------------------------------
• [0.351 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:09.015
    Feb 20 23:21:09.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename ingressclass 02/20/23 23:21:09.019
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.116
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/20/23 23:21:09.133
    STEP: getting /apis/networking.k8s.io 02/20/23 23:21:09.139
    STEP: getting /apis/networking.k8s.iov1 02/20/23 23:21:09.145
    STEP: creating 02/20/23 23:21:09.158
    STEP: getting 02/20/23 23:21:09.193
    STEP: listing 02/20/23 23:21:09.202
    STEP: watching 02/20/23 23:21:09.211
    Feb 20 23:21:09.211: INFO: starting watch
    STEP: patching 02/20/23 23:21:09.214
    STEP: updating 02/20/23 23:21:09.227
    Feb 20 23:21:09.238: INFO: waiting for watch events with expected annotations
    Feb 20 23:21:09.238: INFO: saw patched and updated annotations
    STEP: deleting 02/20/23 23:21:09.243
    STEP: deleting a collection 02/20/23 23:21:09.281
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Feb 20 23:21:09.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-3216" for this suite. 02/20/23 23:21:09.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:09.369
Feb 20 23:21:09.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 23:21:09.373
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.423
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/20/23 23:21:09.429
STEP: submitting the pod to kubernetes 02/20/23 23:21:09.43
STEP: verifying QOS class is set on the pod 02/20/23 23:21:09.525
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Feb 20 23:21:09.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3019" for this suite. 02/20/23 23:21:09.578
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":317,"skipped":5945,"failed":0}
------------------------------
• [0.233 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:09.369
    Feb 20 23:21:09.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 23:21:09.373
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.423
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/20/23 23:21:09.429
    STEP: submitting the pod to kubernetes 02/20/23 23:21:09.43
    STEP: verifying QOS class is set on the pod 02/20/23 23:21:09.525
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Feb 20 23:21:09.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3019" for this suite. 02/20/23 23:21:09.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:09.602
Feb 20 23:21:09.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename lease-test 02/20/23 23:21:09.604
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.672
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Feb 20 23:21:09.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1623" for this suite. 02/20/23 23:21:09.856
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":318,"skipped":5957,"failed":0}
------------------------------
• [0.279 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:09.602
    Feb 20 23:21:09.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename lease-test 02/20/23 23:21:09.604
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.672
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Feb 20 23:21:09.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-1623" for this suite. 02/20/23 23:21:09.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:09.885
Feb 20 23:21:09.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 23:21:09.887
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.953
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Feb 20 23:21:10.070: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/20/23 23:21:10.078
Feb 20 23:21:10.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:10.087: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/20/23 23:21:10.088
Feb 20 23:21:10.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:10.143: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:11.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:11.165: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:12.154: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 23:21:12.154: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/20/23 23:21:12.16
Feb 20 23:21:12.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:12.216: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/20/23 23:21:12.216
Feb 20 23:21:12.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:12.242: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:13.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:13.253: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:14.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:14.260: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:15.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:15.254: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:16.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:16.253: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:17.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:17.252: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:21:18.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 23:21:18.253: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:21:18.265
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6463, will wait for the garbage collector to delete the pods 02/20/23 23:21:18.266
Feb 20 23:21:18.333: INFO: Deleting DaemonSet.extensions daemon-set took: 9.92298ms
Feb 20 23:21:18.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.831075ms
Feb 20 23:21:21.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:21:21.548: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 20 23:21:21.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133519"},"items":null}

Feb 20 23:21:21.566: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133519"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:21:21.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6463" for this suite. 02/20/23 23:21:21.682
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":319,"skipped":5973,"failed":0}
------------------------------
• [SLOW TEST] [11.817 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:09.885
    Feb 20 23:21:09.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 23:21:09.887
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:09.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:09.953
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Feb 20 23:21:10.070: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/20/23 23:21:10.078
    Feb 20 23:21:10.087: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:10.087: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/20/23 23:21:10.088
    Feb 20 23:21:10.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:10.143: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:11.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:11.165: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:12.154: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 23:21:12.154: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/20/23 23:21:12.16
    Feb 20 23:21:12.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:12.216: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/20/23 23:21:12.216
    Feb 20 23:21:12.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:12.242: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:13.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:13.253: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:14.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:14.260: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:15.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:15.254: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:16.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:16.253: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:17.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:17.252: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:21:18.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 23:21:18.253: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:21:18.265
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6463, will wait for the garbage collector to delete the pods 02/20/23 23:21:18.266
    Feb 20 23:21:18.333: INFO: Deleting DaemonSet.extensions daemon-set took: 9.92298ms
    Feb 20 23:21:18.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.831075ms
    Feb 20 23:21:21.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:21:21.548: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 20 23:21:21.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133519"},"items":null}

    Feb 20 23:21:21.566: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133519"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:21:21.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6463" for this suite. 02/20/23 23:21:21.682
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:21.702
Feb 20 23:21:21.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename runtimeclass 02/20/23 23:21:21.703
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:21.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:21.758
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/20/23 23:21:21.766
STEP: getting /apis/node.k8s.io 02/20/23 23:21:21.776
STEP: getting /apis/node.k8s.io/v1 02/20/23 23:21:21.779
STEP: creating 02/20/23 23:21:21.784
STEP: watching 02/20/23 23:21:21.824
Feb 20 23:21:21.824: INFO: starting watch
STEP: getting 02/20/23 23:21:21.837
STEP: listing 02/20/23 23:21:21.845
STEP: patching 02/20/23 23:21:21.854
STEP: updating 02/20/23 23:21:21.865
Feb 20 23:21:21.883: INFO: waiting for watch events with expected annotations
STEP: deleting 02/20/23 23:21:21.883
STEP: deleting a collection 02/20/23 23:21:21.918
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 20 23:21:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4612" for this suite. 02/20/23 23:21:21.979
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":320,"skipped":5973,"failed":0}
------------------------------
• [0.302 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:21.702
    Feb 20 23:21:21.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename runtimeclass 02/20/23 23:21:21.703
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:21.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:21.758
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/20/23 23:21:21.766
    STEP: getting /apis/node.k8s.io 02/20/23 23:21:21.776
    STEP: getting /apis/node.k8s.io/v1 02/20/23 23:21:21.779
    STEP: creating 02/20/23 23:21:21.784
    STEP: watching 02/20/23 23:21:21.824
    Feb 20 23:21:21.824: INFO: starting watch
    STEP: getting 02/20/23 23:21:21.837
    STEP: listing 02/20/23 23:21:21.845
    STEP: patching 02/20/23 23:21:21.854
    STEP: updating 02/20/23 23:21:21.865
    Feb 20 23:21:21.883: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/20/23 23:21:21.883
    STEP: deleting a collection 02/20/23 23:21:21.918
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 20 23:21:21.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4612" for this suite. 02/20/23 23:21:21.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:21:22.012
Feb 20 23:21:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename cronjob 02/20/23 23:21:22.013
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:22.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:22.059
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/20/23 23:21:22.065
W0220 23:21:22.080093      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 02/20/23 23:21:22.08
STEP: Ensuring no job exists by listing jobs explicitly 02/20/23 23:26:22.102
STEP: Removing cronjob 02/20/23 23:26:22.108
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 20 23:26:22.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5597" for this suite. 02/20/23 23:26:22.138
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":321,"skipped":6043,"failed":0}
------------------------------
• [SLOW TEST] [300.187 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:21:22.012
    Feb 20 23:21:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename cronjob 02/20/23 23:21:22.013
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:21:22.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:21:22.059
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/20/23 23:21:22.065
    W0220 23:21:22.080093      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 02/20/23 23:21:22.08
    STEP: Ensuring no job exists by listing jobs explicitly 02/20/23 23:26:22.102
    STEP: Removing cronjob 02/20/23 23:26:22.108
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 20 23:26:22.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5597" for this suite. 02/20/23 23:26:22.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:22.207
Feb 20 23:26:22.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 23:26:22.209
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:22.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:22.294
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 23:26:22.451
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:26:22.592
STEP: Deploying the webhook pod 02/20/23 23:26:22.619
STEP: Wait for the deployment to be ready 02/20/23 23:26:22.645
Feb 20 23:26:22.692: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/20/23 23:26:24.723
STEP: Verifying the service has paired with the endpoint 02/20/23 23:26:24.767
Feb 20 23:26:25.777: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 02/20/23 23:26:25.907
STEP: Creating a configMap that should be mutated 02/20/23 23:26:25.947
STEP: Deleting the collection of validation webhooks 02/20/23 23:26:26.018
STEP: Creating a configMap that should not be mutated 02/20/23 23:26:26.154
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:26:26.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4829" for this suite. 02/20/23 23:26:26.198
STEP: Destroying namespace "webhook-4829-markers" for this suite. 02/20/23 23:26:26.218
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":322,"skipped":6069,"failed":0}
------------------------------
• [4.143 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:22.207
    Feb 20 23:26:22.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 23:26:22.209
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:22.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:22.294
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 23:26:22.451
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:26:22.592
    STEP: Deploying the webhook pod 02/20/23 23:26:22.619
    STEP: Wait for the deployment to be ready 02/20/23 23:26:22.645
    Feb 20 23:26:22.692: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/20/23 23:26:24.723
    STEP: Verifying the service has paired with the endpoint 02/20/23 23:26:24.767
    Feb 20 23:26:25.777: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 02/20/23 23:26:25.907
    STEP: Creating a configMap that should be mutated 02/20/23 23:26:25.947
    STEP: Deleting the collection of validation webhooks 02/20/23 23:26:26.018
    STEP: Creating a configMap that should not be mutated 02/20/23 23:26:26.154
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:26:26.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4829" for this suite. 02/20/23 23:26:26.198
    STEP: Destroying namespace "webhook-4829-markers" for this suite. 02/20/23 23:26:26.218
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:26.358
Feb 20 23:26:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 23:26:26.359
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:26.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:26.46
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 23:26:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5396" for this suite. 02/20/23 23:26:26.614
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":323,"skipped":6092,"failed":0}
------------------------------
• [0.305 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:26.358
    Feb 20 23:26:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 23:26:26.359
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:26.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:26.46
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 23:26:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5396" for this suite. 02/20/23 23:26:26.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:26.675
Feb 20 23:26:26.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename namespaces 02/20/23 23:26:26.677
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:26.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:26.74
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 02/20/23 23:26:26.747
STEP: patching the Namespace 02/20/23 23:26:26.841
STEP: get the Namespace and ensuring it has the label 02/20/23 23:26:26.921
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:26:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1130" for this suite. 02/20/23 23:26:26.966
STEP: Destroying namespace "nspatchtest-ce2c465b-9649-4386-ab91-bbc56726ece0-5068" for this suite. 02/20/23 23:26:26.986
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":324,"skipped":6105,"failed":0}
------------------------------
• [0.368 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:26.675
    Feb 20 23:26:26.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename namespaces 02/20/23 23:26:26.677
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:26.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:26.74
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 02/20/23 23:26:26.747
    STEP: patching the Namespace 02/20/23 23:26:26.841
    STEP: get the Namespace and ensuring it has the label 02/20/23 23:26:26.921
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:26:26.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1130" for this suite. 02/20/23 23:26:26.966
    STEP: Destroying namespace "nspatchtest-ce2c465b-9649-4386-ab91-bbc56726ece0-5068" for this suite. 02/20/23 23:26:26.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:27.046
Feb 20 23:26:27.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubelet-test 02/20/23 23:26:27.049
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:27.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:27.146
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 20 23:26:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7125" for this suite. 02/20/23 23:26:27.351
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":325,"skipped":6114,"failed":0}
------------------------------
• [0.332 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:27.046
    Feb 20 23:26:27.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubelet-test 02/20/23 23:26:27.049
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:27.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:27.146
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 20 23:26:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7125" for this suite. 02/20/23 23:26:27.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:27.38
Feb 20 23:26:27.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename limitrange 02/20/23 23:26:27.382
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:27.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:27.434
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 02/20/23 23:26:27.44
STEP: Setting up watch 02/20/23 23:26:27.441
STEP: Submitting a LimitRange 02/20/23 23:26:27.56
STEP: Verifying LimitRange creation was observed 02/20/23 23:26:27.596
STEP: Fetching the LimitRange to ensure it has proper values 02/20/23 23:26:27.596
Feb 20 23:26:27.615: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 20 23:26:27.615: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/20/23 23:26:27.615
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/20/23 23:26:27.668
Feb 20 23:26:27.698: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 20 23:26:27.698: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/20/23 23:26:27.698
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/20/23 23:26:27.774
Feb 20 23:26:27.803: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 20 23:26:27.803: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/20/23 23:26:27.803
STEP: Failing to create a Pod with more than max resources 02/20/23 23:26:27.847
STEP: Updating a LimitRange 02/20/23 23:26:27.862
STEP: Verifying LimitRange updating is effective 02/20/23 23:26:27.881
STEP: Creating a Pod with less than former min resources 02/20/23 23:26:29.895
STEP: Failing to create a Pod with more than max resources 02/20/23 23:26:29.955
STEP: Deleting a LimitRange 02/20/23 23:26:29.982
STEP: Verifying the LimitRange was deleted 02/20/23 23:26:30.027
Feb 20 23:26:35.042: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/20/23 23:26:35.042
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Feb 20 23:26:35.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9365" for this suite. 02/20/23 23:26:35.097
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":326,"skipped":6131,"failed":0}
------------------------------
• [SLOW TEST] [7.737 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:27.38
    Feb 20 23:26:27.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename limitrange 02/20/23 23:26:27.382
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:27.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:27.434
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 02/20/23 23:26:27.44
    STEP: Setting up watch 02/20/23 23:26:27.441
    STEP: Submitting a LimitRange 02/20/23 23:26:27.56
    STEP: Verifying LimitRange creation was observed 02/20/23 23:26:27.596
    STEP: Fetching the LimitRange to ensure it has proper values 02/20/23 23:26:27.596
    Feb 20 23:26:27.615: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 20 23:26:27.615: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/20/23 23:26:27.615
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/20/23 23:26:27.668
    Feb 20 23:26:27.698: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 20 23:26:27.698: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/20/23 23:26:27.698
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/20/23 23:26:27.774
    Feb 20 23:26:27.803: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 20 23:26:27.803: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/20/23 23:26:27.803
    STEP: Failing to create a Pod with more than max resources 02/20/23 23:26:27.847
    STEP: Updating a LimitRange 02/20/23 23:26:27.862
    STEP: Verifying LimitRange updating is effective 02/20/23 23:26:27.881
    STEP: Creating a Pod with less than former min resources 02/20/23 23:26:29.895
    STEP: Failing to create a Pod with more than max resources 02/20/23 23:26:29.955
    STEP: Deleting a LimitRange 02/20/23 23:26:29.982
    STEP: Verifying the LimitRange was deleted 02/20/23 23:26:30.027
    Feb 20 23:26:35.042: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/20/23 23:26:35.042
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Feb 20 23:26:35.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-9365" for this suite. 02/20/23 23:26:35.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:35.121
Feb 20 23:26:35.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pods 02/20/23 23:26:35.123
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:35.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:35.181
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 02/20/23 23:26:35.19
STEP: setting up watch 02/20/23 23:26:35.19
STEP: submitting the pod to kubernetes 02/20/23 23:26:35.306
STEP: verifying the pod is in kubernetes 02/20/23 23:26:35.348
STEP: verifying pod creation was observed 02/20/23 23:26:35.357
Feb 20 23:26:35.357: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff" in namespace "pods-7203" to be "running"
Feb 20 23:26:35.365: INFO: Pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.991384ms
Feb 20 23:26:37.377: INFO: Pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff": Phase="Running", Reason="", readiness=true. Elapsed: 2.019785231s
Feb 20 23:26:37.377: INFO: Pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff" satisfied condition "running"
STEP: deleting the pod gracefully 02/20/23 23:26:37.403
STEP: verifying pod deletion was observed 02/20/23 23:26:37.423
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 20 23:26:39.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7203" for this suite. 02/20/23 23:26:40.007
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":327,"skipped":6176,"failed":0}
------------------------------
• [4.945 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:35.121
    Feb 20 23:26:35.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pods 02/20/23 23:26:35.123
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:35.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:35.181
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 02/20/23 23:26:35.19
    STEP: setting up watch 02/20/23 23:26:35.19
    STEP: submitting the pod to kubernetes 02/20/23 23:26:35.306
    STEP: verifying the pod is in kubernetes 02/20/23 23:26:35.348
    STEP: verifying pod creation was observed 02/20/23 23:26:35.357
    Feb 20 23:26:35.357: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff" in namespace "pods-7203" to be "running"
    Feb 20 23:26:35.365: INFO: Pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.991384ms
    Feb 20 23:26:37.377: INFO: Pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff": Phase="Running", Reason="", readiness=true. Elapsed: 2.019785231s
    Feb 20 23:26:37.377: INFO: Pod "pod-submit-remove-b3b4755d-dddd-4331-82bc-5a9d99812cff" satisfied condition "running"
    STEP: deleting the pod gracefully 02/20/23 23:26:37.403
    STEP: verifying pod deletion was observed 02/20/23 23:26:37.423
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 20 23:26:39.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7203" for this suite. 02/20/23 23:26:40.007
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:26:40.07
Feb 20 23:26:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename statefulset 02/20/23 23:26:40.073
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:40.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:40.129
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2706 02/20/23 23:26:40.138
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 02/20/23 23:26:40.154
W0220 23:26:40.168490      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 23:26:40.176: INFO: Found 0 stateful pods, waiting for 3
Feb 20 23:26:50.189: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 23:26:50.189: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 23:26:50.189: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/20/23 23:26:50.212
Feb 20 23:26:50.239: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/20/23 23:26:50.239
STEP: Not applying an update when the partition is greater than the number of replicas 02/20/23 23:27:00.276
STEP: Performing a canary update 02/20/23 23:27:00.277
Feb 20 23:27:00.304: INFO: Updating stateful set ss2
Feb 20 23:27:00.319: INFO: Waiting for Pod statefulset-2706/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 02/20/23 23:27:10.352
Feb 20 23:27:10.454: INFO: Found 1 stateful pods, waiting for 3
Feb 20 23:27:20.471: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 23:27:20.471: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 23:27:20.471: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/20/23 23:27:20.487
Feb 20 23:27:20.514: INFO: Updating stateful set ss2
Feb 20 23:27:20.531: INFO: Waiting for Pod statefulset-2706/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 20 23:27:30.579: INFO: Updating stateful set ss2
Feb 20 23:27:30.594: INFO: Waiting for StatefulSet statefulset-2706/ss2 to complete update
Feb 20 23:27:30.594: INFO: Waiting for Pod statefulset-2706/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 20 23:27:40.613: INFO: Deleting all statefulset in ns statefulset-2706
Feb 20 23:27:40.620: INFO: Scaling statefulset ss2 to 0
Feb 20 23:27:50.661: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 23:27:50.670: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 20 23:27:50.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2706" for this suite. 02/20/23 23:27:50.708
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":328,"skipped":6179,"failed":0}
------------------------------
• [SLOW TEST] [70.658 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:26:40.07
    Feb 20 23:26:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename statefulset 02/20/23 23:26:40.073
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:26:40.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:26:40.129
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2706 02/20/23 23:26:40.138
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 02/20/23 23:26:40.154
    W0220 23:26:40.168490      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 23:26:40.176: INFO: Found 0 stateful pods, waiting for 3
    Feb 20 23:26:50.189: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 23:26:50.189: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 23:26:50.189: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/20/23 23:26:50.212
    Feb 20 23:26:50.239: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/20/23 23:26:50.239
    STEP: Not applying an update when the partition is greater than the number of replicas 02/20/23 23:27:00.276
    STEP: Performing a canary update 02/20/23 23:27:00.277
    Feb 20 23:27:00.304: INFO: Updating stateful set ss2
    Feb 20 23:27:00.319: INFO: Waiting for Pod statefulset-2706/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 02/20/23 23:27:10.352
    Feb 20 23:27:10.454: INFO: Found 1 stateful pods, waiting for 3
    Feb 20 23:27:20.471: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 23:27:20.471: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 20 23:27:20.471: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/20/23 23:27:20.487
    Feb 20 23:27:20.514: INFO: Updating stateful set ss2
    Feb 20 23:27:20.531: INFO: Waiting for Pod statefulset-2706/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 20 23:27:30.579: INFO: Updating stateful set ss2
    Feb 20 23:27:30.594: INFO: Waiting for StatefulSet statefulset-2706/ss2 to complete update
    Feb 20 23:27:30.594: INFO: Waiting for Pod statefulset-2706/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 20 23:27:40.613: INFO: Deleting all statefulset in ns statefulset-2706
    Feb 20 23:27:40.620: INFO: Scaling statefulset ss2 to 0
    Feb 20 23:27:50.661: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 20 23:27:50.670: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 20 23:27:50.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2706" for this suite. 02/20/23 23:27:50.708
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:27:50.729
Feb 20 23:27:50.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename taint-multiple-pods 02/20/23 23:27:50.733
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:27:50.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:27:50.785
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Feb 20 23:27:50.794: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 20 23:28:50.920: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Feb 20 23:28:50.939: INFO: Starting informer...
STEP: Starting pods... 02/20/23 23:28:50.939
Feb 20 23:28:51.201: INFO: Pod1 is running on 10.8.38.66. Tainting Node
Feb 20 23:28:51.440: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5448" to be "running"
Feb 20 23:28:51.448: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.283281ms
Feb 20 23:28:53.460: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02061901s
Feb 20 23:28:53.460: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 20 23:28:53.460: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5448" to be "running"
Feb 20 23:28:53.470: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.568789ms
Feb 20 23:28:53.470: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 20 23:28:53.470: INFO: Pod2 is running on 10.8.38.66. Tainting Node
STEP: Trying to apply a taint on the Node 02/20/23 23:28:53.47
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:28:53.513
STEP: Waiting for Pod1 and Pod2 to be deleted 02/20/23 23:28:53.526
Feb 20 23:29:00.764: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 20 23:29:19.859: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:29:19.89
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:29:19.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5448" for this suite. 02/20/23 23:29:19.923
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":329,"skipped":6183,"failed":0}
------------------------------
• [SLOW TEST] [89.244 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:27:50.729
    Feb 20 23:27:50.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename taint-multiple-pods 02/20/23 23:27:50.733
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:27:50.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:27:50.785
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Feb 20 23:27:50.794: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 20 23:28:50.920: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Feb 20 23:28:50.939: INFO: Starting informer...
    STEP: Starting pods... 02/20/23 23:28:50.939
    Feb 20 23:28:51.201: INFO: Pod1 is running on 10.8.38.66. Tainting Node
    Feb 20 23:28:51.440: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5448" to be "running"
    Feb 20 23:28:51.448: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.283281ms
    Feb 20 23:28:53.460: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02061901s
    Feb 20 23:28:53.460: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 20 23:28:53.460: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5448" to be "running"
    Feb 20 23:28:53.470: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.568789ms
    Feb 20 23:28:53.470: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 20 23:28:53.470: INFO: Pod2 is running on 10.8.38.66. Tainting Node
    STEP: Trying to apply a taint on the Node 02/20/23 23:28:53.47
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:28:53.513
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/20/23 23:28:53.526
    Feb 20 23:29:00.764: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 20 23:29:19.859: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:29:19.89
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:29:19.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5448" for this suite. 02/20/23 23:29:19.923
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:29:19.974
Feb 20 23:29:19.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 23:29:19.976
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:29:20.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:29:20.076
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-f2804c31-fc86-4498-91f7-8c2fe15e8a35 02/20/23 23:29:20.092
STEP: Creating a pod to test consume configMaps 02/20/23 23:29:20.11
Feb 20 23:29:20.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f" in namespace "configmap-3690" to be "Succeeded or Failed"
Feb 20 23:29:20.210: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.03813ms
Feb 20 23:29:22.224: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022830902s
Feb 20 23:29:24.254: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Running", Reason="", readiness=false. Elapsed: 4.053354254s
Feb 20 23:29:26.225: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023638376s
STEP: Saw pod success 02/20/23 23:29:26.225
Feb 20 23:29:26.225: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f" satisfied condition "Succeeded or Failed"
Feb 20 23:29:26.258: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f container agnhost-container: <nil>
STEP: delete the pod 02/20/23 23:29:26.295
Feb 20 23:29:26.327: INFO: Waiting for pod pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f to disappear
Feb 20 23:29:26.344: INFO: Pod pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 23:29:26.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3690" for this suite. 02/20/23 23:29:26.359
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":330,"skipped":6186,"failed":0}
------------------------------
• [SLOW TEST] [6.406 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:29:19.974
    Feb 20 23:29:19.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 23:29:19.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:29:20.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:29:20.076
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-f2804c31-fc86-4498-91f7-8c2fe15e8a35 02/20/23 23:29:20.092
    STEP: Creating a pod to test consume configMaps 02/20/23 23:29:20.11
    Feb 20 23:29:20.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f" in namespace "configmap-3690" to be "Succeeded or Failed"
    Feb 20 23:29:20.210: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.03813ms
    Feb 20 23:29:22.224: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022830902s
    Feb 20 23:29:24.254: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Running", Reason="", readiness=false. Elapsed: 4.053354254s
    Feb 20 23:29:26.225: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023638376s
    STEP: Saw pod success 02/20/23 23:29:26.225
    Feb 20 23:29:26.225: INFO: Pod "pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f" satisfied condition "Succeeded or Failed"
    Feb 20 23:29:26.258: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f container agnhost-container: <nil>
    STEP: delete the pod 02/20/23 23:29:26.295
    Feb 20 23:29:26.327: INFO: Waiting for pod pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f to disappear
    Feb 20 23:29:26.344: INFO: Pod pod-configmaps-0980c5d0-8490-4aec-bc55-2b824ee9519f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 23:29:26.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3690" for this suite. 02/20/23 23:29:26.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:29:26.381
Feb 20 23:29:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename sched-preemption 02/20/23 23:29:26.382
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:29:26.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:29:26.446
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 20 23:29:26.476: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 20 23:30:26.841: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 02/20/23 23:30:26.918
Feb 20 23:30:27.029: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 20 23:30:27.061: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 20 23:30:27.120: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 20 23:30:27.151: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 20 23:30:27.205: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 20 23:30:27.258: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/20/23 23:30:27.258
Feb 20 23:30:27.258: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9058" to be "running"
Feb 20 23:30:27.266: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.919031ms
Feb 20 23:30:29.276: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.017824645s
Feb 20 23:30:29.276: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 20 23:30:29.276: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
Feb 20 23:30:29.285: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.388229ms
Feb 20 23:30:29.286: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:30:29.286: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
Feb 20 23:30:29.295: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.881493ms
Feb 20 23:30:31.318: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.031796165s
Feb 20 23:30:31.318: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:30:31.318: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
Feb 20 23:30:31.326: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.63845ms
Feb 20 23:30:31.326: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:30:31.327: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
Feb 20 23:30:31.335: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.585596ms
Feb 20 23:30:31.335: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 20 23:30:31.335: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
Feb 20 23:30:31.343: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.131594ms
Feb 20 23:30:31.343: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/20/23 23:30:31.343
Feb 20 23:30:31.372: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 20 23:30:31.380: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.719968ms
Feb 20 23:30:33.390: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017915106s
Feb 20 23:30:35.390: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017473363s
Feb 20 23:30:35.390: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:30:35.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9058" for this suite. 02/20/23 23:30:35.513
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":331,"skipped":6197,"failed":0}
------------------------------
• [SLOW TEST] [69.291 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:29:26.381
    Feb 20 23:29:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename sched-preemption 02/20/23 23:29:26.382
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:29:26.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:29:26.446
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 20 23:29:26.476: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 20 23:30:26.841: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 02/20/23 23:30:26.918
    Feb 20 23:30:27.029: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 20 23:30:27.061: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 20 23:30:27.120: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 20 23:30:27.151: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 20 23:30:27.205: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 20 23:30:27.258: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/20/23 23:30:27.258
    Feb 20 23:30:27.258: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9058" to be "running"
    Feb 20 23:30:27.266: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.919031ms
    Feb 20 23:30:29.276: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.017824645s
    Feb 20 23:30:29.276: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 20 23:30:29.276: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
    Feb 20 23:30:29.285: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.388229ms
    Feb 20 23:30:29.286: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:30:29.286: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
    Feb 20 23:30:29.295: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.881493ms
    Feb 20 23:30:31.318: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.031796165s
    Feb 20 23:30:31.318: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:30:31.318: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
    Feb 20 23:30:31.326: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.63845ms
    Feb 20 23:30:31.326: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:30:31.327: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
    Feb 20 23:30:31.335: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.585596ms
    Feb 20 23:30:31.335: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 20 23:30:31.335: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9058" to be "running"
    Feb 20 23:30:31.343: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.131594ms
    Feb 20 23:30:31.343: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/20/23 23:30:31.343
    Feb 20 23:30:31.372: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 20 23:30:31.380: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.719968ms
    Feb 20 23:30:33.390: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017915106s
    Feb 20 23:30:35.390: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.017473363s
    Feb 20 23:30:35.390: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:30:35.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9058" for this suite. 02/20/23 23:30:35.513
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:30:35.673
Feb 20 23:30:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename subpath 02/20/23 23:30:35.676
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:30:35.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:30:35.763
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/20/23 23:30:35.783
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-8txl 02/20/23 23:30:35.82
STEP: Creating a pod to test atomic-volume-subpath 02/20/23 23:30:35.82
Feb 20 23:30:35.880: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8txl" in namespace "subpath-9242" to be "Succeeded or Failed"
Feb 20 23:30:35.888: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.386857ms
Feb 20 23:30:37.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019064206s
Feb 20 23:30:39.900: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 4.020172224s
Feb 20 23:30:41.897: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 6.017218374s
Feb 20 23:30:43.901: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 8.021272655s
Feb 20 23:30:45.898: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 10.018562333s
Feb 20 23:30:47.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 12.018780369s
Feb 20 23:30:49.901: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 14.020956916s
Feb 20 23:30:51.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 16.019244724s
Feb 20 23:30:53.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 18.019042204s
Feb 20 23:30:55.900: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 20.020117929s
Feb 20 23:30:57.898: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 22.01832517s
Feb 20 23:30:59.911: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=false. Elapsed: 24.031671602s
Feb 20 23:31:01.905: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.025011486s
STEP: Saw pod success 02/20/23 23:31:01.905
Feb 20 23:31:01.906: INFO: Pod "pod-subpath-test-configmap-8txl" satisfied condition "Succeeded or Failed"
Feb 20 23:31:01.925: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-configmap-8txl container test-container-subpath-configmap-8txl: <nil>
STEP: delete the pod 02/20/23 23:31:01.973
Feb 20 23:31:02.005: INFO: Waiting for pod pod-subpath-test-configmap-8txl to disappear
Feb 20 23:31:02.016: INFO: Pod pod-subpath-test-configmap-8txl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8txl 02/20/23 23:31:02.016
Feb 20 23:31:02.017: INFO: Deleting pod "pod-subpath-test-configmap-8txl" in namespace "subpath-9242"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 20 23:31:02.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9242" for this suite. 02/20/23 23:31:02.042
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":332,"skipped":6200,"failed":0}
------------------------------
• [SLOW TEST] [26.394 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:30:35.673
    Feb 20 23:30:35.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename subpath 02/20/23 23:30:35.676
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:30:35.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:30:35.763
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/20/23 23:30:35.783
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-8txl 02/20/23 23:30:35.82
    STEP: Creating a pod to test atomic-volume-subpath 02/20/23 23:30:35.82
    Feb 20 23:30:35.880: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8txl" in namespace "subpath-9242" to be "Succeeded or Failed"
    Feb 20 23:30:35.888: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.386857ms
    Feb 20 23:30:37.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019064206s
    Feb 20 23:30:39.900: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 4.020172224s
    Feb 20 23:30:41.897: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 6.017218374s
    Feb 20 23:30:43.901: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 8.021272655s
    Feb 20 23:30:45.898: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 10.018562333s
    Feb 20 23:30:47.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 12.018780369s
    Feb 20 23:30:49.901: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 14.020956916s
    Feb 20 23:30:51.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 16.019244724s
    Feb 20 23:30:53.899: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 18.019042204s
    Feb 20 23:30:55.900: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 20.020117929s
    Feb 20 23:30:57.898: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=true. Elapsed: 22.01832517s
    Feb 20 23:30:59.911: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Running", Reason="", readiness=false. Elapsed: 24.031671602s
    Feb 20 23:31:01.905: INFO: Pod "pod-subpath-test-configmap-8txl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.025011486s
    STEP: Saw pod success 02/20/23 23:31:01.905
    Feb 20 23:31:01.906: INFO: Pod "pod-subpath-test-configmap-8txl" satisfied condition "Succeeded or Failed"
    Feb 20 23:31:01.925: INFO: Trying to get logs from node 10.8.38.66 pod pod-subpath-test-configmap-8txl container test-container-subpath-configmap-8txl: <nil>
    STEP: delete the pod 02/20/23 23:31:01.973
    Feb 20 23:31:02.005: INFO: Waiting for pod pod-subpath-test-configmap-8txl to disappear
    Feb 20 23:31:02.016: INFO: Pod pod-subpath-test-configmap-8txl no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8txl 02/20/23 23:31:02.016
    Feb 20 23:31:02.017: INFO: Deleting pod "pod-subpath-test-configmap-8txl" in namespace "subpath-9242"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 20 23:31:02.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9242" for this suite. 02/20/23 23:31:02.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:02.068
Feb 20 23:31:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 23:31:02.071
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:02.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:02.163
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Feb 20 23:31:02.277: INFO: Create a RollingUpdate DaemonSet
Feb 20 23:31:02.287: INFO: Check that daemon pods launch on every node of the cluster
Feb 20 23:31:02.308: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:31:02.309: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:31:03.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:31:03.371: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:31:04.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 23:31:04.332: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:31:05.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 23:31:05.337: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb 20 23:31:05.337: INFO: Update the DaemonSet to trigger a rollout
Feb 20 23:31:05.389: INFO: Updating DaemonSet daemon-set
Feb 20 23:31:09.444: INFO: Roll back the DaemonSet before rollout is complete
Feb 20 23:31:09.460: INFO: Updating DaemonSet daemon-set
Feb 20 23:31:09.460: INFO: Make sure DaemonSet rollback is complete
Feb 20 23:31:09.500: INFO: Wrong image for pod: daemon-set-x7mdh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Feb 20 23:31:09.500: INFO: Pod daemon-set-x7mdh is not available
Feb 20 23:31:14.551: INFO: Pod daemon-set-ts2jv is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:31:14.58
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8260, will wait for the garbage collector to delete the pods 02/20/23 23:31:14.58
Feb 20 23:31:14.649: INFO: Deleting DaemonSet.extensions daemon-set took: 9.153171ms
Feb 20 23:31:14.852: INFO: Terminating DaemonSet.extensions daemon-set pods took: 202.684648ms
Feb 20 23:31:18.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:31:18.362: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 20 23:31:18.368: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"139426"},"items":null}

Feb 20 23:31:18.377: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"139426"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:31:18.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8260" for this suite. 02/20/23 23:31:18.431
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":333,"skipped":6208,"failed":0}
------------------------------
• [SLOW TEST] [16.383 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:02.068
    Feb 20 23:31:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 23:31:02.071
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:02.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:02.163
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Feb 20 23:31:02.277: INFO: Create a RollingUpdate DaemonSet
    Feb 20 23:31:02.287: INFO: Check that daemon pods launch on every node of the cluster
    Feb 20 23:31:02.308: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:31:02.309: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:31:03.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:31:03.371: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:31:04.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 23:31:04.332: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:31:05.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 23:31:05.337: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb 20 23:31:05.337: INFO: Update the DaemonSet to trigger a rollout
    Feb 20 23:31:05.389: INFO: Updating DaemonSet daemon-set
    Feb 20 23:31:09.444: INFO: Roll back the DaemonSet before rollout is complete
    Feb 20 23:31:09.460: INFO: Updating DaemonSet daemon-set
    Feb 20 23:31:09.460: INFO: Make sure DaemonSet rollback is complete
    Feb 20 23:31:09.500: INFO: Wrong image for pod: daemon-set-x7mdh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Feb 20 23:31:09.500: INFO: Pod daemon-set-x7mdh is not available
    Feb 20 23:31:14.551: INFO: Pod daemon-set-ts2jv is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:31:14.58
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8260, will wait for the garbage collector to delete the pods 02/20/23 23:31:14.58
    Feb 20 23:31:14.649: INFO: Deleting DaemonSet.extensions daemon-set took: 9.153171ms
    Feb 20 23:31:14.852: INFO: Terminating DaemonSet.extensions daemon-set pods took: 202.684648ms
    Feb 20 23:31:18.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:31:18.362: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 20 23:31:18.368: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"139426"},"items":null}

    Feb 20 23:31:18.377: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"139426"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:31:18.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8260" for this suite. 02/20/23 23:31:18.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:18.455
Feb 20 23:31:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 23:31:18.459
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:18.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:18.533
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/20/23 23:31:18.541
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/20/23 23:31:18.543
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/20/23 23:31:18.543
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/20/23 23:31:18.543
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/20/23 23:31:18.545
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/20/23 23:31:18.545
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/20/23 23:31:18.548
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:31:18.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9167" for this suite. 02/20/23 23:31:18.559
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":334,"skipped":6238,"failed":0}
------------------------------
• [0.132 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:18.455
    Feb 20 23:31:18.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename custom-resource-definition 02/20/23 23:31:18.459
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:18.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:18.533
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/20/23 23:31:18.541
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/20/23 23:31:18.543
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/20/23 23:31:18.543
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/20/23 23:31:18.543
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/20/23 23:31:18.545
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/20/23 23:31:18.545
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/20/23 23:31:18.548
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:31:18.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9167" for this suite. 02/20/23 23:31:18.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:18.591
Feb 20 23:31:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename endpointslice 02/20/23 23:31:18.593
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:18.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:18.639
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Feb 20 23:31:18.684: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Feb 20 23:31:18.684: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 20 23:31:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-855" for this suite. 02/20/23 23:31:18.695
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":335,"skipped":6247,"failed":0}
------------------------------
• [0.132 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:18.591
    Feb 20 23:31:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename endpointslice 02/20/23 23:31:18.593
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:18.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:18.639
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Feb 20 23:31:18.684: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Feb 20 23:31:18.684: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 20 23:31:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-855" for this suite. 02/20/23 23:31:18.695
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:18.724
Feb 20 23:31:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 23:31:18.726
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:18.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:18.776
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 02/20/23 23:31:18.789
Feb 20 23:31:18.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015" in namespace "downward-api-3032" to be "Succeeded or Failed"
Feb 20 23:31:18.888: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015": Phase="Pending", Reason="", readiness=false. Elapsed: 9.840294ms
Feb 20 23:31:20.899: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020835191s
Feb 20 23:31:22.899: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020333982s
STEP: Saw pod success 02/20/23 23:31:22.899
Feb 20 23:31:22.899: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015" satisfied condition "Succeeded or Failed"
Feb 20 23:31:22.933: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015 container client-container: <nil>
STEP: delete the pod 02/20/23 23:31:22.953
Feb 20 23:31:22.982: INFO: Waiting for pod downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015 to disappear
Feb 20 23:31:22.996: INFO: Pod downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 23:31:22.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3032" for this suite. 02/20/23 23:31:23.01
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6247,"failed":0}
------------------------------
• [4.313 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:18.724
    Feb 20 23:31:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 23:31:18.726
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:18.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:18.776
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 02/20/23 23:31:18.789
    Feb 20 23:31:18.878: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015" in namespace "downward-api-3032" to be "Succeeded or Failed"
    Feb 20 23:31:18.888: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015": Phase="Pending", Reason="", readiness=false. Elapsed: 9.840294ms
    Feb 20 23:31:20.899: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020835191s
    Feb 20 23:31:22.899: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020333982s
    STEP: Saw pod success 02/20/23 23:31:22.899
    Feb 20 23:31:22.899: INFO: Pod "downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015" satisfied condition "Succeeded or Failed"
    Feb 20 23:31:22.933: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015 container client-container: <nil>
    STEP: delete the pod 02/20/23 23:31:22.953
    Feb 20 23:31:22.982: INFO: Waiting for pod downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015 to disappear
    Feb 20 23:31:22.996: INFO: Pod downwardapi-volume-fce6edcc-38f5-419f-970e-99542ee25015 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 23:31:22.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3032" for this suite. 02/20/23 23:31:23.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:23.042
Feb 20 23:31:23.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename cronjob 02/20/23 23:31:23.044
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:23.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:23.119
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/20/23 23:31:23.139
STEP: creating 02/20/23 23:31:23.139
STEP: getting 02/20/23 23:31:23.153
STEP: listing 02/20/23 23:31:23.162
STEP: watching 02/20/23 23:31:23.2
Feb 20 23:31:23.200: INFO: starting watch
STEP: cluster-wide listing 02/20/23 23:31:23.203
STEP: cluster-wide watching 02/20/23 23:31:23.211
Feb 20 23:31:23.211: INFO: starting watch
STEP: patching 02/20/23 23:31:23.213
STEP: updating 02/20/23 23:31:23.226
Feb 20 23:31:23.248: INFO: waiting for watch events with expected annotations
Feb 20 23:31:23.249: INFO: saw patched and updated annotations
STEP: patching /status 02/20/23 23:31:23.249
STEP: updating /status 02/20/23 23:31:23.264
STEP: get /status 02/20/23 23:31:23.283
STEP: deleting 02/20/23 23:31:23.306
STEP: deleting a collection 02/20/23 23:31:23.373
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 20 23:31:23.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9706" for this suite. 02/20/23 23:31:23.416
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":337,"skipped":6254,"failed":0}
------------------------------
• [0.393 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:23.042
    Feb 20 23:31:23.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename cronjob 02/20/23 23:31:23.044
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:23.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:23.119
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/20/23 23:31:23.139
    STEP: creating 02/20/23 23:31:23.139
    STEP: getting 02/20/23 23:31:23.153
    STEP: listing 02/20/23 23:31:23.162
    STEP: watching 02/20/23 23:31:23.2
    Feb 20 23:31:23.200: INFO: starting watch
    STEP: cluster-wide listing 02/20/23 23:31:23.203
    STEP: cluster-wide watching 02/20/23 23:31:23.211
    Feb 20 23:31:23.211: INFO: starting watch
    STEP: patching 02/20/23 23:31:23.213
    STEP: updating 02/20/23 23:31:23.226
    Feb 20 23:31:23.248: INFO: waiting for watch events with expected annotations
    Feb 20 23:31:23.249: INFO: saw patched and updated annotations
    STEP: patching /status 02/20/23 23:31:23.249
    STEP: updating /status 02/20/23 23:31:23.264
    STEP: get /status 02/20/23 23:31:23.283
    STEP: deleting 02/20/23 23:31:23.306
    STEP: deleting a collection 02/20/23 23:31:23.373
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 20 23:31:23.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9706" for this suite. 02/20/23 23:31:23.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:23.441
Feb 20 23:31:23.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename webhook 02/20/23 23:31:23.443
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:23.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:23.491
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/20/23 23:31:23.602
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:31:24.293
STEP: Deploying the webhook pod 02/20/23 23:31:24.326
STEP: Wait for the deployment to be ready 02/20/23 23:31:24.351
Feb 20 23:31:24.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/20/23 23:31:26.412
STEP: Verifying the service has paired with the endpoint 02/20/23 23:31:26.442
Feb 20 23:31:27.451: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 02/20/23 23:31:27.461
STEP: Creating a custom resource definition that should be denied by the webhook 02/20/23 23:31:27.5
Feb 20 23:31:27.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 20 23:31:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5533" for this suite. 02/20/23 23:31:27.584
STEP: Destroying namespace "webhook-5533-markers" for this suite. 02/20/23 23:31:27.61
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":338,"skipped":6281,"failed":0}
------------------------------
• [4.298 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:23.441
    Feb 20 23:31:23.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename webhook 02/20/23 23:31:23.443
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:23.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:23.491
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/20/23 23:31:23.602
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/20/23 23:31:24.293
    STEP: Deploying the webhook pod 02/20/23 23:31:24.326
    STEP: Wait for the deployment to be ready 02/20/23 23:31:24.351
    Feb 20 23:31:24.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/20/23 23:31:26.412
    STEP: Verifying the service has paired with the endpoint 02/20/23 23:31:26.442
    Feb 20 23:31:27.451: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/20/23 23:31:27.461
    STEP: Creating a custom resource definition that should be denied by the webhook 02/20/23 23:31:27.5
    Feb 20 23:31:27.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 20 23:31:27.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5533" for this suite. 02/20/23 23:31:27.584
    STEP: Destroying namespace "webhook-5533-markers" for this suite. 02/20/23 23:31:27.61
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:27.743
Feb 20 23:31:27.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 23:31:27.744
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:27.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:27.834
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 02/20/23 23:31:27.844
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/20/23 23:31:27.855
STEP: patching the secret 02/20/23 23:31:27.994
STEP: deleting the secret using a LabelSelector 02/20/23 23:31:28.015
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/20/23 23:31:28.037
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 20 23:31:28.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7794" for this suite. 02/20/23 23:31:28.208
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":339,"skipped":6314,"failed":0}
------------------------------
• [0.515 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:27.743
    Feb 20 23:31:27.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 23:31:27.744
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:27.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:27.834
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 02/20/23 23:31:27.844
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/20/23 23:31:27.855
    STEP: patching the secret 02/20/23 23:31:27.994
    STEP: deleting the secret using a LabelSelector 02/20/23 23:31:28.015
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/20/23 23:31:28.037
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 23:31:28.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7794" for this suite. 02/20/23 23:31:28.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:28.263
Feb 20 23:31:28.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename pod-network-test 02/20/23 23:31:28.265
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:28.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:28.377
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3610 02/20/23 23:31:28.384
STEP: creating a selector 02/20/23 23:31:28.384
STEP: Creating the service pods in kubernetes 02/20/23 23:31:28.384
Feb 20 23:31:28.384: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 20 23:31:28.650: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3610" to be "running and ready"
Feb 20 23:31:28.662: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.438179ms
Feb 20 23:31:28.662: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:31:30.675: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.024803374s
Feb 20 23:31:30.675: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:32.674: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023641029s
Feb 20 23:31:32.674: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:34.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022292621s
Feb 20 23:31:34.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:36.674: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023243545s
Feb 20 23:31:36.674: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:38.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022127257s
Feb 20 23:31:38.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:40.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022648641s
Feb 20 23:31:40.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:42.672: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.021971614s
Feb 20 23:31:42.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:44.672: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021619847s
Feb 20 23:31:44.672: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:46.672: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.021438823s
Feb 20 23:31:46.672: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:48.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.022965011s
Feb 20 23:31:48.674: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 20 23:31:50.675: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02429309s
Feb 20 23:31:50.675: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 20 23:31:50.675: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 20 23:31:50.686: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3610" to be "running and ready"
Feb 20 23:31:50.694: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.897197ms
Feb 20 23:31:50.694: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 20 23:31:50.694: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 20 23:31:50.732: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3610" to be "running and ready"
Feb 20 23:31:50.741: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.952308ms
Feb 20 23:31:50.743: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 20 23:31:50.743: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/20/23 23:31:50.77
Feb 20 23:31:50.852: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3610" to be "running"
Feb 20 23:31:50.862: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.891484ms
Feb 20 23:31:52.872: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020054743s
Feb 20 23:31:52.872: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 20 23:31:52.881: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3610" to be "running"
Feb 20 23:31:52.890: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.370306ms
Feb 20 23:31:52.891: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 20 23:31:52.900: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 20 23:31:52.900: INFO: Going to poll 172.30.181.252 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 20 23:31:52.910: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.181.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 23:31:52.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 23:31:52.912: INFO: ExecWithOptions: Clientset creation
Feb 20 23:31:52.912: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.181.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 23:31:53.107: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 20 23:31:53.107: INFO: Going to poll 172.30.144.242 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 20 23:31:53.116: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.144.242:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 23:31:53.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 23:31:53.117: INFO: ExecWithOptions: Clientset creation
Feb 20 23:31:53.117: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.144.242%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 23:31:53.307: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 20 23:31:53.308: INFO: Going to poll 172.30.31.180 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 20 23:31:53.318: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.31.180:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 20 23:31:53.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
Feb 20 23:31:53.319: INFO: ExecWithOptions: Clientset creation
Feb 20 23:31:53.319: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.31.180%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 20 23:31:53.508: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 20 23:31:53.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3610" for this suite. 02/20/23 23:31:53.522
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":340,"skipped":6337,"failed":0}
------------------------------
• [SLOW TEST] [25.282 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:28.263
    Feb 20 23:31:28.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename pod-network-test 02/20/23 23:31:28.265
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:28.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:28.377
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3610 02/20/23 23:31:28.384
    STEP: creating a selector 02/20/23 23:31:28.384
    STEP: Creating the service pods in kubernetes 02/20/23 23:31:28.384
    Feb 20 23:31:28.384: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 20 23:31:28.650: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3610" to be "running and ready"
    Feb 20 23:31:28.662: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.438179ms
    Feb 20 23:31:28.662: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:31:30.675: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.024803374s
    Feb 20 23:31:30.675: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:32.674: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023641029s
    Feb 20 23:31:32.674: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:34.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022292621s
    Feb 20 23:31:34.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:36.674: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023243545s
    Feb 20 23:31:36.674: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:38.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022127257s
    Feb 20 23:31:38.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:40.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022648641s
    Feb 20 23:31:40.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:42.672: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.021971614s
    Feb 20 23:31:42.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:44.672: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021619847s
    Feb 20 23:31:44.672: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:46.672: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.021438823s
    Feb 20 23:31:46.672: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:48.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.022965011s
    Feb 20 23:31:48.674: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 20 23:31:50.675: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02429309s
    Feb 20 23:31:50.675: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 20 23:31:50.675: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 20 23:31:50.686: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3610" to be "running and ready"
    Feb 20 23:31:50.694: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.897197ms
    Feb 20 23:31:50.694: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 20 23:31:50.694: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 20 23:31:50.732: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3610" to be "running and ready"
    Feb 20 23:31:50.741: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.952308ms
    Feb 20 23:31:50.743: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 20 23:31:50.743: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/20/23 23:31:50.77
    Feb 20 23:31:50.852: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3610" to be "running"
    Feb 20 23:31:50.862: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.891484ms
    Feb 20 23:31:52.872: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020054743s
    Feb 20 23:31:52.872: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 20 23:31:52.881: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3610" to be "running"
    Feb 20 23:31:52.890: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 9.370306ms
    Feb 20 23:31:52.891: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 20 23:31:52.900: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 20 23:31:52.900: INFO: Going to poll 172.30.181.252 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 20 23:31:52.910: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.181.252:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 23:31:52.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 23:31:52.912: INFO: ExecWithOptions: Clientset creation
    Feb 20 23:31:52.912: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.181.252%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 23:31:53.107: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 20 23:31:53.107: INFO: Going to poll 172.30.144.242 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 20 23:31:53.116: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.144.242:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 23:31:53.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 23:31:53.117: INFO: ExecWithOptions: Clientset creation
    Feb 20 23:31:53.117: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.144.242%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 23:31:53.307: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 20 23:31:53.308: INFO: Going to poll 172.30.31.180 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 20 23:31:53.318: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.31.180:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3610 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 20 23:31:53.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    Feb 20 23:31:53.319: INFO: ExecWithOptions: Clientset creation
    Feb 20 23:31:53.319: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3610/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.31.180%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 20 23:31:53.508: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 20 23:31:53.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3610" for this suite. 02/20/23 23:31:53.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:53.551
Feb 20 23:31:53.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename csistoragecapacity 02/20/23 23:31:53.553
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:53.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:53.632
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/20/23 23:31:53.639
STEP: getting /apis/storage.k8s.io 02/20/23 23:31:53.646
STEP: getting /apis/storage.k8s.io/v1 02/20/23 23:31:53.649
STEP: creating 02/20/23 23:31:53.651
STEP: watching 02/20/23 23:31:53.756
Feb 20 23:31:53.756: INFO: starting watch
STEP: getting 02/20/23 23:31:53.797
STEP: listing in namespace 02/20/23 23:31:53.814
STEP: listing across namespaces 02/20/23 23:31:53.825
STEP: patching 02/20/23 23:31:53.858
STEP: updating 02/20/23 23:31:53.871
Feb 20 23:31:53.884: INFO: waiting for watch events with expected annotations in namespace
Feb 20 23:31:53.884: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/20/23 23:31:53.884
STEP: deleting a collection 02/20/23 23:31:53.925
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Feb 20 23:31:53.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5979" for this suite. 02/20/23 23:31:53.993
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":341,"skipped":6351,"failed":0}
------------------------------
• [0.468 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:53.551
    Feb 20 23:31:53.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename csistoragecapacity 02/20/23 23:31:53.553
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:53.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:53.632
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/20/23 23:31:53.639
    STEP: getting /apis/storage.k8s.io 02/20/23 23:31:53.646
    STEP: getting /apis/storage.k8s.io/v1 02/20/23 23:31:53.649
    STEP: creating 02/20/23 23:31:53.651
    STEP: watching 02/20/23 23:31:53.756
    Feb 20 23:31:53.756: INFO: starting watch
    STEP: getting 02/20/23 23:31:53.797
    STEP: listing in namespace 02/20/23 23:31:53.814
    STEP: listing across namespaces 02/20/23 23:31:53.825
    STEP: patching 02/20/23 23:31:53.858
    STEP: updating 02/20/23 23:31:53.871
    Feb 20 23:31:53.884: INFO: waiting for watch events with expected annotations in namespace
    Feb 20 23:31:53.884: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/20/23 23:31:53.884
    STEP: deleting a collection 02/20/23 23:31:53.925
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Feb 20 23:31:53.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-5979" for this suite. 02/20/23 23:31:53.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:31:54.021
Feb 20 23:31:54.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename daemonsets 02/20/23 23:31:54.022
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:54.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:54.067
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 02/20/23 23:31:54.186
STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 23:31:54.196
Feb 20 23:31:54.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:31:54.216: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:31:55.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:31:55.270: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:31:56.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 20 23:31:56.242: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
Feb 20 23:31:57.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 23:31:57.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/20/23 23:31:57.249
Feb 20 23:31:57.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:31:57.298: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:31:58.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:31:58.322: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:31:59.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:31:59.326: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:32:00.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:32:00.332: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:32:01.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:32:01.339: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:32:02.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:32:02.323: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:32:03.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 20 23:32:03.325: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
Feb 20 23:32:04.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 20 23:32:04.323: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:32:04.33
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3008, will wait for the garbage collector to delete the pods 02/20/23 23:32:04.33
Feb 20 23:32:04.398: INFO: Deleting DaemonSet.extensions daemon-set took: 10.897334ms
Feb 20 23:32:04.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.309413ms
Feb 20 23:32:07.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 20 23:32:07.909: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 20 23:32:07.914: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"140506"},"items":null}

Feb 20 23:32:07.923: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"140506"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:32:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3008" for this suite. 02/20/23 23:32:07.982
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":342,"skipped":6382,"failed":0}
------------------------------
• [SLOW TEST] [13.986 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:31:54.021
    Feb 20 23:31:54.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename daemonsets 02/20/23 23:31:54.022
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:31:54.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:31:54.067
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 02/20/23 23:31:54.186
    STEP: Check that daemon pods launch on every node of the cluster. 02/20/23 23:31:54.196
    Feb 20 23:31:54.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:31:54.216: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:31:55.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:31:55.270: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:31:56.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 20 23:31:56.242: INFO: Node 10.8.38.66 is running 0 daemon pod, expected 1
    Feb 20 23:31:57.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 23:31:57.239: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/20/23 23:31:57.249
    Feb 20 23:31:57.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:31:57.298: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:31:58.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:31:58.322: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:31:59.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:31:59.326: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:32:00.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:32:00.332: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:32:01.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:32:01.339: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:32:02.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:32:02.323: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:32:03.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 20 23:32:03.325: INFO: Node 10.8.38.69 is running 0 daemon pod, expected 1
    Feb 20 23:32:04.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 20 23:32:04.323: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/20/23 23:32:04.33
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3008, will wait for the garbage collector to delete the pods 02/20/23 23:32:04.33
    Feb 20 23:32:04.398: INFO: Deleting DaemonSet.extensions daemon-set took: 10.897334ms
    Feb 20 23:32:04.499: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.309413ms
    Feb 20 23:32:07.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 20 23:32:07.909: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 20 23:32:07.914: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"140506"},"items":null}

    Feb 20 23:32:07.923: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"140506"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:32:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3008" for this suite. 02/20/23 23:32:07.982
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:08.007
Feb 20 23:32:08.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 23:32:08.009
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:08.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:08.062
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 02/20/23 23:32:08.073
STEP: Creating a ResourceQuota 02/20/23 23:32:13.08
STEP: Ensuring resource quota status is calculated 02/20/23 23:32:13.092
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 23:32:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4940" for this suite. 02/20/23 23:32:15.114
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":343,"skipped":6382,"failed":0}
------------------------------
• [SLOW TEST] [7.131 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:08.007
    Feb 20 23:32:08.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 23:32:08.009
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:08.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:08.062
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 02/20/23 23:32:08.073
    STEP: Creating a ResourceQuota 02/20/23 23:32:13.08
    STEP: Ensuring resource quota status is calculated 02/20/23 23:32:13.092
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 23:32:15.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4940" for this suite. 02/20/23 23:32:15.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:15.144
Feb 20 23:32:15.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename emptydir 02/20/23 23:32:15.146
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:15.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:15.203
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/20/23 23:32:15.209
Feb 20 23:32:15.326: INFO: Waiting up to 5m0s for pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297" in namespace "emptydir-5125" to be "Succeeded or Failed"
Feb 20 23:32:15.338: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Pending", Reason="", readiness=false. Elapsed: 11.979028ms
Feb 20 23:32:17.363: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037402946s
Feb 20 23:32:19.349: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022888364s
Feb 20 23:32:21.350: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024272557s
STEP: Saw pod success 02/20/23 23:32:21.35
Feb 20 23:32:21.351: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297" satisfied condition "Succeeded or Failed"
Feb 20 23:32:21.359: INFO: Trying to get logs from node 10.8.38.66 pod pod-dde307b5-da10-481c-a42b-7e3c0602b297 container test-container: <nil>
STEP: delete the pod 02/20/23 23:32:21.378
Feb 20 23:32:21.410: INFO: Waiting for pod pod-dde307b5-da10-481c-a42b-7e3c0602b297 to disappear
Feb 20 23:32:21.418: INFO: Pod pod-dde307b5-da10-481c-a42b-7e3c0602b297 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 20 23:32:21.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5125" for this suite. 02/20/23 23:32:21.437
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":344,"skipped":6403,"failed":0}
------------------------------
• [SLOW TEST] [6.314 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:15.144
    Feb 20 23:32:15.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename emptydir 02/20/23 23:32:15.146
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:15.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:15.203
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/20/23 23:32:15.209
    Feb 20 23:32:15.326: INFO: Waiting up to 5m0s for pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297" in namespace "emptydir-5125" to be "Succeeded or Failed"
    Feb 20 23:32:15.338: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Pending", Reason="", readiness=false. Elapsed: 11.979028ms
    Feb 20 23:32:17.363: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037402946s
    Feb 20 23:32:19.349: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022888364s
    Feb 20 23:32:21.350: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024272557s
    STEP: Saw pod success 02/20/23 23:32:21.35
    Feb 20 23:32:21.351: INFO: Pod "pod-dde307b5-da10-481c-a42b-7e3c0602b297" satisfied condition "Succeeded or Failed"
    Feb 20 23:32:21.359: INFO: Trying to get logs from node 10.8.38.66 pod pod-dde307b5-da10-481c-a42b-7e3c0602b297 container test-container: <nil>
    STEP: delete the pod 02/20/23 23:32:21.378
    Feb 20 23:32:21.410: INFO: Waiting for pod pod-dde307b5-da10-481c-a42b-7e3c0602b297 to disappear
    Feb 20 23:32:21.418: INFO: Pod pod-dde307b5-da10-481c-a42b-7e3c0602b297 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 20 23:32:21.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5125" for this suite. 02/20/23 23:32:21.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:21.463
Feb 20 23:32:21.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename dns 02/20/23 23:32:21.466
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:21.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:21.57
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/20/23 23:32:21.575
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
 02/20/23 23:32:21.599
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
 02/20/23 23:32:21.599
STEP: creating a pod to probe DNS 02/20/23 23:32:21.599
STEP: submitting the pod to kubernetes 02/20/23 23:32:21.599
Feb 20 23:32:21.650: INFO: Waiting up to 15m0s for pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1" in namespace "dns-9644" to be "running"
Feb 20 23:32:21.658: INFO: Pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.784841ms
Feb 20 23:32:23.669: INFO: Pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019179336s
Feb 20 23:32:23.669: INFO: Pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1" satisfied condition "running"
STEP: retrieving the pod 02/20/23 23:32:23.669
STEP: looking for the results for each expected name from probers 02/20/23 23:32:23.678
Feb 20 23:32:23.704: INFO: DNS probes using dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1 succeeded

STEP: deleting the pod 02/20/23 23:32:23.704
STEP: changing the externalName to bar.example.com 02/20/23 23:32:23.735
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
 02/20/23 23:32:23.77
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
 02/20/23 23:32:23.771
STEP: creating a second pod to probe DNS 02/20/23 23:32:23.771
STEP: submitting the pod to kubernetes 02/20/23 23:32:23.772
Feb 20 23:32:23.806: INFO: Waiting up to 15m0s for pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401" in namespace "dns-9644" to be "running"
Feb 20 23:32:23.814: INFO: Pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401": Phase="Pending", Reason="", readiness=false. Elapsed: 8.567062ms
Feb 20 23:32:25.826: INFO: Pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401": Phase="Running", Reason="", readiness=true. Elapsed: 2.01999909s
Feb 20 23:32:25.826: INFO: Pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401" satisfied condition "running"
STEP: retrieving the pod 02/20/23 23:32:25.826
STEP: looking for the results for each expected name from probers 02/20/23 23:32:25.836
Feb 20 23:32:25.927: INFO: File wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local from pod  dns-9644/dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 20 23:32:26.008: INFO: File jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local from pod  dns-9644/dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 20 23:32:26.008: INFO: Lookups using dns-9644/dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 failed for: [wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local]

Feb 20 23:32:31.038: INFO: DNS probes using dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 succeeded

STEP: deleting the pod 02/20/23 23:32:31.038
STEP: changing the service to type=ClusterIP 02/20/23 23:32:31.07
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
 02/20/23 23:32:31.105
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
 02/20/23 23:32:31.105
STEP: creating a third pod to probe DNS 02/20/23 23:32:31.105
STEP: submitting the pod to kubernetes 02/20/23 23:32:31.141
Feb 20 23:32:31.175: INFO: Waiting up to 15m0s for pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f" in namespace "dns-9644" to be "running"
Feb 20 23:32:31.184: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506031ms
Feb 20 23:32:33.197: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0210504s
Feb 20 23:32:35.195: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f": Phase="Running", Reason="", readiness=true. Elapsed: 4.019360576s
Feb 20 23:32:35.195: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f" satisfied condition "running"
STEP: retrieving the pod 02/20/23 23:32:35.195
STEP: looking for the results for each expected name from probers 02/20/23 23:32:35.204
Feb 20 23:32:35.234: INFO: DNS probes using dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f succeeded

STEP: deleting the pod 02/20/23 23:32:35.235
STEP: deleting the test externalName service 02/20/23 23:32:35.264
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 20 23:32:35.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9644" for this suite. 02/20/23 23:32:35.34
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":345,"skipped":6417,"failed":0}
------------------------------
• [SLOW TEST] [13.897 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:21.463
    Feb 20 23:32:21.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename dns 02/20/23 23:32:21.466
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:21.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:21.57
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/20/23 23:32:21.575
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
     02/20/23 23:32:21.599
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
     02/20/23 23:32:21.599
    STEP: creating a pod to probe DNS 02/20/23 23:32:21.599
    STEP: submitting the pod to kubernetes 02/20/23 23:32:21.599
    Feb 20 23:32:21.650: INFO: Waiting up to 15m0s for pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1" in namespace "dns-9644" to be "running"
    Feb 20 23:32:21.658: INFO: Pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.784841ms
    Feb 20 23:32:23.669: INFO: Pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1": Phase="Running", Reason="", readiness=true. Elapsed: 2.019179336s
    Feb 20 23:32:23.669: INFO: Pod "dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 23:32:23.669
    STEP: looking for the results for each expected name from probers 02/20/23 23:32:23.678
    Feb 20 23:32:23.704: INFO: DNS probes using dns-test-5c4d8611-45e5-492f-a427-0f0d053daea1 succeeded

    STEP: deleting the pod 02/20/23 23:32:23.704
    STEP: changing the externalName to bar.example.com 02/20/23 23:32:23.735
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
     02/20/23 23:32:23.77
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
     02/20/23 23:32:23.771
    STEP: creating a second pod to probe DNS 02/20/23 23:32:23.771
    STEP: submitting the pod to kubernetes 02/20/23 23:32:23.772
    Feb 20 23:32:23.806: INFO: Waiting up to 15m0s for pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401" in namespace "dns-9644" to be "running"
    Feb 20 23:32:23.814: INFO: Pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401": Phase="Pending", Reason="", readiness=false. Elapsed: 8.567062ms
    Feb 20 23:32:25.826: INFO: Pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401": Phase="Running", Reason="", readiness=true. Elapsed: 2.01999909s
    Feb 20 23:32:25.826: INFO: Pod "dns-test-39377e3a-8191-4e53-9757-d481bdf9f401" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 23:32:25.826
    STEP: looking for the results for each expected name from probers 02/20/23 23:32:25.836
    Feb 20 23:32:25.927: INFO: File wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local from pod  dns-9644/dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 20 23:32:26.008: INFO: File jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local from pod  dns-9644/dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 20 23:32:26.008: INFO: Lookups using dns-9644/dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 failed for: [wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local]

    Feb 20 23:32:31.038: INFO: DNS probes using dns-test-39377e3a-8191-4e53-9757-d481bdf9f401 succeeded

    STEP: deleting the pod 02/20/23 23:32:31.038
    STEP: changing the service to type=ClusterIP 02/20/23 23:32:31.07
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
     02/20/23 23:32:31.105
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9644.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9644.svc.cluster.local; sleep 1; done
     02/20/23 23:32:31.105
    STEP: creating a third pod to probe DNS 02/20/23 23:32:31.105
    STEP: submitting the pod to kubernetes 02/20/23 23:32:31.141
    Feb 20 23:32:31.175: INFO: Waiting up to 15m0s for pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f" in namespace "dns-9644" to be "running"
    Feb 20 23:32:31.184: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.506031ms
    Feb 20 23:32:33.197: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0210504s
    Feb 20 23:32:35.195: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f": Phase="Running", Reason="", readiness=true. Elapsed: 4.019360576s
    Feb 20 23:32:35.195: INFO: Pod "dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f" satisfied condition "running"
    STEP: retrieving the pod 02/20/23 23:32:35.195
    STEP: looking for the results for each expected name from probers 02/20/23 23:32:35.204
    Feb 20 23:32:35.234: INFO: DNS probes using dns-test-c3de6ec5-2b09-46fd-816a-f2d57309345f succeeded

    STEP: deleting the pod 02/20/23 23:32:35.235
    STEP: deleting the test externalName service 02/20/23 23:32:35.264
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 20 23:32:35.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9644" for this suite. 02/20/23 23:32:35.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:35.361
Feb 20 23:32:35.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 23:32:35.362
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:35.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:35.464
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-195/configmap-test-9e3df587-424b-439a-8580-a5ce73ce954f 02/20/23 23:32:35.473
STEP: Creating a pod to test consume configMaps 02/20/23 23:32:35.487
Feb 20 23:32:35.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354" in namespace "configmap-195" to be "Succeeded or Failed"
Feb 20 23:32:35.547: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Pending", Reason="", readiness=false. Elapsed: 10.458168ms
Feb 20 23:32:37.559: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022239131s
Feb 20 23:32:39.560: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022819891s
Feb 20 23:32:41.558: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021179887s
STEP: Saw pod success 02/20/23 23:32:41.558
Feb 20 23:32:41.558: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354" satisfied condition "Succeeded or Failed"
Feb 20 23:32:41.569: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354 container env-test: <nil>
STEP: delete the pod 02/20/23 23:32:41.589
Feb 20 23:32:41.621: INFO: Waiting for pod pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354 to disappear
Feb 20 23:32:41.630: INFO: Pod pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 23:32:41.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-195" for this suite. 02/20/23 23:32:41.644
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":346,"skipped":6429,"failed":0}
------------------------------
• [SLOW TEST] [6.305 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:35.361
    Feb 20 23:32:35.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 23:32:35.362
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:35.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:35.464
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-195/configmap-test-9e3df587-424b-439a-8580-a5ce73ce954f 02/20/23 23:32:35.473
    STEP: Creating a pod to test consume configMaps 02/20/23 23:32:35.487
    Feb 20 23:32:35.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354" in namespace "configmap-195" to be "Succeeded or Failed"
    Feb 20 23:32:35.547: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Pending", Reason="", readiness=false. Elapsed: 10.458168ms
    Feb 20 23:32:37.559: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022239131s
    Feb 20 23:32:39.560: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022819891s
    Feb 20 23:32:41.558: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021179887s
    STEP: Saw pod success 02/20/23 23:32:41.558
    Feb 20 23:32:41.558: INFO: Pod "pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354" satisfied condition "Succeeded or Failed"
    Feb 20 23:32:41.569: INFO: Trying to get logs from node 10.8.38.66 pod pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354 container env-test: <nil>
    STEP: delete the pod 02/20/23 23:32:41.589
    Feb 20 23:32:41.621: INFO: Waiting for pod pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354 to disappear
    Feb 20 23:32:41.630: INFO: Pod pod-configmaps-be5e0179-6339-44ae-8de9-3687e3bcb354 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 23:32:41.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-195" for this suite. 02/20/23 23:32:41.644
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:41.668
Feb 20 23:32:41.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename kubectl 02/20/23 23:32:41.671
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:41.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:41.72
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 02/20/23 23:32:41.728
Feb 20 23:32:41.728: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 20 23:32:41.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
Feb 20 23:32:42.522: INFO: stderr: ""
Feb 20 23:32:42.522: INFO: stdout: "service/agnhost-replica created\n"
Feb 20 23:32:42.522: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 20 23:32:42.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
Feb 20 23:32:43.294: INFO: stderr: ""
Feb 20 23:32:43.294: INFO: stdout: "service/agnhost-primary created\n"
Feb 20 23:32:43.294: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 20 23:32:43.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
Feb 20 23:32:43.897: INFO: stderr: ""
Feb 20 23:32:43.897: INFO: stdout: "service/frontend created\n"
Feb 20 23:32:43.897: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 20 23:32:43.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
Feb 20 23:32:44.509: INFO: stderr: ""
Feb 20 23:32:44.509: INFO: stdout: "deployment.apps/frontend created\n"
Feb 20 23:32:44.509: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 20 23:32:44.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
Feb 20 23:32:45.084: INFO: stderr: ""
Feb 20 23:32:45.084: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 20 23:32:45.084: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 20 23:32:45.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
Feb 20 23:32:45.730: INFO: stderr: ""
Feb 20 23:32:45.730: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/20/23 23:32:45.73
Feb 20 23:32:45.730: INFO: Waiting for all frontend pods to be Running.
Feb 20 23:32:50.786: INFO: Waiting for frontend to serve content.
Feb 20 23:32:50.826: INFO: Trying to add a new entry to the guestbook.
Feb 20 23:32:50.853: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/20/23 23:32:50.875
Feb 20 23:32:50.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
Feb 20 23:32:51.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 23:32:51.056: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/20/23 23:32:51.056
Feb 20 23:32:51.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
Feb 20 23:32:51.192: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 23:32:51.192: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/20/23 23:32:51.193
Feb 20 23:32:51.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
Feb 20 23:32:51.352: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 23:32:51.352: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/20/23 23:32:51.352
Feb 20 23:32:51.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
Feb 20 23:32:51.487: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 23:32:51.487: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/20/23 23:32:51.487
Feb 20 23:32:51.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
Feb 20 23:32:51.628: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 23:32:51.628: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/20/23 23:32:51.628
Feb 20 23:32:51.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
Feb 20 23:32:51.728: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 23:32:51.728: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 20 23:32:51.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3115" for this suite. 02/20/23 23:32:51.743
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":347,"skipped":6429,"failed":0}
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:41.668
    Feb 20 23:32:41.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename kubectl 02/20/23 23:32:41.671
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:41.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:41.72
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 02/20/23 23:32:41.728
    Feb 20 23:32:41.728: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 20 23:32:41.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
    Feb 20 23:32:42.522: INFO: stderr: ""
    Feb 20 23:32:42.522: INFO: stdout: "service/agnhost-replica created\n"
    Feb 20 23:32:42.522: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 20 23:32:42.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
    Feb 20 23:32:43.294: INFO: stderr: ""
    Feb 20 23:32:43.294: INFO: stdout: "service/agnhost-primary created\n"
    Feb 20 23:32:43.294: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 20 23:32:43.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
    Feb 20 23:32:43.897: INFO: stderr: ""
    Feb 20 23:32:43.897: INFO: stdout: "service/frontend created\n"
    Feb 20 23:32:43.897: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 20 23:32:43.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
    Feb 20 23:32:44.509: INFO: stderr: ""
    Feb 20 23:32:44.509: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 20 23:32:44.509: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 20 23:32:44.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
    Feb 20 23:32:45.084: INFO: stderr: ""
    Feb 20 23:32:45.084: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 20 23:32:45.084: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 20 23:32:45.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 create -f -'
    Feb 20 23:32:45.730: INFO: stderr: ""
    Feb 20 23:32:45.730: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/20/23 23:32:45.73
    Feb 20 23:32:45.730: INFO: Waiting for all frontend pods to be Running.
    Feb 20 23:32:50.786: INFO: Waiting for frontend to serve content.
    Feb 20 23:32:50.826: INFO: Trying to add a new entry to the guestbook.
    Feb 20 23:32:50.853: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/20/23 23:32:50.875
    Feb 20 23:32:50.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
    Feb 20 23:32:51.056: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 23:32:51.056: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/20/23 23:32:51.056
    Feb 20 23:32:51.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
    Feb 20 23:32:51.192: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 23:32:51.192: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/20/23 23:32:51.193
    Feb 20 23:32:51.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
    Feb 20 23:32:51.352: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 23:32:51.352: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/20/23 23:32:51.352
    Feb 20 23:32:51.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
    Feb 20 23:32:51.487: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 23:32:51.487: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/20/23 23:32:51.487
    Feb 20 23:32:51.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
    Feb 20 23:32:51.628: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 23:32:51.628: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/20/23 23:32:51.628
    Feb 20 23:32:51.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=kubectl-3115 delete --grace-period=0 --force -f -'
    Feb 20 23:32:51.728: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 20 23:32:51.728: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 20 23:32:51.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3115" for this suite. 02/20/23 23:32:51.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:51.763
Feb 20 23:32:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:32:51.764
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:51.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:51.813
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-5f20b121-c292-4162-9dcc-1729035e83c9 02/20/23 23:32:51.822
STEP: Creating a pod to test consume secrets 02/20/23 23:32:51.833
Feb 20 23:32:51.894: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61" in namespace "projected-3321" to be "Succeeded or Failed"
Feb 20 23:32:51.903: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61": Phase="Pending", Reason="", readiness=false. Elapsed: 9.566721ms
Feb 20 23:32:53.917: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022956827s
Feb 20 23:32:55.914: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02058425s
STEP: Saw pod success 02/20/23 23:32:55.914
Feb 20 23:32:55.915: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61" satisfied condition "Succeeded or Failed"
Feb 20 23:32:55.924: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 23:32:55.944
Feb 20 23:32:55.975: INFO: Waiting for pod pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61 to disappear
Feb 20 23:32:55.983: INFO: Pod pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 20 23:32:55.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3321" for this suite. 02/20/23 23:32:55.996
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":348,"skipped":6446,"failed":0}
------------------------------
• [4.257 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:51.763
    Feb 20 23:32:51.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:32:51.764
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:51.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:51.813
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-5f20b121-c292-4162-9dcc-1729035e83c9 02/20/23 23:32:51.822
    STEP: Creating a pod to test consume secrets 02/20/23 23:32:51.833
    Feb 20 23:32:51.894: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61" in namespace "projected-3321" to be "Succeeded or Failed"
    Feb 20 23:32:51.903: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61": Phase="Pending", Reason="", readiness=false. Elapsed: 9.566721ms
    Feb 20 23:32:53.917: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022956827s
    Feb 20 23:32:55.914: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02058425s
    STEP: Saw pod success 02/20/23 23:32:55.914
    Feb 20 23:32:55.915: INFO: Pod "pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61" satisfied condition "Succeeded or Failed"
    Feb 20 23:32:55.924: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 23:32:55.944
    Feb 20 23:32:55.975: INFO: Waiting for pod pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61 to disappear
    Feb 20 23:32:55.983: INFO: Pod pod-projected-secrets-681a19a2-0997-4db7-adcb-0453dee81d61 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 20 23:32:55.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3321" for this suite. 02/20/23 23:32:55.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:32:56.025
Feb 20 23:32:56.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename taint-single-pod 02/20/23 23:32:56.027
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:56.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:56.14
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Feb 20 23:32:56.146: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 20 23:33:56.267: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Feb 20 23:33:56.288: INFO: Starting informer...
STEP: Starting pod... 02/20/23 23:33:56.288
Feb 20 23:33:56.548: INFO: Pod is running on 10.8.38.66. Tainting Node
STEP: Trying to apply a taint on the Node 02/20/23 23:33:56.549
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:33:56.584
STEP: Waiting short time to make sure Pod is queued for deletion 02/20/23 23:33:56.593
Feb 20 23:33:56.595: INFO: Pod wasn't evicted. Proceeding
Feb 20 23:33:56.595: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:33:56.633
STEP: Waiting some time to make sure that toleration time passed. 02/20/23 23:33:56.643
Feb 20 23:35:11.644: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Feb 20 23:35:11.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5749" for this suite. 02/20/23 23:35:11.659
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":349,"skipped":6471,"failed":0}
------------------------------
• [SLOW TEST] [135.654 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:32:56.025
    Feb 20 23:32:56.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename taint-single-pod 02/20/23 23:32:56.027
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:32:56.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:32:56.14
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Feb 20 23:32:56.146: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 20 23:33:56.267: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Feb 20 23:33:56.288: INFO: Starting informer...
    STEP: Starting pod... 02/20/23 23:33:56.288
    Feb 20 23:33:56.548: INFO: Pod is running on 10.8.38.66. Tainting Node
    STEP: Trying to apply a taint on the Node 02/20/23 23:33:56.549
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:33:56.584
    STEP: Waiting short time to make sure Pod is queued for deletion 02/20/23 23:33:56.593
    Feb 20 23:33:56.595: INFO: Pod wasn't evicted. Proceeding
    Feb 20 23:33:56.595: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/20/23 23:33:56.633
    STEP: Waiting some time to make sure that toleration time passed. 02/20/23 23:33:56.643
    Feb 20 23:35:11.644: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Feb 20 23:35:11.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-5749" for this suite. 02/20/23 23:35:11.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:35:11.686
Feb 20 23:35:11.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 23:35:11.688
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:11.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:11.745
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-4776 02/20/23 23:35:11.754
STEP: creating service affinity-nodeport-transition in namespace services-4776 02/20/23 23:35:11.755
STEP: creating replication controller affinity-nodeport-transition in namespace services-4776 02/20/23 23:35:11.802
W0220 23:35:11.814242      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0220 23:35:11.814463      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4776, replica count: 3
I0220 23:35:14.865182      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 23:35:14.903: INFO: Creating new exec pod
Feb 20 23:35:14.961: INFO: Waiting up to 5m0s for pod "execpod-affinitygl2nh" in namespace "services-4776" to be "running"
Feb 20 23:35:14.970: INFO: Pod "execpod-affinitygl2nh": Phase="Pending", Reason="", readiness=false. Elapsed: 9.132899ms
Feb 20 23:35:16.982: INFO: Pod "execpod-affinitygl2nh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021272094s
Feb 20 23:35:18.980: INFO: Pod "execpod-affinitygl2nh": Phase="Running", Reason="", readiness=true. Elapsed: 4.019348114s
Feb 20 23:35:18.980: INFO: Pod "execpod-affinitygl2nh" satisfied condition "running"
Feb 20 23:35:20.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 20 23:35:20.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 20 23:35:20.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:35:20.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.73.138 80'
Feb 20 23:35:20.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.73.138 80\nConnection to 172.21.73.138 80 port [tcp/http] succeeded!\n"
Feb 20 23:35:20.622: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:35:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 31856'
Feb 20 23:35:20.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 31856\nConnection to 10.8.38.70 31856 port [tcp/*] succeeded!\n"
Feb 20 23:35:20.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:35:20.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 31856'
Feb 20 23:35:21.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 31856\nConnection to 10.8.38.69 31856 port [tcp/*] succeeded!\n"
Feb 20 23:35:21.179: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 20 23:35:21.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:31856/ ; done'
Feb 20 23:35:21.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n"
Feb 20 23:35:21.614: INFO: stdout: "\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-mlg7z\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-mlg7z\naffinity-nodeport-transition-mlg7z\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-csxv5"
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-csxv5
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-mlg7z
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-mlg7z
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-mlg7z
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
Feb 20 23:35:21.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:31856/ ; done'
Feb 20 23:35:22.012: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n"
Feb 20 23:35:22.012: INFO: stdout: "\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc"
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
Feb 20 23:35:22.012: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4776, will wait for the garbage collector to delete the pods 02/20/23 23:35:22.044
Feb 20 23:35:22.115: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.844012ms
Feb 20 23:35:22.316: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.938902ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 23:35:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4776" for this suite. 02/20/23 23:35:25.585
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":350,"skipped":6490,"failed":0}
------------------------------
• [SLOW TEST] [13.922 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:35:11.686
    Feb 20 23:35:11.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 23:35:11.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:11.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:11.745
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-4776 02/20/23 23:35:11.754
    STEP: creating service affinity-nodeport-transition in namespace services-4776 02/20/23 23:35:11.755
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4776 02/20/23 23:35:11.802
    W0220 23:35:11.814242      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0220 23:35:11.814463      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4776, replica count: 3
    I0220 23:35:14.865182      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 20 23:35:14.903: INFO: Creating new exec pod
    Feb 20 23:35:14.961: INFO: Waiting up to 5m0s for pod "execpod-affinitygl2nh" in namespace "services-4776" to be "running"
    Feb 20 23:35:14.970: INFO: Pod "execpod-affinitygl2nh": Phase="Pending", Reason="", readiness=false. Elapsed: 9.132899ms
    Feb 20 23:35:16.982: INFO: Pod "execpod-affinitygl2nh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021272094s
    Feb 20 23:35:18.980: INFO: Pod "execpod-affinitygl2nh": Phase="Running", Reason="", readiness=true. Elapsed: 4.019348114s
    Feb 20 23:35:18.980: INFO: Pod "execpod-affinitygl2nh" satisfied condition "running"
    Feb 20 23:35:20.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Feb 20 23:35:20.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 20 23:35:20.292: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:35:20.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.73.138 80'
    Feb 20 23:35:20.622: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.73.138 80\nConnection to 172.21.73.138 80 port [tcp/http] succeeded!\n"
    Feb 20 23:35:20.622: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:35:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.70 31856'
    Feb 20 23:35:20.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.70 31856\nConnection to 10.8.38.70 31856 port [tcp/*] succeeded!\n"
    Feb 20 23:35:20.886: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:35:20.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.8.38.69 31856'
    Feb 20 23:35:21.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.8.38.69 31856\nConnection to 10.8.38.69 31856 port [tcp/*] succeeded!\n"
    Feb 20 23:35:21.179: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 20 23:35:21.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:31856/ ; done'
    Feb 20 23:35:21.614: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n"
    Feb 20 23:35:21.614: INFO: stdout: "\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-mlg7z\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-mlg7z\naffinity-nodeport-transition-mlg7z\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-csxv5\naffinity-nodeport-transition-csxv5"
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-csxv5
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-mlg7z
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.614: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-mlg7z
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-mlg7z
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
    Feb 20 23:35:21.615: INFO: Received response from host: affinity-nodeport-transition-csxv5
    Feb 20 23:35:21.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3627716555 --namespace=services-4776 exec execpod-affinitygl2nh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.8.38.66:31856/ ; done'
    Feb 20 23:35:22.012: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.8.38.66:31856/\n"
    Feb 20 23:35:22.012: INFO: stdout: "\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc\naffinity-nodeport-transition-g95jc"
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Received response from host: affinity-nodeport-transition-g95jc
    Feb 20 23:35:22.012: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4776, will wait for the garbage collector to delete the pods 02/20/23 23:35:22.044
    Feb 20 23:35:22.115: INFO: Deleting ReplicationController affinity-nodeport-transition took: 10.844012ms
    Feb 20 23:35:22.316: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.938902ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 23:35:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4776" for this suite. 02/20/23 23:35:25.585
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:35:25.61
Feb 20 23:35:25.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 23:35:25.612
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:25.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:25.692
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 02/20/23 23:35:25.698
Feb 20 23:35:25.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415" in namespace "downward-api-6382" to be "Succeeded or Failed"
Feb 20 23:35:25.748: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415": Phase="Pending", Reason="", readiness=false. Elapsed: 11.670044ms
Feb 20 23:35:27.759: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022498585s
Feb 20 23:35:29.759: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022268271s
STEP: Saw pod success 02/20/23 23:35:29.759
Feb 20 23:35:29.759: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415" satisfied condition "Succeeded or Failed"
Feb 20 23:35:29.768: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415 container client-container: <nil>
STEP: delete the pod 02/20/23 23:35:29.823
Feb 20 23:35:29.875: INFO: Waiting for pod downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415 to disappear
Feb 20 23:35:29.919: INFO: Pod downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 23:35:29.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6382" for this suite. 02/20/23 23:35:29.936
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":351,"skipped":6496,"failed":0}
------------------------------
• [4.348 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:35:25.61
    Feb 20 23:35:25.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 23:35:25.612
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:25.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:25.692
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 02/20/23 23:35:25.698
    Feb 20 23:35:25.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415" in namespace "downward-api-6382" to be "Succeeded or Failed"
    Feb 20 23:35:25.748: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415": Phase="Pending", Reason="", readiness=false. Elapsed: 11.670044ms
    Feb 20 23:35:27.759: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022498585s
    Feb 20 23:35:29.759: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022268271s
    STEP: Saw pod success 02/20/23 23:35:29.759
    Feb 20 23:35:29.759: INFO: Pod "downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415" satisfied condition "Succeeded or Failed"
    Feb 20 23:35:29.768: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415 container client-container: <nil>
    STEP: delete the pod 02/20/23 23:35:29.823
    Feb 20 23:35:29.875: INFO: Waiting for pod downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415 to disappear
    Feb 20 23:35:29.919: INFO: Pod downwardapi-volume-5457b2ca-608d-4c3d-bcfe-8dc81b4f3415 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 23:35:29.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6382" for this suite. 02/20/23 23:35:29.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:35:29.96
Feb 20 23:35:29.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename job 02/20/23 23:35:29.961
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:30.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:30.025
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 02/20/23 23:35:30.032
STEP: Ensuring job reaches completions 02/20/23 23:35:30.045
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 20 23:35:44.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6019" for this suite. 02/20/23 23:35:44.071
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":352,"skipped":6508,"failed":0}
------------------------------
• [SLOW TEST] [14.143 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:35:29.96
    Feb 20 23:35:29.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename job 02/20/23 23:35:29.961
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:30.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:30.025
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 02/20/23 23:35:30.032
    STEP: Ensuring job reaches completions 02/20/23 23:35:30.045
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 20 23:35:44.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6019" for this suite. 02/20/23 23:35:44.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:35:44.109
Feb 20 23:35:44.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 23:35:44.111
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:44.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:44.154
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
Feb 20 23:35:44.173: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-06b3549f-0c5e-40bc-8ac4-580eeedf7a90 02/20/23 23:35:44.173
STEP: Creating configMap with name cm-test-opt-upd-1af7ca3f-9522-4fbf-85a5-8c71302f6c6c 02/20/23 23:35:44.184
STEP: Creating the pod 02/20/23 23:35:44.195
Feb 20 23:35:44.247: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22" in namespace "configmap-3148" to be "running and ready"
Feb 20 23:35:44.264: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962682ms
Feb 20 23:35:44.265: INFO: The phase of Pod pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:35:46.276: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028648438s
Feb 20 23:35:46.276: INFO: The phase of Pod pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22 is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:35:48.275: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22": Phase="Running", Reason="", readiness=true. Elapsed: 4.027704568s
Feb 20 23:35:48.275: INFO: The phase of Pod pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22 is Running (Ready = true)
Feb 20 23:35:48.275: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-06b3549f-0c5e-40bc-8ac4-580eeedf7a90 02/20/23 23:35:48.338
STEP: Updating configmap cm-test-opt-upd-1af7ca3f-9522-4fbf-85a5-8c71302f6c6c 02/20/23 23:35:48.353
STEP: Creating configMap with name cm-test-opt-create-0dea0672-5bdd-4c9c-bf38-ea0e18d1cc10 02/20/23 23:35:48.367
STEP: waiting to observe update in volume 02/20/23 23:35:48.378
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 23:37:17.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3148" for this suite. 02/20/23 23:37:17.626
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":353,"skipped":6538,"failed":0}
------------------------------
• [SLOW TEST] [93.539 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:35:44.109
    Feb 20 23:35:44.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 23:35:44.111
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:35:44.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:35:44.154
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    Feb 20 23:35:44.173: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-06b3549f-0c5e-40bc-8ac4-580eeedf7a90 02/20/23 23:35:44.173
    STEP: Creating configMap with name cm-test-opt-upd-1af7ca3f-9522-4fbf-85a5-8c71302f6c6c 02/20/23 23:35:44.184
    STEP: Creating the pod 02/20/23 23:35:44.195
    Feb 20 23:35:44.247: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22" in namespace "configmap-3148" to be "running and ready"
    Feb 20 23:35:44.264: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962682ms
    Feb 20 23:35:44.265: INFO: The phase of Pod pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:35:46.276: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028648438s
    Feb 20 23:35:46.276: INFO: The phase of Pod pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22 is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:35:48.275: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22": Phase="Running", Reason="", readiness=true. Elapsed: 4.027704568s
    Feb 20 23:35:48.275: INFO: The phase of Pod pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22 is Running (Ready = true)
    Feb 20 23:35:48.275: INFO: Pod "pod-configmaps-9ff5282b-63dc-4403-9255-6007e4544c22" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-06b3549f-0c5e-40bc-8ac4-580eeedf7a90 02/20/23 23:35:48.338
    STEP: Updating configmap cm-test-opt-upd-1af7ca3f-9522-4fbf-85a5-8c71302f6c6c 02/20/23 23:35:48.353
    STEP: Creating configMap with name cm-test-opt-create-0dea0672-5bdd-4c9c-bf38-ea0e18d1cc10 02/20/23 23:35:48.367
    STEP: waiting to observe update in volume 02/20/23 23:35:48.378
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 23:37:17.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3148" for this suite. 02/20/23 23:37:17.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:17.65
Feb 20 23:37:17.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename secrets 02/20/23 23:37:17.653
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:17.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:17.709
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-44579478-8630-401e-8e58-e7936783a591 02/20/23 23:37:17.716
STEP: Creating a pod to test consume secrets 02/20/23 23:37:17.733
Feb 20 23:37:17.777: INFO: Waiting up to 5m0s for pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0" in namespace "secrets-7075" to be "Succeeded or Failed"
Feb 20 23:37:17.786: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.488207ms
Feb 20 23:37:19.843: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065609292s
Feb 20 23:37:21.798: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021412877s
STEP: Saw pod success 02/20/23 23:37:21.799
Feb 20 23:37:21.799: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0" satisfied condition "Succeeded or Failed"
Feb 20 23:37:21.808: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0 container secret-volume-test: <nil>
STEP: delete the pod 02/20/23 23:37:21.828
Feb 20 23:37:21.859: INFO: Waiting for pod pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0 to disappear
Feb 20 23:37:21.868: INFO: Pod pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 20 23:37:21.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7075" for this suite. 02/20/23 23:37:21.883
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":354,"skipped":6549,"failed":0}
------------------------------
• [4.255 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:17.65
    Feb 20 23:37:17.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename secrets 02/20/23 23:37:17.653
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:17.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:17.709
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-44579478-8630-401e-8e58-e7936783a591 02/20/23 23:37:17.716
    STEP: Creating a pod to test consume secrets 02/20/23 23:37:17.733
    Feb 20 23:37:17.777: INFO: Waiting up to 5m0s for pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0" in namespace "secrets-7075" to be "Succeeded or Failed"
    Feb 20 23:37:17.786: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.488207ms
    Feb 20 23:37:19.843: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065609292s
    Feb 20 23:37:21.798: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021412877s
    STEP: Saw pod success 02/20/23 23:37:21.799
    Feb 20 23:37:21.799: INFO: Pod "pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0" satisfied condition "Succeeded or Failed"
    Feb 20 23:37:21.808: INFO: Trying to get logs from node 10.8.38.66 pod pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0 container secret-volume-test: <nil>
    STEP: delete the pod 02/20/23 23:37:21.828
    Feb 20 23:37:21.859: INFO: Waiting for pod pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0 to disappear
    Feb 20 23:37:21.868: INFO: Pod pod-secrets-7f12994f-3922-4999-8cee-6125aefea6b0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 20 23:37:21.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7075" for this suite. 02/20/23 23:37:21.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:21.913
Feb 20 23:37:21.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename services 02/20/23 23:37:21.914
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:21.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:21.964
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 02/20/23 23:37:21.977
Feb 20 23:37:21.977: INFO: Creating e2e-svc-a-24fjk
Feb 20 23:37:22.012: INFO: Creating e2e-svc-b-p5rr7
Feb 20 23:37:22.041: INFO: Creating e2e-svc-c-5pb5z
STEP: deleting service collection 02/20/23 23:37:22.078
Feb 20 23:37:22.182: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 20 23:37:22.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2486" for this suite. 02/20/23 23:37:22.193
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":355,"skipped":6556,"failed":0}
------------------------------
• [0.304 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:21.913
    Feb 20 23:37:21.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename services 02/20/23 23:37:21.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:21.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:21.964
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 02/20/23 23:37:21.977
    Feb 20 23:37:21.977: INFO: Creating e2e-svc-a-24fjk
    Feb 20 23:37:22.012: INFO: Creating e2e-svc-b-p5rr7
    Feb 20 23:37:22.041: INFO: Creating e2e-svc-c-5pb5z
    STEP: deleting service collection 02/20/23 23:37:22.078
    Feb 20 23:37:22.182: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 20 23:37:22.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2486" for this suite. 02/20/23 23:37:22.193
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:22.228
Feb 20 23:37:22.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename projected 02/20/23 23:37:22.228
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:22.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:22.279
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-62171a21-82b5-4f6e-8165-53f8b324129c 02/20/23 23:37:22.284
STEP: Creating a pod to test consume configMaps 02/20/23 23:37:22.295
Feb 20 23:37:22.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995" in namespace "projected-5554" to be "Succeeded or Failed"
Feb 20 23:37:22.373: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995": Phase="Pending", Reason="", readiness=false. Elapsed: 8.933836ms
Feb 20 23:37:24.384: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01981056s
Feb 20 23:37:26.383: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018615434s
STEP: Saw pod success 02/20/23 23:37:26.383
Feb 20 23:37:26.383: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995" satisfied condition "Succeeded or Failed"
Feb 20 23:37:26.396: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995 container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/20/23 23:37:26.414
Feb 20 23:37:26.448: INFO: Waiting for pod pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995 to disappear
Feb 20 23:37:26.458: INFO: Pod pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 20 23:37:26.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5554" for this suite. 02/20/23 23:37:26.47
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":356,"skipped":6567,"failed":0}
------------------------------
• [4.262 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:22.228
    Feb 20 23:37:22.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename projected 02/20/23 23:37:22.228
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:22.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:22.279
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-62171a21-82b5-4f6e-8165-53f8b324129c 02/20/23 23:37:22.284
    STEP: Creating a pod to test consume configMaps 02/20/23 23:37:22.295
    Feb 20 23:37:22.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995" in namespace "projected-5554" to be "Succeeded or Failed"
    Feb 20 23:37:22.373: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995": Phase="Pending", Reason="", readiness=false. Elapsed: 8.933836ms
    Feb 20 23:37:24.384: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01981056s
    Feb 20 23:37:26.383: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018615434s
    STEP: Saw pod success 02/20/23 23:37:26.383
    Feb 20 23:37:26.383: INFO: Pod "pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995" satisfied condition "Succeeded or Failed"
    Feb 20 23:37:26.396: INFO: Trying to get logs from node 10.8.38.66 pod pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/20/23 23:37:26.414
    Feb 20 23:37:26.448: INFO: Waiting for pod pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995 to disappear
    Feb 20 23:37:26.458: INFO: Pod pod-projected-configmaps-8f5d98a0-cf23-4a75-b448-23a0194f2995 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 20 23:37:26.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5554" for this suite. 02/20/23 23:37:26.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:26.496
Feb 20 23:37:26.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename replicaset 02/20/23 23:37:26.499
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:26.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:26.55
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/20/23 23:37:26.562
W0220 23:37:26.592949      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Feb 20 23:37:26.601: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 20 23:37:31.613: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/20/23 23:37:31.613
STEP: getting scale subresource 02/20/23 23:37:31.613
STEP: updating a scale subresource 02/20/23 23:37:31.638
STEP: verifying the replicaset Spec.Replicas was modified 02/20/23 23:37:31.654
STEP: Patch a scale subresource 02/20/23 23:37:31.664
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 20 23:37:31.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7267" for this suite. 02/20/23 23:37:31.714
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":357,"skipped":6577,"failed":0}
------------------------------
• [SLOW TEST] [5.262 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:26.496
    Feb 20 23:37:26.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename replicaset 02/20/23 23:37:26.499
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:26.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:26.55
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/20/23 23:37:26.562
    W0220 23:37:26.592949      20 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Feb 20 23:37:26.601: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 20 23:37:31.613: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/20/23 23:37:31.613
    STEP: getting scale subresource 02/20/23 23:37:31.613
    STEP: updating a scale subresource 02/20/23 23:37:31.638
    STEP: verifying the replicaset Spec.Replicas was modified 02/20/23 23:37:31.654
    STEP: Patch a scale subresource 02/20/23 23:37:31.664
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 20 23:37:31.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7267" for this suite. 02/20/23 23:37:31.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:31.763
Feb 20 23:37:31.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename resourcequota 02/20/23 23:37:31.766
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:31.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:31.824
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 02/20/23 23:37:31.83
STEP: Getting a ResourceQuota 02/20/23 23:37:31.839
STEP: Updating a ResourceQuota 02/20/23 23:37:31.85
STEP: Verifying a ResourceQuota was modified 02/20/23 23:37:31.859
STEP: Deleting a ResourceQuota 02/20/23 23:37:31.874
STEP: Verifying the deleted ResourceQuota 02/20/23 23:37:31.892
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 20 23:37:31.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9021" for this suite. 02/20/23 23:37:31.909
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":358,"skipped":6584,"failed":0}
------------------------------
• [0.168 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:31.763
    Feb 20 23:37:31.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename resourcequota 02/20/23 23:37:31.766
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:31.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:31.824
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 02/20/23 23:37:31.83
    STEP: Getting a ResourceQuota 02/20/23 23:37:31.839
    STEP: Updating a ResourceQuota 02/20/23 23:37:31.85
    STEP: Verifying a ResourceQuota was modified 02/20/23 23:37:31.859
    STEP: Deleting a ResourceQuota 02/20/23 23:37:31.874
    STEP: Verifying the deleted ResourceQuota 02/20/23 23:37:31.892
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 20 23:37:31.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9021" for this suite. 02/20/23 23:37:31.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:31.933
Feb 20 23:37:31.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename downward-api 02/20/23 23:37:31.936
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:31.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:31.991
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 02/20/23 23:37:31.998
Feb 20 23:37:32.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd" in namespace "downward-api-2333" to be "Succeeded or Failed"
Feb 20 23:37:32.113: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.027534ms
Feb 20 23:37:34.123: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023232317s
Feb 20 23:37:36.122: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022891016s
STEP: Saw pod success 02/20/23 23:37:36.123
Feb 20 23:37:36.123: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd" satisfied condition "Succeeded or Failed"
Feb 20 23:37:36.133: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd container client-container: <nil>
STEP: delete the pod 02/20/23 23:37:36.152
Feb 20 23:37:36.183: INFO: Waiting for pod downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd to disappear
Feb 20 23:37:36.194: INFO: Pod downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 20 23:37:36.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2333" for this suite. 02/20/23 23:37:36.219
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":359,"skipped":6595,"failed":0}
------------------------------
• [4.308 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:31.933
    Feb 20 23:37:31.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename downward-api 02/20/23 23:37:31.936
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:31.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:31.991
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 02/20/23 23:37:31.998
    Feb 20 23:37:32.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd" in namespace "downward-api-2333" to be "Succeeded or Failed"
    Feb 20 23:37:32.113: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 13.027534ms
    Feb 20 23:37:34.123: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023232317s
    Feb 20 23:37:36.122: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022891016s
    STEP: Saw pod success 02/20/23 23:37:36.123
    Feb 20 23:37:36.123: INFO: Pod "downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd" satisfied condition "Succeeded or Failed"
    Feb 20 23:37:36.133: INFO: Trying to get logs from node 10.8.38.66 pod downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd container client-container: <nil>
    STEP: delete the pod 02/20/23 23:37:36.152
    Feb 20 23:37:36.183: INFO: Waiting for pod downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd to disappear
    Feb 20 23:37:36.194: INFO: Pod downwardapi-volume-2fd9aacc-a49b-4282-8ece-25095ff40ffd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 20 23:37:36.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2333" for this suite. 02/20/23 23:37:36.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:36.245
Feb 20 23:37:36.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename podtemplate 02/20/23 23:37:36.247
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:36.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:36.304
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 20 23:37:36.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1604" for this suite. 02/20/23 23:37:36.383
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":360,"skipped":6606,"failed":0}
------------------------------
• [0.160 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:36.245
    Feb 20 23:37:36.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename podtemplate 02/20/23 23:37:36.247
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:36.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:36.304
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 20 23:37:36.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1604" for this suite. 02/20/23 23:37:36.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:36.414
Feb 20 23:37:36.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 23:37:36.415
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:36.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:36.472
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
Feb 20 23:37:36.492: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-bba5be36-9179-4c55-a44b-b23ee9106973 02/20/23 23:37:36.492
STEP: Creating the pod 02/20/23 23:37:36.505
Feb 20 23:37:36.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75" in namespace "configmap-4081" to be "running"
Feb 20 23:37:36.590: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75": Phase="Pending", Reason="", readiness=false. Elapsed: 14.519574ms
Feb 20 23:37:38.599: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023410576s
Feb 20 23:37:40.601: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75": Phase="Running", Reason="", readiness=false. Elapsed: 4.025448337s
Feb 20 23:37:40.601: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75" satisfied condition "running"
STEP: Waiting for pod with text data 02/20/23 23:37:40.601
STEP: Waiting for pod with binary data 02/20/23 23:37:40.628
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 23:37:40.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4081" for this suite. 02/20/23 23:37:40.686
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":361,"skipped":6672,"failed":0}
------------------------------
• [4.290 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:36.414
    Feb 20 23:37:36.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 23:37:36.415
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:36.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:36.472
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    Feb 20 23:37:36.492: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-bba5be36-9179-4c55-a44b-b23ee9106973 02/20/23 23:37:36.492
    STEP: Creating the pod 02/20/23 23:37:36.505
    Feb 20 23:37:36.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75" in namespace "configmap-4081" to be "running"
    Feb 20 23:37:36.590: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75": Phase="Pending", Reason="", readiness=false. Elapsed: 14.519574ms
    Feb 20 23:37:38.599: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023410576s
    Feb 20 23:37:40.601: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75": Phase="Running", Reason="", readiness=false. Elapsed: 4.025448337s
    Feb 20 23:37:40.601: INFO: Pod "pod-configmaps-adf60e19-7f44-4398-b329-aa667c3b0c75" satisfied condition "running"
    STEP: Waiting for pod with text data 02/20/23 23:37:40.601
    STEP: Waiting for pod with binary data 02/20/23 23:37:40.628
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 23:37:40.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4081" for this suite. 02/20/23 23:37:40.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/20/23 23:37:40.705
Feb 20 23:37:40.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
STEP: Building a namespace api object, basename configmap 02/20/23 23:37:40.709
STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:40.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:40.764
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
Feb 20 23:37:40.785: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-e7712475-9c9a-42aa-8fce-b38fac97e156 02/20/23 23:37:40.785
STEP: Creating the pod 02/20/23 23:37:40.798
Feb 20 23:37:40.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc" in namespace "configmap-9331" to be "running and ready"
Feb 20 23:37:40.878: INFO: Pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.603128ms
Feb 20 23:37:40.878: INFO: The phase of Pod pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc is Pending, waiting for it to be Running (with Ready = true)
Feb 20 23:37:42.887: INFO: Pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc": Phase="Running", Reason="", readiness=true. Elapsed: 2.031171844s
Feb 20 23:37:42.887: INFO: The phase of Pod pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc is Running (Ready = true)
Feb 20 23:37:42.887: INFO: Pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-e7712475-9c9a-42aa-8fce-b38fac97e156 02/20/23 23:37:42.918
STEP: waiting to observe update in volume 02/20/23 23:37:42.931
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 20 23:37:44.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9331" for this suite. 02/20/23 23:37:44.985
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":362,"skipped":6678,"failed":0}
------------------------------
• [4.300 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/20/23 23:37:40.705
    Feb 20 23:37:40.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3627716555
    STEP: Building a namespace api object, basename configmap 02/20/23 23:37:40.709
    STEP: Waiting for a default service account to be provisioned in namespace 02/20/23 23:37:40.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/20/23 23:37:40.764
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    Feb 20 23:37:40.785: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-e7712475-9c9a-42aa-8fce-b38fac97e156 02/20/23 23:37:40.785
    STEP: Creating the pod 02/20/23 23:37:40.798
    Feb 20 23:37:40.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc" in namespace "configmap-9331" to be "running and ready"
    Feb 20 23:37:40.878: INFO: Pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.603128ms
    Feb 20 23:37:40.878: INFO: The phase of Pod pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc is Pending, waiting for it to be Running (with Ready = true)
    Feb 20 23:37:42.887: INFO: Pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc": Phase="Running", Reason="", readiness=true. Elapsed: 2.031171844s
    Feb 20 23:37:42.887: INFO: The phase of Pod pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc is Running (Ready = true)
    Feb 20 23:37:42.887: INFO: Pod "pod-configmaps-cfea16f5-633b-4959-9b8d-b88df9d42edc" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-e7712475-9c9a-42aa-8fce-b38fac97e156 02/20/23 23:37:42.918
    STEP: waiting to observe update in volume 02/20/23 23:37:42.931
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 20 23:37:44.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9331" for this suite. 02/20/23 23:37:44.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Feb 20 23:37:45.016: INFO: Running AfterSuite actions on all nodes
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Feb 20 23:37:45.016: INFO: Running AfterSuite actions on node 1
Feb 20 23:37:45.016: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 20 23:37:45.016: INFO: Running AfterSuite actions on all nodes
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Feb 20 23:37:45.016: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 20 23:37:45.016: INFO: Running AfterSuite actions on node 1
    Feb 20 23:37:45.016: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.001 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.090 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 6348.604 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h45m49.085847503s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

